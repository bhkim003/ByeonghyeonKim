{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30979/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"0\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 16, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.75,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = True,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yryz40rf\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fasnh6ig with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_155726-fasnh6ig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig' target=\"_blank\">restful-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 3, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 45e076f350e4b27d8ff87367a19d69df\n",
      "cache path doesn't exist\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c61eaeb3b64103b26574a209daeda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iter_acc</td><td>▅▄▄▃▃▃▅▂▂▅▂▃▂▂▂▃▁▃▂▃▄▆▂▃▇▅▄▁▅▂▆▄▄▆▄▆▃▆█▄</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_epoch_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>iter_acc</td><td>0.1875</td></tr><tr><td>tr_acc</td><td>0</td></tr><tr><td>tr_epoch_loss</td><td>0</td></tr><tr><td>val_acc_best</td><td>0</td></tr><tr><td>val_acc_now</td><td>0</td></tr><tr><td>val_loss</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">restful-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fasnh6ig</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_155726-fasnh6ig/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run fasnh6ig errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_30979/263192303.py\", line 94, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_30979/2774805580.py\", line 276, in my_snn_system\n",
      "    for i, data in iterator:\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1183, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1229, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/_utils.py\", line 434, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 138, in __getitem__\n",
      "    data, targets = load_from_disk_cache(file_path)\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 212, in load_from_disk_cache\n",
      "    with h5py.File(file_path, \"r\") as f:\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '/data2/DVSGesture/cache/fast_dataloading_45e076f350e4b27d8ff87367a19d69df/train/40_0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 308, in __getitem__\n",
      "    return self.datasets[dataset_idx][sample_idx]\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 151, in __getitem__\n",
      "    data, targets = load_from_disk_cache(file_path)\n",
      "  File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 214, in load_from_disk_cache\n",
      "    for index in f[name].keys():\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/_collections_abc.py\", line 720, in __iter__\n",
      "    yield from self._mapping\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/group.py\", line 502, in __iter__\n",
      "    for x in self.id.__iter__():\n",
      "  File \"h5py/h5g.pyx\", line 128, in h5py.h5g.GroupIter.__next__\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5l.pyx\", line 316, in h5py.h5l.LinkProxy.iterate\n",
      "RuntimeError: Link iteration failed (bad local heap signature)\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fasnh6ig errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_30979/263192303.py\", line 94, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_30979/2774805580.py\", line 276, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     for i, data in iterator:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = self._next_data()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1183, in _next_data\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._process_data(data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1229, in _process_data\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data.reraise()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/_utils.py\", line 434, in reraise\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise exception\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Caught RuntimeError in DataLoader worker process 1.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Original Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 138, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data, targets = load_from_disk_cache(file_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 212, in load_from_disk_cache\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     with h5py.File(file_path, \"r\") as f:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 562, in __init__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     fid = h5f.open(name, flags, fapl=fapl)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '/data2/DVSGesture/cache/fast_dataloading_45e076f350e4b27d8ff87367a19d69df/train/40_0.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = fetcher.fetch(index)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 308, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.datasets[dataset_idx][sample_idx]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 151, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data, targets = load_from_disk_cache(file_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/tonic/cached_dataset.py\", line 214, in load_from_disk_cache\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     for index in f[name].keys():\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/_collections_abc.py\", line 720, in __iter__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     yield from self._mapping\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/h5py/_hl/group.py\", line 502, in __iter__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     for x in self.id.__iter__():\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/h5g.pyx\", line 128, in h5py.h5g.GroupIter.__next__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"h5py/h5l.pyx\", line 316, in h5py.h5l.LinkProxy.iterate\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Link iteration failed (bad local heap signature)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9o18owo8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_155945-9o18owo8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8' target=\"_blank\">vocal-sweep-7</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  3.375626/  4.924582, val:  43.33%, val_best:  43.33%, tr:  32.18%, tr_best:  32.18%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  4.411354/  5.846550, val:  35.42%, val_best:  43.33%, tr:  45.56%, tr_best:  45.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  4.374959/  6.216932, val:  35.00%, val_best:  43.33%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  3.781863/  6.351960, val:  41.67%, val_best:  43.33%, tr:  55.16%, tr_best:  55.16%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  4.105217/  5.605066, val:  56.25%, val_best:  56.25%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.718383/  3.675399, val:  51.67%, val_best:  56.25%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  3.386384/  5.642118, val:  49.17%, val_best:  56.25%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  3.616551/  5.396501, val:  50.00%, val_best:  56.25%, tr:  58.63%, tr_best:  62.00%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.751287/  4.964119, val:  53.33%, val_best:  56.25%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  4.728420/  4.619531, val:  53.33%, val_best:  56.25%, tr:  60.98%, tr_best:  62.82%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  3.861833/  4.854526, val:  58.75%, val_best:  58.75%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.841662/  4.876754, val:  47.92%, val_best:  58.75%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.943535/  3.461709, val:  61.25%, val_best:  61.25%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.981824/  5.390105, val:  50.83%, val_best:  61.25%, tr:  67.21%, tr_best:  70.28%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.982751/  4.572068, val:  61.67%, val_best:  61.67%, tr:  70.07%, tr_best:  70.28%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.994673/  4.406756, val:  58.33%, val_best:  61.67%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.836171/  4.360404, val:  62.50%, val_best:  62.50%, tr:  71.60%, tr_best:  73.44%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.816617/  5.424423, val:  61.25%, val_best:  62.50%, tr:  73.24%, tr_best:  73.44%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  2.485013/  3.814268, val:  65.83%, val_best:  65.83%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.669511/  3.262528, val:  68.75%, val_best:  68.75%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.870311/  3.994393, val:  67.08%, val_best:  68.75%, tr:  80.18%, tr_best:  81.72%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.892323/  6.426356, val:  72.08%, val_best:  72.08%, tr:  75.38%, tr_best:  81.72%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.916661/  3.352026, val:  71.67%, val_best:  72.08%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.636025/  5.987766, val:  62.08%, val_best:  72.08%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.418722/  4.139493, val:  65.42%, val_best:  72.08%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.466357/  4.220771, val:  67.92%, val_best:  72.08%, tr:  86.01%, tr_best:  88.25%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.810418/  5.617115, val:  63.33%, val_best:  72.08%, tr:  85.90%, tr_best:  88.25%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.458785/  4.161286, val:  76.25%, val_best:  76.25%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.393750/  4.261668, val:  73.75%, val_best:  76.25%, tr:  89.17%, tr_best:  89.48%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.143280/  4.867313, val:  71.25%, val_best:  76.25%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.199079/  4.413030, val:  71.67%, val_best:  76.25%, tr:  92.75%, tr_best:  93.56%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.882417/  4.749913, val:  71.67%, val_best:  76.25%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.081329/  4.926338, val:  67.92%, val_best:  76.25%, tr:  93.46%, tr_best:  95.30%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.197487/  4.877429, val:  72.50%, val_best:  76.25%, tr:  93.56%, tr_best:  95.30%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.984108/  5.131997, val:  72.08%, val_best:  76.25%, tr:  94.89%, tr_best:  95.30%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.750658/  4.895487, val:  70.83%, val_best:  76.25%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.733275/  4.703146, val:  74.17%, val_best:  76.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.834344/  5.506299, val:  70.83%, val_best:  76.25%, tr:  95.91%, tr_best:  98.06%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  1.077495/  5.172935, val:  72.92%, val_best:  76.25%, tr:  94.38%, tr_best:  98.06%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.830219/  4.922905, val:  75.42%, val_best:  76.25%, tr:  97.04%, tr_best:  98.06%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.706840/  4.866054, val:  75.00%, val_best:  76.25%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.592747/  5.185280, val:  74.17%, val_best:  76.25%, tr:  97.96%, tr_best:  98.06%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.607890/  5.022070, val:  78.33%, val_best:  78.33%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.498638/  5.774553, val:  74.17%, val_best:  78.33%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.551077/  6.247904, val:  73.75%, val_best:  78.33%, tr:  98.98%, tr_best:  99.08%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.480349/  5.110059, val:  75.00%, val_best:  78.33%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.403839/  5.547753, val:  76.25%, val_best:  78.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.417751/  5.763806, val:  75.00%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.348740/  5.703766, val:  74.17%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.406445/  5.658116, val:  75.42%, val_best:  78.33%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.346797/  5.996122, val:  74.17%, val_best:  78.33%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.338436/  5.991768, val:  77.08%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.352796/  5.773657, val:  76.67%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.251303/  5.833306, val:  75.42%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.305196/  5.744478, val:  79.58%, val_best:  79.58%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.269031/  5.948262, val:  77.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.260727/  6.389153, val:  76.67%, val_best:  79.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.287932/  6.065574, val:  79.17%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.206378/  6.340549, val:  74.58%, val_best:  79.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.199226/  6.156328, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.165120/  6.334200, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.218347/  6.301515, val:  78.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.229086/  6.548867, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.206777/  6.424737, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.165844/  6.553136, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.148494/  6.218416, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.134092/  6.576536, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.122098/  6.545312, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.149716/  6.500177, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.132718/  6.908567, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.122940/  6.756269, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.143760/  6.667559, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.111834/  6.922075, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.099359/  7.153737, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.095698/  6.775165, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.102958/  6.886825, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.147348/  7.126762, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.098712/  7.233177, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.105645/  7.316442, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.072043/  6.959475, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.067294/  7.111836, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.078573/  7.312032, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.068949/  7.075481, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.067747/  7.240650, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.072425/  7.365749, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.115504/  7.512792, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.068340/  7.061684, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.049083/  7.394644, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.096339/  7.196530, val:  77.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.070422/  7.142729, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.046580/  7.190536, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.057905/  7.529678, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.054658/  7.720180, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.053474/  7.473547, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.067817/  7.334467, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.040718/  7.485956, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.052300/  7.549210, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.056550/  7.520621, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033973/  7.603555, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.041235/  7.617035, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7060ec45f746f7999f66d99d391613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▃▃▄▆▄▄▇▇▇██▇▇███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▄▃▄▅▅▅▆▇▆▅▇▆▇▇▇█▇▇▇▇██████▇█████████▇█</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▅▅▅▆▅▇▇▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆▇▇▆█▅▅▅▃▅▃▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▃▄▅▅▆▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▄▃▄▅▅▅▆▇▆▅▇▆▇▇▇█▇▇▇▇██████▇█████████▇█</td></tr><tr><td>val_loss</td><td>▄▆▅▄▃▁▃▄▁▆▂▅▄▄▄▅▄▄▆▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04123</td></tr><tr><td>val_acc_best</td><td>0.8</td></tr><tr><td>val_acc_now</td><td>0.79167</td></tr><tr><td>val_loss</td><td>7.61703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-7</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9o18owo8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_155945-9o18owo8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: if5mf2g5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_160618-if5mf2g5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5' target=\"_blank\">icy-sweep-9</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.337012/  3.260089, val:  42.50%, val_best:  42.50%, tr:  33.40%, tr_best:  33.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.454684/  2.734299, val:  46.67%, val_best:  46.67%, tr:  52.20%, tr_best:  52.20%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.322649/  3.660435, val:  55.42%, val_best:  55.42%, tr:  56.59%, tr_best:  56.59%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.446365/  3.104996, val:  51.67%, val_best:  55.42%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.984612/  1.943396, val:  61.67%, val_best:  61.67%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.940178/  2.476325, val:  54.58%, val_best:  61.67%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.635616/  1.767809, val:  61.25%, val_best:  61.67%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.487669/  2.575252, val:  52.92%, val_best:  61.67%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.334403/  1.688805, val:  65.83%, val_best:  65.83%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.583318/  1.808418, val:  66.67%, val_best:  66.67%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.377620/  1.468558, val:  66.67%, val_best:  66.67%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.968081/  2.111320, val:  60.42%, val_best:  66.67%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.332990/  1.439684, val:  77.50%, val_best:  77.50%, tr:  76.00%, tr_best:  79.47%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.999688/  1.824517, val:  71.25%, val_best:  77.50%, tr:  79.06%, tr_best:  79.47%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.101177/  2.228175, val:  68.75%, val_best:  77.50%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.594902/  1.615644, val:  76.67%, val_best:  77.50%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.496880/  1.489792, val:  80.00%, val_best:  80.00%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.616244/  1.901250, val:  70.00%, val_best:  80.00%, tr:  90.30%, tr_best:  93.36%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.556900/  1.899036, val:  73.75%, val_best:  80.00%, tr:  91.73%, tr_best:  93.36%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.550071/  1.503246, val:  79.17%, val_best:  80.00%, tr:  91.42%, tr_best:  93.36%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.350235/  1.745610, val:  75.42%, val_best:  80.00%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.415559/  1.839736, val:  72.08%, val_best:  80.00%, tr:  94.18%, tr_best:  96.02%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.343377/  1.825100, val:  75.83%, val_best:  80.00%, tr:  95.30%, tr_best:  96.02%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.267262/  1.783595, val:  81.67%, val_best:  81.67%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.206805/  1.637124, val:  81.67%, val_best:  81.67%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.153955/  1.575179, val:  83.33%, val_best:  83.33%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.133744/  1.703222, val:  82.92%, val_best:  83.33%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.126711/  1.629458, val:  84.17%, val_best:  84.17%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.100742/  1.628612, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.100535/  1.964477, val:  77.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.130499/  1.728528, val:  80.83%, val_best:  84.17%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.072704/  1.690548, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.067556/  1.654439, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.056996/  1.793394, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.057550/  1.753700, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.045737/  1.757158, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.030058/  1.758637, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.026752/  1.788976, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.026644/  1.790571, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.029296/  1.881343, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.023284/  1.834787, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.023142/  1.801984, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.018058/  1.849258, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.016033/  1.905129, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.022627/  1.855869, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.016819/  1.839126, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.017481/  1.910885, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.011696/  1.898611, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.023355/  1.886415, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.019010/  1.941561, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.012663/  1.927688, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.010791/  1.895817, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.007536/  1.908752, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.007220/  1.931646, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.006954/  1.951990, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.005509/  1.970693, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.006775/  2.008574, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.005568/  1.974713, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.005249/  1.990543, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.005621/  1.995314, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.006728/  2.008166, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.006238/  2.005769, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.005487/  2.051635, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.003979/  2.053753, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.003930/  2.054438, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.004248/  2.063787, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.008216/  2.071535, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.004468/  2.056513, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.004708/  2.063874, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.003543/  2.067139, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.003737/  2.088584, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.003297/  2.087028, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.003118/  2.077927, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.003155/  2.079436, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002560/  2.089953, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002990/  2.099266, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002696/  2.113652, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002209/  2.111343, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002074/  2.115354, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.002450/  2.130357, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.002405/  2.126160, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002405/  2.131227, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.002318/  2.125073, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.002282/  2.114488, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001954/  2.105329, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.002109/  2.113721, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002065/  2.123964, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001896/  2.120854, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.002316/  2.134632, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002162/  2.128128, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002581/  2.116694, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.002279/  2.143983, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.002293/  2.172338, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.002123/  2.138163, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.002580/  2.178327, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.002055/  2.133386, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001821/  2.163731, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001780/  2.134904, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.002366/  2.143330, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.002570/  2.150552, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bb1c56545645f2b590355ae1ef4215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▅▂▅▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▅▆▅▅▇▆▇▇▇▇▇█▇█▇█▇███████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▅▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▅▆▅▅▇▆▇▇▇▇▇█▇█▇█▇███████████████████</td></tr><tr><td>val_loss</td><td>▇█▃▅▂▁▃▂▁▂▂▂▃▂▂▂▂▂▂▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00257</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.15055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-9</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5mf2g5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_160618-if5mf2g5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8vxt7zx6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_161147-8vxt7zx6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6' target=\"_blank\">dandy-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.177759/  1.767869, val:  41.67%, val_best:  41.67%, tr:  17.16%, tr_best:  17.16%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.579576/  1.792288, val:  50.42%, val_best:  50.42%, tr:  48.11%, tr_best:  48.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.616680/  1.629443, val:  57.08%, val_best:  57.08%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.419275/  1.941939, val:  51.67%, val_best:  57.08%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.380651/  1.543922, val:  63.75%, val_best:  63.75%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.277786/  1.511709, val:  56.25%, val_best:  63.75%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.321165/  1.669516, val:  60.83%, val_best:  63.75%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.358928/  2.006596, val:  51.67%, val_best:  63.75%, tr:  63.43%, tr_best:  67.31%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.201749/  1.467234, val:  60.00%, val_best:  63.75%, tr:  68.54%, tr_best:  68.54%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.502143/  1.740815, val:  55.42%, val_best:  63.75%, tr:  64.04%, tr_best:  68.54%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.311748/  1.231247, val:  70.00%, val_best:  70.00%, tr:  67.62%, tr_best:  68.54%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.137177/  1.785117, val:  59.17%, val_best:  70.00%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.156562/  1.328666, val:  71.25%, val_best:  71.25%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.009147/  1.670813, val:  60.83%, val_best:  71.25%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.045778/  2.023691, val:  61.67%, val_best:  71.25%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.858041/  1.563960, val:  62.08%, val_best:  71.25%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.887830/  1.468858, val:  74.17%, val_best:  74.17%, tr:  79.98%, tr_best:  82.64%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.900720/  1.557298, val:  64.58%, val_best:  74.17%, tr:  81.92%, tr_best:  82.64%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.758488/  1.677016, val:  64.58%, val_best:  74.17%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.802542/  1.511950, val:  67.92%, val_best:  74.17%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.856475/  1.820350, val:  62.92%, val_best:  74.17%, tr:  84.07%, tr_best:  85.60%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.806115/  1.659008, val:  70.83%, val_best:  74.17%, tr:  84.78%, tr_best:  85.60%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.597384/  1.400848, val:  77.92%, val_best:  77.92%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.625445/  1.780209, val:  65.83%, val_best:  77.92%, tr:  91.52%, tr_best:  92.75%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.567910/  1.592333, val:  75.83%, val_best:  77.92%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.515739/  1.388809, val:  78.33%, val_best:  78.33%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.557114/  1.886964, val:  70.42%, val_best:  78.33%, tr:  92.03%, tr_best:  94.08%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.515287/  1.598993, val:  72.92%, val_best:  78.33%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.612111/  1.623978, val:  78.33%, val_best:  78.33%, tr:  90.81%, tr_best:  94.48%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.455126/  1.576767, val:  77.50%, val_best:  78.33%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.468030/  1.678486, val:  79.17%, val_best:  79.17%, tr:  95.61%, tr_best:  97.85%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.411578/  1.687217, val:  78.75%, val_best:  79.17%, tr:  97.45%, tr_best:  97.85%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.539425/  1.702679, val:  77.50%, val_best:  79.17%, tr:  93.05%, tr_best:  97.85%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.424015/  1.868854, val:  79.17%, val_best:  79.17%, tr:  96.22%, tr_best:  97.85%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.439049/  1.844776, val:  78.33%, val_best:  79.17%, tr:  96.22%, tr_best:  97.85%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.356362/  1.616032, val:  81.25%, val_best:  81.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.312396/  1.551294, val:  84.58%, val_best:  84.58%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.349992/  1.680122, val:  83.33%, val_best:  84.58%, tr:  98.16%, tr_best:  98.88%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.357983/  1.846990, val:  72.50%, val_best:  84.58%, tr:  97.85%, tr_best:  98.88%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.372230/  1.684441, val:  82.50%, val_best:  84.58%, tr:  97.14%, tr_best:  98.88%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.276763/  1.823798, val:  75.83%, val_best:  84.58%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.302270/  1.725174, val:  82.08%, val_best:  84.58%, tr:  97.96%, tr_best:  98.98%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.279079/  1.732792, val:  83.75%, val_best:  84.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.265188/  1.850989, val:  79.58%, val_best:  84.58%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.256333/  1.769657, val:  79.58%, val_best:  84.58%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.230452/  1.736552, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.237203/  1.961126, val:  82.50%, val_best:  86.67%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.217414/  1.800170, val:  85.00%, val_best:  86.67%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.244355/  1.794164, val:  84.58%, val_best:  86.67%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.206079/  1.818823, val:  82.92%, val_best:  86.67%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.180891/  1.840598, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.183160/  1.823378, val:  86.67%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.167777/  1.940293, val:  84.17%, val_best:  86.67%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.149384/  1.932339, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.148469/  1.902918, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.147529/  1.933115, val:  85.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.152409/  2.050434, val:  82.92%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.160633/  1.945492, val:  84.17%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.139429/  1.955546, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.139944/  1.968697, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.128921/  2.047884, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.121671/  2.086908, val:  84.58%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.112908/  2.082826, val:  85.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.113260/  2.057825, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.108503/  2.086049, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.101700/  2.132696, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.091546/  2.112172, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.097310/  2.106270, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.100106/  2.166396, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.095699/  2.202477, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.087246/  2.109648, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.091016/  2.206437, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.084482/  2.195636, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.082248/  2.269924, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.080090/  2.212375, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.077953/  2.214989, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.069510/  2.308018, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.068327/  2.220813, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.068934/  2.358262, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.068913/  2.246019, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.067295/  2.307035, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.071219/  2.294563, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.065294/  2.318426, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.070130/  2.367636, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.062798/  2.369966, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.064366/  2.427171, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.068922/  2.404682, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.074674/  2.361145, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.066103/  2.431033, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.057996/  2.417319, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.055227/  2.464743, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.056826/  2.380807, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.048108/  2.492890, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.060290/  2.486069, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.053566/  2.508778, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.046905/  2.515688, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.048450/  2.548788, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.048925/  2.560366, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.050309/  2.593634, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.048594/  2.518627, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c14f133b04c9b9a61c6ebd4c61a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▂▅▇▇▇█▇█▇█▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▂▃▅▄▄▅▅▆▅▆▆▇▇▇▇▇▇▇▇█▇█▇█▇▇█████████▇▇</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▅▆▆▆▇▇▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▆▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▂▃▅▄▄▅▅▆▅▆▆▇▇▇▇▇▇▇▇█▇█▇█▇▇█████████▇▇</td></tr><tr><td>val_loss</td><td>▃▃▂▅▃▁▅▂▂▃▂▄▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04859</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>2.51863</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8vxt7zx6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_161147-8vxt7zx6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aud4paar with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_161715-aud4paar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar' target=\"_blank\">treasured-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.364254/  3.444064, val:  38.75%, val_best:  38.75%, tr:  29.93%, tr_best:  29.93%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.800476/  2.800792, val:  52.50%, val_best:  52.50%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.511064/  4.942358, val:  44.58%, val_best:  52.50%, tr:  51.99%, tr_best:  51.99%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.756983/  4.411570, val:  50.00%, val_best:  52.50%, tr:  58.53%, tr_best:  58.53%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.786238/  2.951656, val:  55.42%, val_best:  55.42%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.716480/  3.593223, val:  50.42%, val_best:  55.42%, tr:  63.53%, tr_best:  63.53%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.915297/  3.561498, val:  59.58%, val_best:  59.58%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.970849/  3.506444, val:  54.17%, val_best:  59.58%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.279908/  3.326262, val:  57.08%, val_best:  59.58%, tr:  66.39%, tr_best:  68.23%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  3.127134/  3.536241, val:  52.92%, val_best:  59.58%, tr:  63.43%, tr_best:  68.23%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.187211/  3.341280, val:  59.58%, val_best:  59.58%, tr:  67.52%, tr_best:  68.23%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.246208/  3.157217, val:  62.08%, val_best:  62.08%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.994955/  2.990803, val:  61.67%, val_best:  62.08%, tr:  71.20%, tr_best:  71.20%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.896069/  3.906312, val:  55.42%, val_best:  62.08%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.860709/  4.161561, val:  56.25%, val_best:  62.08%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.075395/  4.681872, val:  55.42%, val_best:  62.08%, tr:  73.75%, tr_best:  75.79%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.138122/  3.459884, val:  65.83%, val_best:  65.83%, tr:  75.18%, tr_best:  75.79%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.879036/  4.637856, val:  59.58%, val_best:  65.83%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.842557/  3.405706, val:  68.75%, val_best:  68.75%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.455800/  3.158424, val:  68.75%, val_best:  68.75%, tr:  83.55%, tr_best:  83.55%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.539481/  4.381956, val:  62.08%, val_best:  68.75%, tr:  83.25%, tr_best:  83.55%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.166104/  5.062113, val:  58.75%, val_best:  68.75%, tr:  80.18%, tr_best:  83.55%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.473376/  3.780391, val:  76.25%, val_best:  76.25%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.434365/  4.222356, val:  70.00%, val_best:  76.25%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.229558/  3.878329, val:  77.50%, val_best:  77.50%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.132174/  3.745704, val:  70.83%, val_best:  77.50%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.279749/  4.601786, val:  65.42%, val_best:  77.50%, tr:  89.58%, tr_best:  92.03%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.137110/  3.743865, val:  76.67%, val_best:  77.50%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.266669/  4.538523, val:  67.08%, val_best:  77.50%, tr:  90.40%, tr_best:  94.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.233508/  4.785876, val:  64.17%, val_best:  77.50%, tr:  93.56%, tr_best:  94.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.109261/  4.348667, val:  75.42%, val_best:  77.50%, tr:  94.08%, tr_best:  94.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.975196/  4.362831, val:  76.67%, val_best:  77.50%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.410273/  5.588483, val:  65.83%, val_best:  77.50%, tr:  91.11%, tr_best:  96.53%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.006734/  5.592145, val:  69.58%, val_best:  77.50%, tr:  95.61%, tr_best:  96.53%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.982735/  5.199594, val:  72.08%, val_best:  77.50%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.935123/  4.861407, val:  77.08%, val_best:  77.50%, tr:  96.94%, tr_best:  97.04%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.846158/  4.992792, val:  75.42%, val_best:  77.50%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.892742/  5.650120, val:  69.17%, val_best:  77.50%, tr:  97.45%, tr_best:  98.16%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.916555/  5.139957, val:  77.92%, val_best:  77.92%, tr:  97.65%, tr_best:  98.16%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.877786/  5.477777, val:  79.17%, val_best:  79.17%, tr:  96.42%, tr_best:  98.16%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.710892/  5.012998, val:  79.58%, val_best:  79.58%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.796885/  5.626713, val:  74.17%, val_best:  79.58%, tr:  98.26%, tr_best:  98.88%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.629547/  5.680094, val:  75.83%, val_best:  79.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.694272/  6.191647, val:  72.92%, val_best:  79.58%, tr:  98.67%, tr_best:  99.18%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.779547/  6.453382, val:  74.17%, val_best:  79.58%, tr:  98.77%, tr_best:  99.18%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.639009/  5.863923, val:  79.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.692899/  6.059294, val:  75.00%, val_best:  79.58%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.577556/  5.979143, val:  78.33%, val_best:  79.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.650701/  6.454761, val:  79.17%, val_best:  79.58%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.539503/  6.471018, val:  81.25%, val_best:  81.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.538288/  6.774906, val:  77.50%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.544933/  6.708885, val:  75.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.552473/  6.472380, val:  79.17%, val_best:  81.25%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.429608/  6.634486, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.466758/  6.777270, val:  79.58%, val_best:  81.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.460159/  6.988739, val:  81.67%, val_best:  81.67%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.464994/  7.434267, val:  76.25%, val_best:  81.67%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.460632/  7.163128, val:  79.17%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.394761/  7.267477, val:  79.58%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.425328/  7.464379, val:  80.00%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.445552/  7.310522, val:  82.08%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.429530/  7.667799, val:  78.75%, val_best:  82.08%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.377259/  7.766232, val:  77.92%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.376768/  7.786118, val:  80.42%, val_best:  82.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.319576/  7.940769, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.360402/  8.017546, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.339194/  7.855277, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.316680/  8.015875, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.309215/  8.475647, val:  80.42%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.277169/  8.479498, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.308949/  8.507063, val:  80.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.326781/  8.409334, val:  79.58%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.239241/  8.663848, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.282894/  8.855862, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.265662/  8.563479, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.262529/  8.686619, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.240331/  8.959558, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.237789/  8.932541, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.260343/  9.351934, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.272440/  9.179317, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.226881/  9.122897, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.189232/  9.046495, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.216305/  9.343026, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.250716/  9.472028, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.204579/  9.350897, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.215309/  9.439935, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.193411/  9.321900, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.216113/  9.761829, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.241072/  9.393623, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.190044/  9.726390, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.177576/  9.724614, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.151476/  9.776415, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.125746/  9.959390, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.174645/  9.839767, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.175153/  9.960859, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.160499/ 10.189226, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.147680/ 10.430724, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.203150/ 10.523477, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.162383/ 10.346108, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.161944/ 10.082000, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c1b4cba60c47a785fd57b217244feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▅▄▄▆▅▇▇▇█▇▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▃▅▄▄▆▄▇▅▅▅▇▆█▇▇▇█████▇███▇█▇▇██▇▇▇▇█</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▄▅▆▆▆▆▇▇▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▇▅▇▅▅▅▄▅▃▃▃▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▃▅▄▄▆▄▇▅▅▅▇▆█▇▇▇█████▇███▇█▇▇██▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▁▃▁▂▂▁▂▃▁▃▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.16194</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>10.082</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aud4paar</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_161715-aud4paar/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4wwmotsf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991c079270744e31a1ebaadb608a1f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113489212261306, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_162405-4wwmotsf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf' target=\"_blank\">jolly-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.255974/  1.794649, val:  28.33%, val_best:  28.33%, tr:  12.26%, tr_best:  12.26%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.447584/  1.374007, val:  56.25%, val_best:  56.25%, tr:  49.44%, tr_best:  49.44%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.271685/  1.370422, val:  59.17%, val_best:  59.17%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.076363/  1.430519, val:  55.83%, val_best:  59.17%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.042131/  1.322086, val:  63.33%, val_best:  63.33%, tr:  65.27%, tr_best:  66.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.979358/  1.174465, val:  65.42%, val_best:  65.42%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.980131/  1.141178, val:  66.67%, val_best:  66.67%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.931621/  1.143126, val:  72.50%, val_best:  72.50%, tr:  71.71%, tr_best:  71.81%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.846661/  1.219483, val:  64.17%, val_best:  72.50%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.806770/  1.260874, val:  68.33%, val_best:  72.50%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.848225/  1.211548, val:  69.17%, val_best:  72.50%, tr:  79.78%, tr_best:  81.41%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.714247/  1.151863, val:  79.17%, val_best:  79.17%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.622073/  1.080003, val:  82.92%, val_best:  82.92%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.592821/  1.142021, val:  82.50%, val_best:  82.92%, tr:  91.22%, tr_best:  91.62%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.556177/  1.358047, val:  70.42%, val_best:  82.92%, tr:  91.42%, tr_best:  91.62%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.481127/  1.228372, val:  80.83%, val_best:  82.92%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.439013/  1.212624, val:  81.67%, val_best:  82.92%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.399943/  1.239657, val:  81.25%, val_best:  82.92%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.357254/  1.310441, val:  80.42%, val_best:  82.92%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.347358/  1.198072, val:  85.00%, val_best:  85.00%, tr:  97.45%, tr_best:  97.65%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.297294/  1.380537, val:  78.33%, val_best:  85.00%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.289627/  1.360016, val:  85.00%, val_best:  85.00%, tr:  98.16%, tr_best:  98.67%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.278937/  1.404645, val:  80.42%, val_best:  85.00%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.275446/  1.392614, val:  81.67%, val_best:  85.00%, tr:  98.37%, tr_best:  98.98%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.243805/  1.398935, val:  83.75%, val_best:  85.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.202857/  1.456874, val:  83.33%, val_best:  85.00%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.207546/  1.486244, val:  85.00%, val_best:  85.00%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.181242/  1.544554, val:  84.17%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.165685/  1.493030, val:  84.58%, val_best:  85.00%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.149535/  1.695074, val:  79.17%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.155572/  1.627908, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.151485/  1.657331, val:  82.08%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.167811/  1.685501, val:  82.50%, val_best:  85.00%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.131060/  1.727040, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.119735/  1.805142, val:  81.67%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.115524/  1.790252, val:  82.50%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.093009/  1.748719, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.091824/  1.843053, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.096797/  1.832674, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.081316/  1.843225, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.070998/  1.845201, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.061086/  1.941512, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.064064/  1.906011, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.059933/  1.942955, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.056217/  1.973621, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.052101/  2.007511, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.050164/  1.968748, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.051952/  2.030425, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.050920/  1.981389, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.040537/  2.046527, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.039788/  2.052551, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.045400/  2.067800, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.041909/  2.092551, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.033276/  2.107158, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.028859/  2.100258, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.027081/  2.105584, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.031503/  2.174173, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.027545/  2.164366, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.030097/  2.191032, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.026088/  2.225728, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.023934/  2.151983, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.025296/  2.234951, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.022565/  2.192113, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.021190/  2.228780, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.021179/  2.268256, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.021038/  2.315913, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.019384/  2.284978, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.022650/  2.281950, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.019097/  2.281934, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.018251/  2.309144, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.016003/  2.334571, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.016030/  2.301430, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.015940/  2.332711, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.016914/  2.358677, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014311/  2.377785, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.015170/  2.319848, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.015982/  2.481783, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.016414/  2.346095, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.017350/  2.342394, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.014484/  2.330831, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.011201/  2.391643, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.013324/  2.363090, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.013394/  2.402677, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.013639/  2.418149, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.013833/  2.468368, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.011364/  2.450069, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.015356/  2.446522, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.013673/  2.444415, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.016079/  2.485945, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.017404/  2.481196, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.010788/  2.495262, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.012371/  2.520899, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.011556/  2.551409, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.010907/  2.558093, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.010315/  2.549564, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.009746/  2.506722, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009529/  2.572533, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008709/  2.552748, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.006697/  2.503835, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.007798/  2.572679, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29c7881ffb74760bc293dc8eea05660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▃▂█▇▆▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▆▆█▆▇████▇▇▇██▇██▇██████████████▇████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▇▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▆▆███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▆▆█▆▇████▇▇▇██▇██▇██████████████▇████</td></tr><tr><td>val_loss</td><td>▄▂▂▁▂▁▂▂▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0078</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>2.57268</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4wwmotsf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_162405-4wwmotsf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aun0wfgg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_162934-aun0wfgg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg' target=\"_blank\">soft-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.040516/  1.642006, val:  55.00%, val_best:  55.00%, tr:  27.17%, tr_best:  27.17%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.550143/  1.873266, val:  49.17%, val_best:  55.00%, tr:  51.58%, tr_best:  51.58%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.622550/  1.669430, val:  57.50%, val_best:  57.50%, tr:  54.14%, tr_best:  54.14%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.293943/  1.977846, val:  55.83%, val_best:  57.50%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.354115/  1.542797, val:  62.92%, val_best:  62.92%, tr:  61.59%, tr_best:  63.23%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.276774/  1.588674, val:  54.58%, val_best:  62.92%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.230379/  1.757703, val:  60.00%, val_best:  62.92%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.398552/  2.213275, val:  55.42%, val_best:  62.92%, tr:  64.96%, tr_best:  66.70%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.121745/  1.554287, val:  60.00%, val_best:  62.92%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.502731/  1.743191, val:  62.50%, val_best:  62.92%, tr:  65.37%, tr_best:  70.58%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.187423/  1.219841, val:  75.83%, val_best:  75.83%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.112035/  1.663177, val:  63.33%, val_best:  75.83%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.125620/  1.364929, val:  71.67%, val_best:  75.83%, tr:  74.97%, tr_best:  74.97%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.852706/  1.586628, val:  61.25%, val_best:  75.83%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.950517/  1.812055, val:  64.58%, val_best:  75.83%, tr:  80.08%, tr_best:  80.69%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695665/  1.407705, val:  69.58%, val_best:  75.83%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.690188/  1.454104, val:  76.67%, val_best:  76.67%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.684235/  1.535680, val:  70.42%, val_best:  76.67%, tr:  87.44%, tr_best:  88.66%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.604962/  1.412773, val:  79.17%, val_best:  79.17%, tr:  90.40%, tr_best:  90.40%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.545171/  1.377163, val:  79.17%, val_best:  79.17%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.520745/  1.588545, val:  68.33%, val_best:  79.17%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.581843/  1.550013, val:  75.83%, val_best:  79.17%, tr:  91.83%, tr_best:  92.24%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.437616/  1.418960, val:  79.58%, val_best:  79.58%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.405296/  1.676802, val:  75.83%, val_best:  79.58%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.425702/  1.573171, val:  79.58%, val_best:  79.58%, tr:  95.40%, tr_best:  96.42%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.366101/  1.519008, val:  80.00%, val_best:  80.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.346354/  1.718673, val:  76.67%, val_best:  80.00%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.321981/  1.589448, val:  79.58%, val_best:  80.00%, tr:  97.45%, tr_best:  97.65%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.283775/  1.517297, val:  83.33%, val_best:  83.33%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.272821/  1.765906, val:  76.25%, val_best:  83.33%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.249871/  1.619042, val:  81.67%, val_best:  83.33%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.225793/  1.665401, val:  82.08%, val_best:  83.33%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.327429/  1.642328, val:  82.08%, val_best:  83.33%, tr:  96.94%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.220426/  1.842655, val:  80.42%, val_best:  83.33%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.230818/  1.796905, val:  80.00%, val_best:  83.33%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.191670/  1.653128, val:  83.75%, val_best:  83.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.170803/  1.675033, val:  82.92%, val_best:  83.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.184137/  1.724534, val:  85.83%, val_best:  85.83%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.172902/  1.694957, val:  84.58%, val_best:  85.83%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.190999/  1.638858, val:  87.50%, val_best:  87.50%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.141605/  1.661387, val:  83.75%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.138470/  1.746879, val:  85.83%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.128380/  1.799114, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.125076/  1.830211, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.119388/  1.879802, val:  84.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.103815/  1.851572, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.104387/  1.942153, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.093042/  1.893952, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.099276/  1.910627, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.084603/  1.907686, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.074872/  1.919549, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.065433/  1.925734, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.070708/  2.010888, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.062743/  2.009382, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.066973/  1.990938, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.055979/  2.006928, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.058404/  2.155457, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.067781/  2.013330, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.054138/  2.094468, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.052688/  2.094179, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.048649/  2.116537, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.048545/  2.150365, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.049908/  2.172894, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.051428/  2.120027, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.045165/  2.123672, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.044897/  2.159294, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.046397/  2.190489, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.044869/  2.196114, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.040653/  2.181042, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.037838/  2.225475, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.036113/  2.183403, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.038867/  2.244186, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.033992/  2.258068, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.031342/  2.275935, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.031066/  2.225717, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.032031/  2.272989, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.024831/  2.340320, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.025319/  2.363508, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.024136/  2.429628, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.023436/  2.344496, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.019394/  2.358616, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.025281/  2.396485, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.025266/  2.426712, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.026510/  2.368071, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.027278/  2.390736, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.022376/  2.450972, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.022716/  2.343374, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.024559/  2.373593, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.024390/  2.409000, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.018715/  2.370669, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.015984/  2.432915, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.018121/  2.419651, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.020993/  2.468354, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.018900/  2.473497, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.017190/  2.503860, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.016020/  2.535366, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.016636/  2.521703, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.016006/  2.500131, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.016884/  2.466167, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.017244/  2.505944, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7859b734d5465f981775af64ea9a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▂▅▅▁▆▇▇██▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▁▃▅▃▄▆▅▆▆▆▇▇███▇██▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▆▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▃▅▅▆▆▆▆▆▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▁▃▅▃▄▆▅▆▆▆▇▇███▇██▇██████████████████</td></tr><tr><td>val_loss</td><td>▃▃▂▆▃▁▄▂▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01724</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>2.50594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-25</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aun0wfgg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_162934-aun0wfgg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ip0s7a3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb30ca3ecade4c80b5a5f26dbbbc9790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113427677709195, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_163506-9ip0s7a3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3' target=\"_blank\">genial-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.305396/  2.111218, val:  19.17%, val_best:  19.17%, tr:   9.40%, tr_best:   9.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.699203/  1.455139, val:  50.42%, val_best:  50.42%, tr:  37.39%, tr_best:  37.39%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.251002/  1.293518, val:  59.17%, val_best:  59.17%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.101699/  1.366040, val:  59.58%, val_best:  59.58%, tr:  63.64%, tr_best:  63.64%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.051564/  1.172195, val:  66.67%, val_best:  66.67%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.006531/  1.187820, val:  64.17%, val_best:  66.67%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.962090/  1.214793, val:  61.67%, val_best:  66.67%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.965383/  1.218809, val:  61.25%, val_best:  66.67%, tr:  67.82%, tr_best:  70.38%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.935547/  1.150730, val:  67.92%, val_best:  67.92%, tr:  68.95%, tr_best:  70.38%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.876291/  1.309818, val:  60.00%, val_best:  67.92%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.904200/  1.288254, val:  62.92%, val_best:  67.92%, tr:  73.34%, tr_best:  74.77%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.869970/  1.163118, val:  72.08%, val_best:  72.08%, tr:  73.34%, tr_best:  74.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.824630/  1.160297, val:  71.25%, val_best:  72.08%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.814169/  1.129000, val:  70.00%, val_best:  72.08%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.781957/  1.447923, val:  65.42%, val_best:  72.08%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.754031/  1.145749, val:  75.42%, val_best:  75.42%, tr:  79.06%, tr_best:  81.00%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.736852/  1.140446, val:  71.67%, val_best:  75.42%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.701477/  1.185807, val:  72.50%, val_best:  75.42%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.677294/  1.300672, val:  69.58%, val_best:  75.42%, tr:  86.11%, tr_best:  86.41%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.676341/  1.257019, val:  71.67%, val_best:  75.42%, tr:  85.60%, tr_best:  86.41%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.679741/  1.230954, val:  70.83%, val_best:  75.42%, tr:  85.80%, tr_best:  86.41%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.608238/  1.433563, val:  70.42%, val_best:  75.42%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.584824/  1.293576, val:  67.08%, val_best:  75.42%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.588511/  1.150824, val:  77.92%, val_best:  77.92%, tr:  91.22%, tr_best:  91.62%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.534270/  1.227142, val:  78.33%, val_best:  78.33%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.541524/  1.222217, val:  75.42%, val_best:  78.33%, tr:  91.83%, tr_best:  95.40%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.594831/  1.194868, val:  75.83%, val_best:  78.33%, tr:  88.56%, tr_best:  95.40%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.509628/  1.337001, val:  72.92%, val_best:  78.33%, tr:  94.18%, tr_best:  95.40%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.495934/  1.227807, val:  80.42%, val_best:  80.42%, tr:  94.48%, tr_best:  95.40%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.445957/  1.345022, val:  71.25%, val_best:  80.42%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.448369/  1.298983, val:  73.33%, val_best:  80.42%, tr:  96.32%, tr_best:  96.63%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.432984/  1.282349, val:  76.25%, val_best:  80.42%, tr:  96.42%, tr_best:  96.63%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.470449/  1.292142, val:  76.67%, val_best:  80.42%, tr:  93.36%, tr_best:  96.63%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.431250/  1.331490, val:  73.33%, val_best:  80.42%, tr:  95.81%, tr_best:  96.63%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.412668/  1.336563, val:  77.08%, val_best:  80.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.372017/  1.344720, val:  80.00%, val_best:  80.42%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.358682/  1.326062, val:  82.08%, val_best:  82.08%, tr:  97.85%, tr_best:  98.26%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.350713/  1.440310, val:  73.33%, val_best:  82.08%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.385123/  1.317410, val:  77.92%, val_best:  82.08%, tr:  97.75%, tr_best:  98.26%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.356483/  1.350035, val:  81.25%, val_best:  82.08%, tr:  97.34%, tr_best:  98.26%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.321186/  1.312757, val:  82.08%, val_best:  82.08%, tr:  97.85%, tr_best:  98.26%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.329135/  1.402122, val:  80.00%, val_best:  82.08%, tr:  97.55%, tr_best:  98.26%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.297287/  1.347606, val:  81.25%, val_best:  82.08%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.294904/  1.447161, val:  75.83%, val_best:  82.08%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.300371/  1.445148, val:  76.67%, val_best:  82.08%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.270501/  1.414950, val:  80.42%, val_best:  82.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.272118/  1.438061, val:  79.17%, val_best:  82.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.254753/  1.438618, val:  78.75%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.277224/  1.466854, val:  80.00%, val_best:  82.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.246045/  1.439476, val:  81.25%, val_best:  82.08%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.229539/  1.471748, val:  78.33%, val_best:  82.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.230495/  1.481717, val:  80.00%, val_best:  82.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.231312/  1.531207, val:  81.25%, val_best:  82.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.218309/  1.521851, val:  78.75%, val_best:  82.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.200975/  1.501706, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.193795/  1.565947, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.204722/  1.563462, val:  82.08%, val_best:  84.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.197064/  1.508280, val:  83.33%, val_best:  84.58%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.171372/  1.554160, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.179808/  1.651070, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.172693/  1.615369, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.185133/  1.612537, val:  78.33%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.163319/  1.643281, val:  78.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.171514/  1.658182, val:  80.83%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.171313/  1.609245, val:  82.50%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.147116/  1.653720, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.134929/  1.627900, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.138620/  1.691944, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.154565/  1.726142, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.143177/  1.707724, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.137044/  1.717020, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.128650/  1.751790, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.113639/  1.817807, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.139280/  1.783728, val:  82.08%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.128704/  1.762757, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.105600/  1.793963, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.118291/  1.793824, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.121397/  1.829144, val:  83.75%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.125361/  1.843383, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.131758/  1.839104, val:  81.67%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.116138/  1.854860, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.105949/  1.833159, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.108596/  1.895337, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.114023/  1.933228, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.096126/  1.852567, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.104709/  1.888085, val:  82.08%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.095559/  1.863306, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.091648/  1.930743, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.084029/  1.879617, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.076067/  1.950858, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.089658/  1.932786, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.079884/  1.949038, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.075741/  1.988927, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.078477/  1.979764, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.077056/  2.009168, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.074213/  1.997348, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.072934/  2.054961, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.072094/  2.015152, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.067559/  2.068703, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.071169/  1.976241, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fef5617285148a7b838ea2afc7a3c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▆▅▆▆▅▆▇▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▅▇▆▇▇▆▇▇▇▇█▇██▇▇████▇▇███▇█████▇████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▇▇▇▇█▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▅▇▆▇▇▆▇▇▇▇█▇██▇▇████▇▇███▇█████▇████</td></tr><tr><td>val_loss</td><td>█▂▁▁▂▁▃▁▂▃▁▁▂▂▂▃▂▂▃▃▃▄▄▄▅▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07117</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>1.97624</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-sweep-29</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9ip0s7a3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_163506-9ip0s7a3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fpsj5zyt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a468a733a7c4f75b17a958bdfc7177e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113546576557888, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164034-fpsj5zyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt' target=\"_blank\">azure-sweep-34</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.135064/  2.348142, val:  45.00%, val_best:  45.00%, tr:  26.56%, tr_best:  26.56%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.917111/  2.421806, val:  52.92%, val_best:  52.92%, tr:  50.87%, tr_best:  50.87%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.260753/  2.014679, val:  52.50%, val_best:  52.92%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.700029/  2.913277, val:  57.08%, val_best:  57.08%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.070586/  2.466565, val:  60.83%, val_best:  60.83%, tr:  58.43%, tr_best:  60.98%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.974281/  2.510710, val:  56.25%, val_best:  60.83%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.456344/  2.469453, val:  58.75%, val_best:  60.83%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.739878/  2.664713, val:  54.17%, val_best:  60.83%, tr:  64.76%, tr_best:  68.03%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.713847/  2.751289, val:  55.00%, val_best:  60.83%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.096435/  2.797124, val:  59.17%, val_best:  60.83%, tr:  67.62%, tr_best:  68.64%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.593845/  1.728211, val:  68.33%, val_best:  68.33%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.624188/  2.273625, val:  65.00%, val_best:  68.33%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.366256/  1.997888, val:  65.83%, val_best:  68.33%, tr:  77.43%, tr_best:  77.43%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.306336/  2.404990, val:  62.50%, val_best:  68.33%, tr:  78.45%, tr_best:  78.45%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.562997/  2.758612, val:  65.42%, val_best:  68.33%, tr:  78.35%, tr_best:  78.45%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.974711/  2.258536, val:  70.83%, val_best:  70.83%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.903604/  1.960086, val:  80.00%, val_best:  80.00%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.898043/  2.249439, val:  72.50%, val_best:  80.00%, tr:  88.25%, tr_best:  89.17%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.863013/  2.361640, val:  75.00%, val_best:  80.00%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.835367/  2.126679, val:  80.42%, val_best:  80.42%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.702799/  2.647707, val:  67.50%, val_best:  80.42%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.849705/  2.180631, val:  79.17%, val_best:  80.42%, tr:  91.32%, tr_best:  94.28%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.627266/  2.232721, val:  78.75%, val_best:  80.42%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.675346/  2.746020, val:  74.17%, val_best:  80.42%, tr:  94.89%, tr_best:  95.20%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.572461/  2.423426, val:  80.83%, val_best:  80.83%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.555514/  2.337517, val:  79.58%, val_best:  80.83%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.474505/  2.414984, val:  79.17%, val_best:  80.83%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.469093/  2.691437, val:  76.25%, val_best:  80.83%, tr:  97.55%, tr_best:  97.75%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.456499/  2.521243, val:  82.92%, val_best:  82.92%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.387633/  2.855560, val:  79.58%, val_best:  82.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.372882/  2.779290, val:  82.50%, val_best:  82.92%, tr:  98.37%, tr_best:  98.88%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.364165/  2.667890, val:  82.50%, val_best:  82.92%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.505896/  2.728811, val:  80.42%, val_best:  82.92%, tr:  97.14%, tr_best:  99.18%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.387118/  2.998614, val:  83.33%, val_best:  83.33%, tr:  97.96%, tr_best:  99.18%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.369427/  3.229547, val:  77.92%, val_best:  83.33%, tr:  99.08%, tr_best:  99.18%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.303415/  2.691154, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.249374/  2.695435, val:  85.83%, val_best:  85.83%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.250952/  2.816796, val:  85.00%, val_best:  85.83%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.294056/  2.873309, val:  82.92%, val_best:  85.83%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.309798/  2.796141, val:  86.25%, val_best:  86.25%, tr:  98.88%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.219641/  2.762543, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.211970/  2.841761, val:  84.17%, val_best:  87.08%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.181153/  2.971166, val:  79.58%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.204013/  2.931836, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.181774/  3.080180, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.164444/  3.323238, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.160665/  3.321762, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.164442/  3.211581, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.156313/  3.246644, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.167254/  3.290756, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.135145/  3.414236, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.133105/  3.296542, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.135538/  3.443089, val:  82.92%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.132860/  3.484478, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.125570/  3.407904, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.110793/  3.401881, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.102220/  3.673701, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.113155/  3.458656, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.094956/  3.459064, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.093048/  3.698795, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.096622/  3.679455, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.107340/  3.662870, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.091717/  3.709290, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.198414/  3.899753, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.114333/  3.827221, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.101458/  3.784963, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.094442/  4.059870, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.095331/  4.018095, val:  86.67%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.085894/  3.966964, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.057995/  4.053779, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.056270/  3.920878, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.065530/  4.122053, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.076394/  4.150880, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.069386/  4.169332, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.066232/  4.078281, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.052409/  4.288164, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.053953/  4.289522, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.053649/  4.327401, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.044496/  4.346035, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.048964/  4.318016, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.054606/  4.343048, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.040182/  4.264418, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.045545/  4.403891, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.044003/  4.324257, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044437/  4.273188, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.054824/  4.345763, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.047066/  4.325257, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.040543/  4.423947, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.040323/  4.357699, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.041144/  4.431307, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.039271/  4.545905, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.041248/  4.382458, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.034382/  4.529025, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.038688/  4.499335, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.034813/  4.517359, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.023365/  4.559685, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.030883/  4.574547, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.032475/  4.558115, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.040467/  4.578133, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030222/  4.585063, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5aa78484664049b66a065ce5e7e1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▅▅▅▇▇▇█▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▃▄▄▆▇▇▇▇▇▇███▇▇▇█▇██▇████▇███████▇██</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▇▅▆▄▄▄▃▂▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▄▅▅▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▃▄▄▆▇▇▇▇▇▇███▇▇▇█▇██▇████▇███████▇██</td></tr><tr><td>val_loss</td><td>▂▁▂▃▃▁▃▂▁▁▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▆▇▇▇▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03022</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>4.58506</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-sweep-34</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fpsj5zyt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_164034-fpsj5zyt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pyrwsu69 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164650-pyrwsu69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69' target=\"_blank\">trim-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.062186/  2.395987, val:  44.58%, val_best:  44.58%, tr:  30.54%, tr_best:  30.54%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.932272/  2.177964, val:  51.25%, val_best:  51.25%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.009413/  1.797581, val:  56.67%, val_best:  56.67%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.426914/  2.523172, val:  56.25%, val_best:  56.67%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.787200/  1.792446, val:  62.92%, val_best:  62.92%, tr:  61.18%, tr_best:  66.09%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.719016/  2.198045, val:  58.33%, val_best:  62.92%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.163717/  2.508123, val:  60.83%, val_best:  62.92%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.472037/  2.152541, val:  59.17%, val_best:  62.92%, tr:  68.95%, tr_best:  71.60%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.214181/  1.654069, val:  61.67%, val_best:  62.92%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.613171/  2.570397, val:  53.75%, val_best:  62.92%, tr:  70.48%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.207835/  2.052088, val:  58.75%, val_best:  62.92%, tr:  73.14%, tr_best:  73.14%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.542757/  2.610566, val:  63.33%, val_best:  63.33%, tr:  72.63%, tr_best:  73.14%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.462697/  1.669866, val:  72.92%, val_best:  72.92%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.068347/  2.035656, val:  65.42%, val_best:  72.92%, tr:  78.35%, tr_best:  78.35%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.285327/  2.627961, val:  63.75%, val_best:  72.92%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.877170/  2.274029, val:  67.08%, val_best:  72.92%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.895368/  2.154450, val:  72.50%, val_best:  72.92%, tr:  87.03%, tr_best:  87.03%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.731003/  2.051186, val:  70.00%, val_best:  72.92%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.700490/  2.331778, val:  68.33%, val_best:  72.92%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.629375/  2.138595, val:  75.83%, val_best:  75.83%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.578407/  2.284514, val:  74.17%, val_best:  75.83%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.586689/  2.689306, val:  70.42%, val_best:  75.83%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.493521/  2.378192, val:  75.83%, val_best:  75.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.494291/  2.669397, val:  75.42%, val_best:  75.83%, tr:  96.12%, tr_best:  96.53%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.395209/  2.602473, val:  77.92%, val_best:  77.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.422087/  2.663994, val:  77.08%, val_best:  77.92%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.387727/  2.741148, val:  77.50%, val_best:  77.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.382440/  2.938202, val:  77.92%, val_best:  77.92%, tr:  98.47%, tr_best:  98.77%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.289783/  2.659052, val:  81.25%, val_best:  81.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.311728/  2.983512, val:  79.17%, val_best:  81.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.287006/  3.127795, val:  80.83%, val_best:  81.25%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.237401/  3.171818, val:  76.25%, val_best:  81.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.271529/  3.322640, val:  75.83%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.222970/  3.324409, val:  78.33%, val_best:  81.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.243223/  3.555316, val:  75.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.222751/  3.366916, val:  80.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171153/  3.391801, val:  81.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.163252/  3.529157, val:  78.75%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.150880/  3.376877, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.171795/  3.613594, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.135498/  3.662543, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.112801/  3.644865, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.109654/  3.816520, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.128493/  3.744687, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.129770/  3.745157, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.092817/  3.856912, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.110130/  3.776510, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.098390/  3.879533, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.077045/  3.927565, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.082830/  4.332655, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.082940/  4.092589, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.081213/  4.066563, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.081532/  4.166774, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.071735/  4.143727, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.077904/  4.281452, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.102459/  4.300975, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.087348/  4.528956, val:  75.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.062137/  4.454435, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.061949/  4.447115, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.075246/  4.401570, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.060665/  4.622108, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.048169/  4.708529, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.040685/  4.518310, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.038630/  4.411102, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.033288/  4.501129, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.038497/  4.617074, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.039794/  4.594819, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.046207/  4.557834, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.072226/  4.797017, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.043040/  4.666761, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.039252/  4.662484, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.030064/  4.797895, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.045557/  4.730133, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.033137/  4.767644, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.036926/  4.849226, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.038322/  4.949713, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058650/  4.869298, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.043149/  5.034408, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.043755/  5.122185, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.051200/  5.103598, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.033358/  5.295014, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.047769/  4.883702, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.036011/  5.132040, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.029694/  4.937526, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.044499/  5.091215, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.043552/  5.126673, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.042657/  5.159236, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.031548/  5.157077, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.027336/  5.177079, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.030779/  5.099520, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.025766/  5.258143, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.023230/  5.093557, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.031848/  5.331208, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.018090/  5.196564, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.023957/  5.249485, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.038331/  5.359312, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.037034/  5.286173, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028969/  5.336520, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.022347/  5.271390, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023963/  5.262772, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475845f01c744abd811bf7dddfc63916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▇▅▅▂▆▇▅█▇███▇██▇███████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▄▃▆▅▆▇▆▇▇▇▇█▇█▇██▇▇██▇███▇▇████▇██▇▇█</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▆▆▆▅▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▆▆▆▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▄▃▆▅▆▇▆▇▇▇▇█▇█▇██▇▇██▇███▇▇████▇██▇▇█</td></tr><tr><td>val_loss</td><td>▂▁▁▂▃▁▃▂▂▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▆▇▇▇▇██▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02396</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>5.26277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-37</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pyrwsu69</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_164650-pyrwsu69/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 597cvjd1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_165242-597cvjd1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1' target=\"_blank\">graceful-sweep-39</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.318805/  2.210244, val:  20.00%, val_best:  20.00%, tr:   9.19%, tr_best:   9.19%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.772503/  1.428594, val:  57.92%, val_best:  57.92%, tr:  38.00%, tr_best:  38.00%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.310495/  1.356579, val:  55.83%, val_best:  57.92%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.101270/  1.336439, val:  58.33%, val_best:  58.33%, tr:  64.15%, tr_best:  64.15%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.075656/  1.288629, val:  59.58%, val_best:  59.58%, tr:  62.82%, tr_best:  64.15%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.996204/  1.210784, val:  62.50%, val_best:  62.50%, tr:  68.44%, tr_best:  68.44%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.966449/  1.234998, val:  60.83%, val_best:  62.50%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.931269/  1.220983, val:  61.67%, val_best:  62.50%, tr:  68.64%, tr_best:  69.15%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.919397/  1.122737, val:  64.17%, val_best:  64.17%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.848325/  1.235111, val:  61.25%, val_best:  64.17%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.886516/  1.307322, val:  62.92%, val_best:  64.17%, tr:  75.38%, tr_best:  75.49%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.843915/  1.214096, val:  63.33%, val_best:  64.17%, tr:  74.06%, tr_best:  75.49%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.791331/  1.180076, val:  63.75%, val_best:  64.17%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.789663/  1.240403, val:  64.17%, val_best:  64.17%, tr:  78.65%, tr_best:  78.65%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.715044/  1.433705, val:  64.58%, val_best:  64.58%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.686299/  1.252988, val:  65.00%, val_best:  65.00%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.647108/  1.229423, val:  67.08%, val_best:  67.08%, tr:  85.29%, tr_best:  85.29%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.620437/  1.263187, val:  69.58%, val_best:  69.58%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.586220/  1.360671, val:  66.67%, val_best:  69.58%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.573204/  1.296395, val:  69.58%, val_best:  69.58%, tr:  90.09%, tr_best:  90.81%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.556452/  1.316498, val:  66.67%, val_best:  69.58%, tr:  90.60%, tr_best:  90.81%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.494077/  1.457673, val:  67.08%, val_best:  69.58%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.474826/  1.377802, val:  69.17%, val_best:  69.58%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.488248/  1.335735, val:  74.17%, val_best:  74.17%, tr:  94.48%, tr_best:  95.30%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.418628/  1.415621, val:  72.92%, val_best:  74.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.416415/  1.379255, val:  71.67%, val_best:  74.17%, tr:  94.59%, tr_best:  96.83%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.414250/  1.352293, val:  77.08%, val_best:  77.08%, tr:  95.71%, tr_best:  96.83%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.367924/  1.516313, val:  75.42%, val_best:  77.08%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.332235/  1.443123, val:  74.58%, val_best:  77.08%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.298641/  1.544621, val:  71.25%, val_best:  77.08%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.283481/  1.523626, val:  72.50%, val_best:  77.08%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.280684/  1.530439, val:  78.33%, val_best:  78.33%, tr:  98.67%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.316187/  1.599973, val:  71.67%, val_best:  78.33%, tr:  97.34%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.290718/  1.631603, val:  76.25%, val_best:  78.33%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.253567/  1.609717, val:  74.58%, val_best:  78.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.217485/  1.646970, val:  75.83%, val_best:  78.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.192557/  1.624180, val:  77.92%, val_best:  78.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.194858/  1.681047, val:  77.92%, val_best:  78.33%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.200941/  1.646473, val:  79.58%, val_best:  79.58%, tr:  99.39%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.167189/  1.770751, val:  75.00%, val_best:  79.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.175144/  1.745195, val:  75.00%, val_best:  79.58%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.171604/  1.719100, val:  77.08%, val_best:  79.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.140545/  1.791715, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.128460/  1.771028, val:  77.08%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.128419/  1.858317, val:  74.17%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.118835/  1.854908, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.109903/  1.883399, val:  79.58%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.101471/  1.868499, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.111395/  1.897279, val:  80.42%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.099955/  1.980149, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.086770/  1.994833, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.081568/  1.982269, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.086296/  2.083234, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.077490/  2.026960, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.075247/  2.055609, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.061841/  2.055895, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.063706/  2.112296, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.065352/  2.105452, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.061281/  2.113639, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.063396/  2.136526, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.052958/  2.152933, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.051542/  2.149691, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.048617/  2.226520, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.054391/  2.284744, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.055692/  2.180856, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.045647/  2.280824, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.039179/  2.257237, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.038166/  2.271627, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.041401/  2.296675, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.037734/  2.263813, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.035291/  2.274694, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.037053/  2.324774, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.036267/  2.355351, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.039400/  2.410007, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.037208/  2.414623, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.031648/  2.415062, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.026893/  2.367258, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.033897/  2.523752, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.039499/  2.466230, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.034374/  2.450610, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.027541/  2.469490, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.022908/  2.496147, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.023113/  2.517245, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.022990/  2.524898, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.019521/  2.477964, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.022837/  2.563119, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.023714/  2.532770, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.021604/  2.571520, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.016297/  2.571679, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.019144/  2.572850, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.018133/  2.559130, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.019105/  2.632702, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.018280/  2.638778, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.019552/  2.652483, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.016407/  2.629497, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.015731/  2.668269, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.018343/  2.677304, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.017681/  2.753611, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.016281/  2.676286, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.020668/  2.670245, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da0190541804a58bea04833c52ed905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▄▇▆▇▅▇█▇██▇▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇█▇▇▇█████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇█▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>▆▂▁▁▁▁▂▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02067</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.79583</td></tr><tr><td>val_loss</td><td>2.67025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-39</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/597cvjd1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_165242-597cvjd1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: klrdac15 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_165812-klrdac15</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15' target=\"_blank\">dark-sweep-41</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309219, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.030512/  1.678581, val:  49.17%, val_best:  49.17%, tr:  21.25%, tr_best:  21.25%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.437040/  1.430297, val:  57.08%, val_best:  57.08%, tr:  52.40%, tr_best:  52.40%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.166490/  1.438994, val:  55.00%, val_best:  57.08%, tr:  60.88%, tr_best:  60.88%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.090239/  1.297438, val:  64.17%, val_best:  64.17%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.037390/  1.301868, val:  61.67%, val_best:  64.17%, tr:  69.05%, tr_best:  69.05%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.000910/  1.338546, val:  61.25%, val_best:  64.17%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.994178/  1.323386, val:  62.50%, val_best:  64.17%, tr:  68.64%, tr_best:  70.99%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.981850/  1.270464, val:  65.42%, val_best:  65.42%, tr:  69.36%, tr_best:  70.99%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.913207/  1.388049, val:  60.42%, val_best:  65.42%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.937273/  1.378559, val:  65.00%, val_best:  65.42%, tr:  73.03%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.896307/  1.284199, val:  71.67%, val_best:  71.67%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.847823/  1.231609, val:  73.33%, val_best:  73.33%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.808100/  1.275456, val:  72.50%, val_best:  73.33%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.769586/  1.543463, val:  69.17%, val_best:  73.33%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.743970/  1.289895, val:  73.33%, val_best:  73.33%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.710916/  1.317821, val:  73.75%, val_best:  73.75%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.677988/  1.398424, val:  74.58%, val_best:  74.58%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.625889/  1.493693, val:  70.00%, val_best:  74.58%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.624810/  1.371965, val:  74.17%, val_best:  74.58%, tr:  92.13%, tr_best:  92.44%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.601085/  1.485054, val:  71.25%, val_best:  74.58%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.571917/  1.462354, val:  72.92%, val_best:  74.58%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.529248/  1.528203, val:  72.50%, val_best:  74.58%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.542228/  1.469005, val:  80.00%, val_best:  80.00%, tr:  94.99%, tr_best:  95.10%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.484097/  1.510545, val:  79.58%, val_best:  80.00%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.474766/  1.549213, val:  75.83%, val_best:  80.00%, tr:  96.83%, tr_best:  97.04%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.507410/  1.542966, val:  79.17%, val_best:  80.00%, tr:  95.91%, tr_best:  97.04%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.430397/  1.610787, val:  78.33%, val_best:  80.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.433632/  1.619803, val:  82.50%, val_best:  82.50%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.419218/  1.795434, val:  74.17%, val_best:  82.50%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.411251/  1.627755, val:  80.42%, val_best:  82.50%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.396910/  1.745959, val:  79.58%, val_best:  82.50%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.391482/  1.734411, val:  80.42%, val_best:  82.50%, tr:  97.96%, tr_best:  98.37%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.359470/  1.800742, val:  80.83%, val_best:  82.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.350944/  1.896027, val:  80.00%, val_best:  82.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.342049/  1.930975, val:  82.50%, val_best:  82.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.326643/  1.834182, val:  82.50%, val_best:  82.50%, tr:  98.67%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.304016/  2.002824, val:  78.75%, val_best:  82.50%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.327067/  1.894813, val:  82.08%, val_best:  82.50%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.293819/  1.974611, val:  84.58%, val_best:  84.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.287332/  1.974986, val:  84.58%, val_best:  84.58%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.269948/  2.110264, val:  80.83%, val_best:  84.58%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.268224/  2.078310, val:  81.67%, val_best:  84.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.255084/  2.167964, val:  82.50%, val_best:  84.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.235727/  2.119823, val:  82.50%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.244317/  2.234583, val:  82.50%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.245396/  2.272496, val:  83.33%, val_best:  84.58%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.230579/  2.316329, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.216525/  2.273062, val:  81.67%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.197593/  2.334180, val:  81.25%, val_best:  84.58%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.187620/  2.399100, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.184547/  2.391267, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.195622/  2.521014, val:  83.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.183916/  2.480004, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.165481/  2.504408, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.175693/  2.531630, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.168706/  2.568130, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.173943/  2.586096, val:  84.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.155942/  2.579063, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.144336/  2.632550, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.164397/  2.645462, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.154119/  2.709978, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.162563/  2.770606, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.142049/  2.744117, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.124962/  2.868333, val:  82.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.122618/  2.847694, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.117883/  2.825746, val:  83.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.137307/  2.860445, val:  84.17%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.146424/  2.922956, val:  83.33%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.127995/  3.005267, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.110804/  2.917452, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.113099/  3.010923, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.099290/  3.032896, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.101636/  3.030765, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.102947/  3.077376, val:  82.08%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.106148/  3.136700, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.107884/  3.154456, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.112288/  3.248230, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.106655/  3.235567, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.098614/  3.210622, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.097743/  3.300218, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.087968/  3.221530, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.082949/  3.320432, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.089126/  3.357121, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.079311/  3.327686, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.107448/  3.402605, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.103553/  3.466336, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.095166/  3.384451, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.098599/  3.498098, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.069661/  3.476065, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.077462/  3.421753, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.077030/  3.526197, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.073842/  3.523541, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.078491/  3.696885, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.067479/  3.670784, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.064959/  3.664366, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.059086/  3.669058, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.066375/  3.681804, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.055239/  3.731021, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.070220/  3.678856, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30480c73e2e8464d869431bc413ca1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▄▅▆▄██████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇██▇████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▆▆▇▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇██▇████████████████████████</td></tr><tr><td>val_loss</td><td>▄▂▁▁▁▁▂▁▁▂▂▂▃▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07022</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>3.67886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-41</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/klrdac15</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_165812-klrdac15/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hqsrvmfl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19a667a243849949c1007ed10d01774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112981998465127, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_170453-hqsrvmfl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl' target=\"_blank\">ethereal-sweep-43</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.253323/  1.765929, val:  38.33%, val_best:  38.33%, tr:  13.38%, tr_best:  13.38%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.511758/  1.988629, val:  50.83%, val_best:  50.83%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.611668/  1.840533, val:  56.67%, val_best:  56.67%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.452601/  2.138407, val:  50.00%, val_best:  56.67%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.425102/  1.544960, val:  62.50%, val_best:  62.50%, tr:  61.39%, tr_best:  61.39%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.326070/  1.660033, val:  56.67%, val_best:  62.50%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.351721/  1.740631, val:  60.00%, val_best:  62.50%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.472630/  1.659437, val:  63.75%, val_best:  63.75%, tr:  65.68%, tr_best:  67.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.166623/  1.507021, val:  62.50%, val_best:  63.75%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.481194/  1.640425, val:  65.42%, val_best:  65.42%, tr:  69.66%, tr_best:  71.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.355739/  1.421762, val:  69.58%, val_best:  69.58%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.191274/  1.731272, val:  66.25%, val_best:  69.58%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.157648/  1.571059, val:  74.17%, val_best:  74.17%, tr:  76.00%, tr_best:  76.61%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.047196/  1.905127, val:  63.33%, val_best:  74.17%, tr:  79.88%, tr_best:  79.88%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.138409/  2.007575, val:  70.83%, val_best:  74.17%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.807623/  1.930568, val:  70.42%, val_best:  74.17%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.891349/  1.785194, val:  77.50%, val_best:  77.50%, tr:  87.44%, tr_best:  88.97%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.843770/  1.822308, val:  79.58%, val_best:  79.58%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.731335/  1.875055, val:  79.58%, val_best:  79.58%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.726594/  1.773581, val:  79.58%, val_best:  79.58%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.691131/  1.919801, val:  79.17%, val_best:  79.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.677214/  2.106114, val:  75.00%, val_best:  79.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.617743/  1.978966, val:  79.58%, val_best:  79.58%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.619838/  2.085046, val:  82.08%, val_best:  82.08%, tr:  94.99%, tr_best:  95.81%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.575177/  2.118035, val:  80.00%, val_best:  82.08%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.518957/  1.969795, val:  84.58%, val_best:  84.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.534052/  2.078116, val:  83.33%, val_best:  84.58%, tr:  97.14%, tr_best:  97.34%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.483941/  2.331681, val:  75.83%, val_best:  84.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.509145/  2.250173, val:  83.75%, val_best:  84.58%, tr:  97.65%, tr_best:  97.85%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.452508/  2.539420, val:  79.58%, val_best:  84.58%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.445803/  2.195746, val:  86.25%, val_best:  86.25%, tr:  98.37%, tr_best:  98.67%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.406524/  2.516564, val:  82.08%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.577702/  2.973837, val:  72.50%, val_best:  86.25%, tr:  96.02%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.456043/  2.819457, val:  80.42%, val_best:  86.25%, tr:  98.57%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.410303/  2.874006, val:  82.92%, val_best:  86.25%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.365640/  2.764460, val:  85.00%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.339329/  2.668807, val:  87.08%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.348269/  2.747555, val:  85.00%, val_best:  87.08%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.385121/  2.791430, val:  85.42%, val_best:  87.08%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.330159/  2.913253, val:  85.42%, val_best:  87.08%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.297723/  2.984876, val:  84.17%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.290734/  3.085657, val:  83.33%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.250836/  3.063665, val:  85.00%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.266245/  3.204490, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.269421/  3.308914, val:  84.17%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.273942/  3.302929, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.225014/  3.334401, val:  83.75%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.225493/  3.343555, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.237100/  3.448946, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.224939/  3.488646, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.205718/  3.537910, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.206952/  3.688695, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.207965/  3.698255, val:  85.42%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.201338/  3.905701, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.192889/  3.784663, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.199326/  3.880504, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.184847/  4.016809, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.161488/  4.014282, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.162391/  4.088997, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.154122/  4.290989, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.161288/  4.280174, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.165245/  4.335113, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.167979/  4.380980, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.156888/  4.445717, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.149900/  4.438478, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.131143/  4.483879, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.124440/  4.527740, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.142066/  4.612941, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.143669/  4.712356, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.121949/  4.791534, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.119967/  4.713686, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.131761/  4.720039, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.117413/  4.793860, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.148624/  4.932520, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.122942/  4.752527, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.113134/  4.808766, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.094022/  4.925261, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.095570/  4.946665, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.095363/  5.082295, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.095163/  5.232117, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.091544/  5.069768, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.093498/  5.187819, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.087709/  5.214963, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.092781/  5.356602, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.082836/  5.175879, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.087973/  5.425270, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.102739/  5.286963, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.095318/  5.472827, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.109029/  5.515829, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.085764/  5.330654, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.082066/  5.529750, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.083677/  5.574675, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.076181/  5.598844, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.070736/  5.620394, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.069504/  5.599156, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.075637/  5.783115, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.072909/  5.814156, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.077149/  5.719594, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.074706/  5.652952, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.079862/  5.601643, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d8f823e5084242b2bf13e330595b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▄▂▅▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▅▆▆▇▇▆▇▇▇▆██████████▇█▇█████▇█████▇█</td></tr><tr><td>tr_acc</td><td>▁▅▅▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▆▄▄▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▅▆▆▇▇▆▇▇▇▆██████████▇█▇█████▇█████▇█</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07986</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>5.60164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-43</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hqsrvmfl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_170453-hqsrvmfl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dtgw2w8d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171110-dtgw2w8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d' target=\"_blank\">deep-sweep-45</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.241928/  1.732256, val:  41.25%, val_best:  41.25%, tr:  13.69%, tr_best:  13.69%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.426778/  1.694993, val:  56.25%, val_best:  56.25%, tr:  52.81%, tr_best:  52.81%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.510367/  1.499763, val:  57.92%, val_best:  57.92%, tr:  59.04%, tr_best:  59.04%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.213689/  1.719357, val:  54.58%, val_best:  57.92%, tr:  65.27%, tr_best:  65.27%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.199136/  1.322595, val:  62.08%, val_best:  62.08%, tr:  63.84%, tr_best:  65.27%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.061224/  1.362986, val:  61.25%, val_best:  62.08%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.056197/  1.326440, val:  64.17%, val_best:  64.17%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.085115/  1.224115, val:  68.33%, val_best:  68.33%, tr:  69.46%, tr_best:  70.79%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.055146/  1.446075, val:  64.58%, val_best:  68.33%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.031130/  1.395791, val:  62.50%, val_best:  68.33%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.030412/  1.304922, val:  71.25%, val_best:  71.25%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.890050/  1.339263, val:  71.25%, val_best:  71.25%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.760786/  1.211170, val:  79.17%, val_best:  79.17%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.749820/  1.280827, val:  81.67%, val_best:  81.67%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.752633/  1.666925, val:  74.17%, val_best:  81.67%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.606293/  1.376468, val:  82.50%, val_best:  82.50%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.579093/  1.364524, val:  80.00%, val_best:  82.50%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.508265/  1.427017, val:  82.50%, val_best:  82.50%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.515138/  1.447799, val:  84.17%, val_best:  84.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.496850/  1.419511, val:  84.17%, val_best:  84.17%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.434394/  1.515021, val:  84.17%, val_best:  84.17%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.417364/  1.576975, val:  82.08%, val_best:  84.17%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.392547/  1.619514, val:  80.83%, val_best:  84.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.398906/  1.603530, val:  85.00%, val_best:  85.00%, tr:  97.55%, tr_best:  98.47%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.382188/  1.708744, val:  85.83%, val_best:  85.83%, tr:  98.06%, tr_best:  98.47%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.357598/  1.761010, val:  83.33%, val_best:  85.83%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.337110/  1.734960, val:  86.25%, val_best:  86.25%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.319749/  1.926165, val:  79.17%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.298305/  1.824659, val:  84.58%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.305065/  1.992830, val:  82.08%, val_best:  86.25%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.307400/  1.943748, val:  82.50%, val_best:  86.25%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.278015/  1.979821, val:  85.42%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.275924/  2.010406, val:  85.00%, val_best:  86.25%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.258831/  2.087749, val:  84.17%, val_best:  86.25%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.273977/  2.239629, val:  80.83%, val_best:  86.25%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.245046/  2.129827, val:  86.25%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.235650/  2.117629, val:  85.42%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.205171/  2.178760, val:  83.75%, val_best:  86.25%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.223763/  2.166179, val:  87.08%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.180114/  2.327710, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.189393/  2.397435, val:  86.25%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.170833/  2.413211, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.186980/  2.450851, val:  85.00%, val_best:  87.08%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.163194/  2.463600, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.181356/  2.484248, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.157595/  2.594248, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.158860/  2.575316, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.156610/  2.690062, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.172761/  2.710411, val:  87.50%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.139817/  2.767637, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.125100/  2.759830, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.121371/  2.815148, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.125158/  2.838930, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.113746/  2.974573, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.101725/  2.924242, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.118513/  2.905219, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.109436/  3.066618, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.095460/  3.026918, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.099435/  2.982941, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.108875/  3.168121, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.094910/  3.143734, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.111829/  3.222238, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.105233/  3.269783, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.092370/  3.178758, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.087288/  3.223400, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.078591/  3.341731, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.073290/  3.328445, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.085053/  3.309978, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.081140/  3.424587, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.072963/  3.495421, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.079825/  3.471228, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.078747/  3.533216, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.070523/  3.483029, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.080121/  3.593130, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.072901/  3.680131, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.078790/  3.601433, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.076220/  3.708626, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.078510/  3.826304, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.090337/  3.753058, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.079186/  3.793157, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.055510/  3.917493, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.060350/  3.889609, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.066357/  3.855721, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.072167/  4.013806, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.062263/  3.871153, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.059768/  3.965374, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.061472/  3.954705, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.059634/  4.095017, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.050664/  4.020766, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.061326/  4.007430, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.060682/  3.945527, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.053575/  4.008485, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.047886/  4.105787, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.048141/  4.023148, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.051081/  4.036030, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.038347/  4.129725, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.043175/  4.096999, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.043645/  4.007375, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.044682/  4.085727, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.039490/  4.099744, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6287f62b622845148f8a4f00288621ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▃▁▇▇▆▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▄▇▆▇█▇██▇██▇████████████████████▇███</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▄▇▆▇█▇██▇██▇████████████████████▇███</td></tr><tr><td>val_loss</td><td>▂▂▁▁▁▁▂▂▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03949</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>4.09974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-45</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dtgw2w8d</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171110-dtgw2w8d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llzgtquc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_171701-llzgtquc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc' target=\"_blank\">rare-sweep-47</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.267550/  1.789936, val:  40.00%, val_best:  40.00%, tr:  11.54%, tr_best:  11.54%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.453109/  1.428581, val:  57.08%, val_best:  57.08%, tr:  51.69%, tr_best:  51.69%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.368202/  1.351913, val:  60.42%, val_best:  60.42%, tr:  59.55%, tr_best:  59.55%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.169459/  1.472526, val:  56.25%, val_best:  60.42%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.114242/  1.334611, val:  62.92%, val_best:  62.92%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.009384/  1.248120, val:  62.92%, val_best:  62.92%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.001380/  1.234303, val:  64.58%, val_best:  64.58%, tr:  72.42%, tr_best:  72.42%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.001089/  1.124841, val:  68.33%, val_best:  68.33%, tr:  69.77%, tr_best:  72.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.023363/  1.246347, val:  66.67%, val_best:  68.33%, tr:  72.93%, tr_best:  72.93%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.985452/  1.252468, val:  63.75%, val_best:  68.33%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.970935/  1.171034, val:  71.67%, val_best:  71.67%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.809606/  1.171789, val:  74.58%, val_best:  74.58%, tr:  82.33%, tr_best:  82.33%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.713953/  1.093451, val:  82.50%, val_best:  82.50%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.687643/  1.109947, val:  81.25%, val_best:  82.50%, tr:  87.13%, tr_best:  87.13%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.695422/  1.342220, val:  73.75%, val_best:  82.50%, tr:  86.62%, tr_best:  87.13%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.557454/  1.170050, val:  76.25%, val_best:  82.50%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.566574/  1.104410, val:  82.50%, val_best:  82.50%, tr:  90.60%, tr_best:  91.62%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.497351/  1.095610, val:  84.58%, val_best:  84.58%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.476041/  1.179812, val:  82.50%, val_best:  84.58%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.450163/  1.125065, val:  82.50%, val_best:  84.58%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.425781/  1.206522, val:  82.08%, val_best:  84.58%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.372507/  1.219955, val:  85.42%, val_best:  85.42%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.360393/  1.257549, val:  81.25%, val_best:  85.42%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.372565/  1.229142, val:  85.42%, val_best:  85.42%, tr:  96.32%, tr_best:  97.65%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.325692/  1.218983, val:  84.17%, val_best:  85.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.337816/  1.249061, val:  84.17%, val_best:  85.42%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.322258/  1.225932, val:  85.42%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.302042/  1.436864, val:  77.50%, val_best:  85.42%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.278606/  1.223891, val:  86.25%, val_best:  86.25%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.248801/  1.505903, val:  76.25%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.253824/  1.306579, val:  85.00%, val_best:  86.25%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.237451/  1.308197, val:  85.42%, val_best:  86.25%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.278118/  1.434853, val:  81.67%, val_best:  86.25%, tr:  98.47%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.249512/  1.491594, val:  82.92%, val_best:  86.25%, tr:  98.77%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.253653/  1.531891, val:  80.00%, val_best:  86.25%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.212439/  1.401450, val:  84.58%, val_best:  86.25%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.191506/  1.360086, val:  85.83%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.190469/  1.433068, val:  85.00%, val_best:  86.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.182410/  1.447386, val:  85.83%, val_best:  86.25%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.178098/  1.436105, val:  85.00%, val_best:  86.25%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.152969/  1.450444, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.155400/  1.475345, val:  87.08%, val_best:  87.08%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.141288/  1.575122, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.134649/  1.583155, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.130998/  1.575995, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.129443/  1.650661, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.119951/  1.645833, val:  83.75%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.128443/  1.642946, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.130009/  1.616219, val:  86.67%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.125203/  1.671433, val:  86.67%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.105559/  1.594225, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.098040/  1.701161, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.100097/  1.699679, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.082719/  1.739190, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.079881/  1.690143, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.082690/  1.746564, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.082018/  1.835865, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.074967/  1.799432, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.072538/  1.844192, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.068444/  1.864274, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.068445/  1.875567, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.069637/  1.877753, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.065256/  1.965138, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.065015/  1.902912, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.068019/  1.849861, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.058949/  1.909259, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.055784/  1.934732, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.057801/  1.943013, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.055997/  1.992320, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.048848/  2.057525, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.048404/  2.020181, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.052290/  2.046600, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.050794/  2.031936, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.052275/  2.049447, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.050121/  2.059045, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.041302/  2.095466, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.034624/  2.086566, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.042368/  2.110610, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.042367/  2.128665, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.036410/  2.109977, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.041459/  2.154525, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.039424/  2.170097, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.036685/  2.170605, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.039139/  2.199093, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.033926/  2.137697, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.033870/  2.220136, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.037168/  2.157977, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.034921/  2.167686, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.032135/  2.240903, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.038357/  2.210830, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.033585/  2.241731, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.033065/  2.237284, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.035400/  2.328768, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.032421/  2.250282, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.030193/  2.308053, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027045/  2.311895, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.034059/  2.295158, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.028591/  2.324856, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.026209/  2.293934, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030616/  2.287906, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5bb833e72a41958033245f355353da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▂▁▆█▅▇█████▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▄▇▆▇▇█▇█▆▇▇█████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇███████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▄▇▆▇▇█▇█▆▇▇█████████████████████████</td></tr><tr><td>val_loss</td><td>▅▂▂▁▂▁▂▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03062</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>2.28791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-sweep-47</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llzgtquc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_171701-llzgtquc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llwcavbj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_172219-llwcavbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj' target=\"_blank\">true-sweep-49</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.101465/  3.324221, val:  40.00%, val_best:  40.00%, tr:  34.42%, tr_best:  34.42%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.458319/  2.152571, val:  50.83%, val_best:  50.83%, tr:  48.42%, tr_best:  48.42%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.574161/  1.896996, val:  51.25%, val_best:  51.25%, tr:  50.97%, tr_best:  50.97%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.102342/  3.321609, val:  51.67%, val_best:  51.67%, tr:  59.24%, tr_best:  59.24%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.209733/  2.727570, val:  54.17%, val_best:  54.17%, tr:  57.71%, tr_best:  59.24%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.101476/  2.293980, val:  61.25%, val_best:  61.25%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.509898/  2.312093, val:  63.33%, val_best:  63.33%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.604437/  2.643199, val:  50.00%, val_best:  63.33%, tr:  64.66%, tr_best:  65.17%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.770214/  2.604050, val:  54.17%, val_best:  63.33%, tr:  64.76%, tr_best:  65.17%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.678787/  2.791501, val:  59.17%, val_best:  63.33%, tr:  62.21%, tr_best:  65.17%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.855636/  1.672830, val:  67.50%, val_best:  67.50%, tr:  68.34%, tr_best:  68.34%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.635617/  3.229590, val:  47.50%, val_best:  67.50%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.713413/  1.890387, val:  63.33%, val_best:  67.50%, tr:  72.83%, tr_best:  73.54%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.339501/  2.658611, val:  56.67%, val_best:  67.50%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.706740/  2.333525, val:  62.92%, val_best:  67.50%, tr:  73.34%, tr_best:  73.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.144755/  1.836629, val:  71.67%, val_best:  71.67%, tr:  81.10%, tr_best:  81.10%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.068119/  1.646398, val:  77.50%, val_best:  77.50%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.081779/  2.243674, val:  67.92%, val_best:  77.50%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.831110/  2.115815, val:  71.67%, val_best:  77.50%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.864666/  1.890798, val:  69.17%, val_best:  77.50%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.849365/  2.572092, val:  65.42%, val_best:  77.50%, tr:  86.21%, tr_best:  87.44%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.093296/  2.282149, val:  69.58%, val_best:  77.50%, tr:  84.17%, tr_best:  87.44%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.587583/  2.038528, val:  77.50%, val_best:  77.50%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.853653/  2.578312, val:  69.58%, val_best:  77.50%, tr:  91.42%, tr_best:  92.95%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.573387/  1.890000, val:  76.67%, val_best:  77.50%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.430167/  1.785288, val:  80.00%, val_best:  80.00%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.393878/  2.101542, val:  72.92%, val_best:  80.00%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.413775/  2.281378, val:  67.92%, val_best:  80.00%, tr:  95.81%, tr_best:  96.22%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.539942/  1.859173, val:  85.42%, val_best:  85.42%, tr:  93.36%, tr_best:  96.22%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.327119/  1.942501, val:  79.58%, val_best:  85.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.359799/  2.495146, val:  79.58%, val_best:  85.42%, tr:  96.83%, tr_best:  97.75%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.310374/  2.156969, val:  77.08%, val_best:  85.42%, tr:  97.65%, tr_best:  97.75%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.366888/  1.950995, val:  77.08%, val_best:  85.42%, tr:  96.42%, tr_best:  97.75%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.382363/  2.537815, val:  80.83%, val_best:  85.42%, tr:  96.53%, tr_best:  97.75%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.359548/  2.135107, val:  79.17%, val_best:  85.42%, tr:  97.34%, tr_best:  97.75%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.249746/  2.006282, val:  81.25%, val_best:  85.42%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.179295/  1.851519, val:  85.00%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.207066/  2.110089, val:  80.00%, val_best:  85.42%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.175540/  2.018707, val:  80.83%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.192950/  1.871792, val:  85.42%, val_best:  85.42%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.160561/  2.060950, val:  79.17%, val_best:  85.42%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.129935/  2.039451, val:  84.17%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.110293/  2.027510, val:  84.17%, val_best:  85.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.100277/  2.039883, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.085017/  1.962710, val:  85.00%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.079050/  1.984434, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.089790/  2.241035, val:  82.08%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.098312/  2.002106, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.095746/  2.120288, val:  83.33%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.070806/  2.018092, val:  85.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.057389/  2.114974, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.048694/  2.076350, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.056105/  2.079623, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.053925/  2.047844, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.045665/  2.093442, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.044584/  2.059523, val:  86.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.048268/  2.273936, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.064674/  2.094952, val:  87.08%, val_best:  87.08%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.050105/  2.244826, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.039918/  2.256194, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.037566/  2.148859, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.045643/  2.148104, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.051063/  2.262684, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.032731/  2.266573, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.027800/  2.317322, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.028965/  2.313595, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.026965/  2.276298, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.032717/  2.337733, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.032430/  2.321970, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.022627/  2.302335, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.017729/  2.370704, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.019072/  2.426221, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.018664/  2.383372, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.017321/  2.322412, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014650/  2.362238, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.012349/  2.364682, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.015869/  2.441809, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.014854/  2.432870, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.023957/  2.515612, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.033188/  2.397029, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.019132/  2.433089, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.015510/  2.403132, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.013415/  2.464687, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.017772/  2.487386, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.016816/  2.447635, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.016685/  2.472792, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.013439/  2.446386, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.011043/  2.484624, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.009350/  2.458170, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.011191/  2.484500, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.009700/  2.508697, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.007340/  2.479566, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.007156/  2.553486, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.007934/  2.462073, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.011370/  2.512015, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.010633/  2.479834, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009567/  2.483004, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.007450/  2.459566, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.006292/  2.454797, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.006837/  2.487900, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759aea3b24a4dc8a032a5cfab4f6fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▆▆▅▅▇▇▇▇▇█▇█▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▂▄▄▄▅▅▅▆▆▇▇▇▇█████▇███████████████▇██</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▅▅▆▇▆▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▇▅█▅▅▄▃▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▄▅▅▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▂▄▄▄▅▅▅▆▆▇▇▇▇█████▇███████████████▇██</td></tr><tr><td>val_loss</td><td>█▁▅▅▅▁▃▃▁▃▁▂▁▁▂▂▁▂▁▂▂▂▂▂▃▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00684</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.4879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-49</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llwcavbj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_172219-llwcavbj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zx4hzndk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_172749-zx4hzndk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk' target=\"_blank\">lemon-sweep-51</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.277348/  1.836137, val:  38.33%, val_best:  38.33%, tr:  11.64%, tr_best:  11.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.530444/  1.553826, val:  56.67%, val_best:  56.67%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.463367/  1.667596, val:  58.75%, val_best:  58.75%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.294098/  1.665490, val:  55.83%, val_best:  58.75%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.244285/  1.353583, val:  60.00%, val_best:  60.00%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.151195/  1.490206, val:  57.50%, val_best:  60.00%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.153702/  1.427458, val:  62.08%, val_best:  62.08%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.209397/  1.390607, val:  64.58%, val_best:  64.58%, tr:  66.70%, tr_best:  68.64%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.088095/  1.211686, val:  70.83%, val_best:  70.83%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.171045/  1.371074, val:  67.50%, val_best:  70.83%, tr:  71.09%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.110695/  1.217043, val:  63.75%, val_best:  70.83%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.935977/  1.408988, val:  65.00%, val_best:  70.83%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.914243/  1.222930, val:  79.17%, val_best:  79.17%, tr:  77.02%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.871244/  1.425935, val:  65.42%, val_best:  79.17%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.922090/  1.598731, val:  69.58%, val_best:  79.17%, tr:  80.18%, tr_best:  82.94%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695977/  1.327556, val:  75.00%, val_best:  79.17%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.701702/  1.301763, val:  81.25%, val_best:  81.25%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.680548/  1.317927, val:  79.58%, val_best:  81.25%, tr:  88.66%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.585737/  1.324111, val:  78.33%, val_best:  81.25%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.554468/  1.228999, val:  82.08%, val_best:  82.08%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.535847/  1.342901, val:  80.42%, val_best:  82.08%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.513860/  1.349173, val:  79.58%, val_best:  82.08%, tr:  94.08%, tr_best:  94.48%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.471891/  1.408997, val:  79.58%, val_best:  82.08%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.497822/  1.387035, val:  82.50%, val_best:  82.50%, tr:  94.59%, tr_best:  95.91%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.468783/  1.394951, val:  80.83%, val_best:  82.50%, tr:  95.61%, tr_best:  95.91%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.403341/  1.387302, val:  81.25%, val_best:  82.50%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.413393/  1.404403, val:  83.33%, val_best:  83.33%, tr:  96.12%, tr_best:  97.34%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.361680/  1.597591, val:  75.42%, val_best:  83.33%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.380964/  1.429426, val:  87.50%, val_best:  87.50%, tr:  97.55%, tr_best:  98.16%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.336191/  1.652869, val:  78.33%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.344023/  1.515484, val:  84.17%, val_best:  87.50%, tr:  98.06%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.307300/  1.519455, val:  86.25%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.393205/  1.703383, val:  77.50%, val_best:  87.50%, tr:  96.02%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.306611/  1.724924, val:  78.33%, val_best:  87.50%, tr:  97.75%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.312883/  1.701099, val:  80.83%, val_best:  87.50%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.270493/  1.616362, val:  85.42%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.258408/  1.578617, val:  88.33%, val_best:  88.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.243536/  1.666261, val:  83.33%, val_best:  88.33%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.257409/  1.555349, val:  88.33%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.241105/  1.588049, val:  87.08%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.214323/  1.602924, val:  87.50%, val_best:  88.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.220465/  1.699192, val:  87.92%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.212917/  1.650550, val:  86.25%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.189541/  1.713008, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.209767/  1.759459, val:  85.42%, val_best:  88.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.166522/  1.698560, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.174053/  1.721082, val:  86.25%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.167443/  1.746748, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.186920/  1.805695, val:  87.08%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.156417/  1.805167, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.142309/  1.833036, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.139528/  1.836177, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.162317/  1.891918, val:  86.67%, val_best:  89.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.128954/  1.912712, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.116131/  1.866073, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.115214/  1.888060, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.117821/  1.913230, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.135084/  1.911174, val:  88.75%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.104586/  1.948376, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.104840/  2.015811, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.092165/  1.970539, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.107049/  2.049332, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.106323/  2.027441, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.089233/  2.024719, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089171/  2.074878, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.079638/  2.135362, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.079328/  2.145368, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.094060/  2.111733, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.079194/  2.134221, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.070059/  2.148082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.072394/  2.139723, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.075009/  2.162890, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.078230/  2.215109, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.071419/  2.255758, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.067724/  2.226867, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.068169/  2.239712, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058137/  2.247678, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.054274/  2.261737, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.056432/  2.325218, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.056006/  2.288713, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.055132/  2.354102, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.057748/  2.332878, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.058765/  2.442618, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.059759/  2.437325, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.055195/  2.384619, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.046222/  2.474452, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.052044/  2.423485, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.052900/  2.462993, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.050090/  2.479652, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.046342/  2.468138, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.043438/  2.495166, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.040929/  2.491071, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.037502/  2.527401, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.035432/  2.525148, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.034233/  2.599615, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.036343/  2.571501, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.040404/  2.559836, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.039971/  2.591554, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.034482/  2.584780, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.038479/  2.592654, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605fc7ffd84f4b3e8ad6ce2fab08f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▂▁▆▇▇██▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>tr_acc</td><td>▁▅▅▅▆▆▆▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>val_loss</td><td>▄▃▂▂▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03848</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.59265</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-51</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zx4hzndk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_172749-zx4hzndk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ukzcg63z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173319-ukzcg63z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z' target=\"_blank\">cosmic-sweep-53</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.319476/  2.168869, val:  15.00%, val_best:  15.00%, tr:   9.40%, tr_best:   9.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.546559/  1.394541, val:  57.50%, val_best:  57.50%, tr:  45.56%, tr_best:  45.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.361095/  1.347619, val:  57.92%, val_best:  57.92%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.168066/  1.364703, val:  56.25%, val_best:  57.92%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.111191/  1.360562, val:  64.58%, val_best:  64.58%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.020129/  1.230537, val:  64.17%, val_best:  64.58%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.009647/  1.213413, val:  65.00%, val_best:  65.00%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.003902/  1.176388, val:  72.08%, val_best:  72.08%, tr:  69.36%, tr_best:  72.11%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.008450/  1.274724, val:  64.58%, val_best:  72.08%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.943574/  1.347100, val:  61.67%, val_best:  72.08%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.939643/  1.174111, val:  72.92%, val_best:  72.92%, tr:  74.46%, tr_best:  76.40%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.807263/  1.173712, val:  76.67%, val_best:  76.67%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.713175/  1.111352, val:  82.08%, val_best:  82.08%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.686933/  1.135550, val:  80.00%, val_best:  82.08%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.701552/  1.443170, val:  71.67%, val_best:  82.08%, tr:  86.93%, tr_best:  87.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.576296/  1.147456, val:  81.67%, val_best:  82.08%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.554653/  1.123438, val:  84.17%, val_best:  84.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.500317/  1.145159, val:  84.58%, val_best:  84.58%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.473771/  1.235066, val:  82.50%, val_best:  84.58%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.453774/  1.129544, val:  83.75%, val_best:  84.58%, tr:  95.20%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.417704/  1.190487, val:  85.83%, val_best:  85.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.391449/  1.228569, val:  82.92%, val_best:  85.83%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.363348/  1.228766, val:  83.75%, val_best:  85.83%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.364034/  1.211714, val:  85.42%, val_best:  85.83%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.336103/  1.263916, val:  83.33%, val_best:  85.83%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.324088/  1.262189, val:  87.50%, val_best:  87.50%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.315723/  1.267444, val:  87.92%, val_best:  87.92%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.290609/  1.388012, val:  84.17%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.282847/  1.273947, val:  87.08%, val_best:  87.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.253795/  1.496196, val:  77.92%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.266297/  1.330569, val:  87.92%, val_best:  87.92%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.235620/  1.388238, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.263707/  1.422997, val:  84.58%, val_best:  87.92%, tr:  98.37%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.245322/  1.483279, val:  85.00%, val_best:  87.92%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.231372/  1.586224, val:  78.75%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.214156/  1.484141, val:  86.67%, val_best:  87.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184852/  1.435867, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.173412/  1.483765, val:  85.42%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.180633/  1.489602, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.176510/  1.547179, val:  88.33%, val_best:  88.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.156040/  1.472144, val:  89.58%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.157954/  1.526698, val:  85.00%, val_best:  89.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.154075/  1.538508, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.132665/  1.573861, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.129036/  1.680215, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.125832/  1.658053, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.125388/  1.662654, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.129947/  1.647644, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.119372/  1.675312, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.116225/  1.678526, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102924/  1.735842, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.096691/  1.729942, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.097066/  1.781765, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.082823/  1.764519, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.087560/  1.788373, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.080971/  1.820014, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.075115/  1.886615, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.085490/  1.877929, val:  89.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.071388/  1.890186, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.068798/  1.936669, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076526/  1.908287, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.079891/  1.899697, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.071702/  1.965082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.068042/  1.891309, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.062879/  1.935400, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.062603/  1.935610, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.058572/  2.021835, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.062719/  2.068077, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.056839/  2.032468, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.059113/  2.075200, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.050592/  2.077132, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.053072/  2.075650, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.045561/  2.104312, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.048279/  2.073460, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.050015/  2.103623, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.041874/  2.110681, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.042547/  2.152984, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.038319/  2.201250, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.040931/  2.212138, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.043858/  2.180299, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.045234/  2.221424, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.038475/  2.217281, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.037684/  2.278193, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.043997/  2.310470, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038367/  2.251507, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.041988/  2.268560, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.038767/  2.270885, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.035475/  2.313851, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.039516/  2.279841, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.035011/  2.255051, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.027546/  2.281515, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.029989/  2.306377, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.024913/  2.329683, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.027511/  2.324182, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.026752/  2.322279, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022845/  2.368027, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.023173/  2.374410, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.024822/  2.412791, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.023658/  2.432441, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.029568/  2.377930, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5213ca7a93b1419eb87b16aab49bcb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▂▇▇▅████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▅▇▆█▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▅▇▆█▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02957</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.37793</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-sweep-53</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ukzcg63z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_173319-ukzcg63z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y0kr0fa9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_173838-y0kr0fa9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9' target=\"_blank\">breezy-sweep-55</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.277348/  1.836137, val:  38.33%, val_best:  38.33%, tr:  11.64%, tr_best:  11.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.530444/  1.553826, val:  56.67%, val_best:  56.67%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.463367/  1.667596, val:  58.75%, val_best:  58.75%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.294098/  1.665490, val:  55.83%, val_best:  58.75%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.244285/  1.353583, val:  60.00%, val_best:  60.00%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.151195/  1.490206, val:  57.50%, val_best:  60.00%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.153702/  1.427458, val:  62.08%, val_best:  62.08%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.209397/  1.390607, val:  64.58%, val_best:  64.58%, tr:  66.70%, tr_best:  68.64%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.088095/  1.211686, val:  70.83%, val_best:  70.83%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.171045/  1.371074, val:  67.50%, val_best:  70.83%, tr:  71.09%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.110695/  1.217043, val:  63.75%, val_best:  70.83%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.935977/  1.408988, val:  65.00%, val_best:  70.83%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.914243/  1.222930, val:  79.17%, val_best:  79.17%, tr:  77.02%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.871244/  1.425935, val:  65.42%, val_best:  79.17%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.922090/  1.598731, val:  69.58%, val_best:  79.17%, tr:  80.18%, tr_best:  82.94%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695977/  1.327556, val:  75.00%, val_best:  79.17%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.701702/  1.301763, val:  81.25%, val_best:  81.25%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.680548/  1.317927, val:  79.58%, val_best:  81.25%, tr:  88.66%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.585737/  1.324111, val:  78.33%, val_best:  81.25%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.554468/  1.228999, val:  82.08%, val_best:  82.08%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.535847/  1.342901, val:  80.42%, val_best:  82.08%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.513860/  1.349173, val:  79.58%, val_best:  82.08%, tr:  94.08%, tr_best:  94.48%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.471891/  1.408997, val:  79.58%, val_best:  82.08%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.497822/  1.387035, val:  82.50%, val_best:  82.50%, tr:  94.59%, tr_best:  95.91%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.468783/  1.394951, val:  80.83%, val_best:  82.50%, tr:  95.61%, tr_best:  95.91%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.403341/  1.387302, val:  81.25%, val_best:  82.50%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.413393/  1.404403, val:  83.33%, val_best:  83.33%, tr:  96.12%, tr_best:  97.34%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.361680/  1.597591, val:  75.42%, val_best:  83.33%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.380964/  1.429426, val:  87.50%, val_best:  87.50%, tr:  97.55%, tr_best:  98.16%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.336191/  1.652869, val:  78.33%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.344023/  1.515484, val:  84.17%, val_best:  87.50%, tr:  98.06%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.307300/  1.519455, val:  86.25%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.393205/  1.703383, val:  77.50%, val_best:  87.50%, tr:  96.02%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.306611/  1.724924, val:  78.33%, val_best:  87.50%, tr:  97.75%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.312883/  1.701099, val:  80.83%, val_best:  87.50%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.270493/  1.616362, val:  85.42%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.258408/  1.578617, val:  88.33%, val_best:  88.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.243536/  1.666261, val:  83.33%, val_best:  88.33%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.257409/  1.555349, val:  88.33%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.241105/  1.588049, val:  87.08%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.214323/  1.602924, val:  87.50%, val_best:  88.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.220465/  1.699192, val:  87.92%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.212917/  1.650550, val:  86.25%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.189541/  1.713008, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.209767/  1.759459, val:  85.42%, val_best:  88.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.166522/  1.698560, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.174053/  1.721082, val:  86.25%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.167443/  1.746748, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.186920/  1.805695, val:  87.08%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.156417/  1.805167, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.142309/  1.833036, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.139528/  1.836177, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.162317/  1.891918, val:  86.67%, val_best:  89.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.128954/  1.912712, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.116131/  1.866073, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.115214/  1.888060, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.117821/  1.913230, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.135084/  1.911174, val:  88.75%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.104586/  1.948376, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.104840/  2.015811, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.092165/  1.970539, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.107049/  2.049332, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.106323/  2.027441, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.089233/  2.024719, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089171/  2.074878, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.079638/  2.135362, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.079328/  2.145368, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.094060/  2.111733, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.079194/  2.134221, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.070059/  2.148082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.072394/  2.139723, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.075009/  2.162890, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.078230/  2.215109, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.071419/  2.255758, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.067724/  2.226867, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.068169/  2.239712, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058137/  2.247678, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.054274/  2.261737, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.056432/  2.325218, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.056006/  2.288713, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.055132/  2.354102, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.057748/  2.332878, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.058765/  2.442618, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.059759/  2.437325, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.055195/  2.384619, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.046222/  2.474452, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.052044/  2.423485, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.052900/  2.462993, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.050090/  2.479652, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.046342/  2.468138, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.043438/  2.495166, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.040929/  2.491071, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.037502/  2.527401, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.035432/  2.525148, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.034233/  2.599615, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.036343/  2.571501, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.040404/  2.559836, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.039971/  2.591554, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.034482/  2.584780, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.038479/  2.592654, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e608b4dab4104a672a2f113302237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▂▁▆▇▇██▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>tr_acc</td><td>▁▅▅▅▆▆▆▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>val_loss</td><td>▄▃▂▂▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03848</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.59265</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-55</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0kr0fa9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_173838-y0kr0fa9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nlowfk69 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_174413-nlowfk69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nlowfk69' target=\"_blank\">polished-sweep-57</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nlowfk69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nlowfk69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.159400/  2.371374, val:  50.00%, val_best:  50.00%, tr:  28.91%, tr_best:  28.91%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.983508/  2.600662, val:  49.17%, val_best:  50.00%, tr:  52.20%, tr_best:  52.20%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.480979/  2.276479, val:  55.00%, val_best:  55.00%, tr:  52.60%, tr_best:  52.60%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.730706/  3.145547, val:  51.67%, val_best:  55.00%, tr:  60.47%, tr_best:  60.47%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.046765/  2.336057, val:  59.58%, val_best:  59.58%, tr:  59.65%, tr_best:  60.47%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.012449/  2.233399, val:  55.00%, val_best:  59.58%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.337782/  2.401479, val:  59.58%, val_best:  59.58%, tr:  69.05%, tr_best:  69.05%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.797901/  3.318091, val:  51.67%, val_best:  59.58%, tr:  64.56%, tr_best:  69.05%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.594118/  2.280526, val:  57.92%, val_best:  59.58%, tr:  68.95%, tr_best:  69.05%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.743725/  2.479433, val:  60.83%, val_best:  60.83%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.746361/  1.770244, val:  67.50%, val_best:  67.50%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.421245/  2.568553, val:  50.42%, val_best:  67.50%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.609935/  2.074598, val:  70.83%, val_best:  70.83%, tr:  73.44%, tr_best:  73.75%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.439618/  3.022609, val:  57.92%, val_best:  70.83%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.437182/  2.759172, val:  66.25%, val_best:  70.83%, tr:  78.35%, tr_best:  78.35%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.187996/  2.175325, val:  69.17%, val_best:  70.83%, tr:  80.29%, tr_best:  80.29%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.129577/  2.246802, val:  77.50%, val_best:  77.50%, tr:  82.84%, tr_best:  82.84%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.125810/  2.538599, val:  66.25%, val_best:  77.50%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.865818/  2.567790, val:  70.83%, val_best:  77.50%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.885109/  2.256886, val:  72.50%, val_best:  77.50%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.019860/  3.147593, val:  65.83%, val_best:  77.50%, tr:  88.56%, tr_best:  89.68%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.214252/  2.955586, val:  70.83%, val_best:  77.50%, tr:  86.93%, tr_best:  89.68%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.754008/  2.366640, val:  80.00%, val_best:  80.00%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.673655/  2.821354, val:  71.25%, val_best:  80.00%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.626128/  2.623749, val:  77.50%, val_best:  80.00%, tr:  94.99%, tr_best:  95.51%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.587585/  2.341368, val:  84.17%, val_best:  84.17%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.561169/  2.853699, val:  72.92%, val_best:  84.17%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.485890/  2.681239, val:  79.58%, val_best:  84.17%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.534545/  2.721205, val:  80.83%, val_best:  84.17%, tr:  96.94%, tr_best:  97.34%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.433531/  3.017112, val:  70.42%, val_best:  84.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.419944/  2.687369, val:  80.00%, val_best:  84.17%, tr:  98.16%, tr_best:  98.47%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.394769/  2.902210, val:  79.58%, val_best:  84.17%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.536452/  2.977929, val:  77.08%, val_best:  84.17%, tr:  96.12%, tr_best:  99.18%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.468538/  3.298048, val:  77.08%, val_best:  84.17%, tr:  97.75%, tr_best:  99.18%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.424709/  3.205304, val:  77.50%, val_best:  84.17%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.328792/  2.968733, val:  84.17%, val_best:  84.17%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.282172/  2.941897, val:  83.75%, val_best:  84.17%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.293042/  3.338488, val:  76.25%, val_best:  84.17%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.373760/  3.083848, val:  82.50%, val_best:  84.17%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.380334/  3.091806, val:  82.08%, val_best:  84.17%, tr:  97.75%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.282171/  3.079080, val:  85.00%, val_best:  85.00%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.273584/  3.258779, val:  82.92%, val_best:  85.00%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.238486/  3.267177, val:  83.75%, val_best:  85.00%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.232465/  3.491629, val:  79.17%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.225414/  3.422829, val:  80.42%, val_best:  85.00%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.202620/  3.430459, val:  82.50%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.201001/  3.478976, val:  82.92%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.172071/  3.433912, val:  80.83%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.213620/  3.512249, val:  84.17%, val_best:  85.00%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.169677/  3.366937, val:  83.33%, val_best:  85.00%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.144114/  3.507602, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.149666/  3.561984, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.148872/  3.489289, val:  83.75%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.145578/  3.613032, val:  81.67%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.154138/  3.575318, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.139334/  3.716099, val:  82.92%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.134739/  3.930926, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.166080/  3.750134, val:  83.33%, val_best:  85.42%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.131890/  3.720356, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.108217/  3.790207, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.106177/  3.941693, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.115464/  3.905829, val:  83.33%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.095730/  4.026973, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.127834/  4.015008, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.084163/  3.987362, val:  85.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.073728/  3.999580, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.081621/  3.923455, val:  82.50%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.073096/  4.120152, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.069877/  4.176241, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.065584/  4.130261, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.068314/  4.102582, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.066333/  4.142295, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.070354/  4.200502, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.067195/  4.232339, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.075638/  4.190978, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.052601/  4.240370, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.056173/  4.387027, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.055092/  4.307129, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.063055/  4.407441, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.046304/  4.327637, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.056017/  4.369382, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.056597/  4.355477, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.048568/  4.455296, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.047736/  4.461704, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.053608/  4.443247, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.042339/  4.498124, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.039556/  4.516511, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.053417/  4.517637, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.055324/  4.670304, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.048260/  4.498509, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.057777/  4.639334, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.043783/  4.579358, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.038196/  4.570997, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.031733/  4.596669, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.035616/  4.641059, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.033539/  4.813901, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.041982/  4.721911, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.040522/  4.774065, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033524/  4.775250, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.032960/  4.701923, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240a4676f1e042e7902ba96469f86528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▅▃▆▇▇▆█▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▁▃▅▄▄▅▅▆▅▅▆█▆▇█▇▇▇██▇▇▇█▇█▇██████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▆▆▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▇▆▆▆▅▄▃▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▃▅▅▆▆▆▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▁▃▅▄▄▅▅▆▅▅▆█▆▇█▇▇▇██▇▇▇█▇█▇██████████</td></tr><tr><td>val_loss</td><td>▂▂▂▄▂▁▃▂▁▃▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03296</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>4.70192</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-sweep-57</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nlowfk69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nlowfk69</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_174413-nlowfk69/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 65ccjeqi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_175051-65ccjeqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/65ccjeqi' target=\"_blank\">wandering-sweep-59</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/65ccjeqi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/65ccjeqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.277348/  1.836137, val:  38.33%, val_best:  38.33%, tr:  11.64%, tr_best:  11.64%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.530444/  1.553826, val:  56.67%, val_best:  56.67%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.463367/  1.667596, val:  58.75%, val_best:  58.75%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.294098/  1.665490, val:  55.83%, val_best:  58.75%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.244285/  1.353583, val:  60.00%, val_best:  60.00%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.151195/  1.490206, val:  57.50%, val_best:  60.00%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.153702/  1.427458, val:  62.08%, val_best:  62.08%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.209397/  1.390607, val:  64.58%, val_best:  64.58%, tr:  66.70%, tr_best:  68.64%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.088095/  1.211686, val:  70.83%, val_best:  70.83%, tr:  71.81%, tr_best:  71.81%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.171045/  1.371074, val:  67.50%, val_best:  70.83%, tr:  71.09%, tr_best:  71.81%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.110695/  1.217043, val:  63.75%, val_best:  70.83%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.935977/  1.408988, val:  65.00%, val_best:  70.83%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.914243/  1.222930, val:  79.17%, val_best:  79.17%, tr:  77.02%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.871244/  1.425935, val:  65.42%, val_best:  79.17%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.922090/  1.598731, val:  69.58%, val_best:  79.17%, tr:  80.18%, tr_best:  82.94%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.695977/  1.327556, val:  75.00%, val_best:  79.17%, tr:  87.23%, tr_best:  87.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.701702/  1.301763, val:  81.25%, val_best:  81.25%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.680548/  1.317927, val:  79.58%, val_best:  81.25%, tr:  88.66%, tr_best:  88.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.585737/  1.324111, val:  78.33%, val_best:  81.25%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.554468/  1.228999, val:  82.08%, val_best:  82.08%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.535847/  1.342901, val:  80.42%, val_best:  82.08%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.513860/  1.349173, val:  79.58%, val_best:  82.08%, tr:  94.08%, tr_best:  94.48%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.471891/  1.408997, val:  79.58%, val_best:  82.08%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.497822/  1.387035, val:  82.50%, val_best:  82.50%, tr:  94.59%, tr_best:  95.91%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.468783/  1.394951, val:  80.83%, val_best:  82.50%, tr:  95.61%, tr_best:  95.91%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.403341/  1.387302, val:  81.25%, val_best:  82.50%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.413393/  1.404403, val:  83.33%, val_best:  83.33%, tr:  96.12%, tr_best:  97.34%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.361680/  1.597591, val:  75.42%, val_best:  83.33%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.380964/  1.429426, val:  87.50%, val_best:  87.50%, tr:  97.55%, tr_best:  98.16%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.336191/  1.652869, val:  78.33%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.344023/  1.515484, val:  84.17%, val_best:  87.50%, tr:  98.06%, tr_best:  98.98%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.307300/  1.519455, val:  86.25%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.393205/  1.703383, val:  77.50%, val_best:  87.50%, tr:  96.02%, tr_best:  98.98%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.306611/  1.724924, val:  78.33%, val_best:  87.50%, tr:  97.75%, tr_best:  98.98%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.312883/  1.701099, val:  80.83%, val_best:  87.50%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.270493/  1.616362, val:  85.42%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.258408/  1.578617, val:  88.33%, val_best:  88.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.243536/  1.666261, val:  83.33%, val_best:  88.33%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.257409/  1.555349, val:  88.33%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.241105/  1.588049, val:  87.08%, val_best:  88.33%, tr:  99.08%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.214323/  1.602924, val:  87.50%, val_best:  88.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.220465/  1.699192, val:  87.92%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.212917/  1.650550, val:  86.25%, val_best:  88.33%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.189541/  1.713008, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.209767/  1.759459, val:  85.42%, val_best:  88.33%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.166522/  1.698560, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.174053/  1.721082, val:  86.25%, val_best:  89.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.167443/  1.746748, val:  88.75%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.186920/  1.805695, val:  87.08%, val_best:  89.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.156417/  1.805167, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.142309/  1.833036, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.139528/  1.836177, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.162317/  1.891918, val:  86.67%, val_best:  89.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.128954/  1.912712, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.116131/  1.866073, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.115214/  1.888060, val:  87.50%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.117821/  1.913230, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.135084/  1.911174, val:  88.75%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.104586/  1.948376, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.104840/  2.015811, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.092165/  1.970539, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.107049/  2.049332, val:  85.83%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.106323/  2.027441, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.089233/  2.024719, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089171/  2.074878, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.079638/  2.135362, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.079328/  2.145368, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.094060/  2.111733, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.079194/  2.134221, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.070059/  2.148082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.072394/  2.139723, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.075009/  2.162890, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.078230/  2.215109, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.071419/  2.255758, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.067724/  2.226867, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.068169/  2.239712, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058137/  2.247678, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.054274/  2.261737, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.056432/  2.325218, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.056006/  2.288713, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.055132/  2.354102, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.057748/  2.332878, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.058765/  2.442618, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.059759/  2.437325, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.055195/  2.384619, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.046222/  2.474452, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.052044/  2.423485, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.052900/  2.462993, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.050090/  2.479652, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.046342/  2.468138, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.043438/  2.495166, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.040929/  2.491071, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.037502/  2.527401, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.035432/  2.525148, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.034233/  2.599615, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.036343/  2.571501, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.040404/  2.559836, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.039971/  2.591554, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.034482/  2.584780, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.038479/  2.592654, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb7361fb052471483397bef778d74b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▂▁▆▇▇██▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>tr_acc</td><td>▁▅▅▅▆▆▆▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▄▅▅▇▅▇▇▇▇▇▆▆▇▇█▇▇██████▇████████████▇█</td></tr><tr><td>val_loss</td><td>▄▃▂▂▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03848</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.59265</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-sweep-59</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/65ccjeqi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/65ccjeqi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_175051-65ccjeqi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9rkk2rh3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_175636-9rkk2rh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9rkk2rh3' target=\"_blank\">silver-sweep-61</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9rkk2rh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9rkk2rh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.306479/  1.978804, val:  26.67%, val_best:  26.67%, tr:  10.42%, tr_best:  10.42%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.546456/  1.492151, val:  55.83%, val_best:  55.83%, tr:  48.52%, tr_best:  48.52%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.455957/  1.559261, val:  58.75%, val_best:  58.75%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.263864/  1.596427, val:  56.67%, val_best:  58.75%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.195132/  1.363155, val:  62.92%, val_best:  62.92%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.087733/  1.421423, val:  60.00%, val_best:  62.92%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.103827/  1.318875, val:  62.92%, val_best:  62.92%, tr:  68.64%, tr_best:  69.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.140004/  1.326241, val:  65.42%, val_best:  65.42%, tr:  68.74%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.075583/  1.454021, val:  69.17%, val_best:  69.17%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.101912/  1.342581, val:  66.25%, val_best:  69.17%, tr:  75.08%, tr_best:  75.08%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.091535/  1.255538, val:  68.75%, val_best:  69.17%, tr:  73.85%, tr_best:  75.08%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.901280/  1.342331, val:  68.75%, val_best:  69.17%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.833625/  1.249760, val:  80.00%, val_best:  80.00%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.801552/  1.348360, val:  70.83%, val_best:  80.00%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.823863/  1.505367, val:  74.17%, val_best:  80.00%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.658266/  1.411291, val:  77.50%, val_best:  80.00%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.673231/  1.365630, val:  79.17%, val_best:  80.00%, tr:  89.17%, tr_best:  89.89%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.594394/  1.362328, val:  82.50%, val_best:  82.50%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.549070/  1.467187, val:  79.17%, val_best:  82.50%, tr:  93.87%, tr_best:  94.89%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.525612/  1.392796, val:  82.50%, val_best:  82.50%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.491467/  1.513932, val:  81.25%, val_best:  82.50%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.446569/  1.529853, val:  77.08%, val_best:  82.50%, tr:  96.42%, tr_best:  96.94%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.436940/  1.481583, val:  83.33%, val_best:  83.33%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.420155/  1.548607, val:  87.08%, val_best:  87.08%, tr:  96.94%, tr_best:  97.34%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.381592/  1.611731, val:  82.92%, val_best:  87.08%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.411919/  1.603865, val:  82.50%, val_best:  87.08%, tr:  97.65%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.388225/  1.599371, val:  87.50%, val_best:  87.50%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.340097/  1.770846, val:  82.08%, val_best:  87.50%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.328348/  1.653015, val:  85.83%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.303842/  1.846102, val:  80.00%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.314113/  1.737318, val:  83.33%, val_best:  87.50%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.292898/  1.793271, val:  84.17%, val_best:  87.50%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.306903/  1.922115, val:  83.33%, val_best:  87.50%, tr:  98.67%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.285894/  1.939197, val:  84.17%, val_best:  87.50%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.275821/  2.016003, val:  82.50%, val_best:  87.50%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.238944/  1.946912, val:  85.42%, val_best:  87.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.224114/  2.024787, val:  82.50%, val_best:  87.50%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.219101/  2.033825, val:  84.58%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.213482/  2.034233, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.215539/  2.057276, val:  84.17%, val_best:  87.50%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.192978/  2.031908, val:  86.67%, val_best:  87.50%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.171497/  2.123474, val:  87.08%, val_best:  87.50%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.181568/  2.156923, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.172227/  2.186422, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.170649/  2.231337, val:  84.17%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.154309/  2.184586, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.144361/  2.305473, val:  83.75%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.144098/  2.194733, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.154926/  2.215383, val:  87.50%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.134494/  2.316416, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.124544/  2.344104, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.134537/  2.350584, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.126179/  2.389639, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.115434/  2.437605, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.106151/  2.390757, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.098700/  2.435629, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.103632/  2.527589, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.106306/  2.477939, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.109899/  2.521224, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.097034/  2.592089, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.091460/  2.631016, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.089422/  2.639864, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.084849/  2.707108, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.086167/  2.648879, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.079147/  2.661229, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.077634/  2.742643, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.082425/  2.787807, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.069761/  2.863002, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.072936/  2.804834, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.066306/  2.860699, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.065371/  2.867242, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.063827/  2.904373, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.058132/  2.981011, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.058314/  2.973475, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.062623/  2.937371, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.055129/  2.916863, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058102/  3.086373, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.055161/  3.054375, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.051371/  3.078571, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.062172/  3.129778, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.058441/  3.151033, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.054117/  3.143978, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.046242/  3.116561, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.042750/  3.218899, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.047966/  3.224355, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.037894/  3.285614, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.037193/  3.284096, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.031701/  3.272225, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035544/  3.255426, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.042131/  3.231530, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.037243/  3.269896, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.033733/  3.305058, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.040364/  3.323627, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.034152/  3.288328, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031402/  3.360850, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.039982/  3.391302, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.032554/  3.356653, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.035032/  3.378537, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033013/  3.383974, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030318/  3.427136, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d221bc58ac6f43fca4e637b3f7bee71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▂▂▆▇▅██▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▅▅▇▆▇▇▇▇█▇▇█▇▇█▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▅▅▇▆▇▇▇▇█▇▇█▇▇█▇█████████████████████</td></tr><tr><td>val_loss</td><td>▃▂▁▁▁▁▂▁▁▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▆▇▇▇▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03032</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>3.42714</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-61</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9rkk2rh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9rkk2rh3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_175636-9rkk2rh3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4bt9e463 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_180316-4bt9e463</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4bt9e463' target=\"_blank\">dashing-sweep-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4bt9e463' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4bt9e463</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.131179/  2.921196, val:  41.67%, val_best:  41.67%, tr:  33.40%, tr_best:  33.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.318407/  1.927050, val:  45.42%, val_best:  45.42%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.106323/  1.833180, val:  49.58%, val_best:  49.58%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.961999/  2.978591, val:  49.17%, val_best:  49.58%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.342806/  2.593457, val:  55.42%, val_best:  55.42%, tr:  56.38%, tr_best:  57.10%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.894290/  2.684509, val:  55.83%, val_best:  55.83%, tr:  61.90%, tr_best:  61.90%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.389243/  1.879352, val:  57.92%, val_best:  57.92%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.347949/  2.464292, val:  51.25%, val_best:  57.92%, tr:  65.78%, tr_best:  66.29%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.333530/  2.797017, val:  52.92%, val_best:  57.92%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.618233/  1.961606, val:  61.67%, val_best:  61.67%, tr:  61.08%, tr_best:  68.74%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.685099/  2.059097, val:  63.33%, val_best:  63.33%, tr:  67.62%, tr_best:  68.74%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.639458/  3.562621, val:  47.08%, val_best:  63.33%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.814824/  1.640324, val:  63.75%, val_best:  63.75%, tr:  70.07%, tr_best:  71.60%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.480045/  3.221801, val:  50.00%, val_best:  63.75%, tr:  72.83%, tr_best:  72.83%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.535369/  2.494406, val:  65.83%, val_best:  65.83%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.924040/  2.021667, val:  63.33%, val_best:  65.83%, tr:  83.76%, tr_best:  83.76%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.257139/  1.765640, val:  70.42%, val_best:  70.42%, tr:  78.14%, tr_best:  83.76%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.052523/  2.597091, val:  66.25%, val_best:  70.42%, tr:  83.55%, tr_best:  83.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.822512/  2.269394, val:  73.75%, val_best:  73.75%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.868395/  2.157351, val:  64.58%, val_best:  73.75%, tr:  85.90%, tr_best:  88.56%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.994031/  2.200928, val:  66.25%, val_best:  73.75%, tr:  85.09%, tr_best:  88.56%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.863896/  1.830917, val:  75.83%, val_best:  75.83%, tr:  86.62%, tr_best:  88.56%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.511757/  1.983851, val:  75.83%, val_best:  75.83%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.851553/  2.290055, val:  71.67%, val_best:  75.83%, tr:  91.93%, tr_best:  94.89%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.533068/  1.857795, val:  74.58%, val_best:  75.83%, tr:  94.28%, tr_best:  94.89%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.505388/  1.611989, val:  82.92%, val_best:  82.92%, tr:  94.08%, tr_best:  94.89%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.394062/  2.008663, val:  70.42%, val_best:  82.92%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.407510/  2.084736, val:  72.50%, val_best:  82.92%, tr:  95.71%, tr_best:  96.02%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.484507/  1.890536, val:  80.42%, val_best:  82.92%, tr:  94.89%, tr_best:  96.02%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.292323/  1.865030, val:  77.92%, val_best:  82.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.320256/  1.795187, val:  81.67%, val_best:  82.92%, tr:  97.45%, tr_best:  98.37%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.267348/  2.056541, val:  75.00%, val_best:  82.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.351805/  2.223072, val:  73.33%, val_best:  82.92%, tr:  96.83%, tr_best:  98.57%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.352732/  2.307595, val:  78.33%, val_best:  82.92%, tr:  96.73%, tr_best:  98.57%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.356049/  1.995265, val:  80.42%, val_best:  82.92%, tr:  96.94%, tr_best:  98.57%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.192498/  1.930076, val:  79.58%, val_best:  82.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171801/  1.988819, val:  80.00%, val_best:  82.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.275631/  2.293711, val:  78.75%, val_best:  82.92%, tr:  97.04%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.216628/  2.426083, val:  70.83%, val_best:  82.92%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.234943/  1.901932, val:  85.83%, val_best:  85.83%, tr:  98.26%, tr_best:  99.49%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.163406/  1.959651, val:  86.25%, val_best:  86.25%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.125801/  2.058132, val:  85.42%, val_best:  86.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.113446/  2.005160, val:  84.58%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.091974/  2.204061, val:  80.83%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.101761/  2.143456, val:  82.08%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.075086/  2.069565, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.059707/  2.118727, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.072908/  2.100178, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.084498/  2.165840, val:  82.92%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.064261/  2.127722, val:  82.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.052265/  2.212857, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.077040/  2.248733, val:  85.00%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.062247/  2.227979, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.054933/  2.196485, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.044064/  2.168809, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.045081/  2.271559, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.046817/  2.350136, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.055768/  2.236046, val:  86.67%, val_best:  87.50%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.038805/  2.235974, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.030169/  2.267781, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.032962/  2.256704, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.038586/  2.302046, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.027350/  2.362448, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.047580/  2.433083, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.063495/  2.281705, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.030124/  2.322313, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.022858/  2.380931, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.026205/  2.358196, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.033539/  2.447375, val:  85.83%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.022422/  2.402941, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.018044/  2.341295, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.016203/  2.362293, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.018469/  2.372191, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.022359/  2.511612, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.024595/  2.399031, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.024549/  2.494772, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.014553/  2.481201, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.015386/  2.472580, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.039150/  2.490783, val:  85.00%, val_best:  89.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.105598/  2.510763, val:  85.42%, val_best:  89.58%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.023687/  2.526340, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.015228/  2.511270, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.010179/  2.516142, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.014063/  2.543627, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.016202/  2.551429, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010658/  2.579327, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.009476/  2.535892, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.011438/  2.548270, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.008906/  2.521182, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.011355/  2.521416, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.010099/  2.551033, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.009502/  2.529438, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.008327/  2.583067, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.007001/  2.572896, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.007973/  2.664171, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.007041/  2.636531, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.007327/  2.579216, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.006922/  2.620018, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.007313/  2.608478, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009589/  2.578333, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a9632699ff490985f70e6649e8a25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▃▅▂▄▇▇▅▇▇▇██▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▂▄▄▅▅▅▆▆▅▇▆▇▇██▇▇▇▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▅▆▆▇▇▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇▇▇▅█▆▅▄▃▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▂▄▄▅▅▅▆▆▅▇▆▇▇██▇▇▇▇██████████████████</td></tr><tr><td>val_loss</td><td>█▂▆▆▃▁▆▆▄▂▂▃▂▄▃▅▂▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00959</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>2.57833</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-64</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4bt9e463' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4bt9e463</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_180316-4bt9e463/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: myyzscid with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_180840-myyzscid</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/myyzscid' target=\"_blank\">amber-sweep-65</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/myyzscid' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/myyzscid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.025563/  1.552874, val:  47.50%, val_best:  47.50%, tr:  24.41%, tr_best:  24.41%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.410157/  1.577203, val:  55.83%, val_best:  55.83%, tr:  51.48%, tr_best:  51.48%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.394499/  1.514237, val:  58.75%, val_best:  58.75%, tr:  57.51%, tr_best:  57.51%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.137119/  1.415409, val:  56.67%, val_best:  58.75%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.069973/  1.229668, val:  62.50%, val_best:  62.50%, tr:  62.61%, tr_best:  63.84%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.006922/  1.474578, val:  55.83%, val_best:  62.50%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.959935/  1.206344, val:  64.17%, val_best:  64.17%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.000130/  1.276868, val:  65.42%, val_best:  65.42%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.058437/  1.229683, val:  66.67%, val_best:  66.67%, tr:  70.07%, tr_best:  70.28%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.914577/  1.494915, val:  58.33%, val_best:  66.67%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.902118/  1.189797, val:  65.00%, val_best:  66.67%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.880366/  1.241336, val:  67.92%, val_best:  67.92%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.853295/  1.170504, val:  65.00%, val_best:  67.92%, tr:  74.57%, tr_best:  75.28%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.735985/  1.259224, val:  64.58%, val_best:  67.92%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.794319/  1.427656, val:  61.67%, val_best:  67.92%, tr:  78.65%, tr_best:  79.06%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.613999/  1.385459, val:  66.25%, val_best:  67.92%, tr:  82.84%, tr_best:  82.84%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.679582/  1.282013, val:  67.08%, val_best:  67.92%, tr:  81.92%, tr_best:  82.84%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.613762/  1.294380, val:  65.42%, val_best:  67.92%, tr:  85.09%, tr_best:  85.09%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.503727/  1.433797, val:  65.42%, val_best:  67.92%, tr:  87.84%, tr_best:  87.84%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.482773/  1.381487, val:  70.83%, val_best:  70.83%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.663067/  1.209757, val:  72.92%, val_best:  72.92%, tr:  86.01%, tr_best:  90.60%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.482779/  1.933835, val:  63.33%, val_best:  72.92%, tr:  89.38%, tr_best:  90.60%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.438839/  1.383077, val:  73.33%, val_best:  73.33%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.401300/  1.428786, val:  68.33%, val_best:  73.33%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.346244/  1.415373, val:  70.00%, val_best:  73.33%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.332422/  1.634405, val:  67.92%, val_best:  73.33%, tr:  94.59%, tr_best:  95.40%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.379434/  1.421505, val:  73.33%, val_best:  73.33%, tr:  92.95%, tr_best:  95.40%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.288393/  1.517938, val:  73.33%, val_best:  73.33%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.316836/  1.409687, val:  75.83%, val_best:  75.83%, tr:  96.12%, tr_best:  97.96%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.250641/  1.508922, val:  73.33%, val_best:  75.83%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.224619/  1.500728, val:  71.67%, val_best:  75.83%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.209062/  1.498457, val:  76.25%, val_best:  76.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.271924/  1.733596, val:  67.50%, val_best:  76.25%, tr:  96.63%, tr_best:  99.08%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.207644/  1.633837, val:  73.75%, val_best:  76.25%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.203840/  1.686921, val:  76.25%, val_best:  76.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.173745/  1.710349, val:  75.42%, val_best:  76.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.176059/  1.602038, val:  77.50%, val_best:  77.50%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.147689/  1.727896, val:  71.67%, val_best:  77.50%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.164358/  1.699645, val:  76.67%, val_best:  77.50%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.144292/  1.749793, val:  76.25%, val_best:  77.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.142491/  1.833972, val:  70.83%, val_best:  77.50%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.167253/  1.715828, val:  77.92%, val_best:  77.92%, tr:  98.67%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.102245/  1.776766, val:  75.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.093923/  1.887533, val:  73.75%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.104426/  1.818184, val:  77.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.086821/  1.904115, val:  76.25%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.076530/  1.837421, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.069432/  1.853680, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.061673/  1.877988, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.061501/  1.915408, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.050801/  1.933461, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.049911/  1.962992, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.051269/  2.013447, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.049609/  2.077691, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.053824/  2.047132, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.044869/  2.053073, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.043689/  2.119328, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.042148/  2.123049, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.041582/  2.112582, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.037382/  2.105213, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.034652/  2.168527, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.038774/  2.133924, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.034355/  2.180401, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.030658/  2.200004, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.029960/  2.177619, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.029298/  2.196415, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.027312/  2.235126, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.029121/  2.233534, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.026096/  2.307343, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.028837/  2.284811, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.026123/  2.292153, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.028373/  2.294954, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.027460/  2.304991, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.029637/  2.325419, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.022810/  2.284303, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.023821/  2.357055, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.023083/  2.363163, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.023837/  2.344180, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.020195/  2.400353, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.021702/  2.391578, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.016303/  2.417750, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.021058/  2.410024, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.018767/  2.401389, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.018344/  2.455542, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.016623/  2.437351, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.016868/  2.457016, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.017196/  2.461045, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.019213/  2.489827, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.016202/  2.469150, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.017488/  2.492587, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.013253/  2.469004, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.013562/  2.507911, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.012353/  2.512767, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.014325/  2.505629, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.014300/  2.588787, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.014666/  2.551506, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.012856/  2.544763, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.016876/  2.549519, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.016815/  2.621163, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.014526/  2.587561, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af842d4c4334c2da764fd9e5a0e607d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▃▁▆▇▆▅█▇██▆▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▅▃▅▄▅▆▄▆▇▇▅▇▆▇▇██▇█▇████▇█▇█████▇██▇█</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▆▆▆▇▇▇█▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▅▃▅▄▅▆▄▆▇▇▅▇▆▇▇██▇█▇████▇█▇█████▇██▇█</td></tr><tr><td>val_loss</td><td>▃▃▁▂▃▁▂▂▂▅▂▂▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01453</td></tr><tr><td>val_acc_best</td><td>0.80417</td></tr><tr><td>val_acc_now</td><td>0.77083</td></tr><tr><td>val_loss</td><td>2.58756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-65</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/myyzscid' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/myyzscid</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_180840-myyzscid/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a4hz2y2s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_181348-a4hz2y2s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a4hz2y2s' target=\"_blank\">atomic-sweep-67</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a4hz2y2s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a4hz2y2s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.366790/  2.004554, val:  31.67%, val_best:  31.67%, tr:  16.45%, tr_best:  16.45%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.283163/  1.939020, val:  33.75%, val_best:  33.75%, tr:  36.98%, tr_best:  36.98%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.011997/  2.203367, val:  39.17%, val_best:  39.17%, tr:  36.16%, tr_best:  36.98%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  1.862645/  1.859085, val:  45.42%, val_best:  45.42%, tr:  46.07%, tr_best:  46.07%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.055645/  2.499836, val:  49.17%, val_best:  49.17%, tr:  47.29%, tr_best:  47.29%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  1.891371/  2.076506, val:  40.00%, val_best:  49.17%, tr:  51.48%, tr_best:  51.48%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  1.964257/  2.187281, val:  43.33%, val_best:  49.17%, tr:  47.80%, tr_best:  51.48%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  1.875565/  3.028201, val:  43.75%, val_best:  49.17%, tr:  47.19%, tr_best:  51.48%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  1.818454/  1.992007, val:  52.50%, val_best:  52.50%, tr:  53.32%, tr_best:  53.32%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  2.120152/  2.034386, val:  58.33%, val_best:  58.33%, tr:  53.63%, tr_best:  53.63%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  2.110407/  2.242183, val:  49.58%, val_best:  58.33%, tr:  52.91%, tr_best:  53.63%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  1.966093/  2.936349, val:  43.33%, val_best:  58.33%, tr:  54.14%, tr_best:  54.14%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  2.066043/  2.307370, val:  47.08%, val_best:  58.33%, tr:  52.81%, tr_best:  54.14%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  2.218701/  3.469272, val:  44.17%, val_best:  58.33%, tr:  55.16%, tr_best:  55.16%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  2.136788/  3.176374, val:  52.08%, val_best:  58.33%, tr:  56.28%, tr_best:  56.28%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  2.193726/  2.650786, val:  47.92%, val_best:  58.33%, tr:  56.18%, tr_best:  56.28%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.180467/  2.831278, val:  39.58%, val_best:  58.33%, tr:  57.00%, tr_best:  57.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.413234/  2.765733, val:  50.00%, val_best:  58.33%, tr:  50.56%, tr_best:  57.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.028584/  1.864250, val:  60.00%, val_best:  60.00%, tr:  58.53%, tr_best:  58.53%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  1.868615/  2.854358, val:  46.25%, val_best:  60.00%, tr:  56.18%, tr_best:  58.53%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.487830/  2.420504, val:  50.00%, val_best:  60.00%, tr:  55.77%, tr_best:  58.53%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.214016/  2.663536, val:  55.83%, val_best:  60.00%, tr:  55.87%, tr_best:  58.53%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.481823/  2.517192, val:  53.33%, val_best:  60.00%, tr:  53.93%, tr_best:  58.53%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.129052/  2.725055, val:  55.00%, val_best:  60.00%, tr:  60.88%, tr_best:  60.88%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.905886/  2.366242, val:  54.17%, val_best:  60.00%, tr:  60.47%, tr_best:  60.88%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.320832/  3.457792, val:  50.83%, val_best:  60.00%, tr:  58.53%, tr_best:  60.88%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.295266/  2.808761, val:  56.25%, val_best:  60.00%, tr:  61.08%, tr_best:  61.08%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  1.931107/  2.675203, val:  47.50%, val_best:  60.00%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.518494/  3.590783, val:  47.92%, val_best:  60.00%, tr:  59.86%, tr_best:  62.10%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.092835/  3.150127, val:  53.33%, val_best:  60.00%, tr:  61.80%, tr_best:  62.10%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.168849/  2.234502, val:  52.92%, val_best:  60.00%, tr:  60.57%, tr_best:  62.10%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.749424/  2.800732, val:  50.42%, val_best:  60.00%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.210280/  4.697171, val:  42.92%, val_best:  60.00%, tr:  60.98%, tr_best:  63.02%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.399149/  2.080772, val:  57.92%, val_best:  60.00%, tr:  58.02%, tr_best:  63.02%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  3.681708/  4.763179, val:  45.83%, val_best:  60.00%, tr:  53.73%, tr_best:  63.02%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.487013/  2.693817, val:  58.33%, val_best:  60.00%, tr:  58.12%, tr_best:  63.02%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.055224/  4.239337, val:  54.58%, val_best:  60.00%, tr:  62.10%, tr_best:  63.02%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  4.288781/  4.895588, val:  42.92%, val_best:  60.00%, tr:  58.53%, tr_best:  63.02%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.809151/  2.404169, val:  57.08%, val_best:  60.00%, tr:  57.61%, tr_best:  63.02%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.283608/  2.492659, val:  55.83%, val_best:  60.00%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.741916/  4.227099, val:  47.50%, val_best:  60.00%, tr:  65.88%, tr_best:  65.88%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.904065/  3.039721, val:  55.00%, val_best:  60.00%, tr:  63.94%, tr_best:  65.88%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  2.135597/  3.502201, val:  46.67%, val_best:  60.00%, tr:  62.21%, tr_best:  65.88%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.218869/  3.307412, val:  56.67%, val_best:  60.00%, tr:  63.02%, tr_best:  65.88%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.128802/  2.978942, val:  47.92%, val_best:  60.00%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.129266/  3.660013, val:  51.67%, val_best:  60.00%, tr:  66.70%, tr_best:  67.21%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.135241/  3.264362, val:  56.67%, val_best:  60.00%, tr:  64.66%, tr_best:  67.21%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  1.870432/  4.283775, val:  42.50%, val_best:  60.00%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  3.018870/  3.599102, val:  42.08%, val_best:  60.00%, tr:  63.33%, tr_best:  68.23%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.840741/  2.745701, val:  54.58%, val_best:  60.00%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.584290/  3.972965, val:  45.42%, val_best:  60.00%, tr:  68.54%, tr_best:  69.77%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.362342/  3.536234, val:  44.58%, val_best:  60.00%, tr:  63.84%, tr_best:  69.77%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.986319/  3.015076, val:  51.67%, val_best:  60.00%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.999081/  4.259120, val:  49.17%, val_best:  60.00%, tr:  66.91%, tr_best:  70.38%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.254115/  4.759608, val:  43.33%, val_best:  60.00%, tr:  68.64%, tr_best:  70.38%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.135601/  5.085141, val:  40.83%, val_best:  60.00%, tr:  64.76%, tr_best:  70.38%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.566765/  3.447516, val:  60.00%, val_best:  60.00%, tr:  64.86%, tr_best:  70.38%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.886208/  3.766874, val:  50.83%, val_best:  60.00%, tr:  65.47%, tr_best:  70.38%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.817239/  2.988883, val:  58.33%, val_best:  60.00%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.645055/  3.872822, val:  43.75%, val_best:  60.00%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.692788/  2.322985, val:  56.25%, val_best:  60.00%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.606310/  2.702275, val:  57.08%, val_best:  60.00%, tr:  70.28%, tr_best:  73.24%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  1.710645/  3.716182, val:  42.50%, val_best:  60.00%, tr:  68.74%, tr_best:  73.24%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  2.117108/  2.654475, val:  60.00%, val_best:  60.00%, tr:  67.62%, tr_best:  73.24%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  2.799963/  3.623913, val:  59.58%, val_best:  60.00%, tr:  61.08%, tr_best:  73.24%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  2.614255/  4.302370, val:  49.58%, val_best:  60.00%, tr:  65.27%, tr_best:  73.24%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.311821/  3.754683, val:  47.92%, val_best:  60.00%, tr:  69.97%, tr_best:  73.24%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  2.554913/  4.586428, val:  50.00%, val_best:  60.00%, tr:  65.37%, tr_best:  73.24%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  2.364616/  3.544029, val:  55.00%, val_best:  60.00%, tr:  69.77%, tr_best:  73.24%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  2.128263/  3.342175, val:  58.75%, val_best:  60.00%, tr:  67.93%, tr_best:  73.24%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  2.408602/  4.282592, val:  54.58%, val_best:  60.00%, tr:  71.71%, tr_best:  73.24%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.497231/  2.881497, val:  57.92%, val_best:  60.00%, tr:  71.30%, tr_best:  73.24%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  1.642783/  4.022128, val:  52.92%, val_best:  60.00%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.245907/  3.193911, val:  53.33%, val_best:  60.00%, tr:  69.66%, tr_best:  74.16%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  1.709073/  3.193357, val:  54.17%, val_best:  60.00%, tr:  73.44%, tr_best:  74.16%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  2.046260/  4.336329, val:  47.08%, val_best:  60.00%, tr:  68.74%, tr_best:  74.16%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  1.727530/  2.954718, val:  50.83%, val_best:  60.00%, tr:  72.22%, tr_best:  74.16%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  2.151317/  6.229358, val:  39.58%, val_best:  60.00%, tr:  66.91%, tr_best:  74.16%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  2.960206/  4.079273, val:  56.67%, val_best:  60.00%, tr:  67.31%, tr_best:  74.16%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  2.057526/  3.615307, val:  53.33%, val_best:  60.00%, tr:  73.24%, tr_best:  74.16%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  1.961756/  4.745461, val:  54.17%, val_best:  60.00%, tr:  71.09%, tr_best:  74.16%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  2.659262/  3.516370, val:  62.92%, val_best:  62.92%, tr:  68.03%, tr_best:  74.16%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  1.590127/  3.255071, val:  53.75%, val_best:  62.92%, tr:  78.45%, tr_best:  78.45%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  1.937127/  4.060642, val:  55.42%, val_best:  62.92%, tr:  72.32%, tr_best:  78.45%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.238741/  3.791571, val:  56.25%, val_best:  62.92%, tr:  73.14%, tr_best:  78.45%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  1.927324/  3.147347, val:  60.00%, val_best:  62.92%, tr:  75.79%, tr_best:  78.45%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  1.692989/  3.391896, val:  59.17%, val_best:  62.92%, tr:  75.49%, tr_best:  78.45%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  1.681328/  3.622781, val:  51.25%, val_best:  62.92%, tr:  73.95%, tr_best:  78.45%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  1.730361/  3.423557, val:  56.25%, val_best:  62.92%, tr:  73.85%, tr_best:  78.45%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  1.519471/  3.023130, val:  60.83%, val_best:  62.92%, tr:  75.69%, tr_best:  78.45%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  1.561310/  3.117778, val:  58.75%, val_best:  62.92%, tr:  77.22%, tr_best:  78.45%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  1.750380/  5.201420, val:  42.50%, val_best:  62.92%, tr:  76.20%, tr_best:  78.45%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  1.728732/  2.746574, val:  57.92%, val_best:  62.92%, tr:  76.92%, tr_best:  78.45%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  1.413990/  2.876418, val:  57.08%, val_best:  62.92%, tr:  78.96%, tr_best:  78.96%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  1.581641/  3.172741, val:  53.75%, val_best:  62.92%, tr:  76.30%, tr_best:  78.96%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  1.937286/  3.936234, val:  48.75%, val_best:  62.92%, tr:  74.67%, tr_best:  78.96%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  1.932061/  3.181025, val:  59.17%, val_best:  62.92%, tr:  75.89%, tr_best:  78.96%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  1.818713/  3.589899, val:  55.42%, val_best:  62.92%, tr:  74.46%, tr_best:  78.96%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  1.916332/  3.621418, val:  46.67%, val_best:  62.92%, tr:  73.75%, tr_best:  78.96%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  1.710996/  2.847316, val:  54.17%, val_best:  62.92%, tr:  73.24%, tr_best:  78.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce22b6eba5884ffd81aa66a5c4956d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▁▃▃▂▄▅▄▆▆▄▆▆▄▆▂▆▃▅▄▅▆▅▅▅▄▅▆▆▆▆▅▇▄█▆▅▇▅▄</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▄▇▅▆▅▅▇▆▇▆▄▇▄▇▅▅▄▇▆▄▆▄▇▅▅▇▆▅▃▆▆▇▆█▇▅▇</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇████</td></tr><tr><td>tr_epoch_loss</td><td>▃▂▂▂▃▂▃▃▂▃▂▃▂▃▃█▃▃▃▂▂▂▃▄▁▁▄▄▃▁▂▃▂▁▃▁▁▂▂▂</td></tr><tr><td>val_acc_best</td><td>▁▃▅▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▄▇▅▆▅▅▇▆▇▆▄▇▄▇▅▅▄▇▆▄▆▄▇▅▅▇▆▅▃▆▆▇▆█▇▅▇</td></tr><tr><td>val_loss</td><td>▁▁▂▃▁▂▃▂▂▂▂▂▃▅▂▆▂▃▃▅▂▃▆▄▄▂▅▅▅▄▅█▄▃▄▄▃▂▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.73238</td></tr><tr><td>tr_epoch_loss</td><td>1.711</td></tr><tr><td>val_acc_best</td><td>0.62917</td></tr><tr><td>val_acc_now</td><td>0.54167</td></tr><tr><td>val_loss</td><td>2.84732</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-67</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a4hz2y2s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a4hz2y2s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_181348-a4hz2y2s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mou9y44f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df769139803044eda3e13d839d8df198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113082498518957, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_181917-mou9y44f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mou9y44f' target=\"_blank\">good-sweep-69</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mou9y44f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mou9y44f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 28.568035/ 24.812077, val:  41.67%, val_best:  41.67%, tr:  27.17%, tr_best:  27.17%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 21.204990/ 26.000177, val:  43.75%, val_best:  43.75%, tr:  40.04%, tr_best:  40.04%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 22.907316/ 25.819174, val:  42.92%, val_best:  43.75%, tr:  43.00%, tr_best:  43.00%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 19.077267/ 13.256350, val:  55.00%, val_best:  55.00%, tr:  47.40%, tr_best:  47.40%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 15.323591/ 20.223778, val:  49.17%, val_best:  55.00%, tr:  51.89%, tr_best:  51.89%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 14.726413/ 18.375206, val:  51.25%, val_best:  55.00%, tr:  51.48%, tr_best:  51.89%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 12.784669/ 16.982210, val:  48.33%, val_best:  55.00%, tr:  56.08%, tr_best:  56.08%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 16.743876/ 23.407478, val:  42.50%, val_best:  55.00%, tr:  53.93%, tr_best:  56.08%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 16.265696/ 18.593967, val:  50.83%, val_best:  55.00%, tr:  55.46%, tr_best:  56.08%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 15.997672/ 18.822344, val:  56.67%, val_best:  56.67%, tr:  61.49%, tr_best:  61.49%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 17.862846/ 18.748732, val:  51.67%, val_best:  56.67%, tr:  58.84%, tr_best:  61.49%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 16.236282/ 28.382366, val:  45.42%, val_best:  56.67%, tr:  60.16%, tr_best:  61.49%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 20.138405/ 18.218842, val:  60.42%, val_best:  60.42%, tr:  57.10%, tr_best:  61.49%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 10.964234/ 13.269766, val:  63.75%, val_best:  63.75%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 14.451543/ 18.721558, val:  55.00%, val_best:  63.75%, tr:  64.66%, tr_best:  64.66%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 11.881100/ 17.141441, val:  53.33%, val_best:  63.75%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 14.804561/ 16.908417, val:  61.25%, val_best:  63.75%, tr:  62.72%, tr_best:  66.29%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 11.941324/ 21.866692, val:  54.17%, val_best:  63.75%, tr:  65.47%, tr_best:  66.29%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 12.566126/ 14.191718, val:  56.67%, val_best:  63.75%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  9.866640/ 20.779902, val:  54.17%, val_best:  63.75%, tr:  68.74%, tr_best:  69.15%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 18.915455/ 17.693651, val:  53.75%, val_best:  63.75%, tr:  65.27%, tr_best:  69.15%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 11.143761/ 21.381506, val:  64.17%, val_best:  64.17%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 11.007064/ 18.457684, val:  63.75%, val_best:  64.17%, tr:  73.65%, tr_best:  73.65%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  9.779452/ 12.819591, val:  69.58%, val_best:  69.58%, tr:  72.73%, tr_best:  73.65%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  9.879377/ 13.697647, val:  68.33%, val_best:  69.58%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  9.878430/ 17.061708, val:  65.83%, val_best:  69.58%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 10.799793/ 17.135519, val:  62.92%, val_best:  69.58%, tr:  75.08%, tr_best:  75.79%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  8.752050/ 18.130070, val:  62.50%, val_best:  69.58%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 10.078238/ 19.118877, val:  69.58%, val_best:  69.58%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  7.313392/ 18.212261, val:  60.00%, val_best:  69.58%, tr:  79.98%, tr_best:  81.21%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  9.558197/ 19.546803, val:  60.42%, val_best:  69.58%, tr:  76.20%, tr_best:  81.21%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  5.509794/ 22.502178, val:  61.25%, val_best:  69.58%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  8.487028/ 28.612568, val:  57.50%, val_best:  69.58%, tr:  81.61%, tr_best:  85.60%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  7.747581/ 18.594873, val:  68.33%, val_best:  69.58%, tr:  82.23%, tr_best:  85.60%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  8.961859/ 19.784666, val:  67.50%, val_best:  69.58%, tr:  82.94%, tr_best:  85.60%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.806986/ 15.615176, val:  73.75%, val_best:  73.75%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  4.742241/ 16.538584, val:  70.83%, val_best:  73.75%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  7.454087/ 28.834118, val:  65.42%, val_best:  73.75%, tr:  85.09%, tr_best:  90.81%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  7.388045/ 24.757710, val:  59.58%, val_best:  73.75%, tr:  85.19%, tr_best:  90.81%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  5.231547/ 21.749386, val:  66.25%, val_best:  73.75%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  4.618971/ 18.483461, val:  70.83%, val_best:  73.75%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  4.040721/ 17.357950, val:  72.50%, val_best:  73.75%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  5.185653/ 20.889906, val:  66.67%, val_best:  73.75%, tr:  89.48%, tr_best:  94.99%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  4.902024/ 25.639246, val:  65.42%, val_best:  73.75%, tr:  92.65%, tr_best:  94.99%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  4.430322/ 24.299248, val:  65.00%, val_best:  73.75%, tr:  93.97%, tr_best:  94.99%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  3.660547/ 19.621244, val:  77.50%, val_best:  77.50%, tr:  94.89%, tr_best:  94.99%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.958582/ 21.624258, val:  73.33%, val_best:  77.50%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.774109/ 19.122314, val:  74.58%, val_best:  77.50%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  3.443926/ 17.451767, val:  78.75%, val_best:  78.75%, tr:  95.40%, tr_best:  98.88%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.832803/ 17.806841, val:  77.08%, val_best:  78.75%, tr:  97.55%, tr_best:  98.88%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.793975/ 21.929245, val:  69.17%, val_best:  78.75%, tr:  97.34%, tr_best:  98.88%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.568959/ 21.008436, val:  75.00%, val_best:  78.75%, tr:  98.37%, tr_best:  98.88%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  2.239618/ 20.143829, val:  72.92%, val_best:  78.75%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  2.300223/ 19.170364, val:  80.83%, val_best:  80.83%, tr:  98.37%, tr_best:  98.88%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.827952/ 20.403355, val:  72.08%, val_best:  80.83%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.307919/ 18.457272, val:  76.67%, val_best:  80.83%, tr:  98.47%, tr_best:  99.28%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  3.577150/ 21.181330, val:  77.50%, val_best:  80.83%, tr:  95.30%, tr_best:  99.28%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.587373/ 20.001991, val:  81.67%, val_best:  81.67%, tr:  97.55%, tr_best:  99.28%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.676118/ 19.867390, val:  80.83%, val_best:  81.67%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  2.138083/ 20.097050, val:  81.25%, val_best:  81.67%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.534839/ 20.652697, val:  75.42%, val_best:  81.67%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.075266/ 22.051716, val:  74.58%, val_best:  81.67%, tr:  98.16%, tr_best:  99.49%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  1.830984/ 22.313410, val:  72.08%, val_best:  81.67%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  1.527502/ 21.992443, val:  77.50%, val_best:  81.67%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  1.424824/ 22.101749, val:  77.50%, val_best:  81.67%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  1.321223/ 21.670654, val:  78.75%, val_best:  81.67%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  1.247255/ 21.562325, val:  79.17%, val_best:  81.67%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.989179/ 21.271906, val:  77.92%, val_best:  81.67%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.829906/ 21.312654, val:  80.83%, val_best:  81.67%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.786406/ 22.166340, val:  76.25%, val_best:  81.67%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.987029/ 21.411808, val:  80.00%, val_best:  81.67%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  1.078922/ 21.529043, val:  80.42%, val_best:  81.67%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.767374/ 21.597149, val:  76.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.712895/ 21.533430, val:  78.33%, val_best:  81.67%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.760567/ 22.438887, val:  77.50%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.624684/ 22.317852, val:  77.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.684795/ 22.250692, val:  76.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.644986/ 23.141186, val:  78.75%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.760846/ 23.455740, val:  76.67%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.454701/ 22.526621, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.470591/ 23.135443, val:  77.08%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.592393/ 23.156282, val:  77.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.560910/ 24.644209, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.621372/ 23.742935, val:  76.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.556141/ 23.805441, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.355459/ 23.471266, val:  78.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.315966/ 23.900877, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.326032/ 24.408365, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.341529/ 24.541338, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.279193/ 24.546921, val:  77.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.326587/ 23.944172, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.339994/ 24.548672, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.316303/ 25.049599, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.402085/ 24.044487, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.329687/ 25.249039, val:  77.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.381353/ 24.387718, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.250403/ 23.592484, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.250295/ 25.001928, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.220575/ 24.423246, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.207938/ 25.277924, val:  77.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b28c60c8b0463e96d3c91cdfdba731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▃▅▁▅▇▅▅▇▆▅▇▅▇█▇▇████▇██████▇██████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▁▄▄▃▃▃▅▆▅▄▄▇▅▅▅▅▇▇▆▆██▇▇▇█▇▇▇█▇▇▇▇██▇</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▅▅▅▆▅▄▃▄▃▄▃▃▂▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▁▄▄▃▃▃▅▆▅▄▄▇▅▅▅▅▇▇▆▆██▇▇▇█▇▇▇█▇▇▇▇██▇</td></tr><tr><td>val_loss</td><td>▆▇▄▅▃▃▃▅▄▅▁▃▃█▂█▅▄▆▄▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.20794</td></tr><tr><td>val_acc_best</td><td>0.82083</td></tr><tr><td>val_acc_now</td><td>0.77083</td></tr><tr><td>val_loss</td><td>25.27792</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-69</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mou9y44f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mou9y44f</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_181917-mou9y44f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sxmsc7mm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_182539-sxmsc7mm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sxmsc7mm' target=\"_blank\">smart-sweep-71</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sxmsc7mm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sxmsc7mm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.088269/  1.924162, val:  55.42%, val_best:  55.42%, tr:  25.74%, tr_best:  25.74%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.748646/  2.324303, val:  47.08%, val_best:  55.42%, tr:  50.56%, tr_best:  50.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.031686/  1.784263, val:  52.92%, val_best:  55.42%, tr:  52.09%, tr_best:  52.09%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.393732/  2.483753, val:  55.42%, val_best:  55.42%, tr:  64.56%, tr_best:  64.56%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.655422/  1.797623, val:  62.92%, val_best:  62.92%, tr:  61.29%, tr_best:  64.56%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.606218/  2.108458, val:  52.08%, val_best:  62.92%, tr:  66.29%, tr_best:  66.29%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.304159/  1.853315, val:  63.33%, val_best:  63.33%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.655556/  2.510137, val:  57.50%, val_best:  63.33%, tr:  65.68%, tr_best:  67.62%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.342168/  1.863558, val:  57.92%, val_best:  63.33%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.793281/  2.411734, val:  54.17%, val_best:  63.33%, tr:  65.88%, tr_best:  69.36%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.277720/  1.507704, val:  69.17%, val_best:  69.17%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.190408/  2.084774, val:  61.67%, val_best:  69.17%, tr:  76.81%, tr_best:  76.81%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.411035/  1.720113, val:  74.58%, val_best:  74.58%, tr:  73.03%, tr_best:  76.81%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.049917/  2.018579, val:  65.00%, val_best:  74.58%, tr:  79.37%, tr_best:  79.37%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.141363/  2.376780, val:  67.08%, val_best:  74.58%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.801808/  1.976255, val:  63.75%, val_best:  74.58%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.780425/  1.924234, val:  74.58%, val_best:  74.58%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.853610/  2.052349, val:  71.25%, val_best:  74.58%, tr:  87.23%, tr_best:  88.87%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.754616/  2.223138, val:  68.75%, val_best:  74.58%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.689644/  1.879389, val:  80.42%, val_best:  80.42%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.650230/  2.146589, val:  72.08%, val_best:  80.42%, tr:  92.34%, tr_best:  93.36%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.667237/  2.055383, val:  76.25%, val_best:  80.42%, tr:  93.16%, tr_best:  93.36%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.504362/  2.246800, val:  73.33%, val_best:  80.42%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.527734/  2.143690, val:  80.83%, val_best:  80.83%, tr:  96.63%, tr_best:  96.94%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.440720/  2.153373, val:  82.92%, val_best:  82.92%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.432012/  2.181108, val:  80.00%, val_best:  82.92%, tr:  97.75%, tr_best:  98.16%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.382291/  2.213511, val:  81.67%, val_best:  82.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.346409/  2.431139, val:  76.25%, val_best:  82.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.338539/  2.310768, val:  82.92%, val_best:  82.92%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.308476/  2.622310, val:  79.58%, val_best:  82.92%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.281789/  2.435708, val:  82.08%, val_best:  82.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.316502/  2.595363, val:  80.00%, val_best:  82.92%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.342502/  2.626222, val:  81.25%, val_best:  82.92%, tr:  98.37%, tr_best:  99.59%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.301779/  2.880189, val:  79.17%, val_best:  82.92%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.308472/  2.827374, val:  77.50%, val_best:  82.92%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.278293/  2.700292, val:  83.75%, val_best:  83.75%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.208794/  2.730045, val:  83.33%, val_best:  83.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.204273/  2.830955, val:  83.33%, val_best:  83.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.212009/  2.806296, val:  83.75%, val_best:  83.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.219720/  2.836807, val:  85.00%, val_best:  85.00%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.168380/  2.898573, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.172596/  3.098001, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.162872/  3.072033, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.138201/  3.031003, val:  84.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.131169/  3.144264, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.153327/  3.141791, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.121078/  3.273422, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.157420/  3.195806, val:  83.75%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.157193/  3.299356, val:  84.17%, val_best:  85.42%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.126161/  3.263213, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.103948/  3.333685, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.086619/  3.351974, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.093493/  3.437164, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.079942/  3.525561, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.080959/  3.504726, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.072637/  3.469257, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.077057/  3.671590, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.086090/  3.697213, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.068157/  3.671484, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.061941/  3.738004, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.059704/  3.602182, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.056558/  3.784250, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.051921/  3.836684, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.054976/  3.816503, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.052012/  3.849582, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.060038/  3.757371, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.054242/  3.928802, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.054727/  3.924969, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.056229/  3.935190, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.058208/  3.950271, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.061890/  3.998514, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.052772/  4.116801, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.044919/  4.110452, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.043252/  4.160613, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.038384/  4.197310, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.042586/  4.180501, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.033763/  4.208863, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.032836/  4.244118, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.043583/  4.158999, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.037321/  4.176544, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.031600/  4.290939, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.021805/  4.267632, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.022327/  4.358761, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.028214/  4.370637, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.027035/  4.329673, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.025617/  4.424868, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.027220/  4.353056, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.025327/  4.471702, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.028826/  4.465854, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.028817/  4.496404, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.022984/  4.425671, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.022781/  4.629734, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.020062/  4.510583, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.022614/  4.472102, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.032351/  4.498072, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.020206/  4.683724, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.020453/  4.645747, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.021390/  4.628222, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.021679/  4.543379, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.024919/  4.571357, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413c3b44ccc2451eb6ac2fe5e872cdb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▄▅▃▅▇▆▇████▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▃▂▁▆▄▅▇▆▇▇▇▇█▇██▇██▇████▇████▇████▇███</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▅▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▇▇▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▃▅▅▅▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▃▂▁▆▄▅▇▆▇▇▇▇█▇██▇██▇████▇████▇████▇███</td></tr><tr><td>val_loss</td><td>▁▁▁▃▃▁▃▂▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02492</td></tr><tr><td>val_acc_best</td><td>0.85833</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>4.57136</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-71</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sxmsc7mm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sxmsc7mm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_182539-sxmsc7mm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5igtnagp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_183202-5igtnagp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5igtnagp' target=\"_blank\">fragrant-sweep-73</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5igtnagp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5igtnagp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.306479/  1.978804, val:  26.67%, val_best:  26.67%, tr:  10.42%, tr_best:  10.42%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.546456/  1.492151, val:  55.83%, val_best:  55.83%, tr:  48.52%, tr_best:  48.52%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.455957/  1.559261, val:  58.75%, val_best:  58.75%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.263864/  1.596427, val:  56.67%, val_best:  58.75%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.195132/  1.363155, val:  62.92%, val_best:  62.92%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.087733/  1.421423, val:  60.00%, val_best:  62.92%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.103827/  1.318875, val:  62.92%, val_best:  62.92%, tr:  68.64%, tr_best:  69.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.140004/  1.326241, val:  65.42%, val_best:  65.42%, tr:  68.74%, tr_best:  69.36%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.075583/  1.454021, val:  69.17%, val_best:  69.17%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.101912/  1.342581, val:  66.25%, val_best:  69.17%, tr:  75.08%, tr_best:  75.08%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.091535/  1.255538, val:  68.75%, val_best:  69.17%, tr:  73.85%, tr_best:  75.08%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.901280/  1.342331, val:  68.75%, val_best:  69.17%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.833625/  1.249760, val:  80.00%, val_best:  80.00%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.801552/  1.348360, val:  70.83%, val_best:  80.00%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.823863/  1.505367, val:  74.17%, val_best:  80.00%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.658266/  1.411291, val:  77.50%, val_best:  80.00%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.673231/  1.365630, val:  79.17%, val_best:  80.00%, tr:  89.17%, tr_best:  89.89%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.594394/  1.362328, val:  82.50%, val_best:  82.50%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.549070/  1.467187, val:  79.17%, val_best:  82.50%, tr:  93.87%, tr_best:  94.89%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.525612/  1.392796, val:  82.50%, val_best:  82.50%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.491467/  1.513932, val:  81.25%, val_best:  82.50%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.446569/  1.529853, val:  77.08%, val_best:  82.50%, tr:  96.42%, tr_best:  96.94%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.436940/  1.481583, val:  83.33%, val_best:  83.33%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.420155/  1.548607, val:  87.08%, val_best:  87.08%, tr:  96.94%, tr_best:  97.34%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.381592/  1.611731, val:  82.92%, val_best:  87.08%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.411919/  1.603865, val:  82.50%, val_best:  87.08%, tr:  97.65%, tr_best:  98.37%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.388225/  1.599371, val:  87.50%, val_best:  87.50%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.340097/  1.770846, val:  82.08%, val_best:  87.50%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.328348/  1.653015, val:  85.83%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.303842/  1.846102, val:  80.00%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.314113/  1.737318, val:  83.33%, val_best:  87.50%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.292898/  1.793271, val:  84.17%, val_best:  87.50%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.306903/  1.922115, val:  83.33%, val_best:  87.50%, tr:  98.67%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.285894/  1.939197, val:  84.17%, val_best:  87.50%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.275821/  2.016003, val:  82.50%, val_best:  87.50%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.238944/  1.946912, val:  85.42%, val_best:  87.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.224114/  2.024787, val:  82.50%, val_best:  87.50%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.219101/  2.033825, val:  84.58%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.213482/  2.034233, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.215539/  2.057276, val:  84.17%, val_best:  87.50%, tr:  99.28%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.192978/  2.031908, val:  86.67%, val_best:  87.50%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.171497/  2.123474, val:  87.08%, val_best:  87.50%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.181568/  2.156923, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.172227/  2.186422, val:  85.42%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.170649/  2.231337, val:  84.17%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.154309/  2.184586, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.144361/  2.305473, val:  83.75%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.144098/  2.194733, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.154926/  2.215383, val:  87.50%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.134494/  2.316416, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.124544/  2.344104, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.134537/  2.350584, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.126179/  2.389639, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.115434/  2.437605, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.106151/  2.390757, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.098700/  2.435629, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.103632/  2.527589, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.106306/  2.477939, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.109899/  2.521224, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.097034/  2.592089, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.091460/  2.631016, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.089422/  2.639864, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.084849/  2.707108, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.086167/  2.648879, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.079147/  2.661229, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.077634/  2.742643, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.082425/  2.787807, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.069761/  2.863002, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.072936/  2.804834, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.066306/  2.860699, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.065371/  2.867242, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.063827/  2.904373, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.058132/  2.981011, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.058314/  2.973475, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.062623/  2.937371, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.055129/  2.916863, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.058102/  3.086373, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.055161/  3.054375, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.051371/  3.078571, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.062172/  3.129778, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.058441/  3.151033, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.054117/  3.143978, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.046242/  3.116561, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.042750/  3.218899, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.047966/  3.224355, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.037894/  3.285614, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.037193/  3.284096, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.031701/  3.272225, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.035544/  3.255426, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.042131/  3.231530, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.037243/  3.269896, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.033733/  3.305058, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.040364/  3.323627, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.034152/  3.288328, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.031402/  3.360850, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.039982/  3.391302, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.032554/  3.356653, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.035032/  3.378537, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.033013/  3.383974, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030318/  3.427136, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91156ad38a12436cad601c7dc71ce949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▂▂▆▇▅██▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▅▅▅▇▆▇▇▇▇█▇▇█▇▇█▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▆▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▅▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▅▅▅▇▆▇▇▇▇█▇▇█▇▇█▇█████████████████████</td></tr><tr><td>val_loss</td><td>▃▂▁▁▁▁▂▁▁▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▆▇▇▇▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03032</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>3.42714</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-sweep-73</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5igtnagp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5igtnagp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_183202-5igtnagp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wz12admg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_183829-wz12admg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wz12admg' target=\"_blank\">prime-sweep-76</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wz12admg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wz12admg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 53.966801/ 43.172638, val:  16.67%, val_best:  16.67%, tr:  17.26%, tr_best:  17.26%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 42.336781/ 34.596153, val:  24.17%, val_best:  24.17%, tr:  27.37%, tr_best:  27.37%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 40.801025/ 40.277390, val:  20.00%, val_best:  24.17%, tr:  27.48%, tr_best:  27.48%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 33.960567/ 27.508686, val:  35.00%, val_best:  35.00%, tr:  30.34%, tr_best:  30.34%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 26.463301/ 26.769308, val:  36.25%, val_best:  36.25%, tr:  33.30%, tr_best:  33.30%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 25.878946/ 27.029900, val:  31.25%, val_best:  36.25%, tr:  35.44%, tr_best:  35.44%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 23.537754/ 27.094767, val:  42.50%, val_best:  42.50%, tr:  39.73%, tr_best:  39.73%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 26.907478/ 36.031898, val:  35.83%, val_best:  42.50%, tr:  40.86%, tr_best:  40.86%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 22.251907/ 14.092922, val:  54.17%, val_best:  54.17%, tr:  45.05%, tr_best:  45.05%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 20.057707/ 25.944942, val:  35.00%, val_best:  54.17%, tr:  46.17%, tr_best:  46.17%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 20.386175/ 19.401770, val:  45.42%, val_best:  54.17%, tr:  45.97%, tr_best:  46.17%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 23.264931/ 23.349119, val:  47.92%, val_best:  54.17%, tr:  47.09%, tr_best:  47.09%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 18.710718/ 20.882833, val:  44.58%, val_best:  54.17%, tr:  49.23%, tr_best:  49.23%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 16.794279/ 31.046432, val:  47.08%, val_best:  54.17%, tr:  48.72%, tr_best:  49.23%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 20.151833/ 37.637802, val:  24.17%, val_best:  54.17%, tr:  49.03%, tr_best:  49.23%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 18.373116/ 27.468987, val:  39.58%, val_best:  54.17%, tr:  52.30%, tr_best:  52.30%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 17.846897/ 25.561840, val:  32.92%, val_best:  54.17%, tr:  54.55%, tr_best:  54.55%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 17.041666/ 32.500626, val:  43.33%, val_best:  54.17%, tr:  53.12%, tr_best:  54.55%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 21.330173/ 19.895325, val:  42.50%, val_best:  54.17%, tr:  51.48%, tr_best:  54.55%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 15.208481/ 28.840023, val:  41.25%, val_best:  54.17%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 24.858128/ 19.410351, val:  50.42%, val_best:  54.17%, tr:  50.26%, tr_best:  54.95%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 19.608490/ 19.569023, val:  52.92%, val_best:  54.17%, tr:  52.81%, tr_best:  54.95%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 20.424000/ 21.821602, val:  50.83%, val_best:  54.17%, tr:  52.71%, tr_best:  54.95%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 12.381702/ 18.620281, val:  53.75%, val_best:  54.17%, tr:  60.47%, tr_best:  60.47%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 18.353582/ 18.511986, val:  46.25%, val_best:  54.17%, tr:  54.44%, tr_best:  60.47%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 13.082562/ 21.204584, val:  55.42%, val_best:  55.42%, tr:  58.84%, tr_best:  60.47%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 20.431313/ 30.189772, val:  44.58%, val_best:  55.42%, tr:  58.32%, tr_best:  60.47%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 18.118189/ 22.878191, val:  47.50%, val_best:  55.42%, tr:  54.95%, tr_best:  60.47%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 23.088568/ 19.553608, val:  52.08%, val_best:  55.42%, tr:  54.34%, tr_best:  60.47%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 11.986443/ 16.105331, val:  52.08%, val_best:  55.42%, tr:  64.56%, tr_best:  64.56%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 12.778860/ 22.839716, val:  43.75%, val_best:  55.42%, tr:  62.00%, tr_best:  64.56%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 11.703560/ 22.600040, val:  58.75%, val_best:  58.75%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 16.839447/ 25.594303, val:  55.42%, val_best:  58.75%, tr:  61.59%, tr_best:  66.70%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 16.124054/ 19.832762, val:  55.42%, val_best:  58.75%, tr:  60.16%, tr_best:  66.70%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 19.564253/ 20.884062, val:  43.75%, val_best:  58.75%, tr:  55.67%, tr_best:  66.70%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 16.607412/ 24.479292, val:  42.50%, val_best:  58.75%, tr:  59.86%, tr_best:  66.70%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 18.683914/ 27.510860, val:  50.42%, val_best:  58.75%, tr:  59.24%, tr_best:  66.70%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 18.354660/ 41.830494, val:  31.25%, val_best:  58.75%, tr:  60.57%, tr_best:  66.70%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss: 18.970760/ 21.376898, val:  52.08%, val_best:  58.75%, tr:  58.12%, tr_best:  66.70%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 12.250057/ 18.086334, val:  58.75%, val_best:  58.75%, tr:  65.78%, tr_best:  66.70%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss: 12.661333/ 20.167034, val:  43.33%, val_best:  58.75%, tr:  64.86%, tr_best:  66.70%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss: 13.256148/ 15.722947, val:  55.83%, val_best:  58.75%, tr:  65.07%, tr_best:  66.70%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 12.411112/ 20.134678, val:  47.08%, val_best:  58.75%, tr:  64.66%, tr_best:  66.70%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss: 13.181302/ 22.981920, val:  53.75%, val_best:  58.75%, tr:  64.04%, tr_best:  66.70%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss: 12.521389/ 30.923958, val:  54.17%, val_best:  58.75%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss: 18.100578/ 16.363007, val:  48.75%, val_best:  58.75%, tr:  59.14%, tr_best:  67.42%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss: 16.579096/ 19.458574, val:  50.00%, val_best:  58.75%, tr:  56.69%, tr_best:  67.42%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 18.225632/ 17.677265, val:  57.50%, val_best:  58.75%, tr:  53.73%, tr_best:  67.42%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 18.330896/ 14.070729, val:  60.83%, val_best:  60.83%, tr:  59.14%, tr_best:  67.42%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss: 13.803683/ 20.703074, val:  43.33%, val_best:  60.83%, tr:  59.96%, tr_best:  67.42%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss: 13.784678/ 19.970663, val:  54.58%, val_best:  60.83%, tr:  57.71%, tr_best:  67.42%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss: 13.071560/ 17.152561, val:  47.92%, val_best:  60.83%, tr:  58.84%, tr_best:  67.42%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss: 14.543612/ 16.445135, val:  58.75%, val_best:  60.83%, tr:  58.63%, tr_best:  67.42%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss: 14.351438/ 22.387934, val:  55.83%, val_best:  60.83%, tr:  59.24%, tr_best:  67.42%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss: 13.296359/ 22.052214, val:  60.00%, val_best:  60.83%, tr:  62.31%, tr_best:  67.42%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss: 16.279961/ 17.786846, val:  49.17%, val_best:  60.83%, tr:  57.92%, tr_best:  67.42%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 19.761360/ 18.811758, val:  53.75%, val_best:  60.83%, tr:  57.10%, tr_best:  67.42%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss: 18.812464/ 36.054874, val:  35.00%, val_best:  60.83%, tr:  57.10%, tr_best:  67.42%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss: 12.867599/ 19.247080, val:  58.33%, val_best:  60.83%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss: 10.408278/ 17.956688, val:  56.67%, val_best:  60.83%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  9.868378/ 23.340775, val:  48.33%, val_best:  60.83%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss: 12.823404/ 15.227527, val:  61.67%, val_best:  61.67%, tr:  68.85%, tr_best:  70.79%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  8.366921/ 19.601301, val:  59.58%, val_best:  61.67%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  9.551720/ 16.526930, val:  58.33%, val_best:  61.67%, tr:  70.89%, tr_best:  74.87%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss: 13.068816/ 17.935131, val:  59.17%, val_best:  61.67%, tr:  66.80%, tr_best:  74.87%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss: 15.261246/ 17.551741, val:  59.58%, val_best:  61.67%, tr:  59.86%, tr_best:  74.87%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss: 13.690252/ 20.565430, val:  60.42%, val_best:  61.67%, tr:  68.54%, tr_best:  74.87%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss: 15.194530/ 16.992552, val:  58.33%, val_best:  61.67%, tr:  68.54%, tr_best:  74.87%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss: 10.014381/ 13.471666, val:  69.17%, val_best:  69.17%, tr:  74.57%, tr_best:  74.87%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  6.968821/ 13.998848, val:  60.83%, val_best:  69.17%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss: 11.026892/ 27.529388, val:  40.83%, val_best:  69.17%, tr:  74.77%, tr_best:  79.16%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss: 12.966559/ 17.204922, val:  58.33%, val_best:  69.17%, tr:  69.77%, tr_best:  79.16%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  7.391158/ 27.340412, val:  59.17%, val_best:  69.17%, tr:  78.04%, tr_best:  79.16%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  9.560093/ 27.702362, val:  41.25%, val_best:  69.17%, tr:  75.28%, tr_best:  79.16%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  7.407925/ 15.408957, val:  61.25%, val_best:  69.17%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  8.555941/ 25.335636, val:  55.42%, val_best:  69.17%, tr:  72.93%, tr_best:  80.08%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  9.494204/ 26.459932, val:  33.75%, val_best:  69.17%, tr:  73.75%, tr_best:  80.08%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  6.861116/ 26.093372, val:  41.67%, val_best:  69.17%, tr:  79.26%, tr_best:  80.08%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss: 11.447849/ 17.762526, val:  59.58%, val_best:  69.17%, tr:  69.25%, tr_best:  80.08%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss: 10.258341/ 15.768785, val:  60.00%, val_best:  69.17%, tr:  75.38%, tr_best:  80.08%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  8.250356/ 18.722902, val:  56.25%, val_best:  69.17%, tr:  78.04%, tr_best:  80.08%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss: 10.592889/ 18.576822, val:  59.58%, val_best:  69.17%, tr:  75.08%, tr_best:  80.08%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  6.125603/ 19.875872, val:  55.42%, val_best:  69.17%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  7.515409/ 20.986448, val:  45.83%, val_best:  69.17%, tr:  80.18%, tr_best:  84.88%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  7.595889/ 20.499304, val:  59.58%, val_best:  69.17%, tr:  82.64%, tr_best:  84.88%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  8.071426/ 15.897988, val:  62.92%, val_best:  69.17%, tr:  79.37%, tr_best:  84.88%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  7.106541/ 15.292553, val:  62.08%, val_best:  69.17%, tr:  81.00%, tr_best:  84.88%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  5.784183/ 17.228308, val:  59.17%, val_best:  69.17%, tr:  82.12%, tr_best:  84.88%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  5.354686/ 14.510749, val:  67.08%, val_best:  69.17%, tr:  85.09%, tr_best:  85.09%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  6.956477/ 19.446955, val:  60.83%, val_best:  69.17%, tr:  82.53%, tr_best:  85.09%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  5.577188/ 14.819393, val:  68.33%, val_best:  69.17%, tr:  83.04%, tr_best:  85.09%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  6.352893/ 18.101992, val:  51.25%, val_best:  69.17%, tr:  83.76%, tr_best:  85.09%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  5.644833/ 15.263346, val:  65.00%, val_best:  69.17%, tr:  83.86%, tr_best:  85.09%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  4.229579/ 19.413433, val:  63.75%, val_best:  69.17%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  5.903767/ 17.182171, val:  65.42%, val_best:  69.17%, tr:  86.93%, tr_best:  89.48%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  5.660415/ 14.587227, val:  54.58%, val_best:  69.17%, tr:  83.86%, tr_best:  89.48%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  6.348433/ 18.717577, val:  57.92%, val_best:  69.17%, tr:  80.69%, tr_best:  89.48%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  7.769062/ 17.746925, val:  57.92%, val_best:  69.17%, tr:  79.16%, tr_best:  89.48%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  4.683215/ 20.644182, val:  53.75%, val_best:  69.17%, tr:  88.66%, tr_best:  89.48%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  4.900682/ 16.991974, val:  63.33%, val_best:  69.17%, tr:  87.74%, tr_best:  89.48%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56677c838d384c5a85c51f411ae9074e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▁▃▁▃▅▄▃▆▅▅▅▅▅▃▄▃▅▃▃▅▄▆▆▆▃▅▆▅▆▇▇▅█▇▇▇█▇</td></tr><tr><td>summary_val_acc</td><td>▁▁▄▄▄▅▂▅▅▆▅▅▆▇▅▃▇▅▆▇▅▇▇▄▇█▇▇▅▇▇▅▇▇▇▇▇█▆▇</td></tr><tr><td>tr_acc</td><td>▁▂▃▃▄▄▄▅▅▅▅▅▆▆▅▅▆▆▆▅▅▅▆▅▆▆▅▆▇▇▇▇▇██████▇</td></tr><tr><td>tr_epoch_loss</td><td>█▆▄▄▃▃▃▃▂▃▃▃▂▃▃▃▂▂▂▃▂▂▂▃▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▄▄▄▅▂▅▅▆▅▅▆▇▅▃▇▅▆▇▅▇▇▄▇█▇▇▅▇▇▅▇▇▇▇▇█▆▇</td></tr><tr><td>val_loss</td><td>█▇▄▆▄▃▇▅▄▂▂▅▁▄▃█▂▂▅▂▂▁▃▆▂▁▂▂▄▄▄▄▁▂▂▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.87743</td></tr><tr><td>tr_epoch_loss</td><td>4.90068</td></tr><tr><td>val_acc_best</td><td>0.69167</td></tr><tr><td>val_acc_now</td><td>0.63333</td></tr><tr><td>val_loss</td><td>16.99197</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prime-sweep-76</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wz12admg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wz12admg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_183829-wz12admg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g3mpa96l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_184343-g3mpa96l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g3mpa96l' target=\"_blank\">giddy-sweep-78</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g3mpa96l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g3mpa96l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.319476/  2.168869, val:  15.00%, val_best:  15.00%, tr:   9.40%, tr_best:   9.40%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.546559/  1.394541, val:  57.50%, val_best:  57.50%, tr:  45.56%, tr_best:  45.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.361095/  1.347619, val:  57.92%, val_best:  57.92%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.168066/  1.364703, val:  56.25%, val_best:  57.92%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.111191/  1.360562, val:  64.58%, val_best:  64.58%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.020129/  1.230537, val:  64.17%, val_best:  64.58%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.009647/  1.213413, val:  65.00%, val_best:  65.00%, tr:  72.11%, tr_best:  72.11%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.003902/  1.176388, val:  72.08%, val_best:  72.08%, tr:  69.36%, tr_best:  72.11%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.008450/  1.274724, val:  64.58%, val_best:  72.08%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.943574/  1.347100, val:  61.67%, val_best:  72.08%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.939643/  1.174111, val:  72.92%, val_best:  72.92%, tr:  74.46%, tr_best:  76.40%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.807263/  1.173712, val:  76.67%, val_best:  76.67%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.713175/  1.111352, val:  82.08%, val_best:  82.08%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.686933/  1.135550, val:  80.00%, val_best:  82.08%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.701552/  1.443170, val:  71.67%, val_best:  82.08%, tr:  86.93%, tr_best:  87.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.576296/  1.147456, val:  81.67%, val_best:  82.08%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.554653/  1.123438, val:  84.17%, val_best:  84.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.500317/  1.145159, val:  84.58%, val_best:  84.58%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.473771/  1.235066, val:  82.50%, val_best:  84.58%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.453774/  1.129544, val:  83.75%, val_best:  84.58%, tr:  95.20%, tr_best:  95.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.417704/  1.190487, val:  85.83%, val_best:  85.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.391449/  1.228569, val:  82.92%, val_best:  85.83%, tr:  96.83%, tr_best:  97.34%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.363348/  1.228766, val:  83.75%, val_best:  85.83%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.364034/  1.211714, val:  85.42%, val_best:  85.83%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.336103/  1.263916, val:  83.33%, val_best:  85.83%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.324088/  1.262189, val:  87.50%, val_best:  87.50%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.315723/  1.267444, val:  87.92%, val_best:  87.92%, tr:  98.06%, tr_best:  98.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.290609/  1.388012, val:  84.17%, val_best:  87.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.282847/  1.273947, val:  87.08%, val_best:  87.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.253795/  1.496196, val:  77.92%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.266297/  1.330569, val:  87.92%, val_best:  87.92%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.235620/  1.388238, val:  86.67%, val_best:  87.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.263707/  1.422997, val:  84.58%, val_best:  87.92%, tr:  98.37%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.245322/  1.483279, val:  85.00%, val_best:  87.92%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.231372/  1.586224, val:  78.75%, val_best:  87.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.214156/  1.484141, val:  86.67%, val_best:  87.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.184852/  1.435867, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.173412/  1.483765, val:  85.42%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.180633/  1.489602, val:  86.67%, val_best:  87.92%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.176510/  1.547179, val:  88.33%, val_best:  88.33%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.156040/  1.472144, val:  89.58%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.157954/  1.526698, val:  85.00%, val_best:  89.58%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.154075/  1.538508, val:  87.50%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.132665/  1.573861, val:  88.33%, val_best:  89.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.129036/  1.680215, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.125832/  1.658053, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.125388/  1.662654, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.129947/  1.647644, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.119372/  1.675312, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.116225/  1.678526, val:  87.92%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.102924/  1.735842, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.096691/  1.729942, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.097066/  1.781765, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.082823/  1.764519, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.087560/  1.788373, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.080971/  1.820014, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.075115/  1.886615, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.085490/  1.877929, val:  89.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.071388/  1.890186, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.068798/  1.936669, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.076526/  1.908287, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.079891/  1.899697, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.071702/  1.965082, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.068042/  1.891309, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.062879/  1.935400, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.062603/  1.935610, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.058572/  2.021835, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.062719/  2.068077, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.056839/  2.032468, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.059113/  2.075200, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.050592/  2.077132, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.053072/  2.075650, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.045561/  2.104312, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.048279/  2.073460, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.050015/  2.103623, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.041874/  2.110681, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.042547/  2.152984, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.038319/  2.201250, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.040931/  2.212138, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.043858/  2.180299, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.045234/  2.221424, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.038475/  2.217281, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.037684/  2.278193, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.043997/  2.310470, val:  86.25%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.038367/  2.251507, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.041988/  2.268560, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.038767/  2.270885, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.035475/  2.313851, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.039516/  2.279841, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.035011/  2.255051, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.027546/  2.281515, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.029989/  2.306377, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.024913/  2.329683, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.027511/  2.324182, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.026752/  2.322279, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.022845/  2.368027, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.023173/  2.374410, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.024822/  2.412791, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.023658/  2.432441, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.029568/  2.377930, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928d2e8502ee4cc0b804f905d0e83f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▃▂▇▇▅████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▅▇▆█▇▇▇█▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▅▇▆█▇▇▇█▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▇▂▂▁▂▁▃▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.02957</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>2.37793</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-sweep-78</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g3mpa96l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/g3mpa96l</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_184343-g3mpa96l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mt80fh24 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_184857-mt80fh24</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt80fh24' target=\"_blank\">magic-sweep-80</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt80fh24' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt80fh24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.50%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.50%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.50%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.322383/  2.317189, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.323500/  2.311199, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.318820/  2.316814, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:   9.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.325358/  2.328855, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:   9.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.318062/  2.313319, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:   9.91%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.321499/  2.314362, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.315455/  2.268324, val:  15.00%, val_best:  15.00%, tr:   9.19%, tr_best:   9.91%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.180608/  2.134505, val:  17.08%, val_best:  17.08%, tr:  17.88%, tr_best:  17.88%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.100813/  2.135898, val:  20.42%, val_best:  20.42%, tr:  16.14%, tr_best:  17.88%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.042522/  1.995835, val:  29.58%, val_best:  29.58%, tr:  25.13%, tr_best:  25.13%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.841108/  1.793968, val:  49.17%, val_best:  49.17%, tr:  35.96%, tr_best:  35.96%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.508608/  1.699548, val:  45.42%, val_best:  49.17%, tr:  54.85%, tr_best:  54.85%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.391127/  1.686904, val:  52.08%, val_best:  52.08%, tr:  58.94%, tr_best:  58.94%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.369375/  1.653307, val:  53.75%, val_best:  53.75%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.354509/  1.612083, val:  56.67%, val_best:  56.67%, tr:  65.47%, tr_best:  65.78%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.306321/  1.676125, val:  56.25%, val_best:  56.67%, tr:  65.37%, tr_best:  65.78%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.384850/  1.679016, val:  59.17%, val_best:  59.17%, tr:  63.23%, tr_best:  65.78%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.298179/  1.619424, val:  57.92%, val_best:  59.17%, tr:  65.99%, tr_best:  65.99%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.207502/  1.625028, val:  59.58%, val_best:  59.58%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.193554/  1.623667, val:  61.25%, val_best:  61.25%, tr:  68.34%, tr_best:  69.15%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.195642/  1.636339, val:  60.00%, val_best:  61.25%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.177093/  1.653186, val:  61.67%, val_best:  61.67%, tr:  69.66%, tr_best:  70.99%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.188159/  1.625109, val:  64.58%, val_best:  64.58%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.137910/  1.701173, val:  55.83%, val_best:  64.58%, tr:  74.57%, tr_best:  74.57%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.143706/  1.682936, val:  53.75%, val_best:  64.58%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  1.204027/  1.728420, val:  60.00%, val_best:  64.58%, tr:  74.16%, tr_best:  75.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.193172/  1.666521, val:  58.33%, val_best:  64.58%, tr:  72.42%, tr_best:  75.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.125662/  1.579560, val:  59.17%, val_best:  64.58%, tr:  72.83%, tr_best:  75.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  1.063540/  1.629887, val:  57.92%, val_best:  64.58%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  1.076575/  1.695539, val:  57.08%, val_best:  64.58%, tr:  72.73%, tr_best:  76.20%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  1.006850/  1.561093, val:  61.67%, val_best:  64.58%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.970893/  1.623089, val:  57.92%, val_best:  64.58%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.949605/  1.562135, val:  64.58%, val_best:  64.58%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.905947/  1.548807, val:  64.17%, val_best:  64.58%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.911107/  1.521059, val:  67.92%, val_best:  67.92%, tr:  83.25%, tr_best:  84.17%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.905974/  1.580433, val:  61.25%, val_best:  67.92%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.840682/  1.537777, val:  69.58%, val_best:  69.58%, tr:  87.84%, tr_best:  87.84%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.824363/  1.620064, val:  62.50%, val_best:  69.58%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.830066/  1.588301, val:  67.08%, val_best:  69.58%, tr:  88.36%, tr_best:  88.76%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.792399/  1.573581, val:  69.17%, val_best:  69.58%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.823262/  1.536942, val:  69.17%, val_best:  69.58%, tr:  89.17%, tr_best:  90.60%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.795075/  1.528695, val:  75.83%, val_best:  75.83%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.781421/  1.546168, val:  70.83%, val_best:  75.83%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.771664/  1.593226, val:  73.33%, val_best:  75.83%, tr:  90.19%, tr_best:  93.16%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.740026/  1.603766, val:  68.33%, val_best:  75.83%, tr:  91.93%, tr_best:  93.16%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.739235/  1.556854, val:  74.17%, val_best:  75.83%, tr:  91.73%, tr_best:  93.16%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.705188/  1.610810, val:  68.75%, val_best:  75.83%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.696513/  1.578881, val:  74.58%, val_best:  75.83%, tr:  93.56%, tr_best:  93.67%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.684200/  1.528120, val:  76.67%, val_best:  76.67%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.683544/  1.512608, val:  77.92%, val_best:  77.92%, tr:  94.28%, tr_best:  94.89%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.667024/  1.624207, val:  69.58%, val_best:  77.92%, tr:  94.08%, tr_best:  94.89%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.659039/  1.593258, val:  76.25%, val_best:  77.92%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.655289/  1.543795, val:  79.17%, val_best:  79.17%, tr:  94.28%, tr_best:  95.30%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.653109/  1.596109, val:  77.50%, val_best:  79.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.659283/  1.529913, val:  74.58%, val_best:  79.17%, tr:  94.89%, tr_best:  95.61%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.653963/  1.578164, val:  74.58%, val_best:  79.17%, tr:  93.56%, tr_best:  95.61%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.631414/  1.546707, val:  76.67%, val_best:  79.17%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.624241/  1.550186, val:  75.42%, val_best:  79.17%, tr:  95.61%, tr_best:  96.12%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.609920/  1.541590, val:  77.92%, val_best:  79.17%, tr:  95.40%, tr_best:  96.12%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.605118/  1.541180, val:  75.83%, val_best:  79.17%, tr:  95.91%, tr_best:  96.12%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.600266/  1.541662, val:  77.92%, val_best:  79.17%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.612122/  1.550332, val:  78.33%, val_best:  79.17%, tr:  95.61%, tr_best:  96.73%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.601750/  1.584315, val:  73.33%, val_best:  79.17%, tr:  95.61%, tr_best:  96.73%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.583847/  1.621726, val:  77.50%, val_best:  79.17%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.580674/  1.555484, val:  80.00%, val_best:  80.00%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.595449/  1.632006, val:  72.08%, val_best:  80.00%, tr:  95.61%, tr_best:  96.83%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.570527/  1.636491, val:  75.83%, val_best:  80.00%, tr:  96.73%, tr_best:  96.83%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.581123/  1.692166, val:  75.00%, val_best:  80.00%, tr:  94.89%, tr_best:  96.83%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.561458/  1.570534, val:  78.75%, val_best:  80.00%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.573990/  1.658221, val:  76.67%, val_best:  80.00%, tr:  96.63%, tr_best:  97.04%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.538350/  1.671167, val:  72.50%, val_best:  80.00%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.555138/  1.640437, val:  79.17%, val_best:  80.00%, tr:  96.73%, tr_best:  97.24%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.541379/  1.651770, val:  75.42%, val_best:  80.00%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.555120/  1.644072, val:  77.08%, val_best:  80.00%, tr:  96.63%, tr_best:  97.45%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.501623/  1.649986, val:  75.83%, val_best:  80.00%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.528621/  1.625576, val:  79.17%, val_best:  80.00%, tr:  97.45%, tr_best:  97.96%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.528035/  1.705443, val:  72.08%, val_best:  80.00%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.527868/  1.641335, val:  77.50%, val_best:  80.00%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.535077/  1.627592, val:  81.25%, val_best:  81.25%, tr:  97.75%, tr_best:  98.16%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.499970/  1.694657, val:  79.58%, val_best:  81.25%, tr:  97.14%, tr_best:  98.16%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.478581/  1.713647, val:  75.83%, val_best:  81.25%, tr:  97.55%, tr_best:  98.16%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.531619/  1.710131, val:  77.08%, val_best:  81.25%, tr:  96.02%, tr_best:  98.16%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.494049/  1.728090, val:  77.50%, val_best:  81.25%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.469285/  1.717641, val:  78.75%, val_best:  81.25%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.486246/  1.702885, val:  78.75%, val_best:  81.25%, tr:  97.85%, tr_best:  98.16%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.460152/  1.756738, val:  73.75%, val_best:  81.25%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.436250/  1.706214, val:  82.08%, val_best:  82.08%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.439152/  1.762965, val:  77.50%, val_best:  82.08%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.470272/  1.808247, val:  74.17%, val_best:  82.08%, tr:  98.06%, tr_best:  98.77%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.453131/  1.802458, val:  76.67%, val_best:  82.08%, tr:  98.67%, tr_best:  98.77%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.429948/  1.810816, val:  78.33%, val_best:  82.08%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.452710/  1.734253, val:  77.08%, val_best:  82.08%, tr:  97.85%, tr_best:  98.77%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.425422/  1.862060, val:  75.00%, val_best:  82.08%, tr:  98.57%, tr_best:  98.77%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.445539/  1.804101, val:  74.58%, val_best:  82.08%, tr:  98.47%, tr_best:  98.77%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadc439cefa448edb60bc9517c2dc693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▁▁▂▂▂▅▅▇▆█▆▅▇▇▇█▇▇████████▇██▇███▇███</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▂▄▅▅▆▆▅▆▆▆▆▇▇▇▇▇▇▇█▇▇██▇▇██▇████▇█</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▁▁▂▅▅▅▆▆▆▆▆▆▇▇▇▇▇███████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██████▇▅▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▂▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▂▄▅▅▆▆▅▆▆▆▆▇▇▇▇▇▇▇█▇▇██▇▇██▇████▇█</td></tr><tr><td>val_loss</td><td>█████▇▆▂▂▂▂▂▃▂▂▂▁▁▂▁▂▂▁▂▂▁▁▁▁▂▂▂▂▃▂▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98468</td></tr><tr><td>tr_epoch_loss</td><td>0.44554</td></tr><tr><td>val_acc_best</td><td>0.82083</td></tr><tr><td>val_acc_now</td><td>0.74583</td></tr><tr><td>val_loss</td><td>1.8041</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-80</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt80fh24' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt80fh24</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_184857-mt80fh24/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1lo994z3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_185447-1lo994z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1lo994z3' target=\"_blank\">wild-sweep-82</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1lo994z3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1lo994z3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.041885/  2.076352, val:  44.58%, val_best:  44.58%, tr:  31.15%, tr_best:  31.15%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.919939/  2.017878, val:  50.42%, val_best:  50.42%, tr:  46.07%, tr_best:  46.07%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.102597/  2.440950, val:  49.58%, val_best:  50.42%, tr:  52.40%, tr_best:  52.40%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.667467/  1.858588, val:  55.83%, val_best:  55.83%, tr:  56.69%, tr_best:  56.69%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.537556/  1.756109, val:  60.42%, val_best:  60.42%, tr:  56.28%, tr_best:  56.69%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.613584/  1.677691, val:  52.50%, val_best:  60.42%, tr:  60.47%, tr_best:  60.47%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.219598/  2.217947, val:  54.58%, val_best:  60.42%, tr:  65.27%, tr_best:  65.27%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.534276/  2.350023, val:  54.17%, val_best:  60.42%, tr:  64.04%, tr_best:  65.27%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.462811/  1.646837, val:  59.17%, val_best:  60.42%, tr:  64.45%, tr_best:  65.27%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.638760/  2.248833, val:  61.25%, val_best:  61.25%, tr:  63.84%, tr_best:  65.27%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.506150/  1.730243, val:  57.50%, val_best:  61.25%, tr:  64.86%, tr_best:  65.27%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.442244/  2.298291, val:  54.58%, val_best:  61.25%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.472599/  1.507917, val:  63.75%, val_best:  63.75%, tr:  68.64%, tr_best:  69.97%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.172953/  2.065725, val:  53.75%, val_best:  63.75%, tr:  71.50%, tr_best:  71.50%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.187004/  2.617172, val:  57.08%, val_best:  63.75%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.019180/  1.529975, val:  62.08%, val_best:  63.75%, tr:  75.49%, tr_best:  75.49%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.977326/  1.602777, val:  62.50%, val_best:  63.75%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.773259/  2.039030, val:  66.25%, val_best:  66.25%, tr:  80.29%, tr_best:  80.29%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.852314/  1.690216, val:  66.25%, val_best:  66.25%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.763153/  1.838111, val:  63.33%, val_best:  66.25%, tr:  82.84%, tr_best:  82.84%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.293844/  1.955798, val:  62.92%, val_best:  66.25%, tr:  77.22%, tr_best:  82.84%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.834789/  1.826028, val:  66.67%, val_best:  66.67%, tr:  82.53%, tr_best:  82.84%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.641562/  2.204953, val:  59.17%, val_best:  66.67%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.641878/  2.010458, val:  64.17%, val_best:  66.67%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.496328/  1.684636, val:  72.92%, val_best:  72.92%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.470440/  1.879090, val:  67.08%, val_best:  72.92%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.484821/  2.185425, val:  62.92%, val_best:  72.92%, tr:  90.30%, tr_best:  91.62%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.469518/  1.920905, val:  67.08%, val_best:  72.92%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.580922/  1.845344, val:  70.00%, val_best:  72.92%, tr:  89.68%, tr_best:  92.44%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.349734/  1.847944, val:  72.08%, val_best:  72.92%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.390830/  1.917486, val:  67.92%, val_best:  72.92%, tr:  94.89%, tr_best:  97.34%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.370476/  1.983282, val:  66.25%, val_best:  72.92%, tr:  94.59%, tr_best:  97.34%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.450989/  2.403383, val:  62.50%, val_best:  72.92%, tr:  91.93%, tr_best:  97.34%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.331856/  2.029486, val:  71.25%, val_best:  72.92%, tr:  96.32%, tr_best:  97.34%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.300893/  2.117865, val:  67.50%, val_best:  72.92%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.213517/  2.129124, val:  68.33%, val_best:  72.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.195204/  2.128897, val:  68.33%, val_best:  72.92%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.281512/  2.445825, val:  63.75%, val_best:  72.92%, tr:  97.04%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.298922/  2.171203, val:  73.75%, val_best:  73.75%, tr:  97.14%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.211939/  2.138402, val:  70.83%, val_best:  73.75%, tr:  98.67%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.157011/  2.191360, val:  69.17%, val_best:  73.75%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.149140/  2.223831, val:  73.33%, val_best:  73.75%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.109453/  2.168082, val:  73.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.144011/  2.309118, val:  69.17%, val_best:  73.75%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.116811/  2.357661, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.091270/  2.325072, val:  71.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.072078/  2.323997, val:  72.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.078969/  2.340367, val:  75.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.084592/  2.406011, val:  74.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.063512/  2.403329, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.056010/  2.475352, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.049399/  2.501791, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.049920/  2.522440, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.042104/  2.577947, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.036562/  2.574369, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.036301/  2.584780, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.039251/  2.618756, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.035480/  2.638052, val:  74.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.033687/  2.627846, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.029900/  2.705202, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.028228/  2.700732, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.020516/  2.698668, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.025816/  2.798197, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.024394/  2.829085, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.026720/  2.756153, val:  73.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.028615/  2.779100, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.020818/  2.837780, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.018184/  2.776603, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.018014/  2.830963, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.020884/  2.851723, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.020474/  2.856428, val:  74.58%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.027082/  2.854678, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.021314/  2.852993, val:  72.50%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.018183/  2.887333, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014834/  2.931038, val:  73.75%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.012323/  2.920913, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.014243/  2.954454, val:  72.92%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.013997/  2.966697, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.013793/  2.982874, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.020407/  2.980649, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.014870/  3.022682, val:  73.33%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.012118/  2.974615, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.011642/  2.995658, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.011557/  3.004260, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.012427/  2.987448, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010530/  3.026885, val:  72.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.010753/  3.040316, val:  72.50%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.009324/  3.031390, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010316/  3.038022, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.010692/  3.100129, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.008319/  3.065827, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.011018/  3.091519, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.009288/  3.082875, val:  71.67%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.009084/  3.095332, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.007007/  3.105630, val:  74.58%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.005500/  3.090611, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.006852/  3.137234, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.007340/  3.121943, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.008452/  3.162018, val:  73.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.008242/  3.173137, val:  74.17%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0d4ec852fa46178013249a2d15bd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▂▄▃▅▇▅▆▆███▇▇▇████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▅▃▅▅▄▆▅▆▇▅▇▅▆▅▇▇▇█▇█▇█▇████▇▇█▇██▇█▇██</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▅▅▆▆▆▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▆▆▆▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▅▅▅▅▅▆▆▆▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▅▃▅▅▄▆▅▆▇▅▇▅▆▅▇▇▇█▇█▇█▇████▇▇█▇██▇█▇██</td></tr><tr><td>val_loss</td><td>▃▅▂▅▄▁▆▃▂▂▂▄▂▅▄▅▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00824</td></tr><tr><td>val_acc_best</td><td>0.75833</td></tr><tr><td>val_acc_now</td><td>0.74167</td></tr><tr><td>val_loss</td><td>3.17314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sweep-82</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1lo994z3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1lo994z3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_185447-1lo994z3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rjit5ts1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_185955-rjit5ts1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rjit5ts1' target=\"_blank\">comfy-sweep-84</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rjit5ts1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rjit5ts1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:147.418854/247.807343, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:146.459976/136.682892, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.11%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:133.448166/244.129166, val:  10.00%, val_best:  10.00%, tr:  11.64%, tr_best:  11.64%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:175.267227/142.461990, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  11.64%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:149.178558/128.590622, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.64%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:148.627426/174.332214, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  11.64%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:136.613708/131.862518, val:  10.00%, val_best:  10.00%, tr:  11.64%, tr_best:  11.64%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:143.258835/ 99.791397, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.64%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:137.292038/126.267761, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  11.64%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:147.303070/136.062500, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  11.64%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:117.101883/ 99.583748, val:  16.25%, val_best:  16.25%, tr:  11.03%, tr_best:  11.64%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:114.930443/ 63.553810, val:  16.67%, val_best:  16.67%, tr:  12.67%, tr_best:  12.67%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 76.414619/100.446800, val:  18.75%, val_best:  18.75%, tr:  17.67%, tr_best:  17.67%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 68.039261/ 96.007416, val:  18.33%, val_best:  18.75%, tr:  19.41%, tr_best:  19.41%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 79.865150/ 87.575005, val:  18.33%, val_best:  18.75%, tr:  21.04%, tr_best:  21.04%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 66.308838/ 90.499786, val:  10.00%, val_best:  18.75%, tr:  21.35%, tr_best:  21.35%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 81.774635/ 79.141022, val:  18.75%, val_best:  18.75%, tr:  17.88%, tr_best:  21.35%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 66.979050/ 46.205742, val:  27.50%, val_best:  27.50%, tr:  23.70%, tr_best:  23.70%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 70.011070/ 89.157471, val:  19.17%, val_best:  27.50%, tr:  21.96%, tr_best:  23.70%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 72.829170/ 79.217056, val:  19.58%, val_best:  27.50%, tr:  22.17%, tr_best:  23.70%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 64.013893/ 66.389565, val:  27.08%, val_best:  27.50%, tr:  22.27%, tr_best:  23.70%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 68.953720/ 40.822483, val:  23.33%, val_best:  27.50%, tr:  23.29%, tr_best:  23.70%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 69.917389/ 79.092201, val:  21.25%, val_best:  27.50%, tr:  23.60%, tr_best:  23.70%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 61.562107/141.595551, val:  15.83%, val_best:  27.50%, tr:  25.64%, tr_best:  25.64%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 69.220589/ 87.432610, val:  25.00%, val_best:  27.50%, tr:  21.96%, tr_best:  25.64%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 60.502762/ 67.337646, val:  24.17%, val_best:  27.50%, tr:  23.60%, tr_best:  25.64%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 59.222942/ 51.032372, val:  12.50%, val_best:  27.50%, tr:  22.57%, tr_best:  25.64%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 58.674351/ 67.627342, val:  20.00%, val_best:  27.50%, tr:  24.00%, tr_best:  25.64%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 59.795677/ 61.356445, val:  27.08%, val_best:  27.50%, tr:  24.41%, tr_best:  25.64%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 60.382458/ 86.617928, val:  22.50%, val_best:  27.50%, tr:  25.33%, tr_best:  25.64%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 58.176414/ 58.496941, val:  24.17%, val_best:  27.50%, tr:  25.03%, tr_best:  25.64%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 69.234306/ 34.771275, val:  24.58%, val_best:  27.50%, tr:  26.05%, tr_best:  26.05%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 56.163807/ 86.124931, val:  26.67%, val_best:  27.50%, tr:  24.31%, tr_best:  26.05%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 56.569828/ 70.187469, val:  18.75%, val_best:  27.50%, tr:  25.64%, tr_best:  26.05%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 63.599136/ 73.234642, val:  23.75%, val_best:  27.50%, tr:  24.11%, tr_best:  26.05%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 49.387825/ 52.906498, val:  25.00%, val_best:  27.50%, tr:  25.84%, tr_best:  26.05%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 53.595428/114.924454, val:  24.17%, val_best:  27.50%, tr:  25.33%, tr_best:  26.05%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 89.530365/116.030777, val:  19.58%, val_best:  27.50%, tr:  20.74%, tr_best:  26.05%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss: 59.764168/ 58.505116, val:  26.67%, val_best:  27.50%, tr:  26.86%, tr_best:  26.86%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 83.533264/ 84.172470, val:  25.42%, val_best:  27.50%, tr:  24.72%, tr_best:  26.86%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss: 66.861938/ 77.699486, val:  21.67%, val_best:  27.50%, tr:  24.62%, tr_best:  26.86%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss: 68.578415/ 47.496960, val:  27.08%, val_best:  27.50%, tr:  26.15%, tr_best:  26.86%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 71.044365/ 56.993877, val:  25.42%, val_best:  27.50%, tr:  26.46%, tr_best:  26.86%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss: 53.440582/ 53.309338, val:  25.42%, val_best:  27.50%, tr:  27.68%, tr_best:  27.68%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss: 53.707706/ 71.061211, val:  25.42%, val_best:  27.50%, tr:  30.95%, tr_best:  30.95%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss: 60.427826/ 88.532097, val:  26.25%, val_best:  27.50%, tr:  26.46%, tr_best:  30.95%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss: 73.208649/106.923767, val:  26.67%, val_best:  27.50%, tr:  24.41%, tr_best:  30.95%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 73.590775/ 97.625549, val:  22.08%, val_best:  27.50%, tr:  26.66%, tr_best:  30.95%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 67.755096/ 63.255421, val:  25.42%, val_best:  27.50%, tr:  25.13%, tr_best:  30.95%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss: 57.277042/ 57.657299, val:  20.83%, val_best:  27.50%, tr:  25.33%, tr_best:  30.95%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss: 62.827335/ 86.220764, val:  27.92%, val_best:  27.92%, tr:  24.82%, tr_best:  30.95%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss: 67.004082/ 69.584610, val:  27.08%, val_best:  27.92%, tr:  25.23%, tr_best:  30.95%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss: 51.930058/ 46.445782, val:  20.00%, val_best:  27.92%, tr:  26.56%, tr_best:  30.95%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss: 56.907703/ 60.369339, val:  26.25%, val_best:  27.92%, tr:  27.27%, tr_best:  30.95%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss: 54.232815/ 36.153812, val:  25.00%, val_best:  27.92%, tr:  27.99%, tr_best:  30.95%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss: 49.137096/ 71.152092, val:  24.17%, val_best:  27.92%, tr:  27.17%, tr_best:  30.95%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 70.023346/ 69.974304, val:  22.50%, val_best:  27.92%, tr:  24.00%, tr_best:  30.95%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss: 65.934975/ 43.851048, val:  25.00%, val_best:  27.92%, tr:  29.83%, tr_best:  30.95%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss: 58.252346/ 63.945423, val:  25.00%, val_best:  27.92%, tr:  26.05%, tr_best:  30.95%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss: 77.700188/111.266205, val:  20.00%, val_best:  27.92%, tr:  26.76%, tr_best:  30.95%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss: 71.853088/ 60.593163, val:  24.58%, val_best:  27.92%, tr:  27.58%, tr_best:  30.95%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss: 67.190811/ 50.092827, val:  27.08%, val_best:  27.92%, tr:  24.82%, tr_best:  30.95%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss: 68.527634/ 53.867481, val:  18.75%, val_best:  27.92%, tr:  26.35%, tr_best:  30.95%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss: 51.253586/ 51.067986, val:  15.83%, val_best:  27.92%, tr:  26.46%, tr_best:  30.95%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss: 57.070702/ 62.780003, val:  19.58%, val_best:  27.92%, tr:  25.54%, tr_best:  30.95%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss: 62.384651/ 98.120674, val:  25.42%, val_best:  27.92%, tr:  27.17%, tr_best:  30.95%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss: 78.079948/109.894058, val:  18.75%, val_best:  27.92%, tr:  25.64%, tr_best:  30.95%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss: 78.375092/ 66.633713, val:  25.42%, val_best:  27.92%, tr:  29.01%, tr_best:  30.95%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss: 68.083748/ 80.340309, val:  16.67%, val_best:  27.92%, tr:  25.23%, tr_best:  30.95%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss: 65.202820/ 56.747452, val:  18.75%, val_best:  27.92%, tr:  26.35%, tr_best:  30.95%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss: 55.658478/ 75.969292, val:  27.92%, val_best:  27.92%, tr:  26.46%, tr_best:  30.95%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss: 74.894142/ 65.952477, val:  24.17%, val_best:  27.92%, tr:  23.08%, tr_best:  30.95%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss: 64.089241/139.544678, val:  13.33%, val_best:  27.92%, tr:  25.74%, tr_best:  30.95%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss: 76.766479/ 80.945946, val:  14.17%, val_best:  27.92%, tr:  25.43%, tr_best:  30.95%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss: 53.691425/ 60.142124, val:  27.92%, val_best:  27.92%, tr:  26.86%, tr_best:  30.95%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss: 56.646477/ 55.819756, val:  25.42%, val_best:  27.92%, tr:  28.19%, tr_best:  30.95%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss: 59.124699/ 61.784439, val:  27.08%, val_best:  27.92%, tr:  27.07%, tr_best:  30.95%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss: 67.492386/ 40.826355, val:  24.58%, val_best:  27.92%, tr:  26.25%, tr_best:  30.95%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss: 69.278687/100.503258, val:  11.67%, val_best:  27.92%, tr:  25.84%, tr_best:  30.95%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss: 75.987381/ 93.763023, val:  25.00%, val_best:  27.92%, tr:  25.94%, tr_best:  30.95%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss: 73.661789/ 69.094017, val:  24.17%, val_best:  27.92%, tr:  28.40%, tr_best:  30.95%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss: 62.094978/ 58.734707, val:  20.00%, val_best:  27.92%, tr:  25.84%, tr_best:  30.95%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss: 65.360275/ 87.979263, val:  26.25%, val_best:  27.92%, tr:  25.43%, tr_best:  30.95%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss: 54.510582/ 60.152981, val:  25.00%, val_best:  27.92%, tr:  27.68%, tr_best:  30.95%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss: 71.288017/ 58.295174, val:  20.00%, val_best:  27.92%, tr:  25.03%, tr_best:  30.95%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss: 54.734035/ 84.574196, val:  10.00%, val_best:  27.92%, tr:  25.03%, tr_best:  30.95%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss: 62.238838/ 82.388618, val:  17.08%, val_best:  27.92%, tr:  26.25%, tr_best:  30.95%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss: 62.076595/ 70.610573, val:  22.08%, val_best:  27.92%, tr:  27.37%, tr_best:  30.95%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss: 57.805943/ 46.221405, val:  28.75%, val_best:  28.75%, tr:  28.40%, tr_best:  30.95%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss: 67.219383/103.216026, val:  19.17%, val_best:  28.75%, tr:  27.48%, tr_best:  30.95%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss: 61.817024/ 86.574005, val:  25.83%, val_best:  28.75%, tr:  26.46%, tr_best:  30.95%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss: 53.664642/114.896324, val:  22.08%, val_best:  28.75%, tr:  27.48%, tr_best:  30.95%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss: 66.065651/ 65.756271, val:  27.08%, val_best:  28.75%, tr:  26.56%, tr_best:  30.95%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss: 63.140766/101.597122, val:  18.33%, val_best:  28.75%, tr:  27.58%, tr_best:  30.95%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss: 50.440430/ 72.546532, val:  22.08%, val_best:  28.75%, tr:  28.91%, tr_best:  30.95%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss: 56.866501/ 58.776527, val:  25.83%, val_best:  28.75%, tr:  26.05%, tr_best:  30.95%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss: 69.609360/ 83.387367, val:  23.75%, val_best:  28.75%, tr:  27.58%, tr_best:  30.95%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss: 67.303940/ 89.284271, val:  23.75%, val_best:  28.75%, tr:  26.66%, tr_best:  30.95%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss: 72.832550/ 82.406586, val:  21.67%, val_best:  28.75%, tr:  26.97%, tr_best:  30.95%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss: 56.363007/ 76.511765, val:  17.50%, val_best:  28.75%, tr:  26.56%, tr_best:  30.95%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c168b772ee864b5599c503e447518cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▁▃▁▂▅▆▃▅▅▁▅▅▆██▃█▂▅▇█▃█▆▅▅█▆▂▆▇▂▃▇█▆▇█</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▄▄█▅▆▇▂▆█▇▅▇▇▇▆▅▅▇▇▅█▇▇█▂▇▇▇▇▅▆▅█▇▆</td></tr><tr><td>tr_acc</td><td>▁▂▁▁▁▄▅▆▅▆▅▅▆▆▆▅▆▇█▇▆▇▇█▇▆▇▇▇▆▇▇▆▆▆▇▇▇▆▇</td></tr><tr><td>tr_epoch_loss</td><td>█▇███▃▃▂▃▂▂▂▂▁▁▄▃▃▁▃▂▁▁▂▃▂▂▃▁▂▂▂▃▂▃▂▂▂▂▂</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▄▄█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▄▄█▅▆▇▂▆█▇▅▇▇▇▆▅▅▇▇▅█▇▇█▂▇▇▇▇▅▆▅█▇▆</td></tr><tr><td>val_loss</td><td>██▄▃▄▃▃▁▂▁▃▁▃▃▂▄▃▂▂▃▂▁▁▁▃▁▃▂▂▄▂▁▃▃▂▂▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.33333</td></tr><tr><td>tr_acc</td><td>0.26558</td></tr><tr><td>tr_epoch_loss</td><td>56.36301</td></tr><tr><td>val_acc_best</td><td>0.2875</td></tr><tr><td>val_acc_now</td><td>0.175</td></tr><tr><td>val_loss</td><td>76.51176</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-sweep-84</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rjit5ts1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rjit5ts1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_185955-rjit5ts1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c8um6ujr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_190503-c8um6ujr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c8um6ujr' target=\"_blank\">glamorous-sweep-86</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c8um6ujr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c8um6ujr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.999686/  1.443269, val:  55.83%, val_best:  55.83%, tr:  26.46%, tr_best:  26.46%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.446745/  1.639690, val:  47.92%, val_best:  55.83%, tr:  49.85%, tr_best:  49.85%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.496911/  1.344767, val:  57.08%, val_best:  57.08%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.258478/  1.609094, val:  55.00%, val_best:  57.08%, tr:  59.75%, tr_best:  59.75%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.148987/  1.349512, val:  61.25%, val_best:  61.25%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.049624/  1.384556, val:  55.83%, val_best:  61.25%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.125850/  1.398055, val:  61.25%, val_best:  61.25%, tr:  66.91%, tr_best:  68.74%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.046312/  1.531923, val:  57.92%, val_best:  61.25%, tr:  67.42%, tr_best:  68.74%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.149443/  1.598951, val:  59.58%, val_best:  61.25%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.097604/  1.202135, val:  64.58%, val_best:  64.58%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.032436/  1.231417, val:  58.75%, val_best:  64.58%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.941776/  1.332646, val:  67.08%, val_best:  67.08%, tr:  73.34%, tr_best:  73.34%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.852984/  1.335994, val:  60.42%, val_best:  67.08%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.756575/  1.635168, val:  60.00%, val_best:  67.08%, tr:  78.65%, tr_best:  78.65%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.926091/  1.603417, val:  61.25%, val_best:  67.08%, tr:  75.69%, tr_best:  78.65%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.656483/  1.396220, val:  62.50%, val_best:  67.08%, tr:  78.55%, tr_best:  78.65%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.673060/  1.552729, val:  63.75%, val_best:  67.08%, tr:  81.92%, tr_best:  81.92%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.692860/  1.586314, val:  60.42%, val_best:  67.08%, tr:  81.00%, tr_best:  81.92%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.631848/  1.872030, val:  62.08%, val_best:  67.08%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.626084/  1.805792, val:  60.42%, val_best:  67.08%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.891784/  1.617710, val:  61.67%, val_best:  67.08%, tr:  81.31%, tr_best:  83.86%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.606564/  1.923553, val:  65.00%, val_best:  67.08%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.513770/  1.593437, val:  65.00%, val_best:  67.08%, tr:  85.90%, tr_best:  85.90%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.465005/  1.859224, val:  61.25%, val_best:  67.08%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.410126/  1.712041, val:  65.42%, val_best:  67.08%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.334608/  1.667510, val:  67.08%, val_best:  67.08%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.421603/  1.804300, val:  62.92%, val_best:  67.08%, tr:  90.50%, tr_best:  95.10%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.329035/  1.724266, val:  67.92%, val_best:  67.92%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.435199/  1.674565, val:  62.92%, val_best:  67.92%, tr:  89.89%, tr_best:  95.20%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.330799/  1.740487, val:  67.08%, val_best:  67.92%, tr:  94.38%, tr_best:  95.20%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.265169/  1.776570, val:  67.08%, val_best:  67.92%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.253489/  1.739705, val:  69.58%, val_best:  69.58%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.258629/  1.858797, val:  64.17%, val_best:  69.58%, tr:  96.53%, tr_best:  97.04%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.250520/  1.825562, val:  68.33%, val_best:  69.58%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.239328/  1.766580, val:  72.92%, val_best:  72.92%, tr:  96.73%, tr_best:  97.45%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.173431/  1.939406, val:  65.83%, val_best:  72.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.171445/  1.883686, val:  66.67%, val_best:  72.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.242129/  2.067839, val:  62.50%, val_best:  72.92%, tr:  96.63%, tr_best:  99.49%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.199733/  1.862255, val:  69.17%, val_best:  72.92%, tr:  98.77%, tr_best:  99.49%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.148036/  2.015227, val:  69.58%, val_best:  72.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.142084/  2.081519, val:  66.67%, val_best:  72.92%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.173006/  1.938175, val:  70.00%, val_best:  72.92%, tr:  98.16%, tr_best:  99.59%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.108047/  2.001409, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.115727/  2.120776, val:  67.08%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.133593/  2.040775, val:  70.42%, val_best:  72.92%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.090667/  2.118294, val:  66.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.087745/  2.088645, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.081788/  2.110461, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.072990/  2.106360, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.071195/  2.124622, val:  72.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.064020/  2.177039, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.063191/  2.234328, val:  72.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.052728/  2.210561, val:  73.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.044475/  2.234064, val:  73.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.047866/  2.260487, val:  70.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.044000/  2.331239, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.046795/  2.363945, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.044055/  2.323532, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.044679/  2.375830, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.037777/  2.416205, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.030638/  2.439295, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.029241/  2.419820, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.038564/  2.449210, val:  71.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.029916/  2.502012, val:  70.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.045555/  2.519877, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.038074/  2.460158, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.028537/  2.497388, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.024928/  2.500271, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.025909/  2.576677, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.038949/  2.571659, val:  70.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.022846/  2.658153, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.021909/  2.635533, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.018922/  2.641749, val:  71.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.023825/  2.623178, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.024247/  2.654109, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.025474/  2.734802, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.024105/  2.711988, val:  71.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.017831/  2.678671, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.019012/  2.660959, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.020542/  2.728856, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.018792/  2.688295, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.014795/  2.692259, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.015470/  2.750524, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.012079/  2.706323, val:  73.75%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.015822/  2.760613, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.014840/  2.773698, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.015897/  2.736811, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.015168/  2.788980, val:  70.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.016165/  2.788911, val:  72.92%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.012837/  2.755102, val:  73.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.012397/  2.769315, val:  73.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.012552/  2.829739, val:  71.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.011079/  2.835128, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.014094/  2.838702, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.013126/  2.887403, val:  71.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.014180/  2.853441, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.011442/  2.893660, val:  71.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008217/  2.876884, val:  72.08%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.009302/  2.971213, val:  70.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009507/  2.902197, val:  70.83%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6ad5ccce794fd690b0735c9d625297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▃▁▆▇▇▇▇███▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▂▄▃▃▃▃▄▅▄▅▄▅▄▆▆▇███▇▇▇▇▇▇▇▇▇█▇█▇▇█▇▇▇</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▆▆▇▇▇▇███████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▄▅▅▅▅▅▅▅▆▆██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▂▄▃▃▃▃▄▅▄▅▄▅▄▆▆▇███▇▇▇▇▇▇▇▇▇█▇█▇▇█▇▇▇</td></tr><tr><td>val_loss</td><td>▂▂▂▂▁▂▃▃▄▄▃▄▃▄▄▅▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00951</td></tr><tr><td>val_acc_best</td><td>0.74167</td></tr><tr><td>val_acc_now</td><td>0.70833</td></tr><tr><td>val_loss</td><td>2.9022</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-sweep-86</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c8um6ujr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/c8um6ujr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_190503-c8um6ujr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cg0mxj5w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_191016-cg0mxj5w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cg0mxj5w' target=\"_blank\">dauntless-sweep-88</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cg0mxj5w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cg0mxj5w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.069252/  2.328336, val:  46.25%, val_best:  46.25%, tr:  26.86%, tr_best:  26.86%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.768642/  2.020372, val:  50.83%, val_best:  50.83%, tr:  54.24%, tr_best:  54.24%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.786851/  1.980916, val:  55.42%, val_best:  55.42%, tr:  57.92%, tr_best:  57.92%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.500423/  2.698007, val:  52.08%, val_best:  55.42%, tr:  65.88%, tr_best:  65.88%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.654534/  1.459645, val:  63.75%, val_best:  63.75%, tr:  63.02%, tr_best:  65.88%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.279347/  1.866377, val:  59.58%, val_best:  63.75%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.226794/  1.809686, val:  61.25%, val_best:  63.75%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.596627/  2.523188, val:  57.50%, val_best:  63.75%, tr:  67.62%, tr_best:  71.09%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.281679/  1.721010, val:  59.17%, val_best:  63.75%, tr:  72.22%, tr_best:  72.22%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.593717/  2.437286, val:  55.00%, val_best:  63.75%, tr:  70.48%, tr_best:  72.22%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.219825/  1.865303, val:  62.50%, val_best:  63.75%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.310496/  2.056555, val:  66.25%, val_best:  66.25%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.292731/  1.820960, val:  73.75%, val_best:  73.75%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.915101/  2.055346, val:  66.67%, val_best:  73.75%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.287546/  2.474879, val:  67.08%, val_best:  73.75%, tr:  81.00%, tr_best:  81.41%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.955813/  2.708978, val:  62.08%, val_best:  73.75%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.970072/  2.145620, val:  72.50%, val_best:  73.75%, tr:  84.47%, tr_best:  84.47%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.812911/  2.369860, val:  69.17%, val_best:  73.75%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.810546/  2.333127, val:  74.58%, val_best:  74.58%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.691344/  2.065144, val:  75.42%, val_best:  75.42%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.647877/  2.500495, val:  75.42%, val_best:  75.42%, tr:  92.54%, tr_best:  93.26%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.584294/  2.454709, val:  71.67%, val_best:  75.42%, tr:  96.22%, tr_best:  96.22%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.534521/  2.698519, val:  74.17%, val_best:  75.42%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.570586/  2.474756, val:  80.42%, val_best:  80.42%, tr:  96.12%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.543966/  2.796560, val:  78.75%, val_best:  80.42%, tr:  95.81%, tr_best:  97.04%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.486127/  2.733544, val:  77.50%, val_best:  80.42%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.530425/  2.477382, val:  85.42%, val_best:  85.42%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.389686/  2.979827, val:  76.67%, val_best:  85.42%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.347348/  2.655748, val:  84.58%, val_best:  85.42%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.349823/  3.315695, val:  74.17%, val_best:  85.42%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.409071/  3.121979, val:  78.33%, val_best:  85.42%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.393849/  2.907233, val:  83.33%, val_best:  85.42%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.411001/  3.320042, val:  78.75%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.335797/  3.428208, val:  78.33%, val_best:  85.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.319532/  3.201770, val:  83.75%, val_best:  85.42%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.292220/  3.447205, val:  82.08%, val_best:  85.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.243522/  3.670194, val:  80.42%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.242666/  3.490338, val:  82.08%, val_best:  85.42%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.227026/  3.609593, val:  81.25%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.236108/  3.822099, val:  79.58%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.222989/  3.785275, val:  79.58%, val_best:  85.42%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.174609/  3.738874, val:  82.92%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.177330/  4.186685, val:  80.83%, val_best:  85.42%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.182485/  4.097248, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.181551/  4.281297, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.166471/  4.254925, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.131105/  4.053175, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.149317/  4.232992, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.175207/  4.183161, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.157678/  4.363027, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.137635/  4.568342, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.121017/  4.359985, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.129241/  4.401690, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.094423/  4.502363, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.091759/  4.804368, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.105703/  4.739372, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.136810/  4.880136, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.101860/  4.805092, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.094461/  4.705267, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.081921/  4.918841, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.103162/  4.982613, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.090069/  4.901466, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.105273/  5.229890, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.094725/  5.163425, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.095885/  5.181915, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.095623/  4.913156, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.072780/  5.306148, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.088047/  5.036472, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.072663/  5.312224, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.094675/  5.138620, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.094823/  5.337319, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.101935/  5.345850, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.086865/  5.153520, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.074433/  5.180137, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.077038/  5.262630, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.064234/  5.348071, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.062403/  5.555963, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.055162/  5.473686, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.093334/  5.528257, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.066577/  5.393158, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.062377/  5.785596, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.053936/  5.673056, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.045569/  5.630235, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.051848/  5.572761, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.062259/  5.609038, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.078806/  5.456686, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.051223/  5.442860, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.046819/  5.811224, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.037472/  5.826153, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.043389/  5.553918, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.037672/  5.658308, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.042821/  5.752028, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.046500/  5.812055, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.045470/  5.890929, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.036495/  6.102860, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.032535/  5.979592, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.048866/  5.997090, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.055333/  5.992412, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.035469/  5.946015, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.030773/  5.988231, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c13ed151ba04f58b2690ee901223b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅▅▃▆▆▅▆███████▇███████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▃▆▅▅▆▆▇█▆▇▇▇▇▇▇▇▇█▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▇▆▆▅▅▄▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▆▆▆▆▆▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▃▆▅▅▆▆▇█▆▇▇▇▇▇▇▇▇█▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▂▂▁▃▃▂▃▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.03077</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>5.98823</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-88</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cg0mxj5w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cg0mxj5w</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_191016-cg0mxj5w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3rfrn3ex with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_191602-3rfrn3ex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3rfrn3ex' target=\"_blank\">atomic-sweep-90</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3rfrn3ex' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3rfrn3ex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.092075/  1.762272, val:  50.42%, val_best:  50.42%, tr:  24.51%, tr_best:  24.51%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.679218/  2.136719, val:  46.67%, val_best:  50.42%, tr:  50.46%, tr_best:  50.46%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.872246/  1.753039, val:  55.83%, val_best:  55.83%, tr:  51.99%, tr_best:  51.99%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.401726/  2.085544, val:  53.75%, val_best:  55.83%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.383739/  1.440776, val:  63.75%, val_best:  63.75%, tr:  61.80%, tr_best:  63.02%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.290029/  1.616892, val:  54.17%, val_best:  63.75%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.325820/  1.749264, val:  57.08%, val_best:  63.75%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.301068/  2.040351, val:  57.50%, val_best:  63.75%, tr:  65.58%, tr_best:  66.50%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.105743/  1.344741, val:  65.00%, val_best:  65.00%, tr:  70.07%, tr_best:  70.07%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.350402/  1.473022, val:  67.08%, val_best:  67.08%, tr:  68.95%, tr_best:  70.07%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.198754/  1.363583, val:  65.83%, val_best:  67.08%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.106728/  1.500183, val:  68.75%, val_best:  68.75%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.022646/  1.354155, val:  71.67%, val_best:  71.67%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.821753/  1.655112, val:  63.33%, val_best:  71.67%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.834057/  1.658195, val:  73.33%, val_best:  73.33%, tr:  82.43%, tr_best:  83.25%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.595545/  1.452603, val:  74.58%, val_best:  74.58%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.636681/  1.420200, val:  80.83%, val_best:  80.83%, tr:  90.19%, tr_best:  91.42%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.576926/  1.558800, val:  76.67%, val_best:  80.83%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.511288/  1.654796, val:  80.00%, val_best:  80.83%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.495527/  1.485361, val:  81.67%, val_best:  81.67%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.455103/  1.644135, val:  79.58%, val_best:  81.67%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.406803/  1.645252, val:  81.67%, val_best:  81.67%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.377541/  1.679020, val:  80.42%, val_best:  81.67%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.366188/  1.767926, val:  82.92%, val_best:  82.92%, tr:  96.63%, tr_best:  96.83%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.291782/  1.803150, val:  78.33%, val_best:  82.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.276153/  1.678476, val:  82.50%, val_best:  82.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.286317/  1.723146, val:  86.25%, val_best:  86.25%, tr:  97.75%, tr_best:  98.57%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.230647/  1.881828, val:  80.00%, val_best:  86.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.211043/  1.843673, val:  84.58%, val_best:  86.25%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.181159/  2.052381, val:  80.42%, val_best:  86.25%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.182866/  2.031175, val:  81.67%, val_best:  86.25%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.169109/  1.922044, val:  84.58%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.256702/  2.015266, val:  82.08%, val_best:  86.25%, tr:  97.34%, tr_best:  99.80%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.172130/  2.262712, val:  79.58%, val_best:  86.25%, tr:  98.98%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.159103/  2.182818, val:  82.50%, val_best:  86.25%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.120475/  2.116154, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.110918/  2.127227, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.104918/  2.243919, val:  82.50%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.104377/  2.220066, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.093630/  2.332703, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.084551/  2.318528, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.081368/  2.343675, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.068539/  2.353890, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.069820/  2.415869, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.060139/  2.481302, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.058456/  2.421284, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.057857/  2.494872, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.053481/  2.463175, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.061315/  2.538633, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.041842/  2.580425, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.033123/  2.567827, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.029107/  2.575386, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.028699/  2.635646, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.026675/  2.694358, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.033636/  2.711923, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.035787/  2.726772, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.029997/  2.803854, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.026445/  2.808045, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.020919/  2.790603, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.020488/  2.852844, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.020452/  2.837827, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.022786/  2.914784, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.018351/  2.969941, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.024158/  2.944937, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.021804/  2.912529, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.018980/  2.922616, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.015437/  2.970700, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.011952/  2.945546, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.015214/  2.927584, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.012571/  2.982592, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.014633/  3.042503, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.018936/  3.018257, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.014138/  3.075493, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.014261/  3.003328, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.012495/  3.066469, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.012122/  3.087354, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.012825/  3.109371, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.010602/  3.121180, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.009768/  3.111120, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.010524/  3.070595, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.006896/  3.127019, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.010467/  3.175677, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.014690/  3.204847, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.016804/  3.227818, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.011008/  3.217014, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.013620/  3.142465, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.027249/  3.233086, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.013773/  3.193597, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010169/  3.228229, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.008933/  3.248235, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.008514/  3.250139, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.007874/  3.318109, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.010786/  3.326379, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.024444/  3.252633, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.010271/  3.364883, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.009930/  3.359876, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009744/  3.315311, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.009363/  3.334834, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.008363/  3.329156, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.007083/  3.347428, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83af57c05361476c972506fa0c065cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▄▃▅▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▂▄▅▅▆▇▇▆█▇▇▇▇▇▇▇██▇▇██████▇█████████▇</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▆▆▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▅▆▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▅▅▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▂▄▅▅▆▇▇▆█▇▇▇▇▇▇▇██▇▇██████▇█████████▇</td></tr><tr><td>val_loss</td><td>▂▂▁▃▁▁▂▂▁▂▃▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00708</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>3.34743</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-90</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3rfrn3ex' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3rfrn3ex</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_191602-3rfrn3ex/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a9fo5pwk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_192230-a9fo5pwk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fo5pwk' target=\"_blank\">devout-sweep-92</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fo5pwk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fo5pwk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 19.494623/ 18.726398, val:  45.42%, val_best:  45.42%, tr:  28.09%, tr_best:  28.09%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 16.310549/ 22.431957, val:  29.17%, val_best:  45.42%, tr:  43.72%, tr_best:  43.72%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 14.754438/ 17.222275, val:  34.17%, val_best:  45.42%, tr:  48.62%, tr_best:  48.62%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 14.142974/ 19.396952, val:  45.00%, val_best:  45.42%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 14.544996/ 13.124252, val:  46.67%, val_best:  46.67%, tr:  53.83%, tr_best:  53.83%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 11.993095/ 22.531282, val:  44.58%, val_best:  46.67%, tr:  55.46%, tr_best:  55.46%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 10.604383/ 12.337032, val:  47.50%, val_best:  47.50%, tr:  54.55%, tr_best:  55.46%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 13.456266/ 17.748796, val:  49.58%, val_best:  49.58%, tr:  53.32%, tr_best:  55.46%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 10.712627/ 11.236701, val:  53.33%, val_best:  53.33%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 14.346603/ 13.886116, val:  49.17%, val_best:  53.33%, tr:  56.49%, tr_best:  58.63%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 13.334709/ 14.272437, val:  57.92%, val_best:  57.92%, tr:  57.20%, tr_best:  58.63%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 12.243519/ 16.934431, val:  50.83%, val_best:  57.92%, tr:  59.65%, tr_best:  59.65%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 11.357775/ 10.005019, val:  58.75%, val_best:  58.75%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  8.213638/ 12.728924, val:  56.67%, val_best:  58.75%, tr:  64.25%, tr_best:  64.35%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 11.221534/ 10.324249, val:  57.92%, val_best:  58.75%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  9.802049/ 10.593627, val:  62.08%, val_best:  62.08%, tr:  65.27%, tr_best:  65.78%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  6.943840/ 10.953038, val:  61.25%, val_best:  62.08%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  9.240314/ 15.874469, val:  54.58%, val_best:  62.08%, tr:  66.91%, tr_best:  72.01%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  7.666942/ 12.020039, val:  57.92%, val_best:  62.08%, tr:  68.74%, tr_best:  72.01%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  6.455399/ 10.137188, val:  57.92%, val_best:  62.08%, tr:  71.60%, tr_best:  72.01%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 13.915679/ 18.069281, val:  57.08%, val_best:  62.08%, tr:  62.72%, tr_best:  72.01%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 11.915972/ 21.167437, val:  64.17%, val_best:  64.17%, tr:  68.13%, tr_best:  72.01%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  9.348978/ 16.364346, val:  57.08%, val_best:  64.17%, tr:  71.30%, tr_best:  72.01%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 12.901521/ 26.408913, val:  51.67%, val_best:  64.17%, tr:  69.15%, tr_best:  72.01%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  8.465640/ 20.004953, val:  52.92%, val_best:  64.17%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  7.423549/ 13.280392, val:  66.67%, val_best:  66.67%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  8.140376/ 22.311172, val:  56.25%, val_best:  66.67%, tr:  75.08%, tr_best:  76.20%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  8.716365/ 14.584615, val:  64.58%, val_best:  66.67%, tr:  73.65%, tr_best:  76.20%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  8.517710/ 12.606021, val:  64.17%, val_best:  66.67%, tr:  75.49%, tr_best:  76.20%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  6.260159/ 12.389673, val:  60.42%, val_best:  66.67%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  6.219509/ 13.410769, val:  57.92%, val_best:  66.67%, tr:  79.37%, tr_best:  79.37%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  5.459061/ 13.870585, val:  56.25%, val_best:  66.67%, tr:  78.86%, tr_best:  79.37%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  8.436308/ 21.421499, val:  60.83%, val_best:  66.67%, tr:  76.00%, tr_best:  79.37%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  7.430467/ 16.361990, val:  62.50%, val_best:  66.67%, tr:  78.65%, tr_best:  79.37%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  8.266163/ 11.016639, val:  65.83%, val_best:  66.67%, tr:  75.59%, tr_best:  79.37%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.695691/ 10.682360, val:  67.92%, val_best:  67.92%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  3.100539/ 10.757206, val:  65.83%, val_best:  67.92%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  7.611052/ 23.476198, val:  61.25%, val_best:  67.92%, tr:  79.67%, tr_best:  88.56%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  4.911914/ 16.204641, val:  60.83%, val_best:  67.92%, tr:  84.58%, tr_best:  88.56%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  4.126606/ 14.071255, val:  71.67%, val_best:  71.67%, tr:  87.23%, tr_best:  88.56%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  3.445603/ 10.916368, val:  68.75%, val_best:  71.67%, tr:  87.84%, tr_best:  88.56%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  3.193264/ 13.575790, val:  66.25%, val_best:  71.67%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  2.365380/ 12.410901, val:  63.33%, val_best:  71.67%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  4.569801/ 29.457359, val:  61.67%, val_best:  71.67%, tr:  86.41%, tr_best:  91.93%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  6.499430/ 14.148446, val:  61.67%, val_best:  71.67%, tr:  85.50%, tr_best:  91.93%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.435035/ 12.609305, val:  71.25%, val_best:  71.67%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.114410/ 14.693672, val:  66.25%, val_best:  71.67%, tr:  93.16%, tr_best:  93.26%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.027383/ 13.786798, val:  68.33%, val_best:  71.67%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.841390/ 12.117916, val:  73.33%, val_best:  73.33%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.191627/ 13.780650, val:  65.83%, val_best:  73.33%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.865268/ 14.609939, val:  67.50%, val_best:  73.33%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.981964/ 11.028677, val:  71.67%, val_best:  73.33%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.348437/ 11.215744, val:  72.92%, val_best:  73.33%, tr:  96.32%, tr_best:  98.26%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.069154/ 11.504497, val:  73.75%, val_best:  73.75%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.576312/ 11.459149, val:  75.42%, val_best:  75.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.534803/ 11.483285, val:  74.58%, val_best:  75.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.580250/ 11.605531, val:  72.08%, val_best:  75.42%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.424181/ 10.238351, val:  78.75%, val_best:  78.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.421205/ 11.647812, val:  74.17%, val_best:  78.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.308629/ 11.529670, val:  70.83%, val_best:  78.75%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.235310/ 11.309573, val:  74.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.490378/ 11.275740, val:  75.42%, val_best:  78.75%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.216311/ 11.252614, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.243083/ 11.599932, val:  70.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.173918/ 10.699143, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.135499/ 10.775160, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.099533/ 10.451315, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.081634/ 11.120658, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.137978/ 11.802026, val:  71.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.118015/ 11.307422, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.123236/ 10.649966, val:  76.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.112241/ 11.190357, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.097612/ 10.513980, val:  73.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.114716/ 11.080923, val:  72.50%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.076487/ 10.968020, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.045768/ 11.167178, val:  72.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.032925/ 10.444829, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.033705/ 10.291762, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.068401/ 11.127097, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.048948/ 10.413589, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.037846/ 10.439519, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.037369/ 10.786142, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.025049/ 10.429722, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.040978/ 10.574954, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.028544/ 10.565056, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.021753/ 10.691658, val:  77.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.040457/ 10.970922, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.044109/ 10.670202, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.356696/ 10.705832, val:  74.17%, val_best:  78.75%, tr:  98.47%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.091444/ 10.599787, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.088537/ 10.939420, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.052717/ 11.056369, val:  75.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.042098/ 10.715539, val:  76.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.027064/ 10.782294, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.015894/ 11.463870, val:  74.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.023821/ 10.855332, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.028322/ 10.821623, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.026725/ 10.684858, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.045897/ 10.735356, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.039698/ 11.242908, val:  75.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d03c4d2ac84316922f9333cdb02487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▂▃▁▁▅▅▃▄▆▆▆▇▅▇▇▇█▇▇▇███████████████████</td></tr><tr><td>summary_val_acc</td><td>▃▁▃▃▃▅▅▄▅▆▄▄▅▅▆▅▇▆▅▆▆▇▇█▇▇█▇█▇▇▇▇██▇████</td></tr><tr><td>tr_acc</td><td>▁▃▄▃▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▆▆▆▅▅▄▃▅▄▄▃▄▃▄▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▂▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▃▁▃▃▃▅▅▄▅▆▄▄▅▅▆▅▇▆▅▆▆▇▇█▇▇█▇█▇▇▇▇██▇████</td></tr><tr><td>val_loss</td><td>▆▅▃▅▃▁▁▄▁▇▆▇▂▇▁█▃▂▃▃▃▂▂▁▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0397</td></tr><tr><td>val_acc_best</td><td>0.8</td></tr><tr><td>val_acc_now</td><td>0.75</td></tr><tr><td>val_loss</td><td>11.24291</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-92</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fo5pwk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a9fo5pwk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_192230-a9fo5pwk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oyp7gd1y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_192744-oyp7gd1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oyp7gd1y' target=\"_blank\">pious-sweep-94</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oyp7gd1y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oyp7gd1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 3, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 45e076f350e4b27d8ff87367a19d69df\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  3.923051/  3.762728, val:  41.67%, val_best:  41.67%, tr:  31.46%, tr_best:  31.46%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  4.541589/  3.213481, val:  54.58%, val_best:  54.58%, tr:  46.78%, tr_best:  46.78%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  3.193708/  4.103241, val:  46.67%, val_best:  54.58%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.962505/  4.464121, val:  51.25%, val_best:  54.58%, tr:  56.38%, tr_best:  56.38%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  3.749696/  5.115317, val:  52.08%, val_best:  54.58%, tr:  54.24%, tr_best:  56.38%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  3.557918/  4.107505, val:  51.67%, val_best:  54.58%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  3.251839/  3.639340, val:  59.17%, val_best:  59.17%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  3.789381/  3.961711, val:  54.17%, val_best:  59.17%, tr:  59.14%, tr_best:  63.23%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.264089/  4.566266, val:  57.08%, val_best:  59.17%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  3.419977/  4.425866, val:  54.58%, val_best:  59.17%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.516391/  4.641468, val:  57.50%, val_best:  59.17%, tr:  65.78%, tr_best:  68.13%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  3.369853/  3.508856, val:  64.17%, val_best:  64.17%, tr:  66.29%, tr_best:  68.13%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.847493/  2.103119, val:  70.00%, val_best:  70.00%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.834257/  2.991056, val:  60.42%, val_best:  70.00%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.249372/  4.156686, val:  57.08%, val_best:  70.00%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.184960/  4.846427, val:  58.75%, val_best:  70.00%, tr:  72.11%, tr_best:  78.86%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.052049/  3.000732, val:  66.25%, val_best:  70.00%, tr:  74.77%, tr_best:  78.86%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.262979/  3.315965, val:  63.33%, val_best:  70.00%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.146066/  2.946395, val:  62.92%, val_best:  70.00%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.802277/  3.409765, val:  66.25%, val_best:  70.00%, tr:  88.76%, tr_best:  88.76%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.142624/  4.610180, val:  63.33%, val_best:  70.00%, tr:  85.39%, tr_best:  88.76%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.574427/  4.439340, val:  62.08%, val_best:  70.00%, tr:  83.15%, tr_best:  88.76%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.966091/  3.165113, val:  68.33%, val_best:  70.00%, tr:  88.66%, tr_best:  88.76%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.664263/  3.861323, val:  63.33%, val_best:  70.00%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.660322/  3.417489, val:  68.75%, val_best:  70.00%, tr:  92.95%, tr_best:  93.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.443418/  3.293869, val:  68.75%, val_best:  70.00%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.748753/  4.317977, val:  68.33%, val_best:  70.00%, tr:  92.54%, tr_best:  96.63%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.680730/  4.293045, val:  65.83%, val_best:  70.00%, tr:  94.59%, tr_best:  96.63%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.504812/  3.619553, val:  72.08%, val_best:  72.08%, tr:  95.81%, tr_best:  96.63%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.285271/  4.217305, val:  66.67%, val_best:  72.08%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.257839/  3.868908, val:  70.00%, val_best:  72.08%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.204703/  3.578285, val:  74.58%, val_best:  74.58%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.255266/  4.311204, val:  69.17%, val_best:  74.58%, tr:  98.57%, tr_best:  99.49%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.205466/  3.920600, val:  68.75%, val_best:  74.58%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.248964/  3.960414, val:  68.33%, val_best:  74.58%, tr:  98.47%, tr_best:  99.49%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.193233/  4.215509, val:  70.42%, val_best:  74.58%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.122403/  4.046196, val:  69.17%, val_best:  74.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.102601/  4.564973, val:  65.83%, val_best:  74.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.077962/  4.239452, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.081510/  4.114038, val:  71.25%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.067395/  4.047510, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.066846/  4.274763, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.058631/  4.148816, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.040458/  4.394059, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.043845/  4.192897, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.047101/  4.110836, val:  73.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.031603/  4.133077, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.018027/  4.243464, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.025513/  4.332367, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.029018/  4.260624, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.021707/  4.103077, val:  73.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.027636/  4.341897, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.032947/  4.167568, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.018852/  4.253907, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.011365/  4.280277, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.013686/  4.255951, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.010026/  4.262817, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.010807/  4.163966, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.016308/  4.251074, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.016702/  4.206409, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.012087/  4.336825, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.014643/  4.237631, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.008159/  4.262468, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.006696/  4.345481, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.009453/  4.435259, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.009104/  4.381699, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.008367/  4.413393, val:  74.17%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.008040/  4.455162, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.005329/  4.511346, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.006888/  4.609755, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.004121/  4.472642, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.008091/  4.432045, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.003974/  4.397450, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.004019/  4.489767, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.005623/  4.380260, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.004461/  4.609549, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.003288/  4.512978, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001891/  4.543507, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002254/  4.509491, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.002173/  4.611336, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001704/  4.581217, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.003287/  4.629131, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.004114/  4.533309, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.004033/  4.666637, val:  73.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.004527/  4.645934, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.003242/  4.722059, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002486/  4.695930, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.007556/  4.599330, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.008814/  4.529707, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.003897/  4.603519, val:  73.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.003391/  4.518907, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.003171/  4.600399, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.002191/  4.514292, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001273/  4.613226, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.002165/  4.694199, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.002391/  4.599045, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001051/  4.625677, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000897/  4.579463, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001136/  4.591270, val:  72.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001002/  4.644301, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616114c63a47473fa9d424e4062067bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▂▅▃▆▂▇█▇█▇████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▄▄▇▄▆▆▅▇▇▆▇▇▆▇██▇▇█▇█▇▇█▇▇▇▇▇▇█▇█████</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▅▅▆▆▇▆▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇██▇▆▃▃▂▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▄▄▇▄▆▆▅▇▇▆▇▇▆▇██▇▇█▇█▇▇█▇▇▇▇▇▇█▇█████</td></tr><tr><td>val_loss</td><td>▅▆█▅▆▁▆▄▄▆▄▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.001</td></tr><tr><td>val_acc_best</td><td>0.74583</td></tr><tr><td>val_acc_now</td><td>0.725</td></tr><tr><td>val_loss</td><td>4.6443</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-sweep-94</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oyp7gd1y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/oyp7gd1y</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_192744-oyp7gd1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lzp8zgka with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f977f1657443beb7304ac541973acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111309446601404, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_193400-lzp8zgka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lzp8zgka' target=\"_blank\">wild-sweep-96</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lzp8zgka' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lzp8zgka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.418859/  3.032273, val:  46.67%, val_best:  46.67%, tr:  31.05%, tr_best:  31.05%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.404619/  1.801633, val:  56.67%, val_best:  56.67%, tr:  51.69%, tr_best:  51.69%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.128932/  2.831036, val:  47.50%, val_best:  56.67%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.434326/  3.080602, val:  46.25%, val_best:  56.67%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.137675/  2.479177, val:  61.25%, val_best:  61.25%, tr:  56.18%, tr_best:  58.12%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.067283/  2.775231, val:  54.58%, val_best:  61.25%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.531356/  2.483945, val:  55.83%, val_best:  61.25%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.827941/  2.755044, val:  54.58%, val_best:  61.25%, tr:  64.04%, tr_best:  64.35%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.784365/  2.038122, val:  59.58%, val_best:  61.25%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.005857/  2.817165, val:  57.08%, val_best:  61.25%, tr:  65.27%, tr_best:  67.72%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.765438/  1.767554, val:  66.25%, val_best:  66.25%, tr:  66.70%, tr_best:  67.72%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.609304/  2.603522, val:  54.17%, val_best:  66.25%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.536538/  1.677273, val:  73.33%, val_best:  73.33%, tr:  74.87%, tr_best:  74.87%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.229391/  2.116813, val:  60.00%, val_best:  73.33%, tr:  77.94%, tr_best:  77.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.273295/  2.360527, val:  72.92%, val_best:  73.33%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.120218/  1.859047, val:  73.75%, val_best:  73.75%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.067421/  1.636831, val:  81.67%, val_best:  81.67%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.889632/  1.888258, val:  71.25%, val_best:  81.67%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.730554/  2.013983, val:  75.42%, val_best:  81.67%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.869590/  1.870310, val:  76.25%, val_best:  81.67%, tr:  87.33%, tr_best:  90.50%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.931892/  3.095341, val:  62.50%, val_best:  81.67%, tr:  87.23%, tr_best:  90.50%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.204713/  2.395799, val:  72.08%, val_best:  81.67%, tr:  84.78%, tr_best:  90.50%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.610667/  1.967584, val:  79.58%, val_best:  81.67%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.693056/  2.639390, val:  69.58%, val_best:  81.67%, tr:  93.16%, tr_best:  94.69%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.574843/  2.123129, val:  77.92%, val_best:  81.67%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.483453/  1.913942, val:  83.75%, val_best:  83.75%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.454603/  2.230331, val:  72.50%, val_best:  83.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.428874/  2.055990, val:  81.67%, val_best:  83.75%, tr:  97.04%, tr_best:  97.24%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.383164/  1.956741, val:  83.75%, val_best:  83.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.347085/  2.101122, val:  81.67%, val_best:  83.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.349761/  2.019179, val:  82.08%, val_best:  83.75%, tr:  97.65%, tr_best:  98.47%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.287953/  2.272194, val:  75.42%, val_best:  83.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.402553/  2.329107, val:  77.08%, val_best:  83.75%, tr:  96.63%, tr_best:  99.08%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.395849/  2.535067, val:  78.75%, val_best:  83.75%, tr:  96.83%, tr_best:  99.08%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.282542/  2.525409, val:  77.92%, val_best:  83.75%, tr:  98.67%, tr_best:  99.08%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.247556/  2.313099, val:  82.08%, val_best:  83.75%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.233800/  2.251657, val:  82.50%, val_best:  83.75%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.234254/  2.226787, val:  85.42%, val_best:  85.42%, tr:  98.67%, tr_best:  99.39%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.244938/  2.358527, val:  82.08%, val_best:  85.42%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.218130/  2.301349, val:  85.42%, val_best:  85.42%, tr:  98.88%, tr_best:  99.39%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.155526/  2.311553, val:  85.83%, val_best:  85.83%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.146480/  2.392386, val:  83.33%, val_best:  85.83%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.129785/  2.405860, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.127956/  2.604856, val:  81.25%, val_best:  85.83%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.148485/  2.457201, val:  82.92%, val_best:  85.83%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.139905/  2.583881, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.117714/  2.621881, val:  81.67%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.123289/  2.526884, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.134478/  2.731280, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.113255/  2.660764, val:  84.58%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.107337/  2.594129, val:  85.00%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.084629/  2.766536, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.103903/  2.851704, val:  84.58%, val_best:  85.83%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.077295/  2.822790, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.096530/  2.602163, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.081336/  2.703417, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.071002/  2.812208, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.052675/  2.748192, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.046669/  2.858523, val:  84.58%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.048132/  2.835482, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.045223/  2.833235, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.045505/  2.876010, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.041146/  2.905790, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.034668/  2.906418, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.040692/  2.920238, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.032608/  2.972028, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.036217/  3.101320, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.032526/  3.002340, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.028738/  3.058268, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.026757/  3.042162, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.028648/  3.063305, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.024201/  3.130354, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.025507/  3.161693, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.024136/  3.123837, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.022654/  3.144860, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.018477/  3.174091, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.021187/  3.215185, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.017211/  3.263602, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.029254/  3.278161, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.022653/  3.263101, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.028113/  3.222233, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.019568/  3.255368, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.020078/  3.259096, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.013758/  3.265145, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.017798/  3.247575, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.013504/  3.395262, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.013402/  3.315048, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.012652/  3.327630, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.010315/  3.363625, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.015465/  3.313991, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.017404/  3.421410, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.014053/  3.406162, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.013115/  3.337248, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.016072/  3.315083, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.014937/  3.380092, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.012326/  3.385179, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.012061/  3.327659, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.009618/  3.319268, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.014001/  3.380230, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009895/  3.362226, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca6614f9bf1443597189c96750ef64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▅▅▅▇▆▇█▇█████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▄▂▃▆▆▅▆▅▆▆▇▆▇██▇▇█████▇██████▇███▇████</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▄▅▆▇▇▆▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▇▆▇▅▅▄▃▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▄▂▃▆▆▅▆▅▆▆▇▆▇██▇▇█████▇██████▇███▇████</td></tr><tr><td>val_loss</td><td>▇▆▄▅▆▁▄▂▂▄▃▃▃▄▄▃▄▄▄▄▅▆▅▅▆▆▆▆▇▇▇█▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0099</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>3.36223</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sweep-96</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lzp8zgka' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lzp8zgka</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_193400-lzp8zgka/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xvhhi9ja with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_194027-xvhhi9ja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xvhhi9ja' target=\"_blank\">fast-sweep-98</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xvhhi9ja' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xvhhi9ja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.067937/  1.566175, val:  52.50%, val_best:  52.50%, tr:  23.29%, tr_best:  23.29%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.456786/  1.700936, val:  54.17%, val_best:  54.17%, tr:  52.40%, tr_best:  52.40%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.496437/  1.646755, val:  58.33%, val_best:  58.33%, tr:  56.79%, tr_best:  56.79%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.204683/  1.627249, val:  54.58%, val_best:  58.33%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.166461/  1.224427, val:  64.17%, val_best:  64.17%, tr:  62.31%, tr_best:  64.35%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.040875/  1.455697, val:  58.75%, val_best:  64.17%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.027952/  1.305898, val:  64.58%, val_best:  64.58%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.046993/  1.360753, val:  60.83%, val_best:  64.58%, tr:  70.07%, tr_best:  70.07%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.972194/  1.314518, val:  68.75%, val_best:  68.75%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.982489/  1.236796, val:  67.50%, val_best:  68.75%, tr:  77.22%, tr_best:  77.22%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.976170/  1.164190, val:  69.58%, val_best:  69.58%, tr:  76.10%, tr_best:  77.22%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.808599/  1.154184, val:  75.42%, val_best:  75.42%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.695380/  1.111551, val:  80.83%, val_best:  80.83%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.613763/  1.255036, val:  74.58%, val_best:  80.83%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.615489/  1.405451, val:  74.58%, val_best:  80.83%, tr:  87.13%, tr_best:  88.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.486587/  1.424530, val:  72.08%, val_best:  80.83%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.471477/  1.210009, val:  80.42%, val_best:  80.83%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.385923/  1.240748, val:  79.17%, val_best:  80.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.340231/  1.292255, val:  82.92%, val_best:  82.92%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.309171/  1.205425, val:  85.42%, val_best:  85.42%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.287653/  1.346468, val:  80.00%, val_best:  85.42%, tr:  97.24%, tr_best:  98.16%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.264246/  1.413657, val:  80.00%, val_best:  85.42%, tr:  98.06%, tr_best:  98.16%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.236204/  1.426323, val:  81.25%, val_best:  85.42%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.232399/  1.399531, val:  85.00%, val_best:  85.42%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.186723/  1.441835, val:  86.25%, val_best:  86.25%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.161405/  1.497071, val:  83.33%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.160115/  1.482655, val:  86.25%, val_best:  86.25%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.136864/  1.563116, val:  82.08%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.133766/  1.538477, val:  84.17%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.108094/  1.640833, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.100967/  1.612758, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.102367/  1.674317, val:  85.42%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.113285/  1.663979, val:  85.83%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.094151/  1.793928, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.100411/  1.695659, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.067340/  1.718834, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.058322/  1.764024, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.057612/  1.767920, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.057130/  1.831519, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.050815/  1.868434, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.041657/  1.871137, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.039306/  1.870009, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.037354/  1.926766, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.031801/  1.931467, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.031378/  1.941924, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.035401/  2.021202, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.034461/  2.068179, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.031070/  1.984209, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.027021/  2.034826, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.021008/  2.026868, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.022487/  2.048006, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.021307/  2.101872, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.022049/  2.091780, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.019693/  2.077992, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.015760/  2.101652, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.014979/  2.119963, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.015456/  2.178576, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.015112/  2.173769, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.013201/  2.156791, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.011239/  2.176126, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.012230/  2.220952, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.011550/  2.209249, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.012774/  2.251884, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.011952/  2.221404, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.011977/  2.277893, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.010860/  2.277236, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.009836/  2.305060, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.009504/  2.288736, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.008635/  2.298874, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.008201/  2.341902, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.010916/  2.363268, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.012409/  2.373828, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.010953/  2.377004, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.008431/  2.379032, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.007139/  2.418274, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.006466/  2.401943, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.006144/  2.416922, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.006229/  2.438816, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.007479/  2.448580, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.008178/  2.396963, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.006546/  2.475780, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.006314/  2.430070, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.005003/  2.452066, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.005154/  2.448916, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.006067/  2.442535, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.005199/  2.470368, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.006097/  2.502178, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.005307/  2.507712, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.004424/  2.490529, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.004878/  2.523559, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.005789/  2.504144, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.003779/  2.549274, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.004969/  2.526990, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.004972/  2.551892, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.005590/  2.575011, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.004253/  2.604839, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.003824/  2.567898, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.004309/  2.589599, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.003066/  2.599775, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.002717/  2.605932, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917e64128b4f433ea495a2ba28531668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▃▃▂▁▅█▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▃▄▇▅▆█▇██▇█▇██▇███▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▃▄▇▅▆█▇██▇█▇██▇███▇██████████████████</td></tr><tr><td>val_loss</td><td>▃▄▂▂▂▁▂▂▁▂▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00272</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.60593</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-sweep-98</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xvhhi9ja' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xvhhi9ja</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_194027-xvhhi9ja/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yalk4qj2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_194541-yalk4qj2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yalk4qj2' target=\"_blank\">floral-sweep-100</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yalk4qj2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yalk4qj2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 45.187454/ 30.262115, val:  22.50%, val_best:  22.50%, tr:  22.47%, tr_best:  22.47%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 24.460712/ 27.259308, val:  30.42%, val_best:  30.42%, tr:  39.02%, tr_best:  39.02%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 19.251930/ 25.823017, val:  33.33%, val_best:  33.33%, tr:  44.74%, tr_best:  44.74%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 16.411528/ 24.325218, val:  37.50%, val_best:  37.50%, tr:  46.17%, tr_best:  46.17%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 23.918329/ 24.751261, val:  41.25%, val_best:  41.25%, tr:  42.29%, tr_best:  46.17%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 21.658142/ 38.475395, val:  37.08%, val_best:  41.25%, tr:  46.07%, tr_best:  46.17%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 17.615662/ 28.446363, val:  40.42%, val_best:  41.25%, tr:  55.16%, tr_best:  55.16%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 19.678091/ 28.235804, val:  36.25%, val_best:  41.25%, tr:  53.63%, tr_best:  55.16%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 12.452600/ 16.651087, val:  47.08%, val_best:  47.08%, tr:  57.81%, tr_best:  57.81%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 14.006525/ 15.303699, val:  48.75%, val_best:  48.75%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 16.885641/ 12.281904, val:  49.58%, val_best:  49.58%, tr:  54.55%, tr_best:  58.43%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 16.274343/ 27.763891, val:  45.83%, val_best:  49.58%, tr:  57.41%, tr_best:  58.43%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.451422/ 15.535475, val:  57.92%, val_best:  57.92%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 20.755844/ 29.942078, val:  35.42%, val_best:  57.92%, tr:  52.91%, tr_best:  58.63%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 16.287708/ 19.441818, val:  57.92%, val_best:  57.92%, tr:  57.92%, tr_best:  58.63%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 14.100290/ 26.740898, val:  47.92%, val_best:  57.92%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 14.963054/ 14.755442, val:  64.58%, val_best:  64.58%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 10.108028/ 10.507936, val:  59.58%, val_best:  64.58%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  9.768046/ 13.649356, val:  55.83%, val_best:  64.58%, tr:  65.07%, tr_best:  68.74%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 11.710277/ 15.507362, val:  50.83%, val_best:  64.58%, tr:  64.86%, tr_best:  68.74%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 14.441642/ 22.366478, val:  53.75%, val_best:  64.58%, tr:  61.70%, tr_best:  68.74%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 13.952308/ 16.425591, val:  62.92%, val_best:  64.58%, tr:  65.37%, tr_best:  68.74%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 10.902123/ 14.861794, val:  61.67%, val_best:  64.58%, tr:  68.64%, tr_best:  68.74%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 10.933050/ 12.003739, val:  58.33%, val_best:  64.58%, tr:  65.99%, tr_best:  68.74%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  7.517968/ 12.419998, val:  65.00%, val_best:  65.00%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  9.308770/ 21.193485, val:  59.58%, val_best:  65.00%, tr:  72.32%, tr_best:  75.69%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  9.556599/ 18.358824, val:  55.42%, val_best:  65.00%, tr:  75.49%, tr_best:  75.69%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  9.404782/ 12.656638, val:  66.25%, val_best:  66.25%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 10.482777/ 16.233072, val:  59.58%, val_best:  66.25%, tr:  73.54%, tr_best:  76.20%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  7.457744/ 13.747622, val:  60.00%, val_best:  66.25%, tr:  79.47%, tr_best:  79.47%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  6.843536/ 10.822941, val:  72.50%, val_best:  72.50%, tr:  79.37%, tr_best:  79.47%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  6.394913/ 15.808571, val:  62.08%, val_best:  72.50%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  8.156906/ 16.808943, val:  67.08%, val_best:  72.50%, tr:  77.83%, tr_best:  80.08%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  5.866455/  9.172800, val:  81.67%, val_best:  81.67%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  7.186062/ 13.303928, val:  70.83%, val_best:  81.67%, tr:  80.39%, tr_best:  84.58%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.240639/ 10.885471, val:  72.92%, val_best:  81.67%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  3.317479/ 12.094479, val:  70.00%, val_best:  81.67%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  6.650159/ 16.523012, val:  65.00%, val_best:  81.67%, tr:  83.04%, tr_best:  92.65%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  7.377903/ 18.719357, val:  59.58%, val_best:  81.67%, tr:  82.74%, tr_best:  92.65%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  3.962336/ 11.367832, val:  75.00%, val_best:  81.67%, tr:  91.42%, tr_best:  92.65%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  3.417572/ 12.619657, val:  69.58%, val_best:  81.67%, tr:  92.03%, tr_best:  92.65%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  3.807698/ 15.710510, val:  63.75%, val_best:  81.67%, tr:  91.01%, tr_best:  92.65%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  4.278366/ 13.276336, val:  70.00%, val_best:  81.67%, tr:  89.17%, tr_best:  92.65%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  3.270939/ 13.817962, val:  71.67%, val_best:  81.67%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.517211/ 10.342820, val:  72.50%, val_best:  81.67%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.336766/ 10.359969, val:  84.17%, val_best:  84.17%, tr:  95.51%, tr_best:  96.02%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  4.211002/ 14.049962, val:  72.08%, val_best:  84.17%, tr:  91.73%, tr_best:  96.02%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  3.131889/ 12.546160, val:  77.08%, val_best:  84.17%, tr:  95.10%, tr_best:  96.02%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.504963/ 10.609910, val:  82.50%, val_best:  84.17%, tr:  95.40%, tr_best:  96.02%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.266931/ 11.183918, val:  79.17%, val_best:  84.17%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.766757/ 12.193379, val:  72.08%, val_best:  84.17%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.405324/ 12.149163, val:  81.25%, val_best:  84.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.986905/ 11.996535, val:  73.33%, val_best:  84.17%, tr:  96.12%, tr_best:  98.47%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.855222/ 13.475624, val:  75.42%, val_best:  84.17%, tr:  96.63%, tr_best:  98.47%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.658958/ 11.346016, val:  75.00%, val_best:  84.17%, tr:  97.85%, tr_best:  98.47%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  1.575914/ 11.856565, val:  70.00%, val_best:  84.17%, tr:  97.85%, tr_best:  98.47%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  1.666067/ 14.375697, val:  70.42%, val_best:  84.17%, tr:  96.73%, tr_best:  98.47%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.450398/ 10.157589, val:  83.75%, val_best:  84.17%, tr:  97.45%, tr_best:  98.47%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.037663/ 11.061153, val:  83.33%, val_best:  84.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.889584/ 10.412311, val:  81.67%, val_best:  84.17%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.636643/ 10.682533, val:  82.08%, val_best:  84.17%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.940724/ 11.154093, val:  81.67%, val_best:  84.17%, tr:  98.57%, tr_best:  99.39%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.954775/ 10.365469, val:  80.00%, val_best:  84.17%, tr:  98.06%, tr_best:  99.39%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.956074/ 11.562982, val:  79.17%, val_best:  84.17%, tr:  98.37%, tr_best:  99.39%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.705160/ 11.378657, val:  77.92%, val_best:  84.17%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.900596/ 10.668340, val:  78.75%, val_best:  84.17%, tr:  98.88%, tr_best:  99.59%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.861182/ 11.255153, val:  80.00%, val_best:  84.17%, tr:  98.98%, tr_best:  99.59%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.722933/ 10.570792, val:  82.50%, val_best:  84.17%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.676670/ 11.017790, val:  77.92%, val_best:  84.17%, tr:  98.47%, tr_best:  99.69%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.519213/ 10.087054, val:  82.92%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.454289/ 10.285648, val:  82.92%, val_best:  84.17%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.502309/ 11.158738, val:  80.00%, val_best:  84.17%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.410398/ 10.633657, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.360849/ 10.301484, val:  82.92%, val_best:  84.17%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.268485/ 10.172871, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.277049/ 10.608520, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.268028/ 10.806051, val:  81.67%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.235757/ 10.514414, val:  82.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.352579/ 10.475294, val:  82.50%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.628252/ 11.018059, val:  79.58%, val_best:  84.17%, tr:  98.88%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.360955/ 10.424612, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.353901/ 10.380229, val:  83.33%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.188564/ 11.480348, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.253539/ 10.775042, val:  82.50%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.191175/ 10.467630, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.209400/ 10.515471, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.234407/ 10.806368, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.281402/ 10.878665, val:  82.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.267516/ 10.762115, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.261555/ 10.646919, val:  83.75%, val_best:  84.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.191567/ 11.033361, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.572101/ 11.811758, val:  77.92%, val_best:  85.00%, tr:  98.47%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.248946/ 11.536996, val:  82.08%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.165420/ 10.983056, val:  81.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.167473/ 10.566911, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.237120/ 11.628488, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.214791/ 11.052873, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.171514/ 10.681046, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.142728/ 10.453480, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.152736/ 10.491996, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7f1bc50c684f91a29eb12e7f2ac7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▂▄▃▁▄▅▅▃▅▃█▇▅▇▇███▇█████▇▇█▇███████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▃▃▄▅▅▅▄▆▆▅▅▆▇▆▇▆▇▇▇▇▇███▇█████▇█▇█████</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▄▅▅▅▆▆▆▆▇▆▇▇██████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▅▄▃▃▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▃▃▄▅▅▆▆▆▆▆▆▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▃▃▄▅▅▅▄▆▆▅▅▆▇▆▇▆▇▇▇▇▇███▇█████▇█▇█████</td></tr><tr><td>val_loss</td><td>█▆▆▇▃▃▄▁▃▃▂▄▂▃▁▃▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.15274</td></tr><tr><td>val_acc_best</td><td>0.85</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>10.492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-sweep-100</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yalk4qj2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yalk4qj2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_194541-yalk4qj2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6xxsxmxr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_195100-6xxsxmxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6xxsxmxr' target=\"_blank\">ruby-sweep-102</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6xxsxmxr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6xxsxmxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.230399/  1.722002, val:  29.58%, val_best:  29.58%, tr:  14.10%, tr_best:  14.10%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.425452/  1.331180, val:  57.92%, val_best:  57.92%, tr:  50.56%, tr_best:  50.56%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.283477/  1.300149, val:  55.83%, val_best:  57.92%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.085528/  1.318885, val:  56.25%, val_best:  57.92%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.047466/  1.320826, val:  62.92%, val_best:  62.92%, tr:  64.25%, tr_best:  65.37%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.989933/  1.235204, val:  64.58%, val_best:  64.58%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.950041/  1.137413, val:  67.92%, val_best:  67.92%, tr:  71.50%, tr_best:  71.50%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.927461/  1.098720, val:  70.83%, val_best:  70.83%, tr:  70.28%, tr_best:  71.50%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.890057/  1.195145, val:  66.67%, val_best:  70.83%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.819678/  1.233530, val:  65.42%, val_best:  70.83%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.821474/  1.180988, val:  70.00%, val_best:  70.83%, tr:  78.24%, tr_best:  78.86%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.723687/  1.139411, val:  80.83%, val_best:  80.83%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.632604/  1.048709, val:  82.92%, val_best:  82.92%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.581795/  1.106624, val:  84.17%, val_best:  84.17%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.567770/  1.264370, val:  75.00%, val_best:  84.17%, tr:  91.52%, tr_best:  92.65%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.502842/  1.137259, val:  81.25%, val_best:  84.17%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.452613/  1.071266, val:  85.00%, val_best:  85.00%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.402088/  1.176797, val:  85.42%, val_best:  85.42%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.379270/  1.244641, val:  81.67%, val_best:  85.42%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.346509/  1.188829, val:  85.83%, val_best:  85.83%, tr:  97.45%, tr_best:  97.75%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.328504/  1.241425, val:  82.50%, val_best:  85.83%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.294168/  1.314342, val:  80.42%, val_best:  85.83%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.288889/  1.235187, val:  83.75%, val_best:  85.83%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.283905/  1.280757, val:  84.58%, val_best:  85.83%, tr:  97.85%, tr_best:  98.47%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.232318/  1.318681, val:  85.00%, val_best:  85.83%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.217399/  1.304914, val:  83.33%, val_best:  85.83%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.213575/  1.258858, val:  87.50%, val_best:  87.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.191379/  1.382714, val:  86.25%, val_best:  87.50%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.173023/  1.327170, val:  87.92%, val_best:  87.92%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.156563/  1.506316, val:  82.50%, val_best:  87.92%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.164469/  1.417698, val:  84.58%, val_best:  87.92%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.136857/  1.451314, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.149839/  1.481982, val:  86.67%, val_best:  87.92%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.132479/  1.566233, val:  85.00%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.132223/  1.520646, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.107850/  1.511546, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.099496/  1.531616, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.086306/  1.568229, val:  86.25%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.093396/  1.573727, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.095330/  1.593868, val:  87.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.089308/  1.565638, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.069257/  1.649165, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.065025/  1.637331, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.070663/  1.662379, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.064494/  1.659196, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.058113/  1.686388, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.057234/  1.727987, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.067201/  1.727449, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.055963/  1.706531, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.042858/  1.745415, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.043624/  1.779624, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.036350/  1.783304, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.033099/  1.780597, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.032031/  1.831114, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.031384/  1.884127, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.027664/  1.885745, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.034866/  1.910079, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.030848/  1.881530, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.029015/  1.882704, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.026210/  1.881297, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.027402/  1.913131, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.021192/  1.946663, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.017708/  1.957808, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.019676/  1.932945, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.020666/  1.951430, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.020814/  1.994893, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.018400/  2.016870, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.017806/  2.007109, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.015896/  2.025358, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.018450/  2.064344, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.015341/  2.058082, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.015338/  2.082862, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.015474/  2.128021, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.013325/  2.103775, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.014666/  2.151866, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.013249/  2.113364, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.013112/  2.171122, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.011470/  2.152185, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.011674/  2.171747, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.011224/  2.170567, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.010980/  2.209912, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.013098/  2.170921, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.010130/  2.189194, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.010869/  2.214001, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.011339/  2.184415, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.013307/  2.198367, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.011689/  2.204574, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.009343/  2.196566, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.009638/  2.188074, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.010553/  2.151942, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.013174/  2.243567, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.013726/  2.279444, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.011346/  2.286497, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.010116/  2.268517, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.009389/  2.256894, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.010421/  2.318434, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.009655/  2.321060, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.007744/  2.260092, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.009949/  2.298817, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.009565/  2.292644, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a4b0ca6b124da6a2eaa5d567930c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▅▂▇█▆████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▆▅▇▆██▇██▇███████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▆▅▇▆██▇██▇███████████████████████████</td></tr><tr><td>val_loss</td><td>▅▂▃▁▂▁▂▂▂▂▂▂▄▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00956</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>2.29264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-102</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6xxsxmxr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6xxsxmxr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_195100-6xxsxmxr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a6kxdfw4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_195614-a6kxdfw4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a6kxdfw4' target=\"_blank\">playful-sweep-104</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a6kxdfw4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a6kxdfw4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 49.918007/ 70.732849, val:  14.58%, val_best:  14.58%, tr:  24.21%, tr_best:  24.21%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 43.641293/ 37.413857, val:  35.00%, val_best:  35.00%, tr:  31.77%, tr_best:  31.77%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 28.036272/ 35.909058, val:  20.42%, val_best:  35.00%, tr:  36.06%, tr_best:  36.06%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 40.503906/ 72.543968, val:  33.33%, val_best:  35.00%, tr:  34.12%, tr_best:  36.06%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 36.287651/ 28.056643, val:  42.92%, val_best:  42.92%, tr:  42.49%, tr_best:  42.49%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 20.993511/ 35.443760, val:  39.17%, val_best:  42.92%, tr:  48.83%, tr_best:  48.83%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 20.443468/ 21.198545, val:  48.33%, val_best:  48.33%, tr:  51.07%, tr_best:  51.07%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 26.422749/ 38.329613, val:  36.67%, val_best:  48.33%, tr:  43.92%, tr_best:  51.07%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 20.755806/ 24.488943, val:  37.50%, val_best:  48.33%, tr:  47.70%, tr_best:  51.07%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 22.260468/ 25.586998, val:  48.75%, val_best:  48.75%, tr:  51.69%, tr_best:  51.69%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 28.716480/ 29.733381, val:  43.75%, val_best:  48.75%, tr:  48.62%, tr_best:  51.69%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 20.393291/ 26.478394, val:  45.00%, val_best:  48.75%, tr:  56.59%, tr_best:  56.59%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 25.583225/ 21.729120, val:  44.17%, val_best:  48.75%, tr:  52.60%, tr_best:  56.59%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 21.485222/ 34.252552, val:  43.33%, val_best:  48.75%, tr:  55.06%, tr_best:  56.59%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 23.893930/ 30.567627, val:  54.58%, val_best:  54.58%, tr:  55.26%, tr_best:  56.59%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 17.095465/ 21.224571, val:  43.33%, val_best:  54.58%, tr:  57.61%, tr_best:  57.61%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 18.077858/ 27.190670, val:  50.42%, val_best:  54.58%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 19.464817/ 29.312180, val:  41.67%, val_best:  54.58%, tr:  58.02%, tr_best:  60.78%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 18.140882/ 17.781351, val:  43.75%, val_best:  54.58%, tr:  59.24%, tr_best:  60.78%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 22.582249/ 43.495029, val:  38.75%, val_best:  54.58%, tr:  55.57%, tr_best:  60.78%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 31.486919/ 33.159946, val:  45.00%, val_best:  54.58%, tr:  53.52%, tr_best:  60.78%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 21.698156/ 43.040432, val:  51.67%, val_best:  54.58%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 19.215933/ 18.865646, val:  58.33%, val_best:  58.33%, tr:  60.27%, tr_best:  60.78%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 15.899861/ 31.846115, val:  54.17%, val_best:  58.33%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 21.568354/ 17.620060, val:  63.75%, val_best:  63.75%, tr:  58.73%, tr_best:  63.23%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 24.779272/ 36.896027, val:  50.83%, val_best:  63.75%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 19.475203/ 31.982874, val:  59.17%, val_best:  63.75%, tr:  63.43%, tr_best:  63.43%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 21.897457/ 48.823910, val:  52.08%, val_best:  63.75%, tr:  61.39%, tr_best:  63.43%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 21.349722/ 29.070232, val:  59.58%, val_best:  63.75%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 16.777479/ 18.438236, val:  62.92%, val_best:  63.75%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 16.281050/ 29.252592, val:  49.17%, val_best:  63.75%, tr:  65.27%, tr_best:  66.91%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 15.936129/ 32.132397, val:  55.83%, val_best:  63.75%, tr:  66.50%, tr_best:  66.91%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 19.185179/ 29.009865, val:  58.33%, val_best:  63.75%, tr:  63.13%, tr_best:  66.91%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 18.810369/ 29.957590, val:  54.58%, val_best:  63.75%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 19.952372/ 31.681721, val:  52.08%, val_best:  63.75%, tr:  63.43%, tr_best:  67.21%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 17.191696/ 24.963211, val:  64.58%, val_best:  64.58%, tr:  67.01%, tr_best:  67.21%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 13.897470/ 23.733740, val:  61.67%, val_best:  64.58%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 28.940104/ 46.483356, val:  56.25%, val_best:  64.58%, tr:  67.11%, tr_best:  72.32%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss: 15.753944/ 26.415911, val:  56.25%, val_best:  64.58%, tr:  71.50%, tr_best:  72.32%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 16.048729/ 21.418222, val:  65.42%, val_best:  65.42%, tr:  68.13%, tr_best:  72.32%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  8.970004/ 25.367043, val:  56.67%, val_best:  65.42%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss: 12.484678/ 24.932775, val:  59.58%, val_best:  65.42%, tr:  73.34%, tr_best:  80.08%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 13.379487/ 28.693092, val:  60.83%, val_best:  65.42%, tr:  74.26%, tr_best:  80.08%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss: 17.683704/ 46.907799, val:  57.08%, val_best:  65.42%, tr:  74.77%, tr_best:  80.08%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss: 18.186131/ 22.570232, val:  69.17%, val_best:  69.17%, tr:  70.38%, tr_best:  80.08%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss: 10.208755/ 20.752899, val:  65.00%, val_best:  69.17%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss: 11.821274/ 23.097858, val:  66.25%, val_best:  69.17%, tr:  77.53%, tr_best:  80.80%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 10.897275/ 21.817297, val:  71.25%, val_best:  71.25%, tr:  79.88%, tr_best:  80.80%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 14.695774/ 21.649096, val:  68.33%, val_best:  71.25%, tr:  76.61%, tr_best:  80.80%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  9.072396/ 28.211853, val:  52.08%, val_best:  71.25%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss: 11.107338/ 21.624105, val:  67.08%, val_best:  71.25%, tr:  81.82%, tr_best:  83.04%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  8.803473/ 22.258591, val:  70.42%, val_best:  71.25%, tr:  82.64%, tr_best:  83.04%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss: 10.016854/ 24.689081, val:  62.50%, val_best:  71.25%, tr:  79.88%, tr_best:  83.04%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  9.268137/ 25.000359, val:  64.17%, val_best:  71.25%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  7.020193/ 22.532537, val:  70.42%, val_best:  71.25%, tr:  88.15%, tr_best:  88.15%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  7.394073/ 22.520679, val:  68.75%, val_best:  71.25%, tr:  87.74%, tr_best:  88.15%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 10.556536/ 28.239967, val:  62.50%, val_best:  71.25%, tr:  79.88%, tr_best:  88.15%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  7.975634/ 23.725000, val:  67.92%, val_best:  71.25%, tr:  86.21%, tr_best:  88.15%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  7.929588/ 24.461138, val:  67.08%, val_best:  71.25%, tr:  85.50%, tr_best:  88.15%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  5.913130/ 22.175884, val:  75.83%, val_best:  75.83%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  6.176432/ 23.928097, val:  70.42%, val_best:  75.83%, tr:  92.44%, tr_best:  92.44%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  6.046090/ 26.804178, val:  60.42%, val_best:  75.83%, tr:  91.52%, tr_best:  92.44%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  8.644613/ 24.511095, val:  73.33%, val_best:  75.83%, tr:  86.72%, tr_best:  92.44%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  6.546498/ 25.856478, val:  67.08%, val_best:  75.83%, tr:  89.38%, tr_best:  92.44%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  5.927174/ 22.775642, val:  75.00%, val_best:  75.83%, tr:  91.83%, tr_best:  92.44%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  5.093918/ 23.069735, val:  73.75%, val_best:  75.83%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  6.583910/ 23.651340, val:  65.42%, val_best:  75.83%, tr:  89.48%, tr_best:  93.97%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  6.287733/ 23.611439, val:  67.50%, val_best:  75.83%, tr:  89.48%, tr_best:  93.97%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  5.049763/ 21.967419, val:  70.00%, val_best:  75.83%, tr:  93.87%, tr_best:  93.97%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  4.722868/ 23.917990, val:  76.25%, val_best:  76.25%, tr:  93.77%, tr_best:  93.97%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  5.648280/ 22.467585, val:  72.50%, val_best:  76.25%, tr:  92.54%, tr_best:  93.97%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  4.797920/ 22.593086, val:  76.25%, val_best:  76.25%, tr:  93.67%, tr_best:  93.97%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  4.753484/ 23.403147, val:  70.83%, val_best:  76.25%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  4.465302/ 22.254095, val:  75.83%, val_best:  76.25%, tr:  93.56%, tr_best:  94.28%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  3.572609/ 23.075346, val:  60.83%, val_best:  76.25%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  4.126802/ 24.700232, val:  74.17%, val_best:  76.25%, tr:  95.20%, tr_best:  95.81%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  3.764362/ 24.239693, val:  72.92%, val_best:  76.25%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  3.379516/ 24.051994, val:  69.17%, val_best:  76.25%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  4.070809/ 20.400537, val:  77.08%, val_best:  77.08%, tr:  94.99%, tr_best:  97.14%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  3.254496/ 22.107771, val:  74.58%, val_best:  77.08%, tr:  96.83%, tr_best:  97.14%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  2.887922/ 22.577394, val:  72.50%, val_best:  77.08%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  3.445028/ 22.874613, val:  75.42%, val_best:  77.08%, tr:  95.71%, tr_best:  97.34%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  4.588431/ 25.687836, val:  71.25%, val_best:  77.08%, tr:  94.59%, tr_best:  97.34%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  3.950463/ 24.407372, val:  72.92%, val_best:  77.08%, tr:  95.30%, tr_best:  97.34%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.821759/ 23.845703, val:  75.00%, val_best:  77.08%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  2.498159/ 25.264423, val:  71.67%, val_best:  77.08%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  2.513945/ 27.190363, val:  70.42%, val_best:  77.08%, tr:  97.96%, tr_best:  98.26%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  3.209316/ 23.976170, val:  74.58%, val_best:  77.08%, tr:  95.71%, tr_best:  98.26%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  2.860589/ 23.990435, val:  74.17%, val_best:  77.08%, tr:  97.04%, tr_best:  98.26%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.098098/ 23.539873, val:  75.00%, val_best:  77.08%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  2.061093/ 23.453428, val:  77.50%, val_best:  77.50%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  1.857408/ 22.853891, val:  77.92%, val_best:  77.92%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  1.973173/ 24.274315, val:  77.92%, val_best:  77.92%, tr:  97.55%, tr_best:  99.28%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  2.701829/ 22.509684, val:  73.75%, val_best:  77.92%, tr:  97.65%, tr_best:  99.28%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  1.912312/ 23.843739, val:  74.17%, val_best:  77.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  1.792292/ 25.611328, val:  67.50%, val_best:  77.92%, tr:  98.67%, tr_best:  99.49%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  1.562984/ 24.700880, val:  75.00%, val_best:  77.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  1.341520/ 25.010342, val:  75.42%, val_best:  77.92%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  1.866671/ 23.791185, val:  76.25%, val_best:  77.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  2.056352/ 24.566502, val:  74.17%, val_best:  77.92%, tr:  98.77%, tr_best:  99.49%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74b6c07c2474aa9a267d9c867a2d99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▃▃▃▁▃▄▄▅▆▅▆▆▅▇▃▅▆▆▅▇▆▇▅▇▆▆█▇▆███▇█▆█▇██</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▅▄▅▄▄▅▆▆▆▆▇▆▇▆▇▇▅▆▇▇█▆█▇▇▇█▇█▇████▇█</td></tr><tr><td>tr_acc</td><td>▁▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▅▆▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▆▅▄▄▄▄▄▄▄▄▃▄▃▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▅▄▅▄▄▅▆▆▆▆▇▆▇▆▇▇▅▆▇▇█▆█▇▇▇█▇█▇████▇█</td></tr><tr><td>val_loss</td><td>█▃▂▄▂▂▃▃▄▄▁▃▁▃▂▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.98774</td></tr><tr><td>tr_epoch_loss</td><td>2.05635</td></tr><tr><td>val_acc_best</td><td>0.77917</td></tr><tr><td>val_acc_now</td><td>0.74167</td></tr><tr><td>val_loss</td><td>24.5665</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-104</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a6kxdfw4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a6kxdfw4</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_195614-a6kxdfw4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x88s8yqb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_200247-x88s8yqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x88s8yqb' target=\"_blank\">fresh-sweep-106</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x88s8yqb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x88s8yqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.50%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.50%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.50%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.322383/  2.317189, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.323500/  2.311199, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.257870/  2.164017, val:  19.17%, val_best:  19.17%, tr:  15.12%, tr_best:  15.12%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.126175/  2.128363, val:  15.00%, val_best:  19.17%, tr:  18.28%, tr_best:  18.28%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.977139/  1.925546, val:  35.83%, val_best:  35.83%, tr:  25.64%, tr_best:  25.64%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.609094/  1.630873, val:  48.75%, val_best:  48.75%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.531923/  1.544955, val:  48.75%, val_best:  48.75%, tr:  49.34%, tr_best:  49.34%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.481638/  1.673422, val:  45.83%, val_best:  48.75%, tr:  49.44%, tr_best:  49.44%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.501951/  1.826609, val:  47.50%, val_best:  48.75%, tr:  52.30%, tr_best:  52.30%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.374460/  1.561048, val:  56.25%, val_best:  56.25%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.315255/  1.518714, val:  57.92%, val_best:  57.92%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.165972/  1.497295, val:  55.00%, val_best:  57.92%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.148022/  1.482845, val:  57.92%, val_best:  57.92%, tr:  67.72%, tr_best:  68.03%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.134318/  1.425274, val:  61.67%, val_best:  61.67%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.083486/  1.413315, val:  63.33%, val_best:  63.33%, tr:  69.36%, tr_best:  70.17%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.044665/  1.408473, val:  65.42%, val_best:  65.42%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.078895/  1.501690, val:  57.08%, val_best:  65.42%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.041992/  1.404616, val:  60.83%, val_best:  65.42%, tr:  71.71%, tr_best:  72.52%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.994876/  1.460015, val:  59.17%, val_best:  65.42%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.996271/  1.379311, val:  70.00%, val_best:  70.00%, tr:  77.63%, tr_best:  77.63%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.028991/  1.433958, val:  68.33%, val_best:  70.00%, tr:  75.89%, tr_best:  77.63%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.939940/  1.418037, val:  67.92%, val_best:  70.00%, tr:  79.16%, tr_best:  79.16%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.924673/  1.371417, val:  69.58%, val_best:  70.00%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.881493/  1.531863, val:  61.25%, val_best:  70.00%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.846008/  1.431923, val:  65.00%, val_best:  70.00%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.857089/  1.374058, val:  69.17%, val_best:  70.00%, tr:  83.96%, tr_best:  88.25%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.868096/  1.396791, val:  67.08%, val_best:  70.00%, tr:  82.02%, tr_best:  88.25%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.857675/  1.376997, val:  66.25%, val_best:  70.00%, tr:  82.74%, tr_best:  88.25%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.807055/  1.434608, val:  66.25%, val_best:  70.00%, tr:  86.31%, tr_best:  88.25%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.763694/  1.378412, val:  70.42%, val_best:  70.42%, tr:  88.15%, tr_best:  88.25%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.731771/  1.322043, val:  77.08%, val_best:  77.08%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.728185/  1.390537, val:  67.50%, val_best:  77.08%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.731905/  1.328901, val:  77.08%, val_best:  77.08%, tr:  89.27%, tr_best:  91.01%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.684831/  1.389685, val:  72.92%, val_best:  77.08%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.674588/  1.338896, val:  74.17%, val_best:  77.08%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.660405/  1.351813, val:  77.08%, val_best:  77.08%, tr:  92.75%, tr_best:  92.75%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.618781/  1.311888, val:  79.17%, val_best:  79.17%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.628071/  1.402862, val:  70.00%, val_best:  79.17%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.599726/  1.394749, val:  74.17%, val_best:  79.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.591263/  1.391381, val:  71.67%, val_best:  79.17%, tr:  95.30%, tr_best:  95.61%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.580159/  1.364966, val:  82.50%, val_best:  82.50%, tr:  94.89%, tr_best:  95.61%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.566270/  1.355677, val:  81.67%, val_best:  82.50%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.587285/  1.333029, val:  78.33%, val_best:  82.50%, tr:  94.89%, tr_best:  96.32%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.536776/  1.357877, val:  83.33%, val_best:  83.33%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.526131/  1.393692, val:  78.75%, val_best:  83.33%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.539281/  1.366354, val:  80.00%, val_best:  83.33%, tr:  96.42%, tr_best:  96.94%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.515173/  1.384732, val:  79.58%, val_best:  83.33%, tr:  96.73%, tr_best:  96.94%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.505246/  1.407125, val:  82.50%, val_best:  83.33%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.494837/  1.387186, val:  82.08%, val_best:  83.33%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.461348/  1.421451, val:  78.33%, val_best:  83.33%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.476675/  1.441497, val:  80.42%, val_best:  83.33%, tr:  97.34%, tr_best:  97.45%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.481424/  1.416381, val:  82.50%, val_best:  83.33%, tr:  96.42%, tr_best:  97.45%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.430464/  1.411360, val:  85.42%, val_best:  85.42%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.420798/  1.499494, val:  82.50%, val_best:  85.42%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.404561/  1.491954, val:  81.67%, val_best:  85.42%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.407886/  1.508587, val:  80.42%, val_best:  85.42%, tr:  97.34%, tr_best:  98.47%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.379987/  1.476344, val:  83.33%, val_best:  85.42%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.361993/  1.462868, val:  85.83%, val_best:  85.83%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.356054/  1.493693, val:  83.75%, val_best:  85.83%, tr:  98.57%, tr_best:  98.77%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.351152/  1.483110, val:  82.50%, val_best:  85.83%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.328448/  1.493489, val:  83.75%, val_best:  85.83%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.334293/  1.513694, val:  82.08%, val_best:  85.83%, tr:  99.08%, tr_best:  99.28%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.326358/  1.519262, val:  82.92%, val_best:  85.83%, tr:  98.98%, tr_best:  99.28%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.313683/  1.565576, val:  82.92%, val_best:  85.83%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.300928/  1.546313, val:  84.17%, val_best:  85.83%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.336071/  1.606559, val:  79.58%, val_best:  85.83%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.298157/  1.536245, val:  85.42%, val_best:  85.83%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.319866/  1.670031, val:  79.58%, val_best:  85.83%, tr:  98.57%, tr_best:  99.28%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.285292/  1.578380, val:  84.58%, val_best:  85.83%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.296071/  1.594930, val:  82.92%, val_best:  85.83%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.268858/  1.647013, val:  81.67%, val_best:  85.83%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.292988/  1.629241, val:  82.50%, val_best:  85.83%, tr:  98.98%, tr_best:  99.39%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.254055/  1.663705, val:  81.67%, val_best:  85.83%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.271331/  1.624453, val:  85.83%, val_best:  85.83%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.256380/  1.634581, val:  85.42%, val_best:  85.83%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.247455/  1.642229, val:  85.00%, val_best:  85.83%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.251525/  1.678504, val:  83.33%, val_best:  85.83%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.246035/  1.684846, val:  83.75%, val_best:  85.83%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.232947/  1.687246, val:  84.17%, val_best:  85.83%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.233852/  1.715383, val:  86.25%, val_best:  86.25%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.214862/  1.679374, val:  85.42%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.225110/  1.764147, val:  84.17%, val_best:  86.25%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.199969/  1.701770, val:  83.33%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.194133/  1.714499, val:  85.83%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.183404/  1.702941, val:  85.83%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.199166/  1.783468, val:  82.92%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.180464/  1.756995, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.177434/  1.800203, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.178917/  1.814281, val:  84.58%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.177508/  1.905339, val:  79.17%, val_best:  86.25%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.178084/  1.764624, val:  85.42%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.167156/  1.842420, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.164148/  1.865605, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.164079/  1.861189, val:  84.17%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e246fbed6e476fb96b0e74099d496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▁▁▅▅▃▅▇▇██▇▇▇████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▅▄▅▆▆▆▆▆▆▇▆▇▇▇██▇███▇████████████▇█</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▂▄▄▆▆▆▆▆▇▇▇▇▇███████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>████▇▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▂▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▅▄▅▆▆▆▆▆▆▇▆▇▇▇██▇███▇████████████▇█</td></tr><tr><td>val_loss</td><td>████▇▃▅▂▂▂▂▂▃▂▁▂▂▁▂▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>0.16408</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.84167</td></tr><tr><td>val_loss</td><td>1.86119</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-106</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x88s8yqb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x88s8yqb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_200247-x88s8yqb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2e1um50f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_200810-2e1um50f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2e1um50f' target=\"_blank\">bumbling-sweep-108</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2e1um50f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2e1um50f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 19.363825/ 26.274731, val:  37.50%, val_best:  37.50%, tr:  27.68%, tr_best:  27.68%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 25.204157/ 15.370431, val:  33.75%, val_best:  37.50%, tr:  39.33%, tr_best:  39.33%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 21.515629/ 31.035660, val:  38.75%, val_best:  38.75%, tr:  42.70%, tr_best:  42.70%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 16.638781/ 17.385792, val:  41.67%, val_best:  41.67%, tr:  49.85%, tr_best:  49.85%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 13.483982/ 18.970497, val:  40.83%, val_best:  41.67%, tr:  52.50%, tr_best:  52.50%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 14.297200/  8.997998, val:  51.67%, val_best:  51.67%, tr:  51.48%, tr_best:  52.50%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 11.448384/ 16.220263, val:  50.00%, val_best:  51.67%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 18.054281/ 20.071070, val:  46.25%, val_best:  51.67%, tr:  55.77%, tr_best:  58.12%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 12.104637/ 19.143353, val:  52.50%, val_best:  52.50%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 19.539524/ 18.533329, val:  63.33%, val_best:  63.33%, tr:  55.77%, tr_best:  59.45%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 14.593127/ 14.079158, val:  57.50%, val_best:  63.33%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 13.449772/ 20.838541, val:  49.58%, val_best:  63.33%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 13.524837/ 16.615389, val:  58.33%, val_best:  63.33%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 14.072598/ 15.865359, val:  53.33%, val_best:  63.33%, tr:  62.82%, tr_best:  62.82%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 10.642805/ 26.661020, val:  59.17%, val_best:  63.33%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 10.716327/ 13.347038, val:  55.00%, val_best:  63.33%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 13.564254/ 11.540359, val:  60.42%, val_best:  63.33%, tr:  65.99%, tr_best:  67.93%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 12.489397/ 15.851529, val:  60.42%, val_best:  63.33%, tr:  65.88%, tr_best:  67.93%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 10.100571/ 17.114712, val:  61.67%, val_best:  63.33%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  7.537427/ 18.435211, val:  56.67%, val_best:  63.33%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 15.462780/ 17.426922, val:  61.67%, val_best:  63.33%, tr:  64.15%, tr_best:  74.36%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 10.877453/ 13.225877, val:  62.50%, val_best:  63.33%, tr:  70.89%, tr_best:  74.36%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  9.884834/ 14.773526, val:  64.58%, val_best:  64.58%, tr:  72.93%, tr_best:  74.36%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  9.134900/ 14.462602, val:  70.42%, val_best:  70.42%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  5.646224/ 13.141367, val:  63.75%, val_best:  70.42%, tr:  80.49%, tr_best:  80.49%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 10.461411/ 16.635075, val:  66.67%, val_best:  70.42%, tr:  76.71%, tr_best:  80.49%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  6.348264/ 15.828328, val:  64.17%, val_best:  70.42%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  3.833531/ 14.512117, val:  63.75%, val_best:  70.42%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  5.882711/ 10.978755, val:  74.17%, val_best:  74.17%, tr:  81.31%, tr_best:  89.68%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  3.607038/ 10.515760, val:  66.67%, val_best:  74.17%, tr:  89.07%, tr_best:  89.68%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  3.738750/ 11.480208, val:  70.83%, val_best:  74.17%, tr:  87.95%, tr_best:  89.68%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  4.068417/ 14.825227, val:  66.67%, val_best:  74.17%, tr:  88.15%, tr_best:  89.68%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  5.245786/ 22.359510, val:  60.42%, val_best:  74.17%, tr:  85.39%, tr_best:  89.68%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  4.081972/ 11.418062, val:  76.67%, val_best:  76.67%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  4.491286/ 12.824805, val:  66.67%, val_best:  76.67%, tr:  88.46%, tr_best:  90.19%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.535505/ 10.598758, val:  76.25%, val_best:  76.67%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.218017/ 10.857292, val:  71.67%, val_best:  76.67%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.528151/ 15.118996, val:  65.42%, val_best:  76.67%, tr:  94.08%, tr_best:  95.30%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.828440/ 11.118481, val:  76.25%, val_best:  76.67%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.498288/  9.887196, val:  78.33%, val_best:  78.33%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.700370/ 11.776431, val:  70.00%, val_best:  78.33%, tr:  96.22%, tr_best:  97.34%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.492544/ 10.086941, val:  78.75%, val_best:  78.75%, tr:  96.73%, tr_best:  97.34%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  1.275880/ 10.362587, val:  75.42%, val_best:  78.75%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.261427/ 13.207586, val:  71.25%, val_best:  78.75%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.917937/ 10.671676, val:  74.58%, val_best:  78.75%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.912860/ 11.548289, val:  73.75%, val_best:  78.75%, tr:  98.37%, tr_best:  98.98%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.318588/ 10.622867, val:  75.42%, val_best:  78.75%, tr:  97.14%, tr_best:  98.98%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  3.077631/ 12.819504, val:  72.50%, val_best:  78.75%, tr:  93.16%, tr_best:  98.98%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.820133/ 10.933177, val:  76.67%, val_best:  78.75%, tr:  96.02%, tr_best:  98.98%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.686742/ 10.887197, val:  81.67%, val_best:  81.67%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.651038/ 12.006406, val:  76.67%, val_best:  81.67%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.549459/ 11.882790, val:  76.67%, val_best:  81.67%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.770384/ 10.837472, val:  79.17%, val_best:  81.67%, tr:  98.47%, tr_best:  99.39%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.407701/ 11.357688, val:  78.75%, val_best:  81.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.613945/ 11.255065, val:  77.92%, val_best:  81.67%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.318319/ 11.194988, val:  77.08%, val_best:  81.67%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.303529/ 10.860946, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.455330/ 10.669897, val:  80.83%, val_best:  81.67%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.321481/ 11.191951, val:  80.42%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.258754/ 10.340078, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.193823/ 10.256447, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.150554/ 10.358929, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.186346/ 10.508700, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.128238/ 11.167125, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.116781/ 11.258833, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.103839/ 10.666213, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.093838/ 10.880351, val:  81.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.097684/ 11.510939, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.138975/ 10.826751, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.128315/ 11.177417, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.100688/ 10.851373, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.103276/ 11.170411, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.072605/ 11.031643, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.113836/ 10.793372, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.146095/ 10.736507, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.053107/ 10.728313, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.045256/ 10.755943, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.045928/ 10.679750, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.056721/ 11.110341, val:  77.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.112615/ 10.901106, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.077470/ 11.857102, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.110016/ 10.256759, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.065564/ 11.034366, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.063663/ 10.518937, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.186264/ 10.871165, val:  81.67%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.085465/ 11.019920, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.073511/ 10.410564, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.026481/ 10.665424, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.045000/ 10.813211, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.044726/ 10.609191, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.028934/ 10.976398, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.019669/ 10.710556, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.045221/ 11.367007, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.018846/ 10.719973, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.025516/ 11.025354, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.040532/ 10.659403, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.033382/ 11.042619, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.064548/ 10.751208, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.116654/ 10.127073, val:  82.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.070791/ 10.574641, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491d49051af74d62bed19f991995c414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▃▅▃▆▆▆▅▇▇█▇▇▇█▇██▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▂▅▄▄▅▄▅▅▅▆▅▇▅▇▇▇▆█▇▇██████▇███████▇██</td></tr><tr><td>tr_acc</td><td>▁▂▃▄▄▄▅▅▆▅▆▆▇▇▇▇███▇████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▅▇▇▅▄▅▃▅▃▃▂▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▃▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▂▅▄▄▅▄▅▅▅▆▅▇▅▇▇▇▆█▇▇██████▇███████▇██</td></tr><tr><td>val_loss</td><td>▆█▄▄▄▃▇▃▄▂▂▃▁▅▁▃▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.07079</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>10.57464</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-sweep-108</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2e1um50f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2e1um50f</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_200810-2e1um50f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wgcfels0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_201328-wgcfels0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wgcfels0' target=\"_blank\">amber-sweep-110</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wgcfels0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wgcfels0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 16.815685/ 27.029757, val:  45.42%, val_best:  45.42%, tr:  27.68%, tr_best:  27.68%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 16.970657/ 16.004709, val:  44.17%, val_best:  45.42%, tr:  40.35%, tr_best:  40.35%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 12.777854/ 24.006514, val:  37.92%, val_best:  45.42%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 12.701122/ 19.068876, val:  37.92%, val_best:  45.42%, tr:  50.66%, tr_best:  50.66%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 14.903253/ 15.754815, val:  49.17%, val_best:  49.17%, tr:  50.87%, tr_best:  50.87%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 14.185539/ 15.933651, val:  49.17%, val_best:  49.17%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  8.885807/ 13.143877, val:  54.17%, val_best:  54.17%, tr:  58.53%, tr_best:  58.53%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 13.011757/ 17.775455, val:  45.83%, val_best:  54.17%, tr:  56.08%, tr_best:  58.53%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  9.624635/ 14.730061, val:  51.25%, val_best:  54.17%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 11.522602/ 17.151928, val:  45.00%, val_best:  54.17%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 14.979428/  9.631096, val:  55.83%, val_best:  55.83%, tr:  58.53%, tr_best:  61.18%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 12.601015/ 17.361624, val:  52.92%, val_best:  55.83%, tr:  62.21%, tr_best:  62.21%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 10.544340/  7.883458, val:  65.00%, val_best:  65.00%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  8.349195/ 16.898045, val:  50.00%, val_best:  65.00%, tr:  64.76%, tr_best:  64.76%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  9.936436/ 11.013273, val:  58.33%, val_best:  65.00%, tr:  67.82%, tr_best:  67.82%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  7.279257/ 21.698887, val:  45.83%, val_best:  65.00%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 10.490767/ 13.297498, val:  60.42%, val_best:  65.00%, tr:  68.23%, tr_best:  73.95%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  6.637451/ 10.877182, val:  60.83%, val_best:  65.00%, tr:  75.69%, tr_best:  75.69%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  6.578171/ 12.040715, val:  69.17%, val_best:  69.17%, tr:  76.20%, tr_best:  76.20%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  5.288208/ 12.676525, val:  60.83%, val_best:  69.17%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  5.822713/ 11.913887, val:  61.25%, val_best:  69.17%, tr:  79.37%, tr_best:  81.61%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  5.426922/  8.991439, val:  79.17%, val_best:  79.17%, tr:  82.74%, tr_best:  82.74%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  3.824252/  8.458432, val:  76.25%, val_best:  79.17%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  4.556746/ 13.756208, val:  60.00%, val_best:  79.17%, tr:  87.74%, tr_best:  89.89%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  3.649113/  8.993187, val:  76.67%, val_best:  79.17%, tr:  87.33%, tr_best:  89.89%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.956609/  9.297558, val:  70.83%, val_best:  79.17%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.372667/  9.365897, val:  65.83%, val_best:  79.17%, tr:  93.87%, tr_best:  93.87%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.729419/ 10.834908, val:  67.08%, val_best:  79.17%, tr:  91.22%, tr_best:  93.87%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  3.562497/  9.899565, val:  82.08%, val_best:  82.08%, tr:  89.38%, tr_best:  93.87%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.107096/  8.337523, val:  79.17%, val_best:  82.08%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  1.952644/  9.162052, val:  77.50%, val_best:  82.08%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  1.619692/  8.751285, val:  82.08%, val_best:  82.08%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.020880/ 10.376871, val:  70.42%, val_best:  82.08%, tr:  94.59%, tr_best:  96.63%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.520005/ 11.433329, val:  72.92%, val_best:  82.08%, tr:  93.26%, tr_best:  96.63%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  2.052519/ 10.758392, val:  76.67%, val_best:  82.08%, tr:  96.12%, tr_best:  96.63%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  1.308896/  9.429441, val:  77.92%, val_best:  82.08%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  1.204382/  9.883239, val:  79.17%, val_best:  82.08%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.578160/ 10.190271, val:  76.67%, val_best:  82.08%, tr:  96.42%, tr_best:  98.06%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  1.903617/ 10.427505, val:  75.42%, val_best:  82.08%, tr:  95.20%, tr_best:  98.06%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.147450/  9.110155, val:  82.50%, val_best:  82.50%, tr:  97.75%, tr_best:  98.06%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.955635/ 10.119437, val:  81.25%, val_best:  82.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.818878/  9.859761, val:  82.08%, val_best:  82.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.730354/  9.726891, val:  80.83%, val_best:  82.50%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.693759/ 12.493809, val:  77.50%, val_best:  82.50%, tr:  98.26%, tr_best:  99.59%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.973573/ 11.883199, val:  79.58%, val_best:  82.50%, tr:  98.37%, tr_best:  99.59%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.656639/  9.843023, val:  82.08%, val_best:  82.50%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.544400/ 10.251000, val:  82.92%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.460333/ 10.609394, val:  80.00%, val_best:  82.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.354751/ 11.399644, val:  76.67%, val_best:  82.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.444272/ 10.097233, val:  86.25%, val_best:  86.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.378618/ 10.672157, val:  82.08%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.317977/ 10.165940, val:  81.25%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.319167/ 10.174860, val:  84.58%, val_best:  86.25%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.205398/ 10.236989, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.193372/ 10.184099, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.206077/ 10.180619, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.223703/ 10.222894, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.226408/ 10.446639, val:  84.17%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.308716/ 10.326639, val:  82.92%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.188283/ 10.551930, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.136065/ 10.027318, val:  84.17%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.191993/ 10.434050, val:  83.33%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.180350/ 10.555761, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.141460/ 10.524742, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.193043/ 10.482661, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.158324/ 11.265647, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.128663/ 10.702250, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.188578/ 10.541249, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.139847/ 10.419876, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.108345/ 10.933984, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.119143/ 10.616575, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.067719/ 10.443916, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.103138/ 11.145075, val:  80.42%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.136129/ 10.715548, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.179398/ 11.325824, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.095487/ 10.666568, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.077479/ 10.793625, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.094362/ 10.817443, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.126427/ 10.133183, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.062485/ 10.848783, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.083752/ 10.176646, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.053116/ 10.790735, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.051218/ 10.306052, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.067710/ 11.057085, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.037556/ 10.443121, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.031908/ 11.111103, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.044067/ 10.425251, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.054968/ 10.642139, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.062983/ 11.094399, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.056515/ 10.906089, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.108126/ 10.842010, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.095129/ 11.180002, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.135255/ 11.074325, val:  84.58%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.060688/ 11.204486, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.089086/ 11.518418, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.083311/ 11.463758, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.051325/ 11.347639, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.032523/ 11.308355, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.035029/ 11.528448, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.033404/ 10.797292, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e935b71524d5479a843dc20e5ed4a4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅▃▃▅▇▆▅█▇███▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▃▂▂▅▄▄▄▇▇▅▇▆▇▇▇▇▇▇███████▇▇▇████████▇█</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▅▆▆▆▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▇▆▆▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▃▃▄▄▄▅▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▃▂▂▅▄▄▄▇▇▅▇▆▇▇▇▇▇▇███████▇▇▇████████▇█</td></tr><tr><td>val_loss</td><td>█▇▄▅▄▁▂▂▃▁▁▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0334</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>10.79729</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-110</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wgcfels0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wgcfels0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_201328-wgcfels0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 27ssac90 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffc0a824b084953a3d032e60cbb9bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112913323773278, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_201946-27ssac90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/27ssac90' target=\"_blank\">glorious-sweep-112</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/27ssac90' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/27ssac90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.189591/  2.681013, val:  39.58%, val_best:  39.58%, tr:  32.38%, tr_best:  32.38%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.024278/  2.034580, val:  50.42%, val_best:  50.42%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.272755/  2.285356, val:  47.92%, val_best:  50.42%, tr:  51.99%, tr_best:  51.99%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.529246/  2.141841, val:  56.25%, val_best:  56.25%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.937994/  1.928113, val:  58.33%, val_best:  58.33%, tr:  56.49%, tr_best:  58.63%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.548661/  1.854554, val:  55.42%, val_best:  58.33%, tr:  60.27%, tr_best:  60.27%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.319190/  2.224841, val:  47.92%, val_best:  58.33%, tr:  64.45%, tr_best:  64.45%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.383824/  2.698400, val:  50.42%, val_best:  58.33%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.263639/  2.186053, val:  57.92%, val_best:  58.33%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.221651/  2.289564, val:  60.00%, val_best:  60.00%, tr:  62.10%, tr_best:  67.01%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.763496/  1.621726, val:  61.67%, val_best:  61.67%, tr:  65.99%, tr_best:  67.01%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.701686/  3.731401, val:  43.33%, val_best:  61.67%, tr:  69.66%, tr_best:  69.66%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.881817/  1.625431, val:  64.17%, val_best:  64.17%, tr:  67.31%, tr_best:  69.66%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.398291/  2.740572, val:  55.42%, val_best:  64.17%, tr:  70.58%, tr_best:  70.58%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.317899/  3.276951, val:  55.42%, val_best:  64.17%, tr:  74.06%, tr_best:  74.06%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  1.075235/  1.905311, val:  62.92%, val_best:  64.17%, tr:  77.32%, tr_best:  77.32%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.090451/  1.819213, val:  64.58%, val_best:  64.58%, tr:  76.40%, tr_best:  77.32%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.830268/  2.285954, val:  65.42%, val_best:  65.42%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.047761/  1.865957, val:  68.75%, val_best:  68.75%, tr:  80.80%, tr_best:  82.94%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.809394/  1.960479, val:  66.67%, val_best:  68.75%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  1.598585/  2.817181, val:  61.67%, val_best:  68.75%, tr:  74.67%, tr_best:  85.70%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.097713/  2.007681, val:  65.42%, val_best:  68.75%, tr:  82.43%, tr_best:  85.70%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.607565/  2.125912, val:  64.58%, val_best:  68.75%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.477430/  2.287719, val:  64.17%, val_best:  68.75%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.422797/  1.799357, val:  72.50%, val_best:  72.50%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.400975/  2.085562, val:  69.17%, val_best:  72.50%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.393402/  2.553743, val:  63.75%, val_best:  72.50%, tr:  93.67%, tr_best:  93.97%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.370503/  2.090564, val:  74.58%, val_best:  74.58%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.486922/  2.139990, val:  72.08%, val_best:  74.58%, tr:  91.32%, tr_best:  95.10%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.269251/  2.122205, val:  70.83%, val_best:  74.58%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.245811/  2.157034, val:  69.58%, val_best:  74.58%, tr:  97.04%, tr_best:  97.85%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.200243/  2.051281, val:  72.92%, val_best:  74.58%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.174977/  2.368940, val:  67.50%, val_best:  74.58%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.195590/  2.237581, val:  72.50%, val_best:  74.58%, tr:  98.06%, tr_best:  98.57%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.170724/  2.413483, val:  72.08%, val_best:  74.58%, tr:  98.47%, tr_best:  98.57%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.354969/  2.462908, val:  70.83%, val_best:  74.58%, tr:  96.02%, tr_best:  98.57%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.109972/  2.204970, val:  73.33%, val_best:  74.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.082862/  2.420528, val:  68.75%, val_best:  74.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.096901/  2.327611, val:  75.83%, val_best:  75.83%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.088840/  2.372437, val:  74.17%, val_best:  75.83%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.054533/  2.567433, val:  70.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.069613/  2.439532, val:  74.58%, val_best:  75.83%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.038032/  2.420806, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.063734/  2.589256, val:  70.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.034882/  2.483270, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.029445/  2.567803, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.026892/  2.554765, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.023502/  2.545923, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.021434/  2.598356, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.017920/  2.597820, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.014236/  2.650981, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.015373/  2.622976, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.012628/  2.649110, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.012934/  2.666799, val:  75.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.009493/  2.676865, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.009337/  2.719397, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.007905/  2.677067, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.007918/  2.739219, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.007089/  2.715886, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.007321/  2.726737, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.007403/  2.752232, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.007021/  2.774225, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.006322/  2.799400, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.006663/  2.781136, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.006275/  2.808521, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.006832/  2.783684, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.005164/  2.794207, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.004452/  2.816886, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.005671/  2.813399, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.004335/  2.799936, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.005227/  2.814714, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.005054/  2.883396, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.004237/  2.874449, val:  75.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.004519/  2.896796, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.004305/  2.886806, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.003755/  2.866358, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.003325/  2.863374, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.003810/  2.863811, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.004062/  2.886941, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.003587/  2.865095, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.003175/  2.904144, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002830/  2.890540, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.003386/  2.917043, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.002485/  2.915791, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.002253/  2.943211, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.002467/  2.928400, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002332/  2.949036, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.002148/  2.928194, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.002013/  2.920280, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002057/  2.936995, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002009/  2.930733, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001999/  2.939908, val:  76.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001732/  2.948353, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.002043/  2.942866, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001873/  2.946817, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.002144/  2.951939, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.002130/  2.964011, val:  75.83%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.003695/  2.976264, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.002755/  2.977792, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.002733/  2.980707, val:  74.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d62324373c4594a91f7037a7a28510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▃▂▃▆▆▆▆██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▃▅▆▄▆▆▆▇▆▇▆▇▆▇▇█▇███████▇████████████</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▅▅▆▇▆▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅█▇▅▄▃▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▅▆▆▆▆▆▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▃▅▆▄▆▆▆▇▆▇▆▇▆▇▇█▇███████▇████████████</td></tr><tr><td>val_loss</td><td>▅▄▂▆▄▁█▄▂▃▂▅▃▄▅▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00273</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.74583</td></tr><tr><td>val_loss</td><td>2.98071</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-sweep-112</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/27ssac90' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/27ssac90</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_201946-27ssac90/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nzropmdp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_202516-nzropmdp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzropmdp' target=\"_blank\">fresh-sweep-114</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzropmdp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzropmdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.233205/  2.885348, val:  48.75%, val_best:  48.75%, tr:  32.69%, tr_best:  32.69%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.251796/  2.603237, val:  49.17%, val_best:  49.17%, tr:  51.48%, tr_best:  51.48%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.553468/  2.680786, val:  49.17%, val_best:  49.17%, tr:  53.22%, tr_best:  53.22%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.886286/  2.927650, val:  50.83%, val_best:  50.83%, tr:  59.14%, tr_best:  59.14%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.912172/  1.930701, val:  61.25%, val_best:  61.25%, tr:  59.24%, tr_best:  59.24%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.826656/  2.281605, val:  55.00%, val_best:  61.25%, tr:  63.33%, tr_best:  63.33%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.316392/  2.538579, val:  57.50%, val_best:  61.25%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.456642/  2.575949, val:  50.83%, val_best:  61.25%, tr:  63.43%, tr_best:  66.80%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.410051/  2.053946, val:  55.83%, val_best:  61.25%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.014061/  2.126938, val:  62.50%, val_best:  62.50%, tr:  66.80%, tr_best:  70.38%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.302268/  1.423764, val:  65.83%, val_best:  65.83%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.961804/  2.352137, val:  54.58%, val_best:  65.83%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.075516/  1.462476, val:  70.42%, val_best:  70.42%, tr:  78.55%, tr_best:  78.86%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.138984/  2.377849, val:  54.58%, val_best:  70.42%, tr:  77.12%, tr_best:  78.86%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.171451/  2.697406, val:  64.17%, val_best:  70.42%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.801599/  1.701425, val:  70.42%, val_best:  70.42%, tr:  88.25%, tr_best:  88.25%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.652673/  1.305056, val:  82.08%, val_best:  82.08%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.642809/  1.909999, val:  68.75%, val_best:  82.08%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.540132/  1.850911, val:  70.42%, val_best:  82.08%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.434751/  1.450024, val:  83.75%, val_best:  83.75%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.399854/  2.232773, val:  63.33%, val_best:  83.75%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.431717/  1.849017, val:  71.25%, val_best:  83.75%, tr:  94.69%, tr_best:  94.89%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.302715/  1.598701, val:  79.58%, val_best:  83.75%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.368796/  1.979620, val:  78.75%, val_best:  83.75%, tr:  95.71%, tr_best:  97.24%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.252601/  1.754311, val:  80.83%, val_best:  83.75%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.219377/  1.763829, val:  80.42%, val_best:  83.75%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.177752/  1.752450, val:  79.58%, val_best:  83.75%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.172993/  1.785510, val:  80.00%, val_best:  83.75%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.135874/  1.692886, val:  82.50%, val_best:  83.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.114406/  1.800593, val:  79.58%, val_best:  83.75%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.101037/  1.595530, val:  82.50%, val_best:  83.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.079222/  1.779388, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.202445/  1.902434, val:  79.17%, val_best:  83.75%, tr:  98.16%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.099883/  1.961266, val:  79.17%, val_best:  83.75%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.068068/  1.868779, val:  80.42%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.055006/  1.806557, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.049752/  1.729651, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.033256/  1.832965, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.029641/  1.764388, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.032119/  1.837874, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.037111/  1.786479, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.033550/  1.862473, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.024035/  1.815660, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.019007/  1.846671, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.015517/  1.833543, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.019335/  1.840527, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.019993/  1.880686, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.012844/  1.905780, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.012039/  1.894908, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.009845/  1.879605, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.008556/  1.905463, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.008401/  1.916675, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.010739/  1.961199, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.008515/  1.925189, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.006061/  1.979531, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.005619/  1.963542, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.005994/  1.967488, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.006541/  1.982592, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.006168/  1.984834, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.005706/  1.951628, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.005489/  1.962629, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.007038/  1.986569, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.005824/  1.991554, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.006857/  1.996778, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.004867/  2.011392, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.005206/  2.036138, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.004381/  2.047790, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.004697/  2.024341, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.003519/  2.039173, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002915/  2.064933, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002528/  2.039666, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.003389/  2.042520, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.003049/  2.032365, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.004842/  2.040803, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.003090/  2.044996, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002568/  2.038294, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002548/  2.063113, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002676/  2.100417, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002240/  2.083894, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.002153/  2.069538, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.002470/  2.063998, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002101/  2.086331, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001939/  2.085986, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001894/  2.067583, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.002233/  2.101539, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.003384/  2.082181, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002288/  2.121229, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001934/  2.081111, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001597/  2.103472, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001829/  2.084339, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001616/  2.096443, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001460/  2.116152, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001669/  2.130167, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001624/  2.092165, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001438/  2.125288, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001388/  2.118886, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001513/  2.114205, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001825/  2.133653, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001796/  2.116814, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001677/  2.117766, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bec1a1d6e94dcd9697a94e68ce53dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▆▄▆▇▇█▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▁▄▅▄▅█▅▇▇▇▇█▇▇███████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▆▅▇▄▄▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▄▅▅▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▁▄▅▄▅█▅▇▇▇▇█▇▇███████████████████████</td></tr><tr><td>val_loss</td><td>█▇▃▆▄▁▇▃▁▃▂▂▃▃▃▃▃▃▃▃▃▃▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00168</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>2.11777</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-114</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzropmdp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzropmdp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_202516-nzropmdp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wx5z681b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_203045-wx5z681b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wx5z681b' target=\"_blank\">firm-sweep-116</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wx5z681b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wx5z681b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.75, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  4.200511/  6.479202, val:  43.33%, val_best:  43.33%, tr:  28.19%, tr_best:  28.19%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  7.584684/  5.027276, val:  45.42%, val_best:  45.42%, tr:  40.65%, tr_best:  40.65%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  5.945079/  7.599246, val:  37.50%, val_best:  45.42%, tr:  45.05%, tr_best:  45.05%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  8.716803/ 11.198073, val:  37.92%, val_best:  45.42%, tr:  43.31%, tr_best:  45.05%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  9.046657/  7.869035, val:  54.17%, val_best:  54.17%, tr:  49.03%, tr_best:  49.03%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  8.916561/  7.911869, val:  50.83%, val_best:  54.17%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  8.357661/ 13.733480, val:  45.00%, val_best:  54.17%, tr:  52.20%, tr_best:  52.20%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  7.295323/  8.937086, val:  45.00%, val_best:  54.17%, tr:  56.08%, tr_best:  56.08%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  6.808314/  8.475446, val:  42.92%, val_best:  54.17%, tr:  56.49%, tr_best:  56.49%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  6.455123/ 11.070263, val:  44.58%, val_best:  54.17%, tr:  52.91%, tr_best:  56.49%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  9.050187/ 10.203449, val:  38.33%, val_best:  54.17%, tr:  50.87%, tr_best:  56.49%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  7.813977/  6.322823, val:  55.42%, val_best:  55.42%, tr:  57.71%, tr_best:  57.71%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  8.563612/  7.615721, val:  55.00%, val_best:  55.42%, tr:  55.57%, tr_best:  57.71%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  8.055799/ 13.005069, val:  55.00%, val_best:  55.42%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  9.901088/ 14.838126, val:  54.17%, val_best:  55.42%, tr:  58.43%, tr_best:  60.98%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  7.872737/  6.266997, val:  57.92%, val_best:  57.92%, tr:  60.88%, tr_best:  60.98%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  6.920975/ 10.540997, val:  45.42%, val_best:  57.92%, tr:  61.29%, tr_best:  61.29%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  8.588549/  7.351385, val:  58.75%, val_best:  58.75%, tr:  57.30%, tr_best:  61.29%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  5.848842/  7.411594, val:  54.58%, val_best:  58.75%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  7.201399/ 11.564223, val:  44.58%, val_best:  58.75%, tr:  59.55%, tr_best:  66.91%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  8.247005/  7.692538, val:  57.92%, val_best:  58.75%, tr:  60.98%, tr_best:  66.91%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  7.390272/  7.840501, val:  63.33%, val_best:  63.33%, tr:  64.15%, tr_best:  66.91%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  8.300420/  7.729944, val:  64.17%, val_best:  64.17%, tr:  63.84%, tr_best:  66.91%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  6.815089/  9.671560, val:  58.33%, val_best:  64.17%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  7.441381/  8.762639, val:  63.33%, val_best:  64.17%, tr:  66.70%, tr_best:  66.91%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  7.032763/ 11.061709, val:  61.67%, val_best:  64.17%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  5.620374/  8.518066, val:  63.75%, val_best:  64.17%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  4.550776/  7.276412, val:  60.00%, val_best:  64.17%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  6.234176/ 11.299375, val:  56.25%, val_best:  64.17%, tr:  72.22%, tr_best:  75.18%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  5.147920/ 10.474562, val:  50.42%, val_best:  64.17%, tr:  72.83%, tr_best:  75.18%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  6.207249/  7.884713, val:  65.42%, val_best:  65.42%, tr:  72.63%, tr_best:  75.18%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  5.632901/ 13.593664, val:  59.58%, val_best:  65.42%, tr:  70.68%, tr_best:  75.18%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  7.566924/ 14.471519, val:  65.42%, val_best:  65.42%, tr:  71.50%, tr_best:  75.18%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  5.748251/  7.652814, val:  72.92%, val_best:  72.92%, tr:  74.87%, tr_best:  75.18%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  7.334761/  9.412944, val:  68.75%, val_best:  72.92%, tr:  70.89%, tr_best:  75.18%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.936479/  7.579159, val:  70.00%, val_best:  72.92%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  3.693299/  9.637777, val:  59.58%, val_best:  72.92%, tr:  84.37%, tr_best:  84.37%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  6.810153/ 11.852184, val:  64.17%, val_best:  72.92%, tr:  73.34%, tr_best:  84.37%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  4.422292/ 12.152067, val:  54.58%, val_best:  72.92%, tr:  82.84%, tr_best:  84.37%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  3.449658/  7.397459, val:  74.17%, val_best:  74.17%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  3.173975/  8.335664, val:  63.33%, val_best:  74.17%, tr:  86.01%, tr_best:  86.31%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  3.377406/ 12.148862, val:  53.33%, val_best:  74.17%, tr:  84.27%, tr_best:  86.31%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  4.491756/  9.527860, val:  63.33%, val_best:  74.17%, tr:  81.21%, tr_best:  86.31%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  3.593506/  7.937098, val:  70.00%, val_best:  74.17%, tr:  82.12%, tr_best:  86.31%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  3.352149/  8.708920, val:  68.75%, val_best:  74.17%, tr:  86.52%, tr_best:  86.52%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  3.309326/  7.974119, val:  74.58%, val_best:  74.58%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  3.956278/  7.931472, val:  77.92%, val_best:  77.92%, tr:  81.82%, tr_best:  87.74%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  3.097843/  9.671588, val:  60.83%, val_best:  77.92%, tr:  89.58%, tr_best:  89.58%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.816560/  7.143037, val:  77.08%, val_best:  77.92%, tr:  89.48%, tr_best:  89.58%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.572464/  9.214375, val:  69.17%, val_best:  77.92%, tr:  90.81%, tr_best:  90.81%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  4.046179/  9.865373, val:  65.00%, val_best:  77.92%, tr:  88.25%, tr_best:  90.81%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.439411/  8.495549, val:  67.92%, val_best:  77.92%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  2.932221/  8.552970, val:  72.50%, val_best:  77.92%, tr:  86.93%, tr_best:  91.42%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  2.829469/  9.398013, val:  70.00%, val_best:  77.92%, tr:  90.40%, tr_best:  91.42%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.705038/  9.706039, val:  65.42%, val_best:  77.92%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.332128/  7.808637, val:  70.83%, val_best:  77.92%, tr:  93.56%, tr_best:  93.56%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.601578/  9.431848, val:  70.42%, val_best:  77.92%, tr:  90.30%, tr_best:  93.56%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.164915/  7.366796, val:  76.67%, val_best:  77.92%, tr:  93.26%, tr_best:  93.56%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.827552/  7.405222, val:  74.58%, val_best:  77.92%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.685275/  7.833979, val:  71.25%, val_best:  77.92%, tr:  95.30%, tr_best:  95.30%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.457996/  7.984524, val:  68.75%, val_best:  77.92%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.137420/  9.272992, val:  63.33%, val_best:  77.92%, tr:  92.54%, tr_best:  96.12%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.646350/  9.602538, val:  75.42%, val_best:  77.92%, tr:  90.60%, tr_best:  96.12%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  1.944950/  8.304156, val:  74.58%, val_best:  77.92%, tr:  94.69%, tr_best:  96.12%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  1.762481/  7.990337, val:  77.92%, val_best:  77.92%, tr:  94.79%, tr_best:  96.12%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  1.666044/  7.596965, val:  76.67%, val_best:  77.92%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.319421/  8.680306, val:  72.92%, val_best:  77.92%, tr:  92.34%, tr_best:  96.32%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  1.741449/  8.885927, val:  73.75%, val_best:  77.92%, tr:  95.61%, tr_best:  96.32%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  2.027672/  7.743995, val:  81.67%, val_best:  81.67%, tr:  94.38%, tr_best:  96.32%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  1.403040/  7.933595, val:  79.17%, val_best:  81.67%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  1.096440/  7.968944, val:  79.17%, val_best:  81.67%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  1.406774/  8.258750, val:  75.83%, val_best:  81.67%, tr:  96.94%, tr_best:  98.06%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  1.655388/  8.221876, val:  74.58%, val_best:  81.67%, tr:  96.73%, tr_best:  98.06%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  1.597434/  9.278728, val:  78.33%, val_best:  81.67%, tr:  95.71%, tr_best:  98.06%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  1.501590/  8.877422, val:  72.50%, val_best:  81.67%, tr:  97.34%, tr_best:  98.06%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  1.007985/  7.970828, val:  74.17%, val_best:  81.67%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.926483/  7.721369, val:  78.75%, val_best:  81.67%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.909047/  7.984389, val:  75.00%, val_best:  81.67%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  1.591281/  8.243221, val:  74.58%, val_best:  81.67%, tr:  95.20%, tr_best:  98.98%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  1.015424/  8.117344, val:  82.08%, val_best:  82.08%, tr:  98.37%, tr_best:  98.98%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.933219/  7.914622, val:  76.67%, val_best:  82.08%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.738111/  7.967771, val:  79.17%, val_best:  82.08%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.803190/ 10.541615, val:  70.00%, val_best:  82.08%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.939137/  8.581687, val:  77.08%, val_best:  82.08%, tr:  98.77%, tr_best:  99.18%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.799394/  8.114028, val:  80.42%, val_best:  82.08%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.844110/  8.249681, val:  80.00%, val_best:  82.08%, tr:  98.67%, tr_best:  99.18%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.941263/  7.953658, val:  79.17%, val_best:  82.08%, tr:  98.57%, tr_best:  99.18%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.742199/  9.148170, val:  75.00%, val_best:  82.08%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.971457/  8.174341, val:  78.33%, val_best:  82.08%, tr:  98.57%, tr_best:  99.18%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.882039/  7.678519, val:  83.75%, val_best:  83.75%, tr:  98.88%, tr_best:  99.18%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.648221/  8.293545, val:  80.00%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.974217/  8.467155, val:  81.25%, val_best:  83.75%, tr:  97.65%, tr_best:  99.59%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.625257/  8.290241, val:  82.92%, val_best:  83.75%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.645801/  8.298487, val:  82.08%, val_best:  83.75%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.542191/  7.967348, val:  82.50%, val_best:  83.75%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.504141/  8.268330, val:  77.08%, val_best:  83.75%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.469525/  7.919466, val:  81.25%, val_best:  83.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.594092/  7.942258, val:  80.00%, val_best:  83.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.638021/  8.332540, val:  79.58%, val_best:  83.75%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.744180/  7.613715, val:  80.42%, val_best:  83.75%, tr:  98.57%, tr_best:  99.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087a6a3804e44f54b869bc2189fb3fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▁▅▄▂▅▆▆▅▆▆▅▇▅▇▇▇▇▆▇█▇████▇▇█▇█▆▇██▇█▇██</td></tr><tr><td>summary_val_acc</td><td>▂▁▄▂▂▄▄▄▂▅▅▅▃▅▆▅▇▅▆▅▆▆▅▇▆▅▇▆▇▇▇▇█▆▇▇██▇▇</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▃▄▄▄▄▅▅▅▅▅▆▅▇▆▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>tr_epoch_loss</td><td>▄▅▇▆▅▇█▇▆▆▆▅▄▆▄▆▃▄▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▄▂▂▄▄▄▂▅▅▅▃▅▆▅▇▅▆▅▆▆▅▇▆▅▇▆▇▇▇▇█▆▇▇██▇▇</td></tr><tr><td>val_loss</td><td>▁▂▂▃▅▂█▂▅▂▃▃▄█▂▅▂▄▃▄▃▃▄▂▂▃▂▃▂▂▂▂▂▄▂▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.9857</td></tr><tr><td>tr_epoch_loss</td><td>0.74418</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>7.61371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-sweep-116</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wx5z681b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wx5z681b</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_203045-wx5z681b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ixwadct8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_203702-ixwadct8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ixwadct8' target=\"_blank\">hearty-sweep-118</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ixwadct8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ixwadct8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 3, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 45e076f350e4b27d8ff87367a19d69df\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.158919/  2.202000, val:  53.33%, val_best:  53.33%, tr:  24.21%, tr_best:  24.21%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.262150/  2.493455, val:  45.42%, val_best:  53.33%, tr:  48.11%, tr_best:  48.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.910365/  3.160977, val:  50.42%, val_best:  53.33%, tr:  51.99%, tr_best:  51.99%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.170996/  3.635644, val:  51.25%, val_best:  53.33%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.475592/  2.734103, val:  59.17%, val_best:  59.17%, tr:  58.12%, tr_best:  60.57%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.205483/  2.642354, val:  61.25%, val_best:  61.25%, tr:  61.08%, tr_best:  61.08%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.657819/  2.774061, val:  58.33%, val_best:  61.25%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.774516/  2.562287, val:  55.00%, val_best:  61.25%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.842544/  2.151876, val:  60.83%, val_best:  61.25%, tr:  68.03%, tr_best:  68.03%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.493580/  2.763739, val:  59.17%, val_best:  61.25%, tr:  66.50%, tr_best:  68.03%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.257788/  2.740072, val:  58.75%, val_best:  61.25%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.312843/  3.004942, val:  62.92%, val_best:  62.92%, tr:  68.13%, tr_best:  68.23%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.025832/  2.502827, val:  63.75%, val_best:  63.75%, tr:  72.83%, tr_best:  72.83%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.921135/  3.977357, val:  55.83%, val_best:  63.75%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.085892/  4.130024, val:  56.67%, val_best:  63.75%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.162655/  4.268164, val:  58.33%, val_best:  63.75%, tr:  73.14%, tr_best:  74.46%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  1.987048/  3.509721, val:  61.67%, val_best:  63.75%, tr:  77.63%, tr_best:  77.63%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  1.763820/  3.306667, val:  65.83%, val_best:  65.83%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  1.439524/  3.244778, val:  69.17%, val_best:  69.17%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  1.500467/  3.803234, val:  57.50%, val_best:  69.17%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.491704/  4.318708, val:  56.25%, val_best:  69.17%, tr:  77.63%, tr_best:  85.70%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  1.871091/  5.253419, val:  65.83%, val_best:  69.17%, tr:  83.35%, tr_best:  85.70%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  1.473823/  3.862241, val:  74.17%, val_best:  74.17%, tr:  87.95%, tr_best:  87.95%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  1.329031/  4.181344, val:  66.67%, val_best:  74.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  1.227773/  4.348045, val:  66.67%, val_best:  74.17%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  1.159649/  4.010387, val:  67.92%, val_best:  74.17%, tr:  94.08%, tr_best:  94.08%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  1.431607/  4.413397, val:  67.92%, val_best:  74.17%, tr:  89.58%, tr_best:  94.08%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  1.246762/  4.452457, val:  70.42%, val_best:  74.17%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  1.254726/  4.248132, val:  78.33%, val_best:  78.33%, tr:  93.87%, tr_best:  94.38%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  1.164192/  4.556507, val:  70.42%, val_best:  78.33%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  1.127473/  4.768045, val:  70.83%, val_best:  78.33%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  1.081565/  4.981831, val:  74.17%, val_best:  78.33%, tr:  96.22%, tr_best:  96.42%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  1.382399/  5.156547, val:  73.75%, val_best:  78.33%, tr:  93.97%, tr_best:  96.42%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  1.097411/  5.519323, val:  74.17%, val_best:  78.33%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  1.162542/  5.743013, val:  73.33%, val_best:  78.33%, tr:  95.71%, tr_best:  96.53%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  1.058741/  5.449726, val:  76.67%, val_best:  78.33%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.973193/  5.468093, val:  78.75%, val_best:  78.75%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  1.057029/  6.448984, val:  65.42%, val_best:  78.75%, tr:  97.45%, tr_best:  98.37%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  1.250764/  5.763024, val:  77.92%, val_best:  78.75%, tr:  96.42%, tr_best:  98.37%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.960869/  6.001623, val:  76.25%, val_best:  78.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.930110/  5.944229, val:  75.42%, val_best:  78.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.889508/  5.913131, val:  78.33%, val_best:  78.75%, tr:  98.26%, tr_best:  98.47%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.776518/  6.233888, val:  77.92%, val_best:  78.75%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.768605/  6.375433, val:  73.75%, val_best:  78.75%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.892988/  6.726219, val:  79.17%, val_best:  79.17%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.773486/  6.737723, val:  77.50%, val_best:  79.17%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.820222/  6.943592, val:  77.08%, val_best:  79.17%, tr:  99.28%, tr_best:  99.59%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.741135/  7.009386, val:  77.08%, val_best:  79.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.715883/  7.075821, val:  78.75%, val_best:  79.17%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.698723/  7.289232, val:  79.17%, val_best:  79.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.594797/  7.397673, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.594559/  7.431638, val:  81.25%, val_best:  81.25%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.641589/  7.840540, val:  79.58%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.609408/  8.320494, val:  72.50%, val_best:  81.25%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.537371/  7.903512, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.554360/  8.490902, val:  78.75%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.579146/  8.573096, val:  77.08%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.658164/  8.215792, val:  81.67%, val_best:  81.67%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.517487/  8.242540, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.507075/  8.744087, val:  76.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.409661/  8.664561, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.406527/  8.740046, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.491851/  9.153049, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.475560/  9.320946, val:  76.67%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.597052/  8.883257, val:  79.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.446935/  9.237028, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.429066/  9.651292, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.446421/  9.544794, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.400858/  9.346743, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.335021/  9.799678, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.397918/  9.540175, val:  81.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.415789/  9.912456, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.335177/ 10.227929, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.380072/ 10.129696, val:  81.25%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.361497/ 10.140726, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.353522/ 10.176398, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.324124/ 10.517482, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.316676/ 10.624279, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.325396/ 11.192955, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.345221/ 10.704013, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.293297/ 11.347095, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.352062/ 10.963799, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.341124/ 10.796882, val:  80.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.312834/ 11.285692, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.266964/ 10.956514, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.278603/ 10.972315, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.274417/ 11.472560, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.246025/ 11.611012, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.248364/ 11.657941, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.253143/ 11.408241, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.222052/ 11.635468, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.216078/ 11.243421, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.251691/ 11.584186, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.220315/ 11.745297, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.253707/ 11.995111, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.215093/ 12.018735, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.221995/ 11.872770, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.196896/ 11.997541, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.225672/ 11.891211, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.210808/ 11.907944, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51a1bd5f7a7425db4a1f07c500d2fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▄▄▁▅▆▇▆▆███▆▇█████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▃▂▃▄▂▄▂▄▄▅▅▆▆▄▆▇▇▆▇▇▇▇▆▇▇▇▇▇█▇▇▇█▇█▇▇█</td></tr><tr><td>tr_acc</td><td>▁▄▄▅▅▅▆▆▇▆▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▆█▇▅▇▆▆▅▄▅▄▄▃▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▃▃▃▃▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▃▂▃▄▂▄▂▄▄▅▅▆▆▄▆▇▇▆▇▇▇▇▆▇▇▇▇▇█▇▇▇█▇█▇▇█</td></tr><tr><td>val_loss</td><td>▁▂▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.21081</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>11.90794</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-118</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ixwadct8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ixwadct8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_203702-ixwadct8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mmt02wav with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_204341-mmt02wav</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mmt02wav' target=\"_blank\">ruby-sweep-120</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mmt02wav' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mmt02wav</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323420/  2.309336, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.50%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.50%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.50%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.291458/  2.143747, val:  21.67%, val_best:  21.67%, tr:  11.85%, tr_best:  11.85%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.905825/  1.782934, val:  39.17%, val_best:  39.17%, tr:  29.42%, tr_best:  29.42%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.615709/  1.659846, val:  50.00%, val_best:  50.00%, tr:  48.52%, tr_best:  48.52%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.386631/  1.542441, val:  52.92%, val_best:  52.92%, tr:  60.27%, tr_best:  60.27%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.175486/  1.405655, val:  60.42%, val_best:  60.42%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.116167/  1.325251, val:  66.67%, val_best:  66.67%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.056790/  1.309287, val:  62.08%, val_best:  66.67%, tr:  69.46%, tr_best:  69.46%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  1.016342/  1.289853, val:  64.17%, val_best:  66.67%, tr:  67.52%, tr_best:  69.46%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.984676/  1.532358, val:  60.83%, val_best:  66.67%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.972607/  1.274819, val:  63.33%, val_best:  66.67%, tr:  71.91%, tr_best:  71.91%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.954623/  1.250425, val:  62.92%, val_best:  66.67%, tr:  70.58%, tr_best:  71.91%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.915580/  1.321139, val:  66.25%, val_best:  66.67%, tr:  73.95%, tr_best:  73.95%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.929419/  1.363532, val:  65.00%, val_best:  66.67%, tr:  72.93%, tr_best:  73.95%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.912929/  1.267261, val:  66.67%, val_best:  66.67%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.845255/  1.321691, val:  65.00%, val_best:  66.67%, tr:  75.59%, tr_best:  76.71%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.812203/  1.300644, val:  69.17%, val_best:  69.17%, tr:  80.80%, tr_best:  80.80%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.775248/  1.261587, val:  67.92%, val_best:  69.17%, tr:  81.82%, tr_best:  81.82%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.774768/  1.278479, val:  70.83%, val_best:  70.83%, tr:  85.60%, tr_best:  85.60%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.711207/  1.286118, val:  75.00%, val_best:  75.00%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.718109/  1.269744, val:  74.17%, val_best:  75.00%, tr:  87.03%, tr_best:  90.91%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.753037/  1.277771, val:  74.17%, val_best:  75.00%, tr:  86.41%, tr_best:  90.91%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.682177/  1.311690, val:  70.00%, val_best:  75.00%, tr:  89.17%, tr_best:  90.91%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.666983/  1.278850, val:  81.25%, val_best:  81.25%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.631252/  1.374220, val:  67.92%, val_best:  81.25%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.623162/  1.323416, val:  71.67%, val_best:  81.25%, tr:  93.36%, tr_best:  94.48%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.596485/  1.345698, val:  74.58%, val_best:  81.25%, tr:  92.54%, tr_best:  94.48%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.674504/  1.346274, val:  76.25%, val_best:  81.25%, tr:  89.48%, tr_best:  94.48%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.610440/  1.381903, val:  69.58%, val_best:  81.25%, tr:  91.73%, tr_best:  94.48%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.575398/  1.378321, val:  71.25%, val_best:  81.25%, tr:  93.46%, tr_best:  94.48%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.565801/  1.354012, val:  72.08%, val_best:  81.25%, tr:  94.79%, tr_best:  94.79%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.554612/  1.329122, val:  79.17%, val_best:  81.25%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.544284/  1.459644, val:  71.67%, val_best:  81.25%, tr:  94.89%, tr_best:  95.51%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.548300/  1.332869, val:  80.42%, val_best:  81.25%, tr:  95.40%, tr_best:  95.51%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.506975/  1.476480, val:  72.50%, val_best:  81.25%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.506683/  1.319353, val:  78.33%, val_best:  81.25%, tr:  95.61%, tr_best:  96.12%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.483775/  1.401120, val:  76.67%, val_best:  81.25%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.460515/  1.368401, val:  81.25%, val_best:  81.25%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.445603/  1.446276, val:  72.50%, val_best:  81.25%, tr:  97.14%, tr_best:  97.55%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.433210/  1.462573, val:  75.83%, val_best:  81.25%, tr:  97.34%, tr_best:  97.55%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.414706/  1.404942, val:  79.58%, val_best:  81.25%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.429890/  1.386918, val:  81.25%, val_best:  81.25%, tr:  97.55%, tr_best:  98.06%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.398412/  1.415054, val:  80.83%, val_best:  81.25%, tr:  97.96%, tr_best:  98.06%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.411060/  1.384066, val:  81.25%, val_best:  81.25%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.381497/  1.432294, val:  82.92%, val_best:  82.92%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.359656/  1.461407, val:  80.83%, val_best:  82.92%, tr:  98.26%, tr_best:  98.57%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.363088/  1.448848, val:  81.67%, val_best:  82.92%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.366607/  1.430104, val:  82.08%, val_best:  82.92%, tr:  98.47%, tr_best:  98.67%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.342048/  1.482495, val:  82.50%, val_best:  82.92%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.334191/  1.490056, val:  81.25%, val_best:  82.92%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.335905/  1.503259, val:  78.75%, val_best:  82.92%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.328649/  1.525111, val:  81.25%, val_best:  82.92%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.348626/  1.511465, val:  81.67%, val_best:  82.92%, tr:  98.47%, tr_best:  98.98%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.321803/  1.493647, val:  83.33%, val_best:  83.33%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.323351/  1.528425, val:  84.58%, val_best:  84.58%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.309125/  1.562680, val:  81.67%, val_best:  84.58%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.319893/  1.492612, val:  80.42%, val_best:  84.58%, tr:  98.37%, tr_best:  99.08%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.305815/  1.457483, val:  84.17%, val_best:  84.58%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.297815/  1.517922, val:  82.08%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.292889/  1.539246, val:  82.08%, val_best:  84.58%, tr:  98.57%, tr_best:  99.39%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.256533/  1.551692, val:  81.25%, val_best:  84.58%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.248233/  1.510673, val:  84.17%, val_best:  84.58%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.247878/  1.553670, val:  82.92%, val_best:  84.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.241921/  1.562485, val:  83.75%, val_best:  84.58%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.240875/  1.595793, val:  84.17%, val_best:  84.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.233267/  1.592707, val:  82.08%, val_best:  84.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.234385/  1.635394, val:  82.50%, val_best:  84.58%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.234625/  1.650525, val:  82.92%, val_best:  84.58%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.241504/  1.677817, val:  80.83%, val_best:  84.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.232623/  1.620835, val:  82.92%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.236167/  1.631208, val:  82.08%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.205747/  1.661351, val:  79.17%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.214982/  1.690198, val:  86.25%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.197944/  1.702519, val:  82.92%, val_best:  86.25%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.206654/  1.657902, val:  84.58%, val_best:  86.25%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.190452/  1.669877, val:  84.58%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.178747/  1.708617, val:  83.33%, val_best:  86.25%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.186692/  1.759594, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.191603/  1.764653, val:  82.08%, val_best:  86.25%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.177530/  1.696043, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.181490/  1.722131, val:  85.00%, val_best:  86.25%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.166945/  1.807132, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.188475/  1.753338, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.174358/  1.787797, val:  82.50%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.154527/  1.819121, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.154205/  1.751974, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.146599/  1.799871, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.146129/  1.783239, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.152013/  1.855630, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.151673/  1.872150, val:  82.92%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.132773/  1.939177, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.142216/  1.837168, val:  82.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.140994/  1.877620, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.131438/  1.893683, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.140967/  1.861853, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5511e443e249cc8b7301791bfe199f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▅▆▆▇▃▇█▇██▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▄▅▆▆▆▆▆▇▇▆▇▇▇▇█▇██████▇██████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▃▅▆▆▆▆▇▇▇█▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>███▇▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▄▅▆▆▆▆▆▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▄▅▆▆▆▆▆▇▇▆▇▇▇▇█▇██████▇██████████████</td></tr><tr><td>val_loss</td><td>███▄▃▁▃▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▃▄▄▄▄▄▅▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.14097</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.83333</td></tr><tr><td>val_loss</td><td>1.86185</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-120</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mmt02wav' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mmt02wav</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_204341-mmt02wav/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p8ycd5p1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_204921-p8ycd5p1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p8ycd5p1' target=\"_blank\">still-sweep-122</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p8ycd5p1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p8ycd5p1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 58.799767/ 40.038303, val:  36.67%, val_best:  36.67%, tr:  23.90%, tr_best:  23.90%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 34.441307/ 29.254259, val:  35.42%, val_best:  36.67%, tr:  40.76%, tr_best:  40.76%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 33.135933/ 44.839859, val:  31.67%, val_best:  36.67%, tr:  41.16%, tr_best:  41.16%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 25.868589/ 23.502264, val:  47.92%, val_best:  47.92%, tr:  44.84%, tr_best:  44.84%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 24.009020/ 34.667068, val:  43.33%, val_best:  47.92%, tr:  46.58%, tr_best:  46.58%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 22.715925/ 30.162796, val:  51.25%, val_best:  51.25%, tr:  53.52%, tr_best:  53.52%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 23.130873/ 32.664448, val:  42.92%, val_best:  51.25%, tr:  49.64%, tr_best:  53.52%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 20.414373/ 20.673605, val:  40.00%, val_best:  51.25%, tr:  51.58%, tr_best:  53.52%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 23.312195/ 22.915033, val:  50.42%, val_best:  51.25%, tr:  54.95%, tr_best:  54.95%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 18.434002/ 24.385271, val:  52.08%, val_best:  52.08%, tr:  57.71%, tr_best:  57.71%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 18.565561/ 27.593752, val:  54.17%, val_best:  54.17%, tr:  58.02%, tr_best:  58.02%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 16.754900/ 36.890957, val:  46.67%, val_best:  54.17%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 17.267088/ 14.759045, val:  61.67%, val_best:  61.67%, tr:  60.98%, tr_best:  61.59%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 14.723689/ 24.732594, val:  52.92%, val_best:  61.67%, tr:  62.00%, tr_best:  62.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 19.241249/ 23.489548, val:  52.08%, val_best:  61.67%, tr:  62.92%, tr_best:  62.92%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 18.878157/ 23.732336, val:  57.08%, val_best:  61.67%, tr:  61.49%, tr_best:  62.92%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 17.801861/ 23.804173, val:  65.00%, val_best:  65.00%, tr:  62.10%, tr_best:  62.92%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 13.862562/ 27.913904, val:  57.50%, val_best:  65.00%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 17.115984/ 28.270504, val:  57.08%, val_best:  65.00%, tr:  66.50%, tr_best:  66.91%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 13.397598/ 16.717466, val:  69.58%, val_best:  69.58%, tr:  67.42%, tr_best:  67.42%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 19.864859/ 27.989424, val:  53.33%, val_best:  69.58%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 18.481972/ 25.844431, val:  60.83%, val_best:  69.58%, tr:  68.23%, tr_best:  68.23%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 14.898220/ 19.240629, val:  58.33%, val_best:  69.58%, tr:  70.48%, tr_best:  70.48%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 10.742680/ 18.600393, val:  69.58%, val_best:  69.58%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  9.306538/ 17.344028, val:  66.25%, val_best:  69.58%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  8.968408/ 22.051134, val:  59.17%, val_best:  69.58%, tr:  78.35%, tr_best:  78.86%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 10.726943/ 22.630270, val:  66.25%, val_best:  69.58%, tr:  77.73%, tr_best:  78.86%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  8.291721/ 16.414860, val:  71.25%, val_best:  71.25%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 12.147092/ 18.379631, val:  68.75%, val_best:  71.25%, tr:  80.80%, tr_best:  82.43%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  6.418566/ 19.187040, val:  65.83%, val_best:  71.25%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  7.352163/ 15.802610, val:  75.00%, val_best:  75.00%, tr:  84.47%, tr_best:  88.56%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  6.438288/ 17.722992, val:  72.08%, val_best:  75.00%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  8.244319/ 33.402897, val:  60.83%, val_best:  75.00%, tr:  85.29%, tr_best:  89.17%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  6.936863/ 18.813517, val:  72.50%, val_best:  75.00%, tr:  86.93%, tr_best:  89.17%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  6.811401/ 18.136131, val:  71.67%, val_best:  75.00%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  5.635008/ 16.687641, val:  78.75%, val_best:  78.75%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  4.087686/ 20.139370, val:  70.83%, val_best:  78.75%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  7.493690/ 22.644730, val:  69.17%, val_best:  78.75%, tr:  87.33%, tr_best:  95.20%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  5.298653/ 19.482027, val:  77.50%, val_best:  78.75%, tr:  92.85%, tr_best:  95.20%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  3.975052/ 16.848974, val:  71.67%, val_best:  78.75%, tr:  95.10%, tr_best:  95.20%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  3.741676/ 17.484850, val:  77.92%, val_best:  78.75%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  3.041368/ 18.042204, val:  74.58%, val_best:  78.75%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  3.172138/ 19.089020, val:  79.58%, val_best:  79.58%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  3.292358/ 21.544573, val:  70.42%, val_best:  79.58%, tr:  96.53%, tr_best:  97.14%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  4.398154/ 19.813343, val:  75.00%, val_best:  79.58%, tr:  95.61%, tr_best:  97.14%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  3.483626/ 21.995668, val:  77.50%, val_best:  79.58%, tr:  96.83%, tr_best:  97.14%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.806587/ 19.766939, val:  78.75%, val_best:  79.58%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.409835/ 20.865719, val:  79.58%, val_best:  79.58%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.661749/ 19.908442, val:  80.42%, val_best:  80.42%, tr:  97.45%, tr_best:  98.77%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.134714/ 21.291512, val:  80.42%, val_best:  80.42%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.926872/ 22.743410, val:  73.75%, val_best:  80.42%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.840939/ 19.162273, val:  80.42%, val_best:  80.42%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.544364/ 21.461967, val:  80.00%, val_best:  80.42%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  1.343579/ 21.417252, val:  77.50%, val_best:  80.42%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.167357/ 20.520969, val:  80.83%, val_best:  80.83%, tr:  97.75%, tr_best:  99.69%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  1.623749/ 21.743130, val:  77.08%, val_best:  80.83%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.358147/ 27.188923, val:  71.25%, val_best:  80.83%, tr:  97.55%, tr_best:  99.69%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  3.330584/ 21.206467, val:  81.25%, val_best:  81.25%, tr:  95.81%, tr_best:  99.69%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.464743/ 23.010427, val:  76.67%, val_best:  81.25%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.172403/ 22.951731, val:  78.33%, val_best:  81.25%, tr:  99.39%, tr_best:  99.69%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.005503/ 22.438408, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.901947/ 22.945126, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.849223/ 23.084686, val:  80.83%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  1.125349/ 23.477345, val:  82.08%, val_best:  82.08%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.705087/ 24.460632, val:  79.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.704502/ 22.522053, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.538576/ 21.628027, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.596095/ 22.649508, val:  79.58%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.487051/ 22.359013, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.388862/ 23.162807, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.484134/ 22.257854, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.543717/ 21.579128, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.425320/ 22.155209, val:  80.42%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.346238/ 23.442894, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.325833/ 22.517252, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.269666/ 22.861752, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.320821/ 24.118198, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.289290/ 23.933575, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.480717/ 23.515657, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.438646/ 23.996058, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.303311/ 23.981144, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.301311/ 24.412647, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.281481/ 24.623026, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.471695/ 22.978853, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.408942/ 22.996361, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.261722/ 23.496559, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.326904/ 23.740128, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.321180/ 24.004965, val:  80.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.327803/ 24.551039, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.433643/ 22.984415, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.272357/ 23.889429, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.230813/ 25.015892, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.233371/ 24.669491, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.160260/ 23.626303, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.187243/ 24.292934, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.235763/ 23.422043, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.182090/ 24.436464, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.461722/ 23.751812, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.461853/ 23.676229, val:  80.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.409170/ 26.103920, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f044b21ff3840f68dedfa1c683d4dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▂▅▁▅▅▅▃▇▅▇▇▇▇▇███▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▃▂▄▅▄▅▆▅▆▆▆▅█▆▇█▇█████▇███████████████</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▃▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▃▂▄▅▄▅▆▅▆▆▆▅█▆▇█▇█████▇███████████████</td></tr><tr><td>val_loss</td><td>▇█▆▂▃▁▃▄▁▄▂▃▂▅▁▃▁▂▂▂▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.40917</td></tr><tr><td>val_acc_best</td><td>0.82917</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>26.10392</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-sweep-122</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p8ycd5p1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/p8ycd5p1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_204921-p8ycd5p1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v1nyep69 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_205553-v1nyep69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v1nyep69' target=\"_blank\">autumn-sweep-124</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v1nyep69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v1nyep69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 4, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = d133da00785cbf60fdc9e42f05c56a11\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 34.598541/ 42.845165, val:  34.58%, val_best:  34.58%, tr:  25.43%, tr_best:  25.43%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 28.806881/ 27.639259, val:  36.25%, val_best:  36.25%, tr:  35.75%, tr_best:  35.75%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 16.850803/ 12.739230, val:  48.33%, val_best:  48.33%, tr:  47.09%, tr_best:  47.09%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 14.765196/ 26.317339, val:  37.08%, val_best:  48.33%, tr:  48.72%, tr_best:  48.72%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 19.870354/ 16.831970, val:  56.25%, val_best:  56.25%, tr:  48.21%, tr_best:  48.72%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 13.916819/ 16.374781, val:  57.92%, val_best:  57.92%, tr:  55.26%, tr_best:  55.26%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 12.634999/ 20.286915, val:  40.00%, val_best:  57.92%, tr:  52.50%, tr_best:  55.26%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 15.550255/ 22.341232, val:  44.58%, val_best:  57.92%, tr:  52.20%, tr_best:  55.26%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 15.658232/ 20.462894, val:  47.92%, val_best:  57.92%, tr:  54.03%, tr_best:  55.26%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 16.972057/ 17.018126, val:  53.33%, val_best:  57.92%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 12.723166/ 19.877964, val:  50.42%, val_best:  57.92%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 15.064134/ 25.330473, val:  40.83%, val_best:  57.92%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 14.590794/ 13.676999, val:  57.50%, val_best:  57.92%, tr:  59.75%, tr_best:  59.96%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 19.408733/ 30.163082, val:  38.75%, val_best:  57.92%, tr:  54.34%, tr_best:  59.96%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 16.922546/ 17.455948, val:  57.50%, val_best:  57.92%, tr:  61.70%, tr_best:  61.70%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 12.785129/ 10.438669, val:  58.75%, val_best:  58.75%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 12.170708/ 14.912983, val:  65.00%, val_best:  65.00%, tr:  63.74%, tr_best:  66.09%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 12.790542/ 14.630275, val:  60.00%, val_best:  65.00%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  8.290034/ 13.051970, val:  51.67%, val_best:  65.00%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  9.049175/ 16.592379, val:  54.17%, val_best:  65.00%, tr:  68.54%, tr_best:  71.60%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 13.247643/ 23.011057, val:  44.58%, val_best:  65.00%, tr:  67.93%, tr_best:  71.60%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 11.513073/ 17.450842, val:  55.83%, val_best:  65.00%, tr:  65.88%, tr_best:  71.60%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 10.349063/ 13.818614, val:  55.83%, val_best:  65.00%, tr:  70.28%, tr_best:  71.60%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  8.312199/ 12.130225, val:  65.83%, val_best:  65.83%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  5.085948/ 13.006546, val:  60.00%, val_best:  65.83%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  7.468075/ 14.762388, val:  64.17%, val_best:  65.83%, tr:  76.51%, tr_best:  80.08%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  5.865245/ 12.758136, val:  65.83%, val_best:  65.83%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  6.745067/ 14.552789, val:  61.67%, val_best:  65.83%, tr:  78.45%, tr_best:  81.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  6.972170/ 10.276715, val:  75.42%, val_best:  75.42%, tr:  80.59%, tr_best:  81.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  5.644156/ 12.284469, val:  64.58%, val_best:  75.42%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  5.271861/ 10.291961, val:  67.92%, val_best:  75.42%, tr:  85.90%, tr_best:  85.90%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  3.837924/ 14.088735, val:  69.58%, val_best:  75.42%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  6.994615/ 22.044924, val:  61.67%, val_best:  75.42%, tr:  81.92%, tr_best:  87.54%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  6.684050/ 11.305238, val:  71.25%, val_best:  75.42%, tr:  85.19%, tr_best:  87.54%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  4.915330/ 14.550916, val:  66.25%, val_best:  75.42%, tr:  86.11%, tr_best:  87.54%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  4.369716/ 11.940466, val:  71.25%, val_best:  75.42%, tr:  87.84%, tr_best:  87.84%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  3.394472/ 13.455760, val:  67.50%, val_best:  75.42%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  4.150355/ 13.792921, val:  71.25%, val_best:  75.42%, tr:  89.79%, tr_best:  90.60%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  3.964867/ 13.281427, val:  62.08%, val_best:  75.42%, tr:  89.68%, tr_best:  90.60%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.960257/  9.638389, val:  77.50%, val_best:  77.50%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.837491/ 18.818956, val:  57.92%, val_best:  77.50%, tr:  95.71%, tr_best:  96.02%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  3.466091/ 12.752874, val:  70.42%, val_best:  77.50%, tr:  92.34%, tr_best:  96.02%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  4.104543/ 16.303070, val:  62.50%, val_best:  77.50%, tr:  90.30%, tr_best:  96.02%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  3.110397/ 17.374420, val:  70.00%, val_best:  77.50%, tr:  90.09%, tr_best:  96.02%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  5.411625/ 12.662676, val:  72.08%, val_best:  77.50%, tr:  88.56%, tr_best:  96.02%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.325259/ 12.186578, val:  78.33%, val_best:  78.33%, tr:  95.30%, tr_best:  96.02%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  3.792835/ 14.405923, val:  73.75%, val_best:  78.33%, tr:  91.32%, tr_best:  96.02%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  1.882419/ 12.075043, val:  77.08%, val_best:  78.33%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.629995/ 11.598236, val:  79.58%, val_best:  79.58%, tr:  97.14%, tr_best:  97.24%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  1.277851/ 11.972828, val:  82.92%, val_best:  82.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.511686/ 11.638330, val:  81.25%, val_best:  82.92%, tr:  97.75%, tr_best:  98.37%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.000136/ 11.290065, val:  78.33%, val_best:  82.92%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  1.969820/ 12.517977, val:  77.92%, val_best:  82.92%, tr:  95.81%, tr_best:  99.28%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.854930/ 12.931346, val:  78.75%, val_best:  82.92%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.610731/ 10.814851, val:  82.92%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.753968/ 11.104586, val:  75.83%, val_best:  82.92%, tr:  99.28%, tr_best:  99.69%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  1.329796/ 14.778817, val:  69.17%, val_best:  82.92%, tr:  97.45%, tr_best:  99.69%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.077121/ 11.196333, val:  81.25%, val_best:  82.92%, tr:  97.85%, tr_best:  99.69%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.533943/ 11.434787, val:  80.00%, val_best:  82.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.490593/ 11.258183, val:  80.83%, val_best:  82.92%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.307362/ 10.806359, val:  81.25%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.345878/ 13.163948, val:  77.08%, val_best:  82.92%, tr:  97.24%, tr_best:  99.90%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  1.032491/ 11.857980, val:  79.58%, val_best:  82.92%, tr:  98.37%, tr_best:  99.90%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.432618/ 11.510977, val:  81.67%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.396228/ 11.411971, val:  80.83%, val_best:  82.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.361685/ 11.525159, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.263333/ 11.439760, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.205317/ 11.161554, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.269224/ 11.540326, val:  80.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.261044/ 11.641027, val:  75.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.204058/ 11.297207, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.257191/ 11.400664, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.133518/ 12.036403, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.159716/ 12.077875, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.140074/ 11.208386, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.150379/ 11.986558, val:  79.17%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.111983/ 11.552448, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.105581/ 11.678004, val:  80.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.159906/ 11.761266, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.087946/ 11.541460, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.068217/ 12.145938, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.112168/ 11.672810, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.096205/ 11.939310, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.094595/ 12.875347, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.105927/ 11.582605, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.052675/ 11.475551, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.043458/ 11.165150, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.048212/ 11.725153, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.041335/ 11.887081, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.055741/ 11.430421, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.029225/ 11.608190, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.029468/ 12.222132, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.039472/ 12.584414, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.446144/ 12.317891, val:  80.83%, val_best:  84.58%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.140724/ 11.524422, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.061710/ 11.886935, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.057671/ 11.604146, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.034402/ 11.645186, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.024853/ 11.293026, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.013857/ 11.772838, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354e921835574a2fb9c9e5a25a4d0853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▂▃▃▂▅▄▄▇▃▆▆▅▇▇█▇▇▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▂▄▄▄▅▄▄▅▆▅▅▆▆▇▅▆▇█▇███▇▇███▇████▇█▇▇█</td></tr><tr><td>tr_acc</td><td>▁▃▃▄▄▄▄▅▅▅▆▆▇▆▇▇█▇▇█████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▅▄▄▄▄▄▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▄▄▅▅▅▅▅▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▂▄▄▄▅▄▄▅▆▅▅▆▆▇▅▆▇█▇███▇▇███▇████▇█▇▇█</td></tr><tr><td>val_loss</td><td>█▂▃▄▃▂▃▂▂▃▂▂▂▄▁▂▁▂▂▂▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01386</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>11.77284</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-sweep-124</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v1nyep69' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v1nyep69</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_205553-v1nyep69/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l5cxa24l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_210144-l5cxa24l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5cxa24l' target=\"_blank\">deep-sweep-126</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5cxa24l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5cxa24l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 89a596869d28fa452a440a89440d75ad\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 30.539597/ 56.461449, val:  26.67%, val_best:  26.67%, tr:  25.54%, tr_best:  25.54%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 27.252714/ 23.563463, val:  42.50%, val_best:  42.50%, tr:  37.39%, tr_best:  37.39%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 21.181370/ 11.396823, val:  46.25%, val_best:  46.25%, tr:  40.86%, tr_best:  40.86%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 17.329874/ 24.902559, val:  31.67%, val_best:  46.25%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 18.843246/ 29.238941, val:  39.17%, val_best:  46.25%, tr:  44.84%, tr_best:  44.84%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 19.513775/ 23.759001, val:  40.00%, val_best:  46.25%, tr:  48.31%, tr_best:  48.31%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 18.889967/ 26.869333, val:  49.58%, val_best:  49.58%, tr:  50.66%, tr_best:  50.66%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 21.358738/ 34.459217, val:  27.08%, val_best:  49.58%, tr:  49.54%, tr_best:  50.66%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 16.722500/  9.362221, val:  56.25%, val_best:  56.25%, tr:  52.30%, tr_best:  52.30%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  9.206977/ 19.427645, val:  50.83%, val_best:  56.25%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 18.455362/ 18.230135, val:  35.83%, val_best:  56.25%, tr:  49.13%, tr_best:  60.57%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 12.903935/ 17.189442, val:  45.83%, val_best:  56.25%, tr:  58.63%, tr_best:  60.57%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 15.143075/ 13.939823, val:  51.25%, val_best:  56.25%, tr:  57.10%, tr_best:  60.57%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 16.344433/ 23.478842, val:  47.08%, val_best:  56.25%, tr:  55.67%, tr_best:  60.57%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 19.869406/ 18.895779, val:  50.00%, val_best:  56.25%, tr:  55.06%, tr_best:  60.57%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 13.671486/ 17.547451, val:  47.92%, val_best:  56.25%, tr:  62.10%, tr_best:  62.10%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 13.099560/ 11.812162, val:  60.83%, val_best:  60.83%, tr:  60.98%, tr_best:  62.10%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  9.834303/ 14.611437, val:  53.75%, val_best:  60.83%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 12.535223/ 19.676842, val:  54.58%, val_best:  60.83%, tr:  62.72%, tr_best:  63.13%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 10.545968/ 14.410715, val:  52.08%, val_best:  60.83%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 15.777430/ 19.228205, val:  52.92%, val_best:  60.83%, tr:  60.57%, tr_best:  64.35%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 13.538964/ 18.850348, val:  57.08%, val_best:  60.83%, tr:  64.04%, tr_best:  64.35%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 16.521273/ 17.018257, val:  57.08%, val_best:  60.83%, tr:  62.61%, tr_best:  64.35%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 11.917949/ 11.754488, val:  59.17%, val_best:  60.83%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 14.015205/ 10.782449, val:  67.08%, val_best:  67.08%, tr:  62.21%, tr_best:  66.39%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  9.632771/ 16.137302, val:  66.25%, val_best:  67.08%, tr:  68.34%, tr_best:  68.34%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 15.033981/ 15.489244, val:  53.33%, val_best:  67.08%, tr:  68.44%, tr_best:  68.44%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 11.703545/ 17.212620, val:  52.50%, val_best:  67.08%, tr:  65.88%, tr_best:  68.44%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 12.698419/ 15.718561, val:  62.50%, val_best:  67.08%, tr:  66.50%, tr_best:  68.44%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  7.227598/ 17.619431, val:  56.25%, val_best:  67.08%, tr:  76.30%, tr_best:  76.30%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  9.711735/ 11.811124, val:  62.92%, val_best:  67.08%, tr:  70.79%, tr_best:  76.30%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  8.922586/ 19.213100, val:  56.67%, val_best:  67.08%, tr:  71.71%, tr_best:  76.30%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 11.601423/ 21.611359, val:  53.75%, val_best:  67.08%, tr:  70.58%, tr_best:  76.30%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  8.279494/ 12.525811, val:  65.00%, val_best:  67.08%, tr:  77.63%, tr_best:  77.63%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  9.249083/ 16.266617, val:  54.58%, val_best:  67.08%, tr:  73.75%, tr_best:  77.63%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  8.021082/ 15.497359, val:  64.17%, val_best:  67.08%, tr:  78.35%, tr_best:  78.35%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  8.064224/ 14.594497, val:  63.75%, val_best:  67.08%, tr:  76.81%, tr_best:  78.35%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 12.246971/ 22.407499, val:  62.50%, val_best:  67.08%, tr:  71.60%, tr_best:  78.35%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  8.452099/ 15.527446, val:  62.08%, val_best:  67.08%, tr:  77.63%, tr_best:  78.35%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  6.503627/ 14.075290, val:  62.08%, val_best:  67.08%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  5.295944/ 16.005680, val:  61.67%, val_best:  67.08%, tr:  85.19%, tr_best:  85.19%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  5.164610/ 14.287471, val:  65.42%, val_best:  67.08%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  5.300672/ 13.115335, val:  61.25%, val_best:  67.08%, tr:  83.96%, tr_best:  86.31%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  4.925470/ 17.780567, val:  65.83%, val_best:  67.08%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  4.152695/ 12.915689, val:  70.00%, val_best:  70.00%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  4.343755/ 10.685704, val:  73.75%, val_best:  73.75%, tr:  88.25%, tr_best:  88.66%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  4.636450/ 14.533310, val:  63.33%, val_best:  73.75%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  4.384645/ 11.789474, val:  72.08%, val_best:  73.75%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  4.233063/ 10.411368, val:  77.50%, val_best:  77.50%, tr:  88.25%, tr_best:  91.42%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  3.335036/ 11.180838, val:  79.58%, val_best:  79.58%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  3.223387/ 16.537127, val:  65.00%, val_best:  79.58%, tr:  92.75%, tr_best:  92.95%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  3.015083/  9.799613, val:  75.83%, val_best:  79.58%, tr:  93.05%, tr_best:  93.05%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  3.099069/ 11.444146, val:  73.33%, val_best:  79.58%, tr:  91.73%, tr_best:  93.05%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  3.534588/ 11.014217, val:  73.33%, val_best:  79.58%, tr:  91.42%, tr_best:  93.05%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  3.563453/  9.897239, val:  80.00%, val_best:  80.00%, tr:  92.13%, tr_best:  93.05%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.071296/ 12.369837, val:  65.83%, val_best:  80.00%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.483663/ 10.490096, val:  77.08%, val_best:  80.00%, tr:  94.18%, tr_best:  97.14%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  1.841813/ 10.103529, val:  75.83%, val_best:  80.00%, tr:  97.04%, tr_best:  97.14%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  1.629603/ 10.488729, val:  80.83%, val_best:  80.83%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.756020/  9.966620, val:  78.33%, val_best:  80.83%, tr:  97.04%, tr_best:  97.55%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.350397/ 10.064898, val:  75.42%, val_best:  80.83%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.124119/ 12.265039, val:  68.33%, val_best:  80.83%, tr:  94.48%, tr_best:  98.06%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.688411/ 14.706776, val:  70.00%, val_best:  80.83%, tr:  93.05%, tr_best:  98.06%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  2.036021/ 11.829030, val:  72.50%, val_best:  80.83%, tr:  95.30%, tr_best:  98.06%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  1.496619/ 10.739771, val:  75.83%, val_best:  80.83%, tr:  97.14%, tr_best:  98.06%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  1.730435/ 10.035889, val:  78.33%, val_best:  80.83%, tr:  96.73%, tr_best:  98.06%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  1.982050/ 11.726866, val:  72.08%, val_best:  80.83%, tr:  96.42%, tr_best:  98.06%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  1.642584/ 10.361928, val:  75.42%, val_best:  80.83%, tr:  97.45%, tr_best:  98.06%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  1.244601/ 11.141526, val:  75.42%, val_best:  80.83%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  1.301494/  9.647712, val:  79.58%, val_best:  80.83%, tr:  98.57%, tr_best:  98.67%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.968250/ 10.214186, val:  77.50%, val_best:  80.83%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.125133/ 11.054152, val:  78.75%, val_best:  80.83%, tr:  95.10%, tr_best:  99.18%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.839662/ 12.848054, val:  73.33%, val_best:  80.83%, tr:  91.83%, tr_best:  99.18%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.209676/ 12.572767, val:  77.50%, val_best:  80.83%, tr:  96.22%, tr_best:  99.18%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  2.210265/ 14.839382, val:  70.83%, val_best:  80.83%, tr:  94.48%, tr_best:  99.18%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  1.259286/ 11.690454, val:  84.17%, val_best:  84.17%, tr:  98.98%, tr_best:  99.18%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.734908/ 11.913891, val:  80.42%, val_best:  84.17%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.834822/ 12.643988, val:  71.67%, val_best:  84.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  3.534974/ 13.495354, val:  73.33%, val_best:  84.17%, tr:  92.03%, tr_best:  99.80%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  1.331906/ 12.252546, val:  80.42%, val_best:  84.17%, tr:  98.67%, tr_best:  99.80%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.742656/ 11.989478, val:  80.42%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.844906/ 11.361317, val:  82.08%, val_best:  84.17%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.672256/ 12.877312, val:  80.00%, val_best:  84.17%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.670692/ 12.824355, val:  77.08%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.584893/ 11.392355, val:  82.08%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.585345/ 11.980339, val:  77.50%, val_best:  84.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.578615/ 11.743479, val:  80.42%, val_best:  84.17%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.519794/ 11.692489, val:  80.83%, val_best:  84.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.474776/ 11.573664, val:  78.33%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.416038/ 10.895043, val:  83.75%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.513865/ 11.780375, val:  80.00%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.434290/ 11.486078, val:  77.08%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.299855/ 11.367504, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.305677/ 10.890635, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.278910/ 11.912795, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.287877/ 11.982423, val:  76.67%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.256076/ 10.900169, val:  80.83%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.242460/ 11.397439, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.203988/ 11.440160, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.200777/ 10.646323, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b118b057c5e484593f8e7183709f41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▃▄▃▄▄▄▆▅▄▆▅▅▆▆█▇▇▆▇████▇▇█▆██▇████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▁▄▄▄▄▄▅▆▄▅▄▆▅▅▅▆▇▇▇▇▇▇▆▇▇▇▇█▆█▇████▇█</td></tr><tr><td>tr_acc</td><td>▁▂▃▃▄▄▄▅▅▅▄▅▆▅▆▅▆▆▇▇▇▇▇██▇███▇██████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▆▃▄▆▃▃▄▄▄▃▄▃▄▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▁▄▄▄▄▄▅▆▄▅▄▆▅▅▅▆▇▇▇▇▇▇▆▇▇▇▇█▆█▇████▇█</td></tr><tr><td>val_loss</td><td>█▁▄▅▂▂▂▂▂▂▁▂▂▃▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.20078</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>10.64632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-126</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5cxa24l' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5cxa24l</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_210144-l5cxa24l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hkvqww44 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_210740-hkvqww44</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hkvqww44' target=\"_blank\">snowy-sweep-128</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hkvqww44' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hkvqww44</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 8ba471014b61e50904ecff33d030e937\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325213/  2.309355, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:   9.30%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.278873/  2.068740, val:  27.92%, val_best:  27.92%, tr:  15.32%, tr_best:  15.32%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.706637/  1.543061, val:  51.25%, val_best:  51.25%, tr:  40.76%, tr_best:  40.76%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.302725/  1.399784, val:  55.83%, val_best:  55.83%, tr:  58.32%, tr_best:  58.32%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.177981/  1.290609, val:  60.42%, val_best:  60.42%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.114506/  1.256114, val:  61.67%, val_best:  61.67%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.064526/  1.314157, val:  57.92%, val_best:  61.67%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.057202/  1.261609, val:  65.00%, val_best:  65.00%, tr:  63.94%, tr_best:  65.58%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.034505/  1.264581, val:  64.17%, val_best:  65.00%, tr:  65.68%, tr_best:  65.68%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.989539/  1.403853, val:  56.25%, val_best:  65.00%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.008690/  1.339293, val:  58.75%, val_best:  65.00%, tr:  68.74%, tr_best:  70.99%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.955205/  1.250727, val:  59.58%, val_best:  65.00%, tr:  70.89%, tr_best:  70.99%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.936194/  1.309400, val:  59.58%, val_best:  65.00%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.963210/  1.415863, val:  59.17%, val_best:  65.00%, tr:  71.60%, tr_best:  72.01%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.913008/  1.610222, val:  60.42%, val_best:  65.00%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.914306/  1.289112, val:  62.92%, val_best:  65.00%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.841927/  1.289264, val:  65.00%, val_best:  65.00%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.824527/  1.326847, val:  68.33%, val_best:  68.33%, tr:  79.26%, tr_best:  79.26%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.819023/  1.517724, val:  61.25%, val_best:  68.33%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.815880/  1.445443, val:  65.42%, val_best:  68.33%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.787034/  1.427445, val:  65.00%, val_best:  68.33%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.752176/  1.617138, val:  62.92%, val_best:  68.33%, tr:  81.82%, tr_best:  82.23%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.725123/  1.527445, val:  56.25%, val_best:  68.33%, tr:  84.58%, tr_best:  84.58%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.709203/  1.470852, val:  69.17%, val_best:  69.17%, tr:  84.07%, tr_best:  84.58%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.629910/  1.442271, val:  65.83%, val_best:  69.17%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.650963/  1.408705, val:  68.75%, val_best:  69.17%, tr:  87.03%, tr_best:  91.42%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.704473/  1.447079, val:  67.50%, val_best:  69.17%, tr:  84.17%, tr_best:  91.42%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.602624/  1.532773, val:  65.42%, val_best:  69.17%, tr:  89.99%, tr_best:  91.42%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.582998/  1.480093, val:  65.83%, val_best:  69.17%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.540499/  1.688926, val:  59.17%, val_best:  69.17%, tr:  93.46%, tr_best:  93.77%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.541628/  1.633733, val:  62.50%, val_best:  69.17%, tr:  93.67%, tr_best:  93.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.503141/  1.608848, val:  66.25%, val_best:  69.17%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.563303/  1.684213, val:  62.08%, val_best:  69.17%, tr:  91.73%, tr_best:  95.61%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.540060/  1.607567, val:  71.67%, val_best:  71.67%, tr:  93.26%, tr_best:  95.61%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.501884/  1.652050, val:  69.17%, val_best:  71.67%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.437732/  1.620668, val:  71.67%, val_best:  71.67%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.419497/  1.640327, val:  71.67%, val_best:  71.67%, tr:  97.24%, tr_best:  97.45%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.453644/  1.752817, val:  62.08%, val_best:  71.67%, tr:  96.02%, tr_best:  97.45%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.455628/  1.654278, val:  70.83%, val_best:  71.67%, tr:  96.83%, tr_best:  97.45%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.386395/  1.728267, val:  67.50%, val_best:  71.67%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.381488/  1.727258, val:  74.58%, val_best:  74.58%, tr:  97.45%, tr_best:  98.77%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.390699/  1.818204, val:  66.25%, val_best:  74.58%, tr:  98.26%, tr_best:  98.77%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.340909/  1.740948, val:  71.25%, val_best:  74.58%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.350756/  1.861496, val:  64.58%, val_best:  74.58%, tr:  98.67%, tr_best:  98.98%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.357942/  1.829196, val:  64.58%, val_best:  74.58%, tr:  98.16%, tr_best:  98.98%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.292921/  1.839588, val:  67.50%, val_best:  74.58%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.292397/  1.864377, val:  70.83%, val_best:  74.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.268186/  1.883968, val:  70.42%, val_best:  74.58%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.293714/  1.943354, val:  68.33%, val_best:  74.58%, tr:  99.18%, tr_best:  99.69%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.275830/  1.979586, val:  71.67%, val_best:  74.58%, tr:  98.88%, tr_best:  99.69%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.232400/  1.929041, val:  71.25%, val_best:  74.58%, tr:  99.49%, tr_best:  99.69%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.233216/  1.997132, val:  71.25%, val_best:  74.58%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.228248/  1.985422, val:  71.25%, val_best:  74.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.207386/  1.998639, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.192194/  2.057107, val:  70.00%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.190853/  2.133024, val:  68.75%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.192437/  2.159614, val:  70.42%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.211037/  2.113459, val:  70.00%, val_best:  74.58%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.178366/  2.144843, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.177705/  2.269794, val:  68.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.168811/  2.196496, val:  68.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.158950/  2.266100, val:  71.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.156270/  2.299960, val:  69.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.137444/  2.249778, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.151360/  2.336519, val:  69.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.138009/  2.315435, val:  68.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.131474/  2.281436, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.127199/  2.340977, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.121234/  2.325004, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.119489/  2.377344, val:  68.75%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.116003/  2.450217, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.114563/  2.363270, val:  71.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.103367/  2.472875, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.095550/  2.535606, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.092556/  2.476995, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.104486/  2.584592, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.089202/  2.533388, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.083078/  2.539017, val:  71.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.091684/  2.592721, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.095427/  2.600126, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.085954/  2.640273, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.077835/  2.628261, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.077968/  2.660697, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.078585/  2.625089, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.083429/  2.609190, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.068752/  2.682918, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.067204/  2.738461, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.071255/  2.759149, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.066066/  2.733257, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.062578/  2.767275, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.057770/  2.839627, val:  70.83%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.055365/  2.796641, val:  73.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.067666/  2.763783, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.056616/  2.768914, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.051018/  2.840876, val:  71.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.052893/  2.878608, val:  71.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.051296/  2.900050, val:  72.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.068678/  2.904193, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.047386/  2.883766, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.049604/  2.902995, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7993012895f84885a7d5cf6c0cac9478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▃▄▅▅▅▂▇▇▇██▅▇███▇█████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▆▇▇█▇▇▇▇▇▇█▇▇█▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▅▆▆▆▆▆▇▇▇▇▇██████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▆▇▇█▇▇▇▇▇▇█▇▇█▇█████████████████████</td></tr><tr><td>val_loss</td><td>▅▂▁▁▂▁▂▁▂▃▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▆▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0496</td></tr><tr><td>val_acc_best</td><td>0.74583</td></tr><tr><td>val_acc_now</td><td>0.725</td></tr><tr><td>val_loss</td><td>2.90299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-128</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hkvqww44' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hkvqww44</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_210740-hkvqww44/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gtafgpxi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_211331-gtafgpxi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gtafgpxi' target=\"_blank\">northern-sweep-130</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gtafgpxi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gtafgpxi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 62.437637/ 33.317314, val:  21.25%, val_best:  21.25%, tr:  18.39%, tr_best:  18.39%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 38.116814/ 28.671886, val:  40.42%, val_best:  40.42%, tr:  26.86%, tr_best:  26.86%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 31.445330/ 35.244144, val:  38.75%, val_best:  40.42%, tr:  32.99%, tr_best:  32.99%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 31.065277/ 41.504749, val:  17.08%, val_best:  40.42%, tr:  33.50%, tr_best:  33.50%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 35.505405/ 20.770102, val:  36.67%, val_best:  40.42%, tr:  37.28%, tr_best:  37.28%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 28.192823/ 17.526583, val:  38.33%, val_best:  40.42%, tr:  41.37%, tr_best:  41.37%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 22.973597/ 28.447680, val:  47.92%, val_best:  47.92%, tr:  45.15%, tr_best:  45.15%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 22.060322/ 23.593006, val:  44.17%, val_best:  47.92%, tr:  47.29%, tr_best:  47.29%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 24.783258/ 15.275160, val:  43.33%, val_best:  47.92%, tr:  44.33%, tr_best:  47.29%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 24.665701/ 18.440556, val:  54.58%, val_best:  54.58%, tr:  48.72%, tr_best:  48.72%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 20.483757/ 20.930111, val:  42.50%, val_best:  54.58%, tr:  49.44%, tr_best:  49.44%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 18.853662/ 26.149767, val:  46.67%, val_best:  54.58%, tr:  53.63%, tr_best:  53.63%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 18.003420/ 21.651804, val:  44.17%, val_best:  54.58%, tr:  52.40%, tr_best:  53.63%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 19.915607/ 15.898442, val:  55.42%, val_best:  55.42%, tr:  54.14%, tr_best:  54.14%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 19.157059/ 34.090580, val:  42.92%, val_best:  55.42%, tr:  55.87%, tr_best:  55.87%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 19.052254/ 20.617081, val:  54.58%, val_best:  55.42%, tr:  55.87%, tr_best:  55.87%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 18.734726/ 21.888874, val:  49.58%, val_best:  55.42%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 12.449417/ 14.126304, val:  55.83%, val_best:  55.83%, tr:  59.65%, tr_best:  60.98%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 16.502993/ 25.721210, val:  49.17%, val_best:  55.83%, tr:  59.14%, tr_best:  60.98%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 15.443454/ 21.594557, val:  47.08%, val_best:  55.83%, tr:  59.04%, tr_best:  60.98%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss: 25.702684/ 40.656555, val:  41.25%, val_best:  55.83%, tr:  56.38%, tr_best:  60.98%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss: 23.344664/ 34.695286, val:  50.00%, val_best:  55.83%, tr:  56.69%, tr_best:  60.98%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss: 15.687346/ 24.159702, val:  56.25%, val_best:  56.25%, tr:  60.37%, tr_best:  60.98%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss: 13.038177/ 18.654238, val:  59.58%, val_best:  59.58%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss: 16.427691/ 18.686548, val:  52.92%, val_best:  59.58%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss: 20.085861/ 24.361439, val:  62.50%, val_best:  62.50%, tr:  63.02%, tr_best:  66.80%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss: 14.338651/ 27.664820, val:  50.83%, val_best:  62.50%, tr:  67.72%, tr_best:  67.72%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss: 13.466822/ 28.618980, val:  56.67%, val_best:  62.50%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss: 17.881083/ 20.186642, val:  55.42%, val_best:  62.50%, tr:  66.80%, tr_best:  69.87%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss: 14.220555/ 23.434826, val:  50.00%, val_best:  62.50%, tr:  69.56%, tr_best:  69.87%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss: 11.157239/ 17.374142, val:  63.75%, val_best:  63.75%, tr:  68.44%, tr_best:  69.87%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss: 12.749805/ 27.148821, val:  54.58%, val_best:  63.75%, tr:  69.36%, tr_best:  69.87%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss: 16.839861/ 22.644409, val:  55.42%, val_best:  63.75%, tr:  62.51%, tr_best:  69.87%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss: 13.056420/ 22.878113, val:  54.17%, val_best:  63.75%, tr:  68.34%, tr_best:  69.87%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss: 15.368103/ 25.898123, val:  43.33%, val_best:  63.75%, tr:  67.21%, tr_best:  69.87%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss: 11.439257/ 17.338816, val:  59.17%, val_best:  63.75%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss: 10.075739/ 18.233370, val:  62.92%, val_best:  63.75%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss: 15.537372/ 27.714022, val:  58.75%, val_best:  63.75%, tr:  70.17%, tr_best:  73.54%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss: 12.623818/ 24.789938, val:  57.08%, val_best:  63.75%, tr:  71.20%, tr_best:  73.54%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss: 11.261679/ 19.194885, val:  61.25%, val_best:  63.75%, tr:  69.87%, tr_best:  73.54%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss: 10.007870/ 21.864531, val:  55.00%, val_best:  63.75%, tr:  75.18%, tr_best:  75.18%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  9.929132/ 21.839767, val:  59.58%, val_best:  63.75%, tr:  76.51%, tr_best:  76.51%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss: 10.747188/ 26.289421, val:  55.42%, val_best:  63.75%, tr:  74.16%, tr_best:  76.51%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  8.239505/ 16.330261, val:  65.83%, val_best:  65.83%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  7.292188/ 24.578087, val:  57.08%, val_best:  65.83%, tr:  84.07%, tr_best:  84.07%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss: 10.778883/ 21.864101, val:  60.00%, val_best:  65.83%, tr:  77.32%, tr_best:  84.07%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  8.778929/ 20.930918, val:  59.17%, val_best:  65.83%, tr:  82.23%, tr_best:  84.07%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss: 11.200498/ 19.718670, val:  60.83%, val_best:  65.83%, tr:  76.30%, tr_best:  84.07%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss: 12.229804/ 21.909105, val:  56.67%, val_best:  65.83%, tr:  77.12%, tr_best:  84.07%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  6.450466/ 17.290070, val:  66.25%, val_best:  66.25%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  6.346756/ 21.879623, val:  60.00%, val_best:  66.25%, tr:  86.82%, tr_best:  86.82%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss: 10.321516/ 23.116776, val:  55.42%, val_best:  66.25%, tr:  80.49%, tr_best:  86.82%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  9.995639/ 24.628222, val:  53.75%, val_best:  66.25%, tr:  80.59%, tr_best:  86.82%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  6.493155/ 19.115313, val:  65.83%, val_best:  66.25%, tr:  86.72%, tr_best:  86.82%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  5.548544/ 21.895700, val:  59.58%, val_best:  66.25%, tr:  88.87%, tr_best:  88.87%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  5.914924/ 28.815245, val:  64.58%, val_best:  66.25%, tr:  87.84%, tr_best:  88.87%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss: 10.230659/ 25.718960, val:  60.42%, val_best:  66.25%, tr:  81.41%, tr_best:  88.87%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  5.348524/ 18.817240, val:  66.25%, val_best:  66.25%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  4.771044/ 21.396666, val:  62.50%, val_best:  66.25%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  4.708876/ 21.985106, val:  65.83%, val_best:  66.25%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  4.682801/ 20.317766, val:  72.50%, val_best:  72.50%, tr:  92.65%, tr_best:  92.65%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  4.777287/ 25.099939, val:  59.58%, val_best:  72.50%, tr:  90.81%, tr_best:  92.65%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  4.853632/ 20.143959, val:  67.92%, val_best:  72.50%, tr:  92.24%, tr_best:  92.65%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  3.170525/ 23.084545, val:  62.92%, val_best:  72.50%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  4.441900/ 22.067398, val:  64.17%, val_best:  72.50%, tr:  91.22%, tr_best:  97.14%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  4.926742/ 21.007956, val:  72.08%, val_best:  72.50%, tr:  92.03%, tr_best:  97.14%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  3.641293/ 22.655376, val:  63.33%, val_best:  72.50%, tr:  96.12%, tr_best:  97.14%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  3.332903/ 22.485544, val:  66.67%, val_best:  72.50%, tr:  95.51%, tr_best:  97.14%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  3.171156/ 22.916794, val:  66.67%, val_best:  72.50%, tr:  95.71%, tr_best:  97.14%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  3.034410/ 27.950346, val:  55.42%, val_best:  72.50%, tr:  95.71%, tr_best:  97.14%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  2.665602/ 20.305122, val:  68.75%, val_best:  72.50%, tr:  96.73%, tr_best:  97.14%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.626579/ 21.006824, val:  66.25%, val_best:  72.50%, tr:  96.73%, tr_best:  97.14%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.620324/ 23.086849, val:  60.83%, val_best:  72.50%, tr:  96.32%, tr_best:  97.14%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.471043/ 21.959734, val:  67.08%, val_best:  72.50%, tr:  95.91%, tr_best:  97.14%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  1.748184/ 20.171423, val:  72.92%, val_best:  72.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  1.726918/ 22.468172, val:  67.50%, val_best:  72.92%, tr:  98.06%, tr_best:  99.08%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  1.901754/ 23.055922, val:  61.25%, val_best:  72.92%, tr:  98.67%, tr_best:  99.08%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  1.647637/ 22.812099, val:  65.00%, val_best:  72.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  2.150304/ 22.143375, val:  66.67%, val_best:  72.92%, tr:  97.24%, tr_best:  99.49%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  1.634023/ 22.481432, val:  66.25%, val_best:  72.92%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  1.518622/ 19.946285, val:  68.33%, val_best:  72.92%, tr:  99.08%, tr_best:  99.49%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  1.764498/ 21.286434, val:  70.00%, val_best:  72.92%, tr:  98.67%, tr_best:  99.49%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  1.634515/ 21.946991, val:  65.42%, val_best:  72.92%, tr:  98.16%, tr_best:  99.49%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  1.339578/ 20.564520, val:  70.83%, val_best:  72.92%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  1.274485/ 23.233665, val:  66.67%, val_best:  72.92%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  1.776833/ 21.246353, val:  73.33%, val_best:  73.33%, tr:  98.26%, tr_best:  99.49%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  1.092805/ 20.493076, val:  69.17%, val_best:  73.33%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.977826/ 20.871231, val:  71.25%, val_best:  73.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.724824/ 21.282122, val:  69.17%, val_best:  73.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.846584/ 20.585758, val:  71.25%, val_best:  73.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.791881/ 21.571890, val:  72.50%, val_best:  73.33%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  1.025545/ 21.025265, val:  72.50%, val_best:  73.33%, tr:  99.18%, tr_best:  99.90%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.952571/ 21.892223, val:  70.83%, val_best:  73.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.951265/ 20.878355, val:  73.75%, val_best:  73.75%, tr:  99.49%, tr_best:  99.90%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  1.131794/ 22.002346, val:  69.17%, val_best:  73.75%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  1.021874/ 21.060951, val:  74.17%, val_best:  74.17%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  1.064383/ 22.232662, val:  68.33%, val_best:  74.17%, tr:  99.08%, tr_best:  99.90%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.827115/ 23.645592, val:  65.42%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.648135/ 21.275774, val:  72.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  1.040104/ 22.379406, val:  70.83%, val_best:  74.17%, tr:  99.80%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b03eeb0284448d93a182af7cc4161c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▂▃▄▆▆▅▅▆▅▆▇▅▇▆▅▆▅▆▇▇▇███▇██▇██████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▃▄▅▄▄▆▄▅▅▅▅▆▆▆▆▆▆▆▇▅▆▇▇▆█▇▇▆▇▇▇▇▇████▇</td></tr><tr><td>tr_acc</td><td>▁▂▃▃▄▄▄▅▄▄▅▅▅▅▆▅▅▆▇▆▇▆▇▇▇▇▇█████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▄▅▃▄▃▃▂▃▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▃▄▅▄▄▆▄▅▅▅▅▆▆▆▆▆▆▆▇▅▆▇▇▆█▇▇▆▇▇▇▇▇████▇</td></tr><tr><td>val_loss</td><td>▇█▃▄▂▃█▁▃█▃▅▄▄▂▆▃▅▄▃▂▄▄▃▄▅▃▄▃▄▄▄▄▄▄▃▃▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.0401</td></tr><tr><td>val_acc_best</td><td>0.74167</td></tr><tr><td>val_acc_now</td><td>0.70833</td></tr><tr><td>val_loss</td><td>22.37941</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-130</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gtafgpxi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gtafgpxi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_211331-gtafgpxi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fyjx3j28 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_211954-fyjx3j28</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fyjx3j28' target=\"_blank\">good-sweep-132</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fyjx3j28' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fyjx3j28</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.997207/  2.638526, val:  45.00%, val_best:  45.00%, tr:  35.34%, tr_best:  35.34%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.958745/  1.893745, val:  51.25%, val_best:  51.25%, tr:  54.03%, tr_best:  54.03%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.757265/  1.727143, val:  55.83%, val_best:  55.83%, tr:  55.87%, tr_best:  55.87%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.294790/  2.076087, val:  52.92%, val_best:  55.83%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.537793/  1.431288, val:  62.50%, val_best:  62.50%, tr:  62.00%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.324366/  1.631231, val:  59.58%, val_best:  62.50%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.046699/  1.764139, val:  61.25%, val_best:  62.50%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.385156/  1.765264, val:  61.67%, val_best:  62.50%, tr:  66.91%, tr_best:  70.17%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.064545/  1.469572, val:  61.25%, val_best:  62.50%, tr:  70.68%, tr_best:  70.68%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.454829/  1.836472, val:  56.67%, val_best:  62.50%, tr:  68.54%, tr_best:  70.68%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.920664/  1.295876, val:  63.75%, val_best:  63.75%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.933863/  1.443096, val:  65.42%, val_best:  65.42%, tr:  78.45%, tr_best:  78.45%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.018166/  1.257935, val:  68.75%, val_best:  68.75%, tr:  77.12%, tr_best:  78.45%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.816775/  1.961776, val:  57.08%, val_best:  68.75%, tr:  80.90%, tr_best:  80.90%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  1.075790/  1.931424, val:  67.08%, val_best:  68.75%, tr:  79.47%, tr_best:  80.90%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.697109/  1.582256, val:  67.50%, val_best:  68.75%, tr:  82.23%, tr_best:  82.23%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.595542/  1.600313, val:  70.00%, val_best:  70.00%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.459971/  1.554596, val:  65.83%, val_best:  70.00%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.484798/  2.009511, val:  65.42%, val_best:  70.00%, tr:  89.79%, tr_best:  91.52%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.445808/  1.425599, val:  75.00%, val_best:  75.00%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.513831/  1.845084, val:  68.75%, val_best:  75.00%, tr:  90.91%, tr_best:  91.83%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.499731/  1.594445, val:  72.08%, val_best:  75.00%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.268135/  1.665271, val:  75.83%, val_best:  75.83%, tr:  97.04%, tr_best:  97.04%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.261230/  1.693303, val:  75.83%, val_best:  75.83%, tr:  96.73%, tr_best:  97.04%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.200347/  1.538907, val:  78.75%, val_best:  78.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.189047/  1.615113, val:  76.25%, val_best:  78.75%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.205134/  1.537058, val:  77.92%, val_best:  78.75%, tr:  98.37%, tr_best:  98.57%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.144471/  1.521892, val:  80.42%, val_best:  80.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.100705/  1.567349, val:  78.75%, val_best:  80.42%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.095199/  1.707814, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.102959/  1.626258, val:  78.33%, val_best:  80.42%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.072149/  1.566970, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.082781/  1.858342, val:  75.00%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.069553/  1.722542, val:  79.58%, val_best:  80.83%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.061589/  1.709959, val:  77.92%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.054558/  1.659953, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.047258/  1.622125, val:  81.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.038519/  1.686588, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.040614/  1.666312, val:  80.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.050452/  1.783133, val:  78.33%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.037447/  1.733389, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.037836/  1.712772, val:  80.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.021293/  1.773555, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.020846/  1.780452, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.018894/  1.769426, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.018004/  1.817884, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.019148/  1.769346, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.021137/  1.854094, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.038918/  1.811309, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.019079/  1.797493, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.015343/  1.845378, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.013223/  1.838206, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.011208/  1.885736, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.011573/  1.843320, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.010683/  1.900801, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.010562/  1.905615, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.008833/  1.937863, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.007452/  1.914409, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.008558/  1.966647, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.009574/  1.946349, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.007323/  1.899182, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.007179/  1.922583, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.007278/  1.958200, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.008434/  1.918529, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.008161/  1.937876, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.008699/  1.971469, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.007037/  1.947758, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.006555/  1.969831, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.006364/  1.963385, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.007094/  1.977594, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.006448/  1.962516, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.006228/  1.971711, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.005687/  1.999231, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.004803/  1.984722, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.005616/  1.973777, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.004796/  2.013041, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.006083/  1.991548, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.006142/  1.981760, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.005865/  2.012769, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.006986/  2.050497, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.005521/  2.008073, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.005205/  2.000502, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.004245/  1.962025, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.003979/  2.005187, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.003714/  2.034968, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.004591/  2.028042, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.003565/  2.034664, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.003168/  2.008579, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.003598/  2.054279, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.004317/  2.037667, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.004865/  2.052855, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.004002/  2.004546, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.003433/  2.023724, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.003168/  2.023226, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.002507/  2.031059, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.003252/  2.043410, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.002819/  2.009201, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.002680/  2.041508, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.002867/  2.051435, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.002877/  2.065287, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0010481d5497430e9ad32114d87ed7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▂▁▅▅▅▅▇▇▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▄▃▅▅▅▆▆▇▇▇▆█▇▇██▇█▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▄▅▆▆▇▇▇██████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▆▆▅▅▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▄▄▅▅▆▆▆▇▇▇▇██████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▄▃▅▅▅▆▆▇▇▇▆█▇▇██▇█▇██████████████████</td></tr><tr><td>val_loss</td><td>█▃▂▄▄▁▄▃▂▃▂▂▃▄▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00288</td></tr><tr><td>val_acc_best</td><td>0.8375</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>2.06529</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-132</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fyjx3j28' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fyjx3j28</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_211954-fyjx3j28/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7h86ox4q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_212534-7h86ox4q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7h86ox4q' target=\"_blank\">vital-sweep-134</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7h86ox4q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7h86ox4q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 7, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = dc86e27cc77265c749b0d1b6c08202e2\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.227792/  3.035384, val:  43.75%, val_best:  43.75%, tr:  32.79%, tr_best:  32.79%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.271533/  2.014869, val:  52.50%, val_best:  52.50%, tr:  52.09%, tr_best:  52.09%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.503033/  3.292245, val:  52.50%, val_best:  52.50%, tr:  51.07%, tr_best:  52.09%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.937248/  2.947307, val:  48.75%, val_best:  52.50%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.001321/  2.111120, val:  61.67%, val_best:  61.67%, tr:  61.39%, tr_best:  61.39%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.696553/  2.374784, val:  52.92%, val_best:  61.67%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.261873/  2.369228, val:  60.00%, val_best:  61.67%, tr:  67.93%, tr_best:  67.93%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.915551/  2.657349, val:  57.50%, val_best:  61.67%, tr:  66.91%, tr_best:  67.93%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.325919/  2.127616, val:  59.17%, val_best:  61.67%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.733700/  2.268036, val:  57.50%, val_best:  61.67%, tr:  68.54%, tr_best:  70.79%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.263152/  1.538258, val:  68.33%, val_best:  68.33%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  1.307398/  2.032834, val:  62.92%, val_best:  68.33%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  1.147544/  1.585757, val:  77.08%, val_best:  77.08%, tr:  80.29%, tr_best:  80.29%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.750488/  1.841110, val:  67.08%, val_best:  77.08%, tr:  88.15%, tr_best:  88.15%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.839443/  2.234234, val:  70.83%, val_best:  77.08%, tr:  86.72%, tr_best:  88.15%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.658866/  1.600712, val:  78.75%, val_best:  78.75%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.551721/  1.621783, val:  79.58%, val_best:  79.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.519086/  1.791767, val:  80.00%, val_best:  80.00%, tr:  93.05%, tr_best:  93.77%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.475071/  1.899016, val:  76.67%, val_best:  80.00%, tr:  94.38%, tr_best:  94.38%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.500408/  1.888775, val:  76.67%, val_best:  80.00%, tr:  93.97%, tr_best:  94.38%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.391236/  1.741719, val:  82.08%, val_best:  82.08%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.332476/  1.827884, val:  76.67%, val_best:  82.08%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.294407/  1.842896, val:  80.42%, val_best:  82.08%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.317104/  1.922103, val:  83.33%, val_best:  83.33%, tr:  97.24%, tr_best:  98.37%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.244671/  1.807403, val:  84.17%, val_best:  84.17%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.213078/  1.773962, val:  82.50%, val_best:  84.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.198139/  1.823802, val:  84.17%, val_best:  84.17%, tr:  99.18%, tr_best:  99.28%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.168628/  2.015732, val:  81.67%, val_best:  84.17%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.153001/  1.861964, val:  82.92%, val_best:  84.17%, tr:  99.59%, tr_best:  99.69%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.143575/  2.074306, val:  79.17%, val_best:  84.17%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.148154/  1.889449, val:  84.17%, val_best:  84.17%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.112019/  1.971178, val:  82.92%, val_best:  84.17%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.173054/  2.163765, val:  82.08%, val_best:  84.17%, tr:  98.16%, tr_best:  99.80%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.112453/  2.296498, val:  80.42%, val_best:  84.17%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.093253/  2.183382, val:  81.25%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.092229/  2.167548, val:  84.17%, val_best:  84.17%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.076587/  2.153870, val:  80.42%, val_best:  84.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.062328/  2.260540, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.084096/  2.188240, val:  83.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.064648/  2.242184, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.049050/  2.258012, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.040945/  2.234858, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.034063/  2.260498, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.032469/  2.316663, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.033826/  2.326114, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.031862/  2.421015, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.027635/  2.338775, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.025686/  2.340800, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.040358/  2.369209, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.022984/  2.410456, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.032965/  2.379662, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.021519/  2.414151, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.024792/  2.420980, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.020135/  2.480588, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.017867/  2.434723, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.016737/  2.485010, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.012221/  2.561710, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.011581/  2.504368, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.008483/  2.541454, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.009207/  2.580835, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.008215/  2.515130, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.009302/  2.556250, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.012459/  2.494353, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.013380/  2.563939, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.011938/  2.587133, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.009687/  2.539522, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.007537/  2.571191, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.007712/  2.529684, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.009269/  2.630759, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.007234/  2.577301, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.005250/  2.576274, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.006171/  2.580976, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.005640/  2.605862, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.004655/  2.569928, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.005841/  2.576478, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.006582/  2.639473, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.007952/  2.654293, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.005939/  2.676716, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.006410/  2.674467, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.005521/  2.681213, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.004584/  2.676655, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.004968/  2.761745, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.003698/  2.740090, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.005765/  2.753495, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.004655/  2.717226, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.003411/  2.778147, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.003779/  2.762122, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.003439/  2.770570, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.003224/  2.760664, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.002692/  2.781213, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.002267/  2.785089, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.003076/  2.774276, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.002751/  2.815952, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.003124/  2.745327, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.002302/  2.737025, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.002591/  2.763586, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.002354/  2.784111, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.002669/  2.810686, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.002164/  2.776499, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.003357/  2.864623, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ed371f65ca4edc8164344cd1e76b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▃▅▅▅█▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▃▃▆▅▇▆▆██▇▇█▇████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▆▇▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▇▆▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▄▆▆▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▃▃▆▅▇▆▆██▇▇█▇████████████████████████</td></tr><tr><td>val_loss</td><td>▇█▃▅▄▁▄▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00336</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>2.86462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-sweep-134</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7h86ox4q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7h86ox4q</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_212534-7h86ox4q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: suow5a88 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_213201-suow5a88</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/suow5a88' target=\"_blank\">colorful-sweep-136</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/yryz40rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/suow5a88' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/suow5a88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': True, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = b57dfc72d05d304ce829f424a70e455b\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.125, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 75.085876/ 65.982536, val:  17.92%, val_best:  17.92%, tr:  14.81%, tr_best:  14.81%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 45.436089/ 32.114220, val:  32.92%, val_best:  32.92%, tr:  25.64%, tr_best:  25.64%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 37.132626/ 50.562630, val:  32.92%, val_best:  32.92%, tr:  31.77%, tr_best:  31.77%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 30.957150/ 17.579737, val:  46.25%, val_best:  46.25%, tr:  32.28%, tr_best:  32.28%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 24.293314/ 27.675522, val:  33.33%, val_best:  46.25%, tr:  37.49%, tr_best:  37.49%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 25.051901/ 40.343189, val:  32.92%, val_best:  46.25%, tr:  37.18%, tr_best:  37.49%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 25.347967/ 29.134729, val:  40.00%, val_best:  46.25%, tr:  43.21%, tr_best:  43.21%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 21.710968/ 25.302719, val:  36.67%, val_best:  46.25%, tr:  44.64%, tr_best:  44.64%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 23.073973/ 19.246397, val:  52.50%, val_best:  52.50%, tr:  43.21%, tr_best:  44.64%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 24.306261/ 22.259491, val:  44.17%, val_best:  52.50%, tr:  47.29%, tr_best:  47.29%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss: 21.162378/ 23.244934, val:  42.92%, val_best:  52.50%, tr:  46.48%, tr_best:  47.29%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss: 28.274015/ 22.511148, val:  46.67%, val_best:  52.50%, tr:  42.49%, tr_best:  47.29%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss: 23.985672/ 16.212124, val:  47.50%, val_best:  52.50%, tr:  48.72%, tr_best:  48.72%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss: 18.125326/ 31.480667, val:  39.17%, val_best:  52.50%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss: 23.375120/ 34.368149, val:  49.17%, val_best:  52.50%, tr:  48.52%, tr_best:  50.36%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss: 18.633593/ 22.639454, val:  51.67%, val_best:  52.50%, tr:  51.48%, tr_best:  51.48%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss: 19.694166/ 23.126749, val:  52.92%, val_best:  52.92%, tr:  50.87%, tr_best:  51.48%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss: 15.044006/ 17.743029, val:  45.42%, val_best:  52.92%, tr:  56.49%, tr_best:  56.49%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss: 16.440485/ 25.203707, val:  48.75%, val_best:  52.92%, tr:  54.44%, tr_best:  56.49%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss: 21.265879/ 16.248304, val:  50.83%, val_best:  52.92%, tr:  51.69%, tr_best:  56.49%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.125,0.25,0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.01,0.1]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [1,2,3,4,5,6,7]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True, False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [True,False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [True]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay, #wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = '6pj3lh8j'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
