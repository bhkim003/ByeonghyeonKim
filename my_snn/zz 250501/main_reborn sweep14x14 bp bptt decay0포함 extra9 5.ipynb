{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34408/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"0\",\n",
    "#                 single_step = False, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 1.0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 0.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 3.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = decay, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = True,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 6, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = False, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xas47qh3\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sljd5z5y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_215453-sljd5z5y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sljd5z5y' target=\"_blank\">noble-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sljd5z5y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sljd5z5y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.266900/  2.207652, val:  22.92%, val_best:  22.92%, tr:  14.59%, tr_best:  14.59%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.134581/  2.095816, val:  30.83%, val_best:  30.83%, tr:  27.01%, tr_best:  27.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.009634/  1.988225, val:  38.33%, val_best:  38.33%, tr:  36.09%, tr_best:  36.09%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  1.882645/  1.874648, val:  41.67%, val_best:  41.67%, tr:  43.21%, tr_best:  43.21%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.756175/  1.758618, val:  44.58%, val_best:  44.58%, tr:  46.55%, tr_best:  46.55%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.638547/  1.660027, val:  46.67%, val_best:  46.67%, tr:  50.07%, tr_best:  50.07%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.533574/  1.573274, val:  47.08%, val_best:  47.08%, tr:  51.85%, tr_best:  51.85%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.446158/  1.499520, val:  48.75%, val_best:  48.75%, tr:  54.76%, tr_best:  54.76%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.374977/  1.438889, val:  51.25%, val_best:  51.25%, tr:  56.56%, tr_best:  56.56%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.313385/  1.384491, val:  51.67%, val_best:  51.67%, tr:  57.66%, tr_best:  57.66%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.261787/  1.339985, val:  52.08%, val_best:  52.08%, tr:  59.63%, tr_best:  59.63%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.217134/  1.299610, val:  52.08%, val_best:  52.08%, tr:  60.41%, tr_best:  60.41%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.180643/  1.261304, val:  53.33%, val_best:  53.33%, tr:  60.32%, tr_best:  60.41%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.147187/  1.229540, val:  54.17%, val_best:  54.17%, tr:  61.20%, tr_best:  61.20%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.119203/  1.200135, val:  55.00%, val_best:  55.00%, tr:  61.81%, tr_best:  61.81%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.092511/  1.181086, val:  55.00%, val_best:  55.00%, tr:  62.89%, tr_best:  62.89%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.070046/  1.156432, val:  55.42%, val_best:  55.42%, tr:  62.76%, tr_best:  62.89%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.048934/  1.137937, val:  56.67%, val_best:  56.67%, tr:  62.80%, tr_best:  62.89%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.029617/  1.120042, val:  57.92%, val_best:  57.92%, tr:  63.23%, tr_best:  63.23%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.010602/  1.101642, val:  57.92%, val_best:  57.92%, tr:  63.62%, tr_best:  63.62%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  0.995935/  1.087225, val:  58.33%, val_best:  58.33%, tr:  63.59%, tr_best:  63.62%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  0.981817/  1.072620, val:  57.50%, val_best:  58.33%, tr:  63.98%, tr_best:  63.98%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  0.967208/  1.059461, val:  60.00%, val_best:  60.00%, tr:  64.11%, tr_best:  64.11%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  0.953673/  1.046700, val:  58.75%, val_best:  60.00%, tr:  64.22%, tr_best:  64.22%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  0.940991/  1.037450, val:  60.42%, val_best:  60.42%, tr:  65.06%, tr_best:  65.06%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  0.928821/  1.027025, val:  61.67%, val_best:  61.67%, tr:  65.37%, tr_best:  65.37%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  0.917311/  1.018004, val:  60.42%, val_best:  61.67%, tr:  65.89%, tr_best:  65.89%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  0.908286/  1.009769, val:  59.58%, val_best:  61.67%, tr:  65.80%, tr_best:  65.89%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  0.896888/  1.001318, val:  60.42%, val_best:  61.67%, tr:  66.07%, tr_best:  66.07%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  0.885928/  0.993262, val:  60.83%, val_best:  61.67%, tr:  66.68%, tr_best:  66.68%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  0.876439/  0.985915, val:  60.00%, val_best:  61.67%, tr:  66.52%, tr_best:  66.68%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.868947/  0.981664, val:  60.42%, val_best:  61.67%, tr:  66.75%, tr_best:  66.75%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.858279/  0.973066, val:  61.67%, val_best:  61.67%, tr:  66.77%, tr_best:  66.77%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.850094/  0.967068, val:  61.67%, val_best:  61.67%, tr:  66.55%, tr_best:  66.77%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.842231/  0.957203, val:  60.83%, val_best:  61.67%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.834813/  0.946628, val:  60.83%, val_best:  61.67%, tr:  67.40%, tr_best:  67.40%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.825051/  0.941394, val:  61.25%, val_best:  61.67%, tr:  67.61%, tr_best:  67.61%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.818249/  0.932789, val:  61.25%, val_best:  61.67%, tr:  67.81%, tr_best:  67.81%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.810588/  0.927249, val:  62.08%, val_best:  62.08%, tr:  67.83%, tr_best:  67.83%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.803611/  0.922036, val:  62.08%, val_best:  62.08%, tr:  68.26%, tr_best:  68.26%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.796083/  0.914300, val:  61.25%, val_best:  62.08%, tr:  68.49%, tr_best:  68.49%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.789206/  0.908464, val:  61.67%, val_best:  62.08%, tr:  68.26%, tr_best:  68.49%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.782749/  0.903982, val:  63.75%, val_best:  63.75%, tr:  68.49%, tr_best:  68.49%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.777331/  0.899595, val:  61.25%, val_best:  63.75%, tr:  68.62%, tr_best:  68.62%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.769963/  0.895615, val:  62.50%, val_best:  63.75%, tr:  69.00%, tr_best:  69.00%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.764745/  0.890088, val:  62.92%, val_best:  63.75%, tr:  69.34%, tr_best:  69.34%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.758702/  0.883598, val:  62.08%, val_best:  63.75%, tr:  69.18%, tr_best:  69.34%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.752254/  0.876847, val:  62.92%, val_best:  63.75%, tr:  69.63%, tr_best:  69.63%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.746812/  0.869477, val:  63.75%, val_best:  63.75%, tr:  69.77%, tr_best:  69.77%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.742301/  0.863256, val:  64.17%, val_best:  64.17%, tr:  70.24%, tr_best:  70.24%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.735165/  0.862153, val:  64.58%, val_best:  64.58%, tr:  69.72%, tr_best:  70.24%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.730379/  0.861285, val:  64.58%, val_best:  64.58%, tr:  70.22%, tr_best:  70.24%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.725799/  0.853961, val:  64.58%, val_best:  64.58%, tr:  70.78%, tr_best:  70.78%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.720326/  0.855814, val:  64.58%, val_best:  64.58%, tr:  70.78%, tr_best:  70.78%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.714832/  0.851440, val:  64.58%, val_best:  64.58%, tr:  70.92%, tr_best:  70.92%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.710597/  0.847417, val:  65.83%, val_best:  65.83%, tr:  71.24%, tr_best:  71.24%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.705414/  0.840219, val:  65.00%, val_best:  65.83%, tr:  71.53%, tr_best:  71.53%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.699708/  0.840559, val:  65.83%, val_best:  65.83%, tr:  71.53%, tr_best:  71.53%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.696593/  0.836499, val:  65.42%, val_best:  65.83%, tr:  71.39%, tr_best:  71.53%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.690856/  0.830074, val:  65.42%, val_best:  65.83%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.687257/  0.826471, val:  66.67%, val_best:  66.67%, tr:  72.39%, tr_best:  72.39%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.681947/  0.821454, val:  67.08%, val_best:  67.08%, tr:  72.00%, tr_best:  72.39%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.680511/  0.819491, val:  67.08%, val_best:  67.08%, tr:  71.91%, tr_best:  72.39%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.674662/  0.817399, val:  67.92%, val_best:  67.92%, tr:  72.34%, tr_best:  72.39%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.670538/  0.811107, val:  67.92%, val_best:  67.92%, tr:  72.61%, tr_best:  72.61%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.666288/  0.809117, val:  67.50%, val_best:  67.92%, tr:  72.79%, tr_best:  72.79%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.663668/  0.806643, val:  66.67%, val_best:  67.92%, tr:  73.11%, tr_best:  73.11%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.659499/  0.803086, val:  66.67%, val_best:  67.92%, tr:  73.04%, tr_best:  73.11%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.655165/  0.800506, val:  67.50%, val_best:  67.92%, tr:  73.22%, tr_best:  73.22%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.650053/  0.798804, val:  67.92%, val_best:  67.92%, tr:  73.51%, tr_best:  73.51%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.647311/  0.795178, val:  68.33%, val_best:  68.33%, tr:  73.11%, tr_best:  73.51%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.643141/  0.791496, val:  69.58%, val_best:  69.58%, tr:  73.56%, tr_best:  73.56%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.641986/  0.792410, val:  67.08%, val_best:  69.58%, tr:  74.32%, tr_best:  74.32%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.637132/  0.788478, val:  67.08%, val_best:  69.58%, tr:  73.72%, tr_best:  74.32%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.633591/  0.781451, val:  68.75%, val_best:  69.58%, tr:  73.76%, tr_best:  74.32%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.628969/  0.779606, val:  69.17%, val_best:  69.58%, tr:  73.58%, tr_best:  74.32%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.625478/  0.777917, val:  70.00%, val_best:  70.00%, tr:  73.67%, tr_best:  74.32%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.622437/  0.777216, val:  69.17%, val_best:  70.00%, tr:  74.03%, tr_best:  74.32%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.621279/  0.774592, val:  67.92%, val_best:  70.00%, tr:  75.02%, tr_best:  75.02%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.618362/  0.771103, val:  68.75%, val_best:  70.00%, tr:  73.62%, tr_best:  75.02%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.613633/  0.768010, val:  67.92%, val_best:  70.00%, tr:  74.48%, tr_best:  75.02%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.610719/  0.765524, val:  68.75%, val_best:  70.00%, tr:  74.44%, tr_best:  75.02%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.607984/  0.764992, val:  67.08%, val_best:  70.00%, tr:  74.64%, tr_best:  75.02%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.604853/  0.765907, val:  67.08%, val_best:  70.00%, tr:  75.88%, tr_best:  75.88%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.602037/  0.766473, val:  66.67%, val_best:  70.00%, tr:  75.16%, tr_best:  75.88%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.599105/  0.763732, val:  67.08%, val_best:  70.00%, tr:  75.32%, tr_best:  75.88%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.596332/  0.759634, val:  67.08%, val_best:  70.00%, tr:  75.14%, tr_best:  75.88%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.593523/  0.760113, val:  67.50%, val_best:  70.00%, tr:  75.54%, tr_best:  75.88%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.590523/  0.759438, val:  67.92%, val_best:  70.00%, tr:  75.52%, tr_best:  75.88%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.588720/  0.754809, val:  68.75%, val_best:  70.00%, tr:  75.77%, tr_best:  75.88%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.585111/  0.752894, val:  69.17%, val_best:  70.00%, tr:  76.17%, tr_best:  76.17%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.581915/  0.752701, val:  68.75%, val_best:  70.00%, tr:  76.35%, tr_best:  76.35%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.582003/  0.749536, val:  67.50%, val_best:  70.00%, tr:  76.22%, tr_best:  76.35%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.578276/  0.745849, val:  69.17%, val_best:  70.00%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.575655/  0.743174, val:  69.17%, val_best:  70.00%, tr:  76.98%, tr_best:  76.98%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.573612/  0.743190, val:  68.33%, val_best:  70.00%, tr:  76.49%, tr_best:  76.98%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.569228/  0.743745, val:  69.17%, val_best:  70.00%, tr:  77.43%, tr_best:  77.43%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.569282/  0.746271, val:  70.00%, val_best:  70.00%, tr:  76.31%, tr_best:  77.43%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.564963/  0.744331, val:  69.17%, val_best:  70.00%, tr:  77.25%, tr_best:  77.43%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.564362/  0.740429, val:  70.00%, val_best:  70.00%, tr:  77.32%, tr_best:  77.43%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d230a296b04bf3aef2b254d2ad0df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▅▆▆▄▆▅▅▆▄▇▇▇▇▇▇▅▅▆▆▅▇▅▇▇▇▇▇▆▇▅▇▆▇▅█▇▄▇</td></tr><tr><td>summary_val_acc</td><td>▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▇▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_acc_best</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.5</td></tr><tr><td>tr_acc</td><td>0.77322</td></tr><tr><td>tr_epoch_loss</td><td>0.56436</td></tr><tr><td>val_acc_best</td><td>0.7</td></tr><tr><td>val_acc_now</td><td>0.7</td></tr><tr><td>val_loss</td><td>0.74043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sljd5z5y' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sljd5z5y</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_215453-sljd5z5y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2ihstwq2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_220601-2ihstwq2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ihstwq2' target=\"_blank\">vivid-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ihstwq2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ihstwq2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.300406/  2.303886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.298519/  2.304635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.297336/  2.305443, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.296554/  2.306226, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.296175/  2.306992, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.296047/  2.307540, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.295772/  2.308033, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.295895/  2.308480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.296216/  2.308680, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.295915/  2.308826, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.295852/  2.308956, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.295693/  2.309156, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.295662/  2.309363, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.295913/  2.309332, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  2.296051/  2.309362, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  2.295724/  2.309383, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  2.295885/  2.309391, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  2.295423/  2.309439, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  2.295652/  2.309632, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  2.295643/  2.309649, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  2.295590/  2.309705, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  2.295918/  2.309849, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  2.295757/  2.309842, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  2.295655/  2.309749, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  2.295807/  2.309844, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  2.295745/  2.309667, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  2.295788/  2.309696, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  2.295882/  2.309606, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  2.295771/  2.309717, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  2.295679/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  2.295848/  2.309716, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  2.295971/  2.309722, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  2.295733/  2.309644, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  2.295907/  2.309664, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  2.295579/  2.309586, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  2.296159/  2.309631, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  2.295869/  2.309693, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  2.295740/  2.309628, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  2.295551/  2.309598, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  2.295965/  2.309603, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  2.296029/  2.309681, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  2.296090/  2.309719, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  2.295889/  2.309521, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  2.295922/  2.309554, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  2.295670/  2.309632, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  2.295724/  2.309685, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  2.296068/  2.309621, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  2.295866/  2.309682, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  2.295807/  2.309652, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  2.295917/  2.309690, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  2.295846/  2.309694, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  2.295856/  2.309697, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  2.295696/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  2.295523/  2.309763, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  2.295472/  2.309760, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  2.295569/  2.309824, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  2.295598/  2.309912, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  2.295660/  2.309693, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  2.295855/  2.309856, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  2.295852/  2.309891, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  2.295629/  2.309701, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  2.296022/  2.309732, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  2.295729/  2.309689, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  2.295848/  2.309700, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  2.296057/  2.309669, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  2.295710/  2.309537, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  2.295738/  2.309714, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  2.295935/  2.309666, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  2.295817/  2.309711, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  2.295775/  2.309609, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  2.295650/  2.309650, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  2.295809/  2.309635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  2.295998/  2.309713, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  2.295664/  2.309677, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  2.295693/  2.309724, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  2.295711/  2.309790, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  2.295864/  2.309779, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  2.295828/  2.309688, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  2.295550/  2.309566, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  2.295718/  2.309776, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  2.295860/  2.309709, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  2.296118/  2.309738, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  2.295657/  2.309602, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  2.295612/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  2.296072/  2.309790, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  2.295879/  2.309681, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  2.295759/  2.309716, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  2.295803/  2.309691, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  2.295742/  2.309650, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  2.295716/  2.309737, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  2.295929/  2.309744, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  2.296079/  2.309625, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  2.295828/  2.309630, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  2.295821/  2.309492, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  2.296116/  2.309484, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  2.296196/  2.309489, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  2.295864/  2.309556, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  2.295639/  2.309552, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  2.295890/  2.309603, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  2.295725/  2.309451, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570e693908c448f499da2aced309d1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃██▆▃▅▅█▅▁▅▁▆▃▆▁▃▆▃█▃█▅▅█▃█▆▃▅▃▅▁▃▃▅▃▃▃</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.12647</td></tr><tr><td>tr_epoch_loss</td><td>2.29573</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30945</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ihstwq2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2ihstwq2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_220601-2ihstwq2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7fyrxhs5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_221819-7fyrxhs5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fyrxhs5' target=\"_blank\">leafy-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fyrxhs5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fyrxhs5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.301286/  2.303460, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.300988/  2.303480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.300646/  2.303511, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.300203/  2.303545, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.299970/  2.303592, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.299778/  2.303646, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.299497/  2.303704, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.299424/  2.303769, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.299265/  2.303828, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.298968/  2.303891, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.298871/  2.303962, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.298592/  2.304030, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.298459/  2.304115, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.298379/  2.304188, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.298295/  2.304265, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.298030/  2.304340, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.298017/  2.304422, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.297553/  2.304496, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.297601/  2.304589, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.297451/  2.304670, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.297352/  2.304762, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.297352/  2.304854, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.297287/  2.304944, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.297099/  2.305020, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.297077/  2.305111, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  2.296936/  2.305189, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  2.296890/  2.305276, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  2.296940/  2.305352, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  2.296811/  2.305445, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  2.296637/  2.305523, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  2.296700/  2.305607, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  2.296727/  2.305690, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  2.296513/  2.305765, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  2.296525/  2.305847, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  2.296349/  2.305921, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  2.296649/  2.305994, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  2.296420/  2.306073, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  2.296336/  2.306145, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  2.296126/  2.306217, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  2.296369/  2.306292, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  2.296394/  2.306368, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  2.296393/  2.306446, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  2.296189/  2.306497, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  2.296202/  2.306564, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  2.295956/  2.306634, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  2.296020/  2.306701, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  2.296277/  2.306759, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  2.296092/  2.306827, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  2.296033/  2.306889, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  2.296098/  2.306953, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  2.296002/  2.307009, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  2.295949/  2.307062, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  2.295844/  2.307120, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  2.295670/  2.307187, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  2.295594/  2.307243, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  2.295657/  2.307309, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  2.295711/  2.307380, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  2.295710/  2.307413, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  2.295845/  2.307484, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  2.295861/  2.307544, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  2.295661/  2.307580, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  2.295970/  2.307635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  2.295709/  2.307679, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  2.295809/  2.307726, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  2.295975/  2.307768, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  2.295654/  2.307801, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  2.295733/  2.307851, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  2.295884/  2.307886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  2.295734/  2.307938, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  2.295697/  2.307965, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  2.295595/  2.308010, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  2.295741/  2.308047, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  2.295842/  2.308087, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  2.295578/  2.308119, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  2.295573/  2.308160, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  2.295596/  2.308207, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  2.295764/  2.308246, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  2.295714/  2.308275, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  2.295440/  2.308289, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  2.295578/  2.308338, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  2.295703/  2.308366, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  2.295931/  2.308404, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  2.295539/  2.308422, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  2.295487/  2.308453, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  2.295888/  2.308490, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  2.295693/  2.308505, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  2.295612/  2.308537, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  2.295655/  2.308565, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  2.295583/  2.308585, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  2.295536/  2.308618, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  2.295739/  2.308648, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  2.295894/  2.308658, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  2.295656/  2.308684, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  2.295647/  2.308695, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  2.295885/  2.308713, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  2.295976/  2.308733, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  2.295692/  2.308754, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  2.295486/  2.308773, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  2.295690/  2.308799, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  2.295526/  2.308801, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f25dd2152746efbc4ac1149363a6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃██▆▃▅▅█▅▁▅▁▆▃▆▁▃▆▃█▃█▅▅█▃█▆▃▅▃▅▁▃▃▅▃▃▃</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.12647</td></tr><tr><td>tr_epoch_loss</td><td>2.29553</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.3088</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fyrxhs5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7fyrxhs5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_221819-7fyrxhs5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9cx016j6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_222915-9cx016j6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9cx016j6' target=\"_blank\">twilight-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9cx016j6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9cx016j6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.300406/  2.303886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.298519/  2.304635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.297336/  2.305443, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.296554/  2.306226, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.296175/  2.306992, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.296047/  2.307540, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.295772/  2.308033, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.295895/  2.308480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.296216/  2.308680, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.295915/  2.308826, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.295852/  2.308956, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.295693/  2.309156, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.295662/  2.309363, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.295913/  2.309332, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  2.296051/  2.309362, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  2.295724/  2.309383, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  2.295885/  2.309391, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  2.295423/  2.309439, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  2.295652/  2.309632, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  2.295643/  2.309649, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  2.295590/  2.309705, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  2.295918/  2.309849, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  2.295757/  2.309842, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  2.295655/  2.309749, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  2.295807/  2.309844, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  2.295745/  2.309667, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  2.295788/  2.309696, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  2.295882/  2.309606, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  2.295771/  2.309717, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  2.295679/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  2.295848/  2.309716, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  2.295971/  2.309722, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  2.295733/  2.309644, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  2.295907/  2.309664, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  2.295579/  2.309586, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  2.296159/  2.309631, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  2.295869/  2.309693, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  2.295740/  2.309628, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  2.295551/  2.309598, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  2.295965/  2.309603, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  2.296029/  2.309681, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  2.296090/  2.309719, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  2.295889/  2.309521, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  2.295922/  2.309554, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  2.295670/  2.309632, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  2.295724/  2.309685, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  2.296068/  2.309621, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  2.295866/  2.309682, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  2.295807/  2.309652, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  2.295917/  2.309690, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  2.295846/  2.309694, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  2.295856/  2.309697, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  2.295696/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  2.295523/  2.309763, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  2.295472/  2.309760, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  2.295569/  2.309824, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  2.295598/  2.309912, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  2.295660/  2.309693, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  2.295855/  2.309856, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  2.295852/  2.309891, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  2.295629/  2.309701, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  2.296022/  2.309732, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  2.295729/  2.309689, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  2.295848/  2.309700, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  2.296057/  2.309669, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  2.295710/  2.309537, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  2.295738/  2.309714, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  2.295935/  2.309666, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  2.295817/  2.309711, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  2.295775/  2.309609, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  2.295650/  2.309650, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  2.295809/  2.309635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  2.295998/  2.309713, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  2.295664/  2.309677, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  2.295693/  2.309724, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  2.295711/  2.309790, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  2.295864/  2.309779, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  2.295828/  2.309688, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  2.295550/  2.309566, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  2.295718/  2.309776, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  2.295860/  2.309709, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  2.296118/  2.309738, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  2.295657/  2.309602, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  2.295612/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  2.296072/  2.309790, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  2.295879/  2.309681, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  2.295759/  2.309716, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  2.295803/  2.309691, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  2.295742/  2.309650, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  2.295716/  2.309737, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  2.295929/  2.309744, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  2.296079/  2.309625, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  2.295828/  2.309630, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  2.295821/  2.309492, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  2.296116/  2.309484, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  2.296196/  2.309489, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  2.295864/  2.309556, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  2.295639/  2.309552, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  2.295890/  2.309603, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  2.295725/  2.309451, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195891820497485c92ceaaf4b75932d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃██▆▃▅▅█▅▁▅▁▆▃▆▁▃▆▃█▃█▅▅█▃█▆▃▅▃▅▁▃▃▅▃▃▃</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.12647</td></tr><tr><td>tr_epoch_loss</td><td>2.29573</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30945</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sweep-6</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9cx016j6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9cx016j6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_222915-9cx016j6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nr7r0cc2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_224130-nr7r0cc2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr7r0cc2' target=\"_blank\">winter-sweep-8</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr7r0cc2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr7r0cc2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.937262/  0.815185, val:  65.42%, val_best:  65.42%, tr:  59.11%, tr_best:  59.11%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.621889/  0.850188, val:  60.00%, val_best:  65.42%, tr:  71.93%, tr_best:  71.93%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.458881/  0.705409, val:  75.00%, val_best:  75.00%, tr:  81.15%, tr_best:  81.15%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.364116/  0.639037, val:  81.67%, val_best:  81.67%, tr:  86.54%, tr_best:  86.54%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.216560/  0.537141, val:  83.75%, val_best:  83.75%, tr:  92.20%, tr_best:  92.20%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.154904/  0.527783, val:  85.00%, val_best:  85.00%, tr:  94.68%, tr_best:  94.68%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.123168/  0.547883, val:  85.00%, val_best:  85.00%, tr:  95.87%, tr_best:  95.87%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.101727/  0.458826, val:  90.83%, val_best:  90.83%, tr:  96.46%, tr_best:  96.46%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.077154/  0.511415, val:  87.92%, val_best:  90.83%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.035812/  0.474290, val:  89.17%, val_best:  90.83%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.048552/  0.527904, val:  89.58%, val_best:  90.83%, tr:  98.24%, tr_best:  99.01%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.050785/  0.620202, val:  86.67%, val_best:  90.83%, tr:  98.35%, tr_best:  99.01%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.033672/  0.567299, val:  87.92%, val_best:  90.83%, tr:  98.87%, tr_best:  99.01%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.015432/  0.640954, val:  89.17%, val_best:  90.83%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.034923/  0.587983, val:  87.92%, val_best:  90.83%, tr:  98.96%, tr_best:  99.57%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.021201/  0.601501, val:  87.92%, val_best:  90.83%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.015506/  0.459994, val:  90.42%, val_best:  90.83%, tr:  99.50%, tr_best:  99.57%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.003783/  0.525900, val:  90.00%, val_best:  90.83%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001669/  0.540889, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001090/  0.530677, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000912/  0.546428, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000791/  0.524711, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000725/  0.527520, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000641/  0.522226, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000566/  0.537194, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000524/  0.535535, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000494/  0.539577, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000459/  0.534046, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000424/  0.535550, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000406/  0.539363, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000384/  0.530667, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000363/  0.534131, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000348/  0.539306, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000337/  0.543372, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000322/  0.541279, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000307/  0.543588, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000299/  0.546806, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000288/  0.554800, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000278/  0.557397, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000272/  0.556288, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000260/  0.558762, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000252/  0.559537, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000244/  0.560730, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000236/  0.559925, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000227/  0.559428, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.559861, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000217/  0.549735, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000214/  0.556991, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000204/  0.567211, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000200/  0.569299, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000195/  0.573859, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000194/  0.571811, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000188/  0.569466, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000182/  0.574532, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000179/  0.576192, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.571063, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000177/  0.570948, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000169/  0.566309, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.567574, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000158/  0.568970, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000156/  0.575875, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000154/  0.571182, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000155/  0.576333, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000150/  0.578331, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.580241, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000143/  0.583426, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.582450, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000139/  0.582037, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000137/  0.583224, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000134/  0.586986, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000132/  0.583753, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000129/  0.588107, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000128/  0.582755, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000126/  0.584808, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000123/  0.586361, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000122/  0.584793, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000120/  0.582899, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.579190, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000114/  0.582361, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000113/  0.583482, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000111/  0.586028, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000110/  0.585406, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000108/  0.587859, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000108/  0.578510, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000105/  0.582357, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000104/  0.580975, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000103/  0.578182, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000102/  0.579897, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000102/  0.578804, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000100/  0.577548, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000098/  0.576983, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000097/  0.577229, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000096/  0.577828, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.579341, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000094/  0.575906, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000094/  0.577418, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000092/  0.575834, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000091/  0.580442, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000090/  0.580609, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.579757, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f778118c3ba8437a95324573090c5450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▄▁▇█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▆▆▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.57976</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-8</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr7r0cc2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr7r0cc2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_224130-nr7r0cc2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9gmaok6c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_225300-9gmaok6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9gmaok6c' target=\"_blank\">super-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9gmaok6c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9gmaok6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.924084/  0.900167, val:  62.50%, val_best:  62.50%, tr:  60.17%, tr_best:  60.17%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.608945/  0.868340, val:  60.00%, val_best:  62.50%, tr:  72.86%, tr_best:  72.86%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411019/  0.862224, val:  72.08%, val_best:  72.08%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.290071/  0.516031, val:  84.17%, val_best:  84.17%, tr:  89.34%, tr_best:  89.34%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.148748/  0.462285, val:  87.92%, val_best:  87.92%, tr:  95.38%, tr_best:  95.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.110478/  0.453902, val:  89.17%, val_best:  89.17%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.087798/  0.488559, val:  88.75%, val_best:  89.17%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.056540/  0.460554, val:  87.92%, val_best:  89.17%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.032420/  0.453790, val:  90.00%, val_best:  90.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.044882/  0.377394, val:  90.00%, val_best:  90.00%, tr:  98.67%, tr_best:  99.12%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.031081/  0.458891, val:  90.00%, val_best:  90.00%, tr:  99.01%, tr_best:  99.12%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.012163/  0.410768, val:  90.83%, val_best:  90.83%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003469/  0.424667, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002129/  0.401361, val:  91.25%, val_best:  91.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001459/  0.412334, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001118/  0.409213, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000979/  0.411735, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000870/  0.414670, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000784/  0.411883, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000699/  0.427088, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000645/  0.428068, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000597/  0.433039, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000562/  0.453180, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000520/  0.458498, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000493/  0.451457, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000468/  0.455886, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000441/  0.445366, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000417/  0.445858, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000400/  0.447511, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000372/  0.454448, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000364/  0.447946, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000352/  0.454611, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000334/  0.458291, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000324/  0.462057, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000310/  0.460060, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000302/  0.459708, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000290/  0.464959, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000283/  0.463789, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000270/  0.462140, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000260/  0.459259, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000252/  0.460512, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000247/  0.460966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000241/  0.461501, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000232/  0.467219, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000227/  0.465373, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000218/  0.464732, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000214/  0.462301, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000206/  0.463863, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000203/  0.462686, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000201/  0.461293, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000193/  0.463994, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000188/  0.463246, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000183/  0.461719, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000177/  0.464404, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000174/  0.462007, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000172/  0.465646, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000167/  0.467702, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000163/  0.466615, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000162/  0.465064, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000155/  0.466431, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000156/  0.470764, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000151/  0.474162, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000149/  0.475791, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000144/  0.477446, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000143/  0.473104, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000139/  0.477203, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000137/  0.482258, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000136/  0.479527, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000132/  0.478409, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000131/  0.478923, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.479966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.481726, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000125/  0.477696, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000124/  0.479414, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.479214, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000121/  0.478879, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.476968, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.477993, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.480774, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000115/  0.478548, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000113/  0.480919, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000112/  0.478824, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.480684, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000109/  0.478337, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000106/  0.473828, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000105/  0.479483, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000105/  0.480563, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000103/  0.480323, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000103/  0.481059, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000102/  0.479574, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.478145, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000098/  0.482602, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000097/  0.483561, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.486765, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.482070, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000093/  0.486749, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000091/  0.483553, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000091/  0.492586, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.491570, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.493886, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e398e7c9fc8147678accbcc5a83e209f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.49389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9gmaok6c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9gmaok6c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_225300-9gmaok6c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rc4yh9ks with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_230507-rc4yh9ks</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rc4yh9ks' target=\"_blank\">classic-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rc4yh9ks' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rc4yh9ks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.922245/  0.893013, val:  62.50%, val_best:  62.50%, tr:  59.38%, tr_best:  59.38%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619733/  0.880989, val:  60.42%, val_best:  62.50%, tr:  72.18%, tr_best:  72.18%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.435798/  0.683104, val:  77.08%, val_best:  77.08%, tr:  81.02%, tr_best:  81.02%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.299372/  0.505556, val:  85.00%, val_best:  85.00%, tr:  89.02%, tr_best:  89.02%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.197358/  0.403630, val:  85.42%, val_best:  85.42%, tr:  93.10%, tr_best:  93.10%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.129766/  0.439173, val:  87.08%, val_best:  87.08%, tr:  95.56%, tr_best:  95.56%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.104668/  0.714281, val:  84.58%, val_best:  87.08%, tr:  96.44%, tr_best:  96.44%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.090463/  0.478552, val:  87.08%, val_best:  87.08%, tr:  97.00%, tr_best:  97.00%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.049652/  0.436157, val:  88.33%, val_best:  88.33%, tr:  98.60%, tr_best:  98.60%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.023756/  0.413970, val:  90.00%, val_best:  90.00%, tr:  99.50%, tr_best:  99.50%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032942/  0.472970, val:  88.75%, val_best:  90.00%, tr:  99.03%, tr_best:  99.50%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.027201/  0.430632, val:  89.17%, val_best:  90.00%, tr:  99.12%, tr_best:  99.50%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.013121/  0.514948, val:  89.17%, val_best:  90.00%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.010347/  0.563254, val:  87.92%, val_best:  90.00%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.009572/  0.435153, val:  90.00%, val_best:  90.00%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.003371/  0.482731, val:  90.83%, val_best:  90.83%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001488/  0.473772, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001112/  0.486397, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000995/  0.495737, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000847/  0.505289, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000754/  0.504387, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000669/  0.500575, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000629/  0.510733, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000573/  0.509627, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000528/  0.522962, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000497/  0.518812, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000478/  0.511635, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000442/  0.520021, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000434/  0.512689, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000405/  0.525894, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000383/  0.520380, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000363/  0.527806, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000349/  0.518233, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000336/  0.520770, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000322/  0.527361, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000309/  0.520290, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000296/  0.522064, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000285/  0.526341, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.531222, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000272/  0.532444, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000261/  0.533822, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000254/  0.536716, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000246/  0.538109, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000241/  0.536533, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000233/  0.537335, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000232/  0.527503, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000220/  0.538040, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000214/  0.531304, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000209/  0.538361, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.543586, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.541322, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000196/  0.542786, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000192/  0.547309, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000189/  0.547139, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000184/  0.542990, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000183/  0.544180, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000179/  0.548399, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000175/  0.548830, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000173/  0.553346, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000169/  0.551210, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000168/  0.558970, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000162/  0.561837, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000162/  0.560709, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000157/  0.561463, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000154/  0.560693, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000149/  0.560770, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000150/  0.559654, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000146/  0.561553, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000144/  0.562505, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000142/  0.564370, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000137/  0.567566, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000136/  0.571191, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000135/  0.564148, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000132/  0.569437, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000131/  0.568583, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000128/  0.572920, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000126/  0.570853, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000124/  0.573105, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.572809, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000122/  0.576635, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000119/  0.575801, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000117/  0.575580, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.577640, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000114/  0.579572, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000112/  0.580623, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000110/  0.577019, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000110/  0.579865, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000108/  0.580845, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000107/  0.577829, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000106/  0.577943, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000105/  0.577916, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000102/  0.581132, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000102/  0.583284, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000101/  0.579166, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.579297, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000097/  0.579135, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000096/  0.582260, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000095/  0.580457, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000094/  0.579609, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000093/  0.581171, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1998ae5afb4373ba41dec05feb445d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▄▅▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.91667</td></tr><tr><td>val_loss</td><td>0.58117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classic-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rc4yh9ks' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rc4yh9ks</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_230507-rc4yh9ks/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ogu55u0c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_231707-ogu55u0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ogu55u0c' target=\"_blank\">treasured-sweep-13</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ogu55u0c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ogu55u0c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  0.982620/  0.763252, val:  64.58%, val_best:  64.58%, tr:  59.83%, tr_best:  59.83%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.595134/  0.787460, val:  62.92%, val_best:  64.58%, tr:  71.12%, tr_best:  71.12%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.494593/  0.690815, val:  70.00%, val_best:  70.00%, tr:  75.47%, tr_best:  75.47%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.451859/  0.738624, val:  72.50%, val_best:  72.50%, tr:  78.79%, tr_best:  78.79%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.386030/  0.733375, val:  70.83%, val_best:  72.50%, tr:  82.64%, tr_best:  82.64%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.344184/  0.678375, val:  75.42%, val_best:  75.42%, tr:  84.83%, tr_best:  84.83%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.293505/  0.758990, val:  77.92%, val_best:  77.92%, tr:  88.14%, tr_best:  88.14%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.248461/  0.809975, val:  70.83%, val_best:  77.92%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.205007/  0.656650, val:  77.50%, val_best:  77.92%, tr:  92.29%, tr_best:  92.29%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.170074/  0.709826, val:  78.75%, val_best:  78.75%, tr:  94.00%, tr_best:  94.00%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.122124/  0.581119, val:  84.58%, val_best:  84.58%, tr:  96.37%, tr_best:  96.37%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.106570/  0.706611, val:  80.42%, val_best:  84.58%, tr:  96.46%, tr_best:  96.46%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.095229/  0.593320, val:  85.00%, val_best:  85.00%, tr:  96.80%, tr_best:  96.80%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.058587/  0.629303, val:  84.58%, val_best:  85.00%, tr:  98.31%, tr_best:  98.31%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.051209/  0.585983, val:  85.00%, val_best:  85.00%, tr:  98.49%, tr_best:  98.49%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.058890/  0.579519, val:  84.17%, val_best:  85.00%, tr:  97.99%, tr_best:  98.49%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.030624/  0.712461, val:  82.92%, val_best:  85.00%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.020707/  0.584158, val:  86.67%, val_best:  86.67%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.011792/  0.524050, val:  85.00%, val_best:  86.67%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.022928/  0.633163, val:  83.75%, val_best:  86.67%, tr:  99.39%, tr_best:  99.93%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.009906/  0.606179, val:  85.83%, val_best:  86.67%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.006724/  0.665348, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.007707/  0.618630, val:  85.42%, val_best:  86.67%, tr:  99.91%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.023205/  0.669738, val:  84.17%, val_best:  86.67%, tr:  99.48%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.005553/  0.621535, val:  84.58%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.003967/  0.664081, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.003706/  0.669434, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.002976/  0.650336, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.002667/  0.625103, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.002437/  0.628173, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.002196/  0.647678, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.002046/  0.647234, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.001928/  0.653110, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.001766/  0.663407, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.001711/  0.668078, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.001660/  0.660693, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.001525/  0.675055, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.001503/  0.653547, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.001464/  0.671045, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.001369/  0.679915, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.001327/  0.668826, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.001240/  0.670618, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.001232/  0.673496, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.001178/  0.682140, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.001145/  0.687650, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001108/  0.692142, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.001079/  0.683559, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.001021/  0.684208, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.000998/  0.695942, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.000982/  0.698146, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.000961/  0.695201, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.000933/  0.712461, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.000895/  0.708046, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.000881/  0.701730, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.000841/  0.715689, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.000831/  0.721041, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000823/  0.712479, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000800/  0.726725, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000785/  0.725347, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000750/  0.720342, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000726/  0.728045, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000727/  0.720476, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000703/  0.725871, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000689/  0.730748, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000682/  0.715350, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000671/  0.728565, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000657/  0.730741, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000640/  0.728287, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000630/  0.731618, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000622/  0.727333, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000608/  0.740138, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000596/  0.742107, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000591/  0.740085, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000580/  0.741508, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000566/  0.745314, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000556/  0.742544, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000539/  0.746382, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000535/  0.753533, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000529/  0.748593, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000523/  0.755587, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000508/  0.762483, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000497/  0.760601, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000499/  0.750359, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000484/  0.755237, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000482/  0.756247, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000473/  0.755977, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000460/  0.758920, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000455/  0.759381, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000445/  0.771889, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000447/  0.763992, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000438/  0.760672, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000432/  0.771026, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000426/  0.773566, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000420/  0.769110, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000414/  0.763652, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000407/  0.772488, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000403/  0.773063, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000397/  0.777332, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000390/  0.776414, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000393/  0.774920, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab0ace56c294387a4269cf5a21dc633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▆██▆█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇█▇▇▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00039</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>0.77492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-13</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ogu55u0c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ogu55u0c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_231707-ogu55u0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nuovjakz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac91491b8441455fb2ad5042f2aaa0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112936322266858, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_232836-nuovjakz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nuovjakz' target=\"_blank\">smart-sweep-14</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nuovjakz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nuovjakz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.300406/  2.303886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.298519/  2.304635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.297336/  2.305443, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.296554/  2.306226, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.296175/  2.306992, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.296047/  2.307540, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.295772/  2.308033, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.295895/  2.308480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.296216/  2.308680, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.295915/  2.308826, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.295852/  2.308956, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.295693/  2.309156, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.295662/  2.309363, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.295913/  2.309332, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  2.296051/  2.309362, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  2.295724/  2.309383, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  2.295885/  2.309391, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  2.295423/  2.309439, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  2.295652/  2.309632, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  2.295643/  2.309649, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  2.295590/  2.309705, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  2.295918/  2.309849, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  2.295757/  2.309842, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  2.295655/  2.309749, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  2.295807/  2.309844, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  2.295745/  2.309667, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  2.295788/  2.309696, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  2.295882/  2.309606, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  2.295771/  2.309717, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  2.295679/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  2.295848/  2.309716, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  2.295971/  2.309722, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  2.295733/  2.309644, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  2.295907/  2.309664, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  2.295579/  2.309586, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  2.296159/  2.309631, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  2.295869/  2.309693, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  2.295740/  2.309628, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  2.295551/  2.309598, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  2.295965/  2.309603, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  2.296029/  2.309681, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  2.296090/  2.309719, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  2.295889/  2.309521, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  2.295922/  2.309554, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  2.295670/  2.309632, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  2.295724/  2.309685, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  2.296068/  2.309621, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  2.295866/  2.309682, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  2.295807/  2.309652, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  2.295917/  2.309690, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  2.295846/  2.309694, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  2.295856/  2.309697, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  2.295696/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  2.295523/  2.309763, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  2.295472/  2.309760, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  2.295569/  2.309824, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  2.295598/  2.309912, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  2.295660/  2.309693, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  2.295855/  2.309856, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  2.295852/  2.309891, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  2.295629/  2.309701, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  2.296022/  2.309732, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  2.295729/  2.309689, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  2.295848/  2.309700, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  2.296057/  2.309669, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  2.295710/  2.309537, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  2.295738/  2.309714, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  2.295935/  2.309666, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  2.295817/  2.309711, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  2.295775/  2.309609, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  2.295650/  2.309650, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  2.295809/  2.309635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  2.295998/  2.309713, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  2.295664/  2.309677, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  2.295693/  2.309724, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  2.295711/  2.309790, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  2.295864/  2.309779, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  2.295828/  2.309688, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  2.295550/  2.309566, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  2.295718/  2.309776, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  2.295860/  2.309709, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  2.296118/  2.309738, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  2.295657/  2.309602, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  2.295612/  2.309659, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  2.296072/  2.309790, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  2.295879/  2.309681, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  2.295759/  2.309716, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  2.295803/  2.309691, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  2.295742/  2.309650, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  2.295716/  2.309737, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  2.295929/  2.309744, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  2.296079/  2.309625, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  2.295828/  2.309630, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  2.295821/  2.309492, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  2.296116/  2.309484, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  2.296196/  2.309489, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  2.295864/  2.309556, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  2.295639/  2.309552, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  2.295890/  2.309603, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  2.295725/  2.309451, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbd783498bf4f41aec9525071dee123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃██▆▃▅▅█▅▁▅▁▆▃▆▁▃▆▃█▃█▅▅█▃█▆▃▅▃▅▁▃▃▅▃▃▃</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.12647</td></tr><tr><td>tr_epoch_loss</td><td>2.29573</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30945</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-14</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nuovjakz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nuovjakz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_232836-nuovjakz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jwkqc50k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_234108-jwkqc50k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jwkqc50k' target=\"_blank\">radiant-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jwkqc50k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jwkqc50k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  0.952379/  0.760758, val:  65.00%, val_best:  65.00%, tr:  60.17%, tr_best:  60.17%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.597405/  0.802127, val:  65.83%, val_best:  65.83%, tr:  71.57%, tr_best:  71.57%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.494290/  0.675536, val:  70.42%, val_best:  70.42%, tr:  75.83%, tr_best:  75.83%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.433566/  0.634103, val:  75.42%, val_best:  75.42%, tr:  80.37%, tr_best:  80.37%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.353385/  0.705093, val:  71.25%, val_best:  75.42%, tr:  85.53%, tr_best:  85.53%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.292772/  0.566377, val:  81.67%, val_best:  81.67%, tr:  88.41%, tr_best:  88.41%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.228818/  0.717473, val:  80.83%, val_best:  81.67%, tr:  92.36%, tr_best:  92.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.179044/  0.676184, val:  80.00%, val_best:  81.67%, tr:  93.89%, tr_best:  93.89%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.151077/  0.564310, val:  82.50%, val_best:  82.50%, tr:  95.15%, tr_best:  95.15%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.101316/  0.589415, val:  85.00%, val_best:  85.00%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.076378/  0.522163, val:  85.83%, val_best:  85.83%, tr:  97.99%, tr_best:  97.99%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.059930/  0.665952, val:  85.00%, val_best:  85.83%, tr:  98.51%, tr_best:  98.51%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.059655/  0.573941, val:  83.33%, val_best:  85.83%, tr:  98.49%, tr_best:  98.51%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.036806/  0.559398, val:  86.25%, val_best:  86.25%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.032969/  0.516635, val:  87.08%, val_best:  87.08%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.019159/  0.520381, val:  87.50%, val_best:  87.50%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.022022/  0.533267, val:  84.58%, val_best:  87.50%, tr:  99.66%, tr_best:  99.73%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.016358/  0.542127, val:  85.42%, val_best:  87.50%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.011523/  0.489388, val:  87.08%, val_best:  87.50%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.009425/  0.564911, val:  86.25%, val_best:  87.50%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.006947/  0.567658, val:  85.42%, val_best:  87.50%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.006307/  0.570462, val:  84.17%, val_best:  87.50%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.005114/  0.567243, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.005138/  0.583205, val:  86.25%, val_best:  87.50%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.003950/  0.593902, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.003587/  0.594694, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.003648/  0.609173, val:  84.58%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.003088/  0.596028, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.003133/  0.580937, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.002858/  0.590504, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.002632/  0.612238, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.002386/  0.611080, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.002324/  0.595979, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.002179/  0.602397, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.002005/  0.616105, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.001963/  0.611226, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.001897/  0.634478, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.001767/  0.623152, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.001745/  0.624898, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.001658/  0.618266, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.001578/  0.626798, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.001510/  0.615344, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.001487/  0.619377, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.001420/  0.625227, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.001356/  0.631350, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001325/  0.625323, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.001310/  0.627410, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.001300/  0.623080, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.001201/  0.628064, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.001162/  0.617641, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.001150/  0.624351, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.001110/  0.636615, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.001063/  0.634187, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.001055/  0.621223, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001005/  0.642231, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001018/  0.644688, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000980/  0.637063, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000947/  0.631706, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000934/  0.647234, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000905/  0.637347, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000878/  0.647510, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000869/  0.639772, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000884/  0.638948, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000825/  0.642893, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000805/  0.643366, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000788/  0.637524, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000779/  0.647130, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000773/  0.649504, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000759/  0.646191, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000738/  0.642649, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000732/  0.645177, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000713/  0.643868, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000702/  0.650823, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000692/  0.652844, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000676/  0.647574, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000661/  0.645422, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000642/  0.642438, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000640/  0.642848, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000625/  0.645078, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000614/  0.641013, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000610/  0.644358, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000598/  0.640954, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000584/  0.637847, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000586/  0.643601, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000569/  0.636117, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000557/  0.645541, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000561/  0.646002, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000546/  0.643248, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000545/  0.649327, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000537/  0.644704, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000524/  0.642928, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000526/  0.648899, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000512/  0.654671, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000510/  0.651562, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000505/  0.641383, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000496/  0.653441, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000493/  0.650448, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000483/  0.658626, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000483/  0.659032, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000477/  0.653240, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3921221e310e4050af9d707ace6dcb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▃▆██▆█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00048</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>0.65324</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jwkqc50k' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jwkqc50k</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_234108-jwkqc50k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: en1lu80g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_235237-en1lu80g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/en1lu80g' target=\"_blank\">driven-sweep-16</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/en1lu80g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/en1lu80g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  0.898481/  0.823793, val:  65.83%, val_best:  65.83%, tr:  62.20%, tr_best:  62.20%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.611604/  0.882543, val:  61.67%, val_best:  65.83%, tr:  70.11%, tr_best:  70.11%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.510053/  0.748155, val:  66.67%, val_best:  66.67%, tr:  74.59%, tr_best:  74.59%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.469412/  0.757868, val:  67.08%, val_best:  67.08%, tr:  76.80%, tr_best:  76.80%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.399633/  0.772439, val:  67.92%, val_best:  67.92%, tr:  80.55%, tr_best:  80.55%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.363896/  0.692198, val:  72.92%, val_best:  72.92%, tr:  82.24%, tr_best:  82.24%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.357733/  0.759419, val:  70.00%, val_best:  72.92%, tr:  83.54%, tr_best:  83.54%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.291650/  0.733637, val:  75.00%, val_best:  75.00%, tr:  86.86%, tr_best:  86.86%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.276770/  0.745728, val:  73.33%, val_best:  75.00%, tr:  88.10%, tr_best:  88.10%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.245344/  0.698292, val:  76.25%, val_best:  76.25%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.197213/  0.739935, val:  75.42%, val_best:  76.25%, tr:  92.00%, tr_best:  92.00%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.187397/  0.775623, val:  75.00%, val_best:  76.25%, tr:  92.45%, tr_best:  92.45%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.155108/  0.899217, val:  76.67%, val_best:  76.67%, tr:  94.00%, tr_best:  94.00%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.115828/  0.763209, val:  77.92%, val_best:  77.92%, tr:  95.58%, tr_best:  95.58%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.117284/  0.731325, val:  78.75%, val_best:  78.75%, tr:  95.76%, tr_best:  95.76%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.068853/  0.876907, val:  76.25%, val_best:  78.75%, tr:  97.59%, tr_best:  97.59%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.091877/  0.756856, val:  78.75%, val_best:  78.75%, tr:  96.57%, tr_best:  97.59%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.055636/  0.881745, val:  75.00%, val_best:  78.75%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.064637/  0.777041, val:  79.17%, val_best:  79.17%, tr:  97.70%, tr_best:  98.26%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.033209/  0.884130, val:  77.50%, val_best:  79.17%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.017796/  0.871285, val:  80.00%, val_best:  80.00%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.008210/  0.834815, val:  80.00%, val_best:  80.00%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.009677/  0.914292, val:  80.00%, val_best:  80.00%, tr:  99.80%, tr_best:  99.93%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.033340/  0.885271, val:  79.58%, val_best:  80.00%, tr:  99.01%, tr_best:  99.93%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.018724/  0.889664, val:  80.83%, val_best:  80.83%, tr:  99.55%, tr_best:  99.93%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.004772/  0.880268, val:  80.00%, val_best:  80.83%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.004033/  0.877016, val:  79.58%, val_best:  80.83%, tr:  99.95%, tr_best:  99.98%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.002895/  0.862193, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.002631/  0.864004, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.002286/  0.866756, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.002069/  0.889516, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.001899/  0.898125, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.001745/  0.903290, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.001598/  0.904688, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.001491/  0.913507, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.001392/  0.899336, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.001292/  0.919515, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.001238/  0.916405, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.001236/  0.925066, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.001145/  0.921001, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.001104/  0.924500, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.001039/  0.915401, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.000998/  0.921598, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.000961/  0.926706, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.000926/  0.929054, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.000915/  0.927307, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.000869/  0.934137, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.000834/  0.929947, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.000809/  0.931911, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.000775/  0.946297, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.000753/  0.949497, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.000737/  0.950188, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.000717/  0.958488, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.000698/  0.953354, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.000673/  0.962603, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.000667/  0.960421, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000655/  0.971835, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000635/  0.980483, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000610/  0.984593, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000593/  0.982647, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000579/  0.989581, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000572/  0.984431, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000567/  0.994421, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000548/  0.992698, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000528/  0.998123, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000520/  0.996310, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000509/  1.005011, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000498/  0.998903, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000491/  1.000359, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000483/  1.003252, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000473/  1.016220, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000467/  1.017255, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000464/  1.012084, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000447/  1.015808, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000440/  1.013890, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000427/  1.020182, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000423/  1.023472, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000418/  1.026235, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000412/  1.030910, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000403/  1.032620, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000394/  1.033295, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000390/  1.041270, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000385/  1.032060, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000384/  1.035897, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000373/  1.030756, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000367/  1.037791, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000361/  1.041065, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000358/  1.036252, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000352/  1.035342, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000349/  1.039061, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000344/  1.045172, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000337/  1.048042, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000334/  1.048856, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000329/  1.051989, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000326/  1.054497, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000322/  1.041216, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000318/  1.055244, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000315/  1.053684, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000310/  1.056977, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000308/  1.048941, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c8fd5157434b72aeeb1a3e36011379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▅▁▅▅▆▆█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇█▇████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▇▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇█▇████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁▇▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00031</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.8125</td></tr><tr><td>val_loss</td><td>1.04894</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-sweep-16</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/en1lu80g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/en1lu80g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_235237-en1lu80g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bfgrskef with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_000433-bfgrskef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfgrskef' target=\"_blank\">stilted-sweep-17</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfgrskef' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfgrskef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.230488/  1.894483, val:  24.58%, val_best:  24.58%, tr:  15.53%, tr_best:  15.53%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.171160/  0.925719, val:  60.42%, val_best:  60.42%, tr:  53.13%, tr_best:  53.13%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.714499/  0.760628, val:  65.83%, val_best:  65.83%, tr:  67.31%, tr_best:  67.31%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.590655/  0.734308, val:  69.17%, val_best:  69.17%, tr:  71.39%, tr_best:  71.39%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.509804/  0.668199, val:  70.83%, val_best:  70.83%, tr:  74.48%, tr_best:  74.48%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.456735/  0.679717, val:  72.50%, val_best:  72.50%, tr:  77.59%, tr_best:  77.59%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.409618/  0.732960, val:  73.75%, val_best:  73.75%, tr:  80.75%, tr_best:  80.75%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.357855/  0.639079, val:  76.25%, val_best:  76.25%, tr:  85.21%, tr_best:  85.21%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.310420/  0.671866, val:  76.25%, val_best:  76.25%, tr:  87.71%, tr_best:  87.71%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.251336/  0.637769, val:  81.25%, val_best:  81.25%, tr:  91.12%, tr_best:  91.12%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.201465/  0.581991, val:  84.17%, val_best:  84.17%, tr:  93.55%, tr_best:  93.55%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.164416/  0.628033, val:  81.25%, val_best:  84.17%, tr:  94.77%, tr_best:  94.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.137533/  0.588066, val:  82.92%, val_best:  84.17%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.098348/  0.565540, val:  83.75%, val_best:  84.17%, tr:  97.63%, tr_best:  97.63%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.079255/  0.547163, val:  84.17%, val_best:  84.17%, tr:  98.24%, tr_best:  98.24%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.060503/  0.588293, val:  82.08%, val_best:  84.17%, tr:  98.65%, tr_best:  98.65%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.051354/  0.596437, val:  85.00%, val_best:  85.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.041532/  0.535959, val:  87.08%, val_best:  87.08%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.031546/  0.528492, val:  86.25%, val_best:  87.08%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.028511/  0.589350, val:  84.17%, val_best:  87.08%, tr:  99.62%, tr_best:  99.71%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.025349/  0.534227, val:  85.83%, val_best:  87.08%, tr:  99.62%, tr_best:  99.71%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.019058/  0.577408, val:  85.42%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.014315/  0.558813, val:  84.17%, val_best:  87.08%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.012737/  0.588146, val:  85.42%, val_best:  87.08%, tr:  99.89%, tr_best:  99.98%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.010746/  0.578140, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.010760/  0.582522, val:  85.42%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.009629/  0.617223, val:  83.75%, val_best:  87.08%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.007863/  0.574396, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.008132/  0.559445, val:  86.25%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.007406/  0.581850, val:  85.42%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.006720/  0.579857, val:  86.25%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.005811/  0.604387, val:  84.17%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.005610/  0.594436, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.005430/  0.573741, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.004741/  0.605000, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.004614/  0.600777, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.004291/  0.613966, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.004105/  0.590188, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.004022/  0.596265, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.003720/  0.597433, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.003529/  0.610074, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.003320/  0.589955, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.003288/  0.601375, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.003163/  0.610427, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.003013/  0.599859, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.002865/  0.601631, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.002866/  0.603223, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.002665/  0.594470, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.002680/  0.607053, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.002605/  0.606846, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.002635/  0.594937, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002370/  0.611717, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002310/  0.599046, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002233/  0.587752, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.002200/  0.595162, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.002121/  0.604269, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002051/  0.614163, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.001998/  0.604480, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001968/  0.598754, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001921/  0.598088, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001847/  0.603993, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001817/  0.609484, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001752/  0.591017, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001718/  0.614747, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001696/  0.591860, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001636/  0.607278, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001616/  0.606340, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001569/  0.608506, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001597/  0.611010, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001487/  0.609785, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001448/  0.610556, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001477/  0.602742, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001397/  0.623278, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001393/  0.620418, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001379/  0.611465, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001384/  0.612189, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001315/  0.632627, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001329/  0.623954, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001281/  0.609634, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001263/  0.624568, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001228/  0.642201, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001214/  0.633396, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001194/  0.630446, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001173/  0.626845, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001133/  0.636312, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001121/  0.631985, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001125/  0.626667, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001103/  0.620037, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001080/  0.619351, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001089/  0.614042, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001057/  0.617202, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001032/  0.621814, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001010/  0.624435, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001004/  0.619219, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000977/  0.621659, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000976/  0.632266, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000958/  0.624663, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000943/  0.635307, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000934/  0.635421, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000932/  0.631524, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff6742ba7c947e9a7444b4ab9ee6773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▆▇▇█▇█▇███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▆▅▅▅▅▅▅▅▅▆▅▆▆▆▅▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00093</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>0.63152</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-sweep-17</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfgrskef' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bfgrskef</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_000433-bfgrskef/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q9jccg6g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_001636-q9jccg6g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q9jccg6g' target=\"_blank\">smart-sweep-18</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q9jccg6g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q9jccg6g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  1.793628/  1.367353, val:  50.42%, val_best:  50.42%, tr:  36.59%, tr_best:  36.59%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.078896/  1.044046, val:  60.00%, val_best:  60.00%, tr:  61.74%, tr_best:  61.74%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  0.863244/  0.906804, val:  62.92%, val_best:  62.92%, tr:  66.30%, tr_best:  66.30%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.755520/  0.825081, val:  63.75%, val_best:  63.75%, tr:  68.51%, tr_best:  68.51%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.684600/  0.777398, val:  67.92%, val_best:  67.92%, tr:  70.85%, tr_best:  70.85%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.634513/  0.746337, val:  67.92%, val_best:  67.92%, tr:  72.34%, tr_best:  72.34%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.592997/  0.718604, val:  70.00%, val_best:  70.00%, tr:  73.83%, tr_best:  73.83%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.559752/  0.718282, val:  67.92%, val_best:  70.00%, tr:  75.23%, tr_best:  75.23%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.534730/  0.719083, val:  70.00%, val_best:  70.00%, tr:  75.65%, tr_best:  75.65%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.511788/  0.687442, val:  71.25%, val_best:  71.25%, tr:  77.46%, tr_best:  77.46%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.491294/  0.699757, val:  70.42%, val_best:  71.25%, tr:  77.86%, tr_best:  77.86%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.471112/  0.686370, val:  72.08%, val_best:  72.08%, tr:  79.24%, tr_best:  79.24%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.456408/  0.674098, val:  73.33%, val_best:  73.33%, tr:  80.12%, tr_best:  80.12%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.444525/  0.682317, val:  72.08%, val_best:  73.33%, tr:  80.86%, tr_best:  80.86%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.430189/  0.671425, val:  71.25%, val_best:  73.33%, tr:  81.45%, tr_best:  81.45%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.413160/  0.663911, val:  74.17%, val_best:  74.17%, tr:  82.73%, tr_best:  82.73%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.400152/  0.674311, val:  72.50%, val_best:  74.17%, tr:  82.82%, tr_best:  82.82%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.391464/  0.663677, val:  74.17%, val_best:  74.17%, tr:  83.61%, tr_best:  83.61%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.380452/  0.691022, val:  70.83%, val_best:  74.17%, tr:  84.45%, tr_best:  84.45%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.365539/  0.694252, val:  71.25%, val_best:  74.17%, tr:  85.21%, tr_best:  85.21%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.360076/  0.697419, val:  73.75%, val_best:  74.17%, tr:  85.64%, tr_best:  85.64%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.342587/  0.721997, val:  71.67%, val_best:  74.17%, tr:  86.47%, tr_best:  86.47%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.335225/  0.669644, val:  73.75%, val_best:  74.17%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.325485/  0.671481, val:  74.17%, val_best:  74.17%, tr:  87.78%, tr_best:  87.78%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.313137/  0.674333, val:  74.17%, val_best:  74.17%, tr:  88.48%, tr_best:  88.48%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.305158/  0.668468, val:  75.00%, val_best:  75.00%, tr:  88.84%, tr_best:  88.84%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.289999/  0.709316, val:  74.58%, val_best:  75.00%, tr:  90.22%, tr_best:  90.22%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.282737/  0.683195, val:  74.58%, val_best:  75.00%, tr:  90.46%, tr_best:  90.46%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.271201/  0.657128, val:  75.42%, val_best:  75.42%, tr:  90.87%, tr_best:  90.87%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.262708/  0.665003, val:  77.50%, val_best:  77.50%, tr:  91.48%, tr_best:  91.48%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.246958/  0.694957, val:  75.83%, val_best:  77.50%, tr:  92.38%, tr_best:  92.38%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.241823/  0.667330, val:  77.08%, val_best:  77.50%, tr:  92.25%, tr_best:  92.38%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.227941/  0.702040, val:  75.83%, val_best:  77.50%, tr:  93.17%, tr_best:  93.17%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.216325/  0.673012, val:  77.08%, val_best:  77.50%, tr:  93.30%, tr_best:  93.30%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.210879/  0.676511, val:  78.33%, val_best:  78.33%, tr:  93.49%, tr_best:  93.49%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.195846/  0.651897, val:  80.42%, val_best:  80.42%, tr:  93.96%, tr_best:  93.96%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.186394/  0.674199, val:  77.08%, val_best:  80.42%, tr:  94.52%, tr_best:  94.52%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.179134/  0.641201, val:  80.42%, val_best:  80.42%, tr:  94.95%, tr_best:  94.95%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.167923/  0.661525, val:  82.08%, val_best:  82.08%, tr:  95.42%, tr_best:  95.42%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.160678/  0.644491, val:  81.67%, val_best:  82.08%, tr:  95.67%, tr_best:  95.67%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.149732/  0.687089, val:  78.75%, val_best:  82.08%, tr:  96.44%, tr_best:  96.44%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.140695/  0.693330, val:  75.83%, val_best:  82.08%, tr:  96.66%, tr_best:  96.66%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.131993/  0.653493, val:  80.83%, val_best:  82.08%, tr:  97.09%, tr_best:  97.09%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.126227/  0.656346, val:  81.25%, val_best:  82.08%, tr:  97.05%, tr_best:  97.09%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.122717/  0.646890, val:  82.08%, val_best:  82.08%, tr:  97.07%, tr_best:  97.09%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.113838/  0.657322, val:  80.42%, val_best:  82.08%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.109537/  0.650073, val:  81.67%, val_best:  82.08%, tr:  97.77%, tr_best:  97.77%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.103056/  0.657151, val:  80.00%, val_best:  82.08%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.097923/  0.651409, val:  81.25%, val_best:  82.08%, tr:  98.08%, tr_best:  98.08%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.092461/  0.630081, val:  82.08%, val_best:  82.08%, tr:  98.08%, tr_best:  98.08%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.088861/  0.660385, val:  81.25%, val_best:  82.08%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.078752/  0.656940, val:  80.42%, val_best:  82.08%, tr:  98.83%, tr_best:  98.83%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.075266/  0.661648, val:  79.17%, val_best:  82.08%, tr:  98.81%, tr_best:  98.83%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.071436/  0.644607, val:  80.00%, val_best:  82.08%, tr:  98.94%, tr_best:  98.94%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.067946/  0.671214, val:  80.83%, val_best:  82.08%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.068751/  0.651913, val:  83.75%, val_best:  83.75%, tr:  98.94%, tr_best:  99.10%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.060359/  0.665463, val:  82.92%, val_best:  83.75%, tr:  99.32%, tr_best:  99.32%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.056799/  0.675353, val:  82.08%, val_best:  83.75%, tr:  99.26%, tr_best:  99.32%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.054479/  0.674025, val:  80.83%, val_best:  83.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.053209/  0.652202, val:  82.08%, val_best:  83.75%, tr:  99.57%, tr_best:  99.59%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.048397/  0.708025, val:  80.83%, val_best:  83.75%, tr:  99.64%, tr_best:  99.64%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.047649/  0.653994, val:  82.92%, val_best:  83.75%, tr:  99.62%, tr_best:  99.64%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.045836/  0.643945, val:  83.33%, val_best:  83.75%, tr:  99.62%, tr_best:  99.64%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.043942/  0.668366, val:  81.25%, val_best:  83.75%, tr:  99.62%, tr_best:  99.64%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.041073/  0.628036, val:  82.08%, val_best:  83.75%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.039835/  0.691711, val:  78.33%, val_best:  83.75%, tr:  99.73%, tr_best:  99.77%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.037615/  0.656798, val:  83.33%, val_best:  83.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.036271/  0.651796, val:  82.08%, val_best:  83.75%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.034543/  0.695574, val:  81.67%, val_best:  83.75%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.033145/  0.669129, val:  83.33%, val_best:  83.75%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.032022/  0.670852, val:  82.50%, val_best:  83.75%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.031446/  0.670762, val:  81.67%, val_best:  83.75%, tr:  99.82%, tr_best:  99.91%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.029396/  0.681327, val:  81.67%, val_best:  83.75%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.028694/  0.677644, val:  83.75%, val_best:  83.75%, tr:  99.86%, tr_best:  99.93%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.027682/  0.677544, val:  81.25%, val_best:  83.75%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.026064/  0.669309, val:  83.33%, val_best:  83.75%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.025788/  0.665865, val:  82.92%, val_best:  83.75%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.025212/  0.671724, val:  83.33%, val_best:  83.75%, tr:  99.91%, tr_best:  99.98%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.025033/  0.681937, val:  82.08%, val_best:  83.75%, tr:  99.93%, tr_best:  99.98%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.023403/  0.678549, val:  84.17%, val_best:  84.17%, tr:  99.93%, tr_best:  99.98%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.022496/  0.687387, val:  81.67%, val_best:  84.17%, tr:  99.95%, tr_best:  99.98%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.021997/  0.681508, val:  82.08%, val_best:  84.17%, tr:  99.93%, tr_best:  99.98%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.020940/  0.680731, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.021529/  0.674475, val:  83.33%, val_best:  84.17%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.020631/  0.673137, val:  82.50%, val_best:  84.17%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.019111/  0.677773, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.019216/  0.693006, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.018836/  0.665318, val:  83.75%, val_best:  84.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.017978/  0.682655, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.017460/  0.693883, val:  82.92%, val_best:  84.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.017389/  0.713840, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.016344/  0.703705, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.015846/  0.690073, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.015733/  0.688991, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.014940/  0.674133, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.014919/  0.702501, val:  82.92%, val_best:  85.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.014722/  0.680513, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.014203/  0.688146, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.013680/  0.695066, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.013319/  0.701979, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b850c6f2a6a471da1f171865986460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▂▄▅▅▅▅▆▅██▇██▆██▇█████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████▇█████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▆▆▅▆▅▆▆▅▆▆▅▅▅▅▅▅▅▅▆▅▅▆▆▅▆▅▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01332</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>0.70198</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-18</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q9jccg6g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q9jccg6g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_001636-q9jccg6g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9iey8ds1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_002827-9iey8ds1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9iey8ds1' target=\"_blank\">graceful-sweep-19</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9iey8ds1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9iey8ds1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.212888/  1.550227, val:  47.50%, val_best:  47.50%, tr:  14.63%, tr_best:  14.63%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.863692/  0.822730, val:  64.58%, val_best:  64.58%, tr:  62.49%, tr_best:  62.49%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.594032/  0.726027, val:  71.25%, val_best:  71.25%, tr:  72.79%, tr_best:  72.79%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.480924/  0.611924, val:  80.00%, val_best:  80.00%, tr:  78.92%, tr_best:  78.92%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.343217/  0.510550, val:  82.08%, val_best:  82.08%, tr:  86.90%, tr_best:  86.90%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.240666/  0.493132, val:  85.00%, val_best:  85.00%, tr:  90.67%, tr_best:  90.67%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.180179/  0.561431, val:  84.17%, val_best:  85.00%, tr:  93.58%, tr_best:  93.58%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.126411/  0.479499, val:  86.67%, val_best:  86.67%, tr:  95.42%, tr_best:  95.42%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.099649/  0.405717, val:  87.92%, val_best:  87.92%, tr:  96.91%, tr_best:  96.91%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.099874/  0.541641, val:  87.92%, val_best:  87.92%, tr:  96.44%, tr_best:  96.91%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.067338/  0.521280, val:  87.50%, val_best:  87.92%, tr:  97.84%, tr_best:  97.84%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.060343/  0.393681, val:  89.17%, val_best:  89.17%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.027776/  0.722884, val:  85.42%, val_best:  89.17%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.036712/  0.591728, val:  87.08%, val_best:  89.17%, tr:  98.74%, tr_best:  99.26%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.018036/  0.498672, val:  90.00%, val_best:  90.00%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.011055/  0.457591, val:  89.17%, val_best:  90.00%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.007854/  0.487191, val:  89.17%, val_best:  90.00%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.004766/  0.473161, val:  90.00%, val_best:  90.00%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.003183/  0.491462, val:  90.42%, val_best:  90.42%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.002560/  0.504951, val:  89.17%, val_best:  90.42%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001861/  0.501797, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001666/  0.497783, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001374/  0.502224, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.001177/  0.496483, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.001119/  0.499728, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000998/  0.508125, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000882/  0.487050, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000828/  0.498473, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000772/  0.491784, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000750/  0.488011, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000668/  0.494335, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000651/  0.495666, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000600/  0.494008, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000579/  0.499207, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000551/  0.499366, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000535/  0.501518, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000510/  0.504910, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000500/  0.508775, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000482/  0.516555, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000460/  0.517028, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000446/  0.508581, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000430/  0.502377, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000415/  0.497249, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000404/  0.503429, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000389/  0.497937, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000394/  0.505759, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000376/  0.509157, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000370/  0.510680, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000352/  0.495390, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000343/  0.499319, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000336/  0.502723, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000327/  0.500840, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000318/  0.499977, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000308/  0.505543, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000304/  0.497531, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000294/  0.509585, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000296/  0.494125, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000286/  0.506746, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000282/  0.504156, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000271/  0.515959, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000265/  0.512896, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000261/  0.511528, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000261/  0.512500, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000251/  0.516205, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000244/  0.516595, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000240/  0.527267, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000233/  0.531680, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000229/  0.530671, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000222/  0.534246, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000219/  0.531994, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000221/  0.530033, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000216/  0.535528, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000214/  0.525075, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000208/  0.534516, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000213/  0.525777, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000200/  0.531677, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000198/  0.527204, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000197/  0.527867, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000197/  0.529828, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000191/  0.535394, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000188/  0.530425, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000189/  0.541051, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000183/  0.538176, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000180/  0.542602, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000175/  0.544168, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000178/  0.547992, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000172/  0.549805, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000172/  0.548631, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000171/  0.548449, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000167/  0.543855, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000165/  0.542818, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000163/  0.546298, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000160/  0.544559, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000161/  0.550800, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000159/  0.544305, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000156/  0.545949, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000153/  0.549730, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000151/  0.551638, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000152/  0.550430, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000146/  0.547260, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5311403ded541f1ae12eac1bfb4829a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▇█▇▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▆▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▅▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00015</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.54726</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-19</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9iey8ds1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9iey8ds1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_002827-9iey8ds1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aiklbiep with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_003950-aiklbiep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aiklbiep' target=\"_blank\">valiant-sweep-20</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aiklbiep' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aiklbiep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.003803/  0.952379, val:  62.50%, val_best:  62.50%, tr:  57.35%, tr_best:  57.35%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.614073/  0.917597, val:  59.17%, val_best:  62.50%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.481302/  0.785513, val:  69.17%, val_best:  69.17%, tr:  78.07%, tr_best:  78.07%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.410185/  0.781723, val:  76.67%, val_best:  76.67%, tr:  82.66%, tr_best:  82.66%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.293548/  0.716446, val:  77.92%, val_best:  77.92%, tr:  88.82%, tr_best:  88.82%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.218316/  0.585268, val:  85.83%, val_best:  85.83%, tr:  91.84%, tr_best:  91.84%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.164681/  0.719784, val:  81.25%, val_best:  85.83%, tr:  93.91%, tr_best:  93.91%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.134989/  0.842692, val:  75.83%, val_best:  85.83%, tr:  95.42%, tr_best:  95.42%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.127835/  0.607878, val:  81.67%, val_best:  85.83%, tr:  95.31%, tr_best:  95.42%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.066715/  0.723736, val:  84.17%, val_best:  85.83%, tr:  97.68%, tr_best:  97.68%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.078710/  0.854980, val:  82.50%, val_best:  85.83%, tr:  97.18%, tr_best:  97.68%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.045099/  0.716950, val:  86.25%, val_best:  86.25%, tr:  98.49%, tr_best:  98.49%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.062954/  0.843977, val:  82.92%, val_best:  86.25%, tr:  97.93%, tr_best:  98.49%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.035194/  0.728267, val:  85.42%, val_best:  86.25%, tr:  98.90%, tr_best:  98.90%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.027569/  0.685216, val:  87.08%, val_best:  87.08%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.017140/  0.669296, val:  84.58%, val_best:  87.08%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.015873/  0.686598, val:  86.67%, val_best:  87.08%, tr:  99.50%, tr_best:  99.50%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.006190/  0.708347, val:  85.42%, val_best:  87.08%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.002521/  0.699988, val:  85.83%, val_best:  87.08%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001307/  0.733316, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001112/  0.744146, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000830/  0.728805, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000743/  0.747063, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000670/  0.745048, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000599/  0.752652, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000553/  0.753763, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000516/  0.748726, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000479/  0.752467, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000467/  0.751830, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000433/  0.760407, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000418/  0.766179, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000399/  0.765443, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000368/  0.764809, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000352/  0.768509, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000335/  0.767192, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000320/  0.763866, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000310/  0.770725, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000298/  0.775423, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000290/  0.773633, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000279/  0.780664, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000272/  0.785801, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000259/  0.796478, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000255/  0.792141, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000243/  0.789158, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000238/  0.784647, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000229/  0.787700, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000221/  0.785847, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000218/  0.797250, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000209/  0.799652, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000202/  0.798745, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000203/  0.792663, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000197/  0.799738, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000185/  0.790642, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000186/  0.790717, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.801270, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000174/  0.804110, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000173/  0.797576, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000168/  0.800349, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.800862, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.812970, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000157/  0.811124, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000153/  0.812547, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000152/  0.816181, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000146/  0.818139, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000145/  0.810910, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000144/  0.815354, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000141/  0.809694, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000138/  0.817306, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000138/  0.821746, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000134/  0.818362, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000132/  0.827592, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000130/  0.824344, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000128/  0.826003, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000125/  0.820239, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000123/  0.815924, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000122/  0.822561, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.825653, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.823705, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000117/  0.823103, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000116/  0.820919, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000113/  0.820598, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000112/  0.830585, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000110/  0.834032, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000109/  0.835511, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000107/  0.835891, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.836539, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000105/  0.833315, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000103/  0.833488, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000101/  0.839081, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000101/  0.839468, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.839886, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000098/  0.839717, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000096/  0.838428, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.840105, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000093/  0.842812, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000093/  0.842723, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000091/  0.843141, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000090/  0.842599, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.843953, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000087/  0.844940, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b3fa01fb0e48adaed1d2501a06f8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆▇█▇██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>0.84494</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-sweep-20</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aiklbiep' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/aiklbiep</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_003950-aiklbiep/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x8as39dd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_005137-x8as39dd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8as39dd' target=\"_blank\">wobbly-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8as39dd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8as39dd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.133061/  1.572093, val:  35.42%, val_best:  35.42%, tr:  18.10%, tr_best:  18.10%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.994892/  0.898520, val:  61.25%, val_best:  61.25%, tr:  59.92%, tr_best:  59.92%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.690355/  0.741596, val:  67.92%, val_best:  67.92%, tr:  67.99%, tr_best:  67.99%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.586430/  0.738136, val:  68.33%, val_best:  68.33%, tr:  72.27%, tr_best:  72.27%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.520379/  0.646826, val:  72.50%, val_best:  72.50%, tr:  75.16%, tr_best:  75.16%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.460102/  0.636416, val:  75.00%, val_best:  75.00%, tr:  79.01%, tr_best:  79.01%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.410600/  0.613945, val:  78.33%, val_best:  78.33%, tr:  82.17%, tr_best:  82.17%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.343908/  0.606031, val:  77.50%, val_best:  78.33%, tr:  86.65%, tr_best:  86.65%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.293210/  0.561435, val:  82.50%, val_best:  82.50%, tr:  89.11%, tr_best:  89.11%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.238628/  0.537054, val:  84.58%, val_best:  84.58%, tr:  91.25%, tr_best:  91.25%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.179344/  0.515396, val:  87.08%, val_best:  87.08%, tr:  94.75%, tr_best:  94.75%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.166956/  0.533581, val:  84.58%, val_best:  87.08%, tr:  94.48%, tr_best:  94.75%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.133927/  0.514630, val:  86.25%, val_best:  87.08%, tr:  95.67%, tr_best:  95.67%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.105890/  0.463130, val:  87.50%, val_best:  87.50%, tr:  97.09%, tr_best:  97.09%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.087142/  0.483121, val:  87.50%, val_best:  87.50%, tr:  97.68%, tr_best:  97.68%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.068438/  0.462520, val:  90.00%, val_best:  90.00%, tr:  98.56%, tr_best:  98.56%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.064257/  0.471357, val:  88.75%, val_best:  90.00%, tr:  98.40%, tr_best:  98.56%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.053538/  0.416564, val:  89.17%, val_best:  90.00%, tr:  98.76%, tr_best:  98.76%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.042981/  0.441747, val:  89.58%, val_best:  90.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.036774/  0.513146, val:  87.50%, val_best:  90.00%, tr:  99.12%, tr_best:  99.28%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.032402/  0.492519, val:  87.50%, val_best:  90.00%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.025460/  0.496162, val:  87.50%, val_best:  90.00%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.026175/  0.535341, val:  87.08%, val_best:  90.00%, tr:  99.57%, tr_best:  99.66%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.026208/  0.506933, val:  87.50%, val_best:  90.00%, tr:  99.46%, tr_best:  99.66%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.017519/  0.504978, val:  89.17%, val_best:  90.00%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.016604/  0.505860, val:  87.08%, val_best:  90.00%, tr:  99.73%, tr_best:  99.84%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.015235/  0.502722, val:  87.92%, val_best:  90.00%, tr:  99.77%, tr_best:  99.84%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.011652/  0.521334, val:  87.92%, val_best:  90.00%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.011123/  0.476851, val:  87.92%, val_best:  90.00%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.009702/  0.489998, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.008842/  0.512524, val:  87.92%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.007995/  0.541641, val:  87.08%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.009271/  0.525698, val:  88.75%, val_best:  90.00%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.006790/  0.531094, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.006742/  0.548581, val:  88.75%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.006563/  0.534596, val:  89.17%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.005738/  0.559087, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.005139/  0.511999, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.005628/  0.537192, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.004800/  0.557081, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.005182/  0.564539, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.004341/  0.547831, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.004135/  0.545668, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.003751/  0.572056, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.003685/  0.557938, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.003725/  0.563747, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.003556/  0.556637, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.003530/  0.566187, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.003303/  0.566625, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.003236/  0.580766, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.003031/  0.577167, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002933/  0.581633, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002907/  0.576391, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002867/  0.594080, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.002640/  0.599215, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.002582/  0.588750, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002487/  0.602304, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.002513/  0.595928, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.002340/  0.596591, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.002373/  0.592715, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.002269/  0.591350, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.002116/  0.601211, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.002141/  0.595552, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002037/  0.588750, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.002044/  0.588574, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001973/  0.598116, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001976/  0.599341, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001907/  0.613283, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001797/  0.618377, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001761/  0.601886, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001689/  0.606172, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001742/  0.613927, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001608/  0.622408, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001623/  0.615367, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001579/  0.608969, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001564/  0.611708, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001490/  0.620499, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001478/  0.626577, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001491/  0.613839, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001461/  0.625139, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001421/  0.612088, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001394/  0.625127, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001391/  0.617477, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001355/  0.619678, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001340/  0.612963, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001306/  0.617118, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001293/  0.615445, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001278/  0.614223, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001219/  0.622244, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001215/  0.619299, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001212/  0.615973, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001165/  0.631671, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001160/  0.621385, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001168/  0.627310, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001137/  0.620799, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001105/  0.634188, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001109/  0.635280, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001103/  0.642691, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001071/  0.642570, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001044/  0.633255, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076b410c3d3044009deac2a07e00d455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▆███▇█▇▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00104</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>0.63326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8as39dd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x8as39dd</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_005137-x8as39dd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8h8v9mts with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_010300-8h8v9mts</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8h8v9mts' target=\"_blank\">zesty-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8h8v9mts' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8h8v9mts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.301286/  2.303460, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.300988/  2.303480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.300646/  2.303511, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.300203/  2.303545, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.299970/  2.303592, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.299778/  2.303646, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.299497/  2.303704, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.299424/  2.303769, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.299265/  2.303828, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.298968/  2.303891, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.298871/  2.303962, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.298592/  2.304030, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.298459/  2.304115, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.298379/  2.304188, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.298295/  2.304265, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.298030/  2.304340, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.298017/  2.304422, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.297553/  2.304496, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.297601/  2.304589, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.297451/  2.304670, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.297352/  2.304762, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.297352/  2.304854, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.297287/  2.304944, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.297099/  2.305020, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.297077/  2.305111, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  2.296936/  2.305189, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  2.296890/  2.305276, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  2.296940/  2.305352, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  2.296811/  2.305445, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  2.296637/  2.305523, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  2.296700/  2.305607, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  2.296727/  2.305690, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  2.296513/  2.305765, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  2.296525/  2.305847, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  2.296349/  2.305921, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  2.296649/  2.305994, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  2.296420/  2.306073, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  2.296336/  2.306145, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  2.296126/  2.306217, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  2.296369/  2.306292, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  2.296394/  2.306368, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  2.296393/  2.306446, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  2.296189/  2.306497, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  2.296202/  2.306564, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  2.295956/  2.306634, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  2.296020/  2.306701, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  2.296277/  2.306759, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  2.296092/  2.306827, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  2.296033/  2.306889, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  2.296098/  2.306953, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  2.296002/  2.307009, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  2.295949/  2.307062, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  2.295844/  2.307120, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  2.295670/  2.307187, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  2.295594/  2.307243, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  2.295657/  2.307309, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  2.295711/  2.307380, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  2.295710/  2.307413, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  2.295845/  2.307484, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  2.295861/  2.307544, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  2.295661/  2.307580, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  2.295970/  2.307635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  2.295709/  2.307679, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  2.295809/  2.307726, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  2.295975/  2.307768, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  2.295654/  2.307801, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  2.295733/  2.307851, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  2.295884/  2.307886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  2.295734/  2.307938, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  2.295697/  2.307965, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  2.295595/  2.308010, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  2.295741/  2.308047, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  2.295842/  2.308087, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  2.295578/  2.308119, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  2.295573/  2.308160, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  2.295596/  2.308207, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  2.295764/  2.308246, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  2.295714/  2.308275, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  2.295440/  2.308289, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  2.295578/  2.308338, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  2.295703/  2.308366, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  2.295931/  2.308404, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  2.295539/  2.308422, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  2.295487/  2.308453, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  2.295888/  2.308490, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  2.295693/  2.308505, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  2.295612/  2.308537, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  2.295655/  2.308565, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  2.295583/  2.308585, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  2.295536/  2.308618, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  2.295739/  2.308622, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  2.295893/  2.308677, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  2.295655/  2.308703, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  2.295644/  2.308644, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  2.295861/  2.308699, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  2.295928/  2.308697, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  2.295592/  2.308601, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  2.295272/  2.308514, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  2.295302/  2.308587, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  2.294880/  2.308234, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb41302d8a4f58b692efe4a46915e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃██▆▃▅▅█▅▁▅▁▆▃▆▁▃▆▃█▃█▅▅█▃█▆▃▅▃▅▁▃▃▅▃▃▃</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.12647</td></tr><tr><td>tr_epoch_loss</td><td>2.29488</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.30823</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8h8v9mts' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8h8v9mts</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_010300-8h8v9mts/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: igmj65dw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_011415-igmj65dw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/igmj65dw' target=\"_blank\">smooth-sweep-23</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/igmj65dw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/igmj65dw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.933479/  0.985820, val:  58.75%, val_best:  58.75%, tr:  59.27%, tr_best:  59.27%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.627707/  0.797846, val:  62.50%, val_best:  62.50%, tr:  72.02%, tr_best:  72.02%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.456233/  0.720916, val:  73.75%, val_best:  73.75%, tr:  80.14%, tr_best:  80.14%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.363727/  0.626425, val:  82.92%, val_best:  82.92%, tr:  85.53%, tr_best:  85.53%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.220737/  0.575404, val:  82.92%, val_best:  82.92%, tr:  92.40%, tr_best:  92.40%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.168834/  0.511101, val:  85.00%, val_best:  85.00%, tr:  94.03%, tr_best:  94.03%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.156123/  0.562982, val:  85.83%, val_best:  85.83%, tr:  94.34%, tr_best:  94.34%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.107443/  0.591157, val:  87.50%, val_best:  87.50%, tr:  96.33%, tr_best:  96.33%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.085352/  0.580956, val:  87.50%, val_best:  87.50%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.066232/  0.580653, val:  83.75%, val_best:  87.50%, tr:  97.79%, tr_best:  97.79%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.061717/  0.581183, val:  82.92%, val_best:  87.50%, tr:  98.04%, tr_best:  98.04%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.030976/  0.519453, val:  87.92%, val_best:  87.92%, tr:  99.17%, tr_best:  99.17%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.017432/  0.637827, val:  87.50%, val_best:  87.92%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.024602/  0.555964, val:  87.92%, val_best:  87.92%, tr:  99.37%, tr_best:  99.53%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.013355/  0.543016, val:  89.17%, val_best:  89.17%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.006090/  0.574279, val:  89.17%, val_best:  89.17%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.003286/  0.544430, val:  89.17%, val_best:  89.17%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001901/  0.582992, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001195/  0.573460, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001043/  0.589401, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001023/  0.590079, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000813/  0.578408, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000708/  0.578760, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000652/  0.579283, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000604/  0.565558, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000569/  0.568200, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000539/  0.565055, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000512/  0.564618, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000486/  0.568315, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000458/  0.572440, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000442/  0.572322, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000421/  0.572771, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000401/  0.579117, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000387/  0.575671, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000372/  0.584486, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000354/  0.591487, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000347/  0.586944, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000334/  0.587008, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000326/  0.587502, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000314/  0.587270, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000303/  0.583007, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000295/  0.581247, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000286/  0.582119, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000277/  0.586454, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000270/  0.583617, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000264/  0.593016, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000255/  0.593887, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000248/  0.594120, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000246/  0.591770, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000237/  0.592656, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000234/  0.595968, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000226/  0.596717, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000220/  0.592455, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000213/  0.591295, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000208/  0.599408, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000207/  0.594280, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000200/  0.599355, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000194/  0.607146, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000192/  0.603537, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000188/  0.598881, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000186/  0.601798, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000182/  0.608395, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000181/  0.612492, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000177/  0.607245, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000173/  0.609380, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000170/  0.610260, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000166/  0.613123, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000164/  0.610734, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000162/  0.610821, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000159/  0.609360, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000155/  0.612485, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000153/  0.611960, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000150/  0.615293, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000149/  0.616788, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000145/  0.618384, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000142/  0.623620, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000139/  0.618607, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000138/  0.619710, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000137/  0.623447, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000134/  0.625537, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000135/  0.627234, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000130/  0.631996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000129/  0.628590, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000127/  0.632970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633291, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000123/  0.630818, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000122/  0.630688, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631291, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000118/  0.631953, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000117/  0.631131, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000116/  0.630936, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629977, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000112/  0.627767, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000110/  0.631717, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000110/  0.635952, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000109/  0.636434, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000107/  0.633812, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000106/  0.635118, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000106/  0.636634, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000104/  0.629479, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff86fd2243642a8ba3de5b7178d7335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▃█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.62948</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-23</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/igmj65dw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/igmj65dw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_011415-igmj65dw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gg2r6foq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_012642-gg2r6foq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gg2r6foq' target=\"_blank\">solar-sweep-24</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gg2r6foq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gg2r6foq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.300406/  2.303886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.298519/  2.304635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.297336/  2.305443, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.296554/  2.306226, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.296175/  2.306992, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.296047/  2.307540, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.295772/  2.308033, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.295895/  2.308480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.296216/  2.308680, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.295835/  2.308558, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.292248/  2.299587, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.259738/  2.241203, val:  17.50%, val_best:  17.50%, tr:  16.66%, tr_best:  16.66%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.134043/  2.098728, val:  17.50%, val_best:  17.50%, tr:  21.19%, tr_best:  21.19%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.935410/  1.927308, val:  23.75%, val_best:  23.75%, tr:  25.92%, tr_best:  25.92%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.753391/  1.782625, val:  31.25%, val_best:  31.25%, tr:  30.66%, tr_best:  30.66%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.613383/  1.651384, val:  32.92%, val_best:  32.92%, tr:  36.11%, tr_best:  36.11%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.463344/  1.489389, val:  44.17%, val_best:  44.17%, tr:  45.96%, tr_best:  45.96%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.311142/  1.350466, val:  46.25%, val_best:  46.25%, tr:  50.25%, tr_best:  50.25%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.190231/  1.244449, val:  50.83%, val_best:  50.83%, tr:  54.15%, tr_best:  54.15%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.096903/  1.172999, val:  55.00%, val_best:  55.00%, tr:  58.23%, tr_best:  58.23%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.022245/  1.092800, val:  58.33%, val_best:  58.33%, tr:  61.83%, tr_best:  61.83%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.954882/  1.040579, val:  59.58%, val_best:  59.58%, tr:  63.48%, tr_best:  63.48%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.898351/  0.971588, val:  64.17%, val_best:  64.17%, tr:  64.36%, tr_best:  64.36%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.851355/  0.948447, val:  62.50%, val_best:  64.17%, tr:  65.49%, tr_best:  65.49%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.812370/  0.906983, val:  63.33%, val_best:  64.17%, tr:  66.50%, tr_best:  66.50%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.781353/  0.876057, val:  67.08%, val_best:  67.08%, tr:  67.49%, tr_best:  67.49%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.751902/  0.862858, val:  65.42%, val_best:  67.08%, tr:  68.91%, tr_best:  68.91%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.730613/  0.849067, val:  65.42%, val_best:  67.08%, tr:  68.71%, tr_best:  68.91%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.708609/  0.817031, val:  65.42%, val_best:  67.08%, tr:  70.00%, tr_best:  70.00%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.689571/  0.807394, val:  66.25%, val_best:  67.08%, tr:  69.79%, tr_best:  70.00%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.672644/  0.807275, val:  67.92%, val_best:  67.92%, tr:  70.33%, tr_best:  70.33%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.658875/  0.797922, val:  66.67%, val_best:  67.92%, tr:  70.29%, tr_best:  70.33%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.640290/  0.784343, val:  69.17%, val_best:  69.17%, tr:  71.44%, tr_best:  71.44%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.623705/  0.771243, val:  68.33%, val_best:  69.17%, tr:  71.78%, tr_best:  71.78%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.610352/  0.768973, val:  69.17%, val_best:  69.17%, tr:  72.18%, tr_best:  72.18%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.596139/  0.744822, val:  68.75%, val_best:  69.17%, tr:  72.97%, tr_best:  72.97%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.582226/  0.756960, val:  69.58%, val_best:  69.58%, tr:  73.35%, tr_best:  73.35%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.573374/  0.736409, val:  70.83%, val_best:  70.83%, tr:  73.67%, tr_best:  73.67%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.561425/  0.736700, val:  68.75%, val_best:  70.83%, tr:  74.59%, tr_best:  74.59%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.550453/  0.727427, val:  70.00%, val_best:  70.83%, tr:  74.41%, tr_best:  74.59%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.541691/  0.720331, val:  69.17%, val_best:  70.83%, tr:  75.09%, tr_best:  75.09%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.530663/  0.716629, val:  68.75%, val_best:  70.83%, tr:  74.89%, tr_best:  75.09%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.521699/  0.715798, val:  69.58%, val_best:  70.83%, tr:  75.52%, tr_best:  75.52%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.513468/  0.711819, val:  69.58%, val_best:  70.83%, tr:  75.79%, tr_best:  75.79%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.505777/  0.704904, val:  68.75%, val_best:  70.83%, tr:  75.56%, tr_best:  75.79%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.497525/  0.711123, val:  69.17%, val_best:  70.83%, tr:  76.49%, tr_best:  76.49%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.490341/  0.706909, val:  70.00%, val_best:  70.83%, tr:  76.69%, tr_best:  76.69%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.482123/  0.704659, val:  70.00%, val_best:  70.83%, tr:  77.57%, tr_best:  77.57%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.476508/  0.706241, val:  70.00%, val_best:  70.83%, tr:  76.98%, tr_best:  77.57%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.469946/  0.703348, val:  70.00%, val_best:  70.83%, tr:  78.43%, tr_best:  78.43%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.466646/  0.709340, val:  68.75%, val_best:  70.83%, tr:  77.89%, tr_best:  78.43%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.455909/  0.717476, val:  70.00%, val_best:  70.83%, tr:  78.90%, tr_best:  78.90%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.453923/  0.706779, val:  68.75%, val_best:  70.83%, tr:  78.94%, tr_best:  78.94%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.447306/  0.688155, val:  69.58%, val_best:  70.83%, tr:  78.72%, tr_best:  78.94%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.439791/  0.725893, val:  70.83%, val_best:  70.83%, tr:  79.33%, tr_best:  79.33%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.437070/  0.694754, val:  72.50%, val_best:  72.50%, tr:  79.35%, tr_best:  79.35%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.428534/  0.699913, val:  69.58%, val_best:  72.50%, tr:  80.09%, tr_best:  80.09%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.422546/  0.694701, val:  69.58%, val_best:  72.50%, tr:  80.61%, tr_best:  80.61%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.418563/  0.696214, val:  71.67%, val_best:  72.50%, tr:  80.61%, tr_best:  80.61%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.412128/  0.683443, val:  71.25%, val_best:  72.50%, tr:  81.27%, tr_best:  81.27%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.407199/  0.715782, val:  71.25%, val_best:  72.50%, tr:  81.90%, tr_best:  81.90%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.404454/  0.687033, val:  70.83%, val_best:  72.50%, tr:  81.20%, tr_best:  81.90%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.400846/  0.676477, val:  72.08%, val_best:  72.50%, tr:  82.44%, tr_best:  82.44%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.391769/  0.696986, val:  71.67%, val_best:  72.50%, tr:  82.62%, tr_best:  82.62%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.387029/  0.668894, val:  72.08%, val_best:  72.50%, tr:  83.27%, tr_best:  83.27%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.383265/  0.681759, val:  71.25%, val_best:  72.50%, tr:  83.54%, tr_best:  83.54%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.377071/  0.690346, val:  72.08%, val_best:  72.50%, tr:  84.56%, tr_best:  84.56%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.372379/  0.682006, val:  73.75%, val_best:  73.75%, tr:  84.11%, tr_best:  84.56%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.368997/  0.677795, val:  74.17%, val_best:  74.17%, tr:  84.40%, tr_best:  84.56%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.362577/  0.675875, val:  75.00%, val_best:  75.00%, tr:  84.87%, tr_best:  84.87%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.357425/  0.690530, val:  73.33%, val_best:  75.00%, tr:  85.53%, tr_best:  85.53%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.354881/  0.674782, val:  74.17%, val_best:  75.00%, tr:  85.37%, tr_best:  85.53%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.347969/  0.685782, val:  75.42%, val_best:  75.42%, tr:  85.91%, tr_best:  85.91%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.344551/  0.677688, val:  74.17%, val_best:  75.42%, tr:  85.73%, tr_best:  85.91%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.340217/  0.679886, val:  73.75%, val_best:  75.42%, tr:  86.83%, tr_best:  86.83%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.333419/  0.682752, val:  72.92%, val_best:  75.42%, tr:  86.38%, tr_best:  86.83%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.329340/  0.675858, val:  72.08%, val_best:  75.42%, tr:  87.51%, tr_best:  87.51%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.326220/  0.680640, val:  75.42%, val_best:  75.42%, tr:  87.15%, tr_best:  87.51%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.322076/  0.677727, val:  75.00%, val_best:  75.42%, tr:  87.80%, tr_best:  87.80%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.321268/  0.673672, val:  75.42%, val_best:  75.42%, tr:  86.74%, tr_best:  87.80%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.313937/  0.692592, val:  75.00%, val_best:  75.42%, tr:  88.14%, tr_best:  88.14%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.306333/  0.680277, val:  74.58%, val_best:  75.42%, tr:  88.89%, tr_best:  88.89%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.302513/  0.669147, val:  74.58%, val_best:  75.42%, tr:  89.09%, tr_best:  89.09%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.297025/  0.697064, val:  74.58%, val_best:  75.42%, tr:  89.63%, tr_best:  89.63%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.292874/  0.692987, val:  74.17%, val_best:  75.42%, tr:  89.02%, tr_best:  89.63%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.285865/  0.682365, val:  75.00%, val_best:  75.42%, tr:  89.90%, tr_best:  89.90%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.283854/  0.683342, val:  75.42%, val_best:  75.42%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.278780/  0.691694, val:  74.17%, val_best:  75.42%, tr:  90.22%, tr_best:  90.22%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.273376/  0.677667, val:  75.83%, val_best:  75.83%, tr:  90.33%, tr_best:  90.33%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.268427/  0.692713, val:  76.25%, val_best:  76.25%, tr:  90.96%, tr_best:  90.96%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.262703/  0.694457, val:  74.17%, val_best:  76.25%, tr:  91.16%, tr_best:  91.16%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.257254/  0.712384, val:  74.17%, val_best:  76.25%, tr:  91.16%, tr_best:  91.16%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.253919/  0.679916, val:  75.42%, val_best:  76.25%, tr:  91.37%, tr_best:  91.37%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.250518/  0.671020, val:  77.08%, val_best:  77.08%, tr:  91.77%, tr_best:  91.77%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.241748/  0.672831, val:  76.25%, val_best:  77.08%, tr:  92.20%, tr_best:  92.20%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.236634/  0.683049, val:  75.83%, val_best:  77.08%, tr:  92.79%, tr_best:  92.79%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.231997/  0.660204, val:  77.08%, val_best:  77.08%, tr:  92.90%, tr_best:  92.90%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.227299/  0.671540, val:  77.08%, val_best:  77.08%, tr:  93.17%, tr_best:  93.17%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.221612/  0.691966, val:  76.67%, val_best:  77.08%, tr:  93.49%, tr_best:  93.49%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.216945/  0.680723, val:  77.08%, val_best:  77.08%, tr:  93.73%, tr_best:  93.73%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df0e3ff91504cb3bb2ab125717a7f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▂▂▂▁▃▄▆▅▄▇▅▆▇▇▇▆▅█▆▆█▆▆█▇█▇▇██▇████▇▇█</td></tr><tr><td>summary_val_acc</td><td>▁▂▂▂▂▃▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████████████</td></tr><tr><td>tr_acc</td><td>▁▂▂▂▂▂▃▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█████▆▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_acc_best</td><td>▁▂▂▂▂▃▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▂▂▂▃▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████████████████</td></tr><tr><td>val_loss</td><td>▁█████▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.93733</td></tr><tr><td>tr_epoch_loss</td><td>0.21695</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.77083</td></tr><tr><td>val_loss</td><td>0.68072</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-24</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gg2r6foq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gg2r6foq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_012642-gg2r6foq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tncoqnji with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_013815-tncoqnji</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tncoqnji' target=\"_blank\">atomic-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tncoqnji' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tncoqnji</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=1, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=1, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.218877/  1.102229, val:  51.67%, val_best:  51.67%, tr:  48.04%, tr_best:  48.04%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.693676/  0.982527, val:  58.33%, val_best:  58.33%, tr:  67.83%, tr_best:  67.83%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.540195/  0.847060, val:  65.00%, val_best:  65.00%, tr:  72.63%, tr_best:  72.63%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.498620/  0.864943, val:  70.83%, val_best:  70.83%, tr:  75.86%, tr_best:  75.86%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.410526/  0.982998, val:  68.33%, val_best:  70.83%, tr:  81.15%, tr_best:  81.15%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.381763/  0.763375, val:  73.33%, val_best:  73.33%, tr:  82.98%, tr_best:  82.98%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.328819/  0.768524, val:  77.50%, val_best:  77.50%, tr:  86.29%, tr_best:  86.29%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.316456/  0.925701, val:  73.75%, val_best:  77.50%, tr:  87.35%, tr_best:  87.35%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.304837/  0.787014, val:  74.58%, val_best:  77.50%, tr:  87.26%, tr_best:  87.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.269082/  0.849336, val:  77.08%, val_best:  77.50%, tr:  89.77%, tr_best:  89.77%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.225634/  0.736476, val:  78.75%, val_best:  78.75%, tr:  91.23%, tr_best:  91.23%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.189364/  0.910911, val:  76.25%, val_best:  78.75%, tr:  92.97%, tr_best:  92.97%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.191461/  0.833343, val:  80.00%, val_best:  80.00%, tr:  92.81%, tr_best:  92.97%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.180759/  0.830255, val:  82.50%, val_best:  82.50%, tr:  92.74%, tr_best:  92.97%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.146683/  0.756636, val:  81.67%, val_best:  82.50%, tr:  94.52%, tr_best:  94.52%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.144236/  0.738811, val:  80.42%, val_best:  82.50%, tr:  94.66%, tr_best:  94.66%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.147098/  0.870545, val:  80.00%, val_best:  82.50%, tr:  94.59%, tr_best:  94.66%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.141926/  0.944244, val:  76.25%, val_best:  82.50%, tr:  94.72%, tr_best:  94.72%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.168592/  0.869916, val:  78.33%, val_best:  82.50%, tr:  93.64%, tr_best:  94.72%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.143478/  0.938942, val:  78.33%, val_best:  82.50%, tr:  94.95%, tr_best:  94.95%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.150232/  0.678955, val:  82.50%, val_best:  82.50%, tr:  94.36%, tr_best:  94.95%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.125350/  0.957898, val:  82.08%, val_best:  82.50%, tr:  95.24%, tr_best:  95.24%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.076121/  0.957029, val:  80.00%, val_best:  82.50%, tr:  97.48%, tr_best:  97.48%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.105970/  1.076651, val:  80.83%, val_best:  82.50%, tr:  96.19%, tr_best:  97.48%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.101563/  0.946289, val:  82.08%, val_best:  82.50%, tr:  96.42%, tr_best:  97.48%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.107968/  0.742962, val:  82.50%, val_best:  82.50%, tr:  96.17%, tr_best:  97.48%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.100048/  1.035256, val:  81.67%, val_best:  82.50%, tr:  96.39%, tr_best:  97.48%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.065186/  0.766008, val:  82.08%, val_best:  82.50%, tr:  97.32%, tr_best:  97.48%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.095815/  0.848980, val:  79.17%, val_best:  82.50%, tr:  96.69%, tr_best:  97.48%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.088035/  0.816839, val:  86.67%, val_best:  86.67%, tr:  97.18%, tr_best:  97.48%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.069110/  1.103009, val:  80.42%, val_best:  86.67%, tr:  97.63%, tr_best:  97.63%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.106107/  1.073288, val:  81.67%, val_best:  86.67%, tr:  96.26%, tr_best:  97.63%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.073311/  1.021795, val:  80.83%, val_best:  86.67%, tr:  97.77%, tr_best:  97.77%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.054279/  0.873647, val:  83.75%, val_best:  86.67%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.082155/  1.024121, val:  82.08%, val_best:  86.67%, tr:  96.96%, tr_best:  98.29%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.104675/  0.946901, val:  82.92%, val_best:  86.67%, tr:  96.35%, tr_best:  98.29%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.061193/  0.850092, val:  83.33%, val_best:  86.67%, tr:  97.97%, tr_best:  98.29%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.084748/  0.861021, val:  82.92%, val_best:  86.67%, tr:  96.73%, tr_best:  98.29%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.036817/  1.130780, val:  80.83%, val_best:  86.67%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.048315/  1.130986, val:  82.50%, val_best:  86.67%, tr:  98.33%, tr_best:  98.81%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.078023/  0.861926, val:  84.17%, val_best:  86.67%, tr:  97.23%, tr_best:  98.81%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.051259/  0.779376, val:  85.42%, val_best:  86.67%, tr:  98.31%, tr_best:  98.81%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.025592/  0.914290, val:  83.75%, val_best:  86.67%, tr:  99.21%, tr_best:  99.21%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.085134/  0.903181, val:  82.50%, val_best:  86.67%, tr:  96.91%, tr_best:  99.21%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.066876/  0.984393, val:  80.42%, val_best:  86.67%, tr:  97.29%, tr_best:  99.21%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.058445/  0.951604, val:  82.08%, val_best:  86.67%, tr:  98.08%, tr_best:  99.21%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.048557/  1.135387, val:  78.75%, val_best:  86.67%, tr:  98.13%, tr_best:  99.21%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.055498/  1.075621, val:  80.83%, val_best:  86.67%, tr:  98.13%, tr_best:  99.21%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.044017/  1.015577, val:  81.67%, val_best:  86.67%, tr:  98.62%, tr_best:  99.21%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.032112/  1.026136, val:  82.92%, val_best:  86.67%, tr:  98.99%, tr_best:  99.21%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.047779/  1.021878, val:  84.17%, val_best:  86.67%, tr:  98.53%, tr_best:  99.21%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.031823/  1.096735, val:  84.17%, val_best:  86.67%, tr:  98.87%, tr_best:  99.21%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.043269/  1.124646, val:  80.00%, val_best:  86.67%, tr:  98.42%, tr_best:  99.21%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.024973/  1.044991, val:  81.67%, val_best:  86.67%, tr:  99.19%, tr_best:  99.21%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.019043/  1.037655, val:  82.92%, val_best:  86.67%, tr:  99.32%, tr_best:  99.32%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.017094/  0.989099, val:  83.33%, val_best:  86.67%, tr:  99.37%, tr_best:  99.37%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.010161/  0.940870, val:  83.33%, val_best:  86.67%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.022447/  1.168750, val:  82.50%, val_best:  86.67%, tr:  99.19%, tr_best:  99.71%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.028116/  1.061218, val:  81.25%, val_best:  86.67%, tr:  99.21%, tr_best:  99.71%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.025416/  1.111342, val:  82.08%, val_best:  86.67%, tr:  99.10%, tr_best:  99.71%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.027482/  1.158595, val:  82.50%, val_best:  86.67%, tr:  98.85%, tr_best:  99.71%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.039315/  0.994850, val:  83.75%, val_best:  86.67%, tr:  98.72%, tr_best:  99.71%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.029223/  1.028728, val:  84.58%, val_best:  86.67%, tr:  98.92%, tr_best:  99.71%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.014238/  1.259404, val:  82.08%, val_best:  86.67%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.003784/  1.134752, val:  85.00%, val_best:  86.67%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.002340/  1.109832, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.001347/  1.155173, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000938/  1.145199, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000791/  1.150091, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000743/  1.161909, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000628/  1.159411, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000586/  1.183559, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000544/  1.189181, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000489/  1.197729, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000484/  1.185899, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000441/  1.178912, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000432/  1.189415, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000404/  1.189385, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000389/  1.189553, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000385/  1.185051, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000374/  1.183969, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000351/  1.200949, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000342/  1.208400, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000330/  1.215636, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000316/  1.208341, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000305/  1.210116, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000297/  1.214976, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000287/  1.213247, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000280/  1.199357, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000279/  1.200863, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000265/  1.200441, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000268/  1.198552, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000257/  1.204600, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000252/  1.208547, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000245/  1.199082, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000235/  1.212234, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000231/  1.200084, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000230/  1.209319, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000219/  1.211552, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000213/  1.211473, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494d780fc98f4be68d833d3ff033808c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▅▅▆▅█▆█▆███▆█▆█▆▆████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇█▇▇▇███████████▇██████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇█▇▇▇███████████▇██████████████████</td></tr><tr><td>val_loss</td><td>▁▇▇▆▆▆▅▆▆▇▆▅▆▇▇▆█▆▆▇▇▇▇█▇▇▇█████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00021</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>1.21147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-25</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tncoqnji' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tncoqnji</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_013815-tncoqnji/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gii1gsyl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_015027-gii1gsyl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gii1gsyl' target=\"_blank\">morning-sweep-26</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gii1gsyl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gii1gsyl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.286242/  2.242540, val:  21.25%, val_best:  21.25%, tr:  13.14%, tr_best:  13.14%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.192747/  2.170043, val:  23.75%, val_best:  23.75%, tr:  22.95%, tr_best:  22.95%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.110503/  2.096137, val:  31.67%, val_best:  31.67%, tr:  29.62%, tr_best:  29.62%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.027504/  2.023771, val:  35.83%, val_best:  35.83%, tr:  35.80%, tr_best:  35.80%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.941505/  1.944862, val:  39.58%, val_best:  39.58%, tr:  40.24%, tr_best:  40.24%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.853047/  1.866410, val:  41.25%, val_best:  41.25%, tr:  44.05%, tr_best:  44.05%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.763003/  1.781982, val:  46.67%, val_best:  46.67%, tr:  46.96%, tr_best:  46.96%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.675020/  1.703870, val:  47.08%, val_best:  47.08%, tr:  49.41%, tr_best:  49.41%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.595124/  1.632989, val:  47.08%, val_best:  47.08%, tr:  50.88%, tr_best:  50.88%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.520569/  1.565606, val:  47.50%, val_best:  47.50%, tr:  51.96%, tr_best:  51.96%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.453751/  1.507957, val:  50.00%, val_best:  50.00%, tr:  54.31%, tr_best:  54.31%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.394136/  1.455008, val:  50.00%, val_best:  50.00%, tr:  55.41%, tr_best:  55.41%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.343765/  1.411840, val:  50.00%, val_best:  50.00%, tr:  56.22%, tr_best:  56.22%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.300159/  1.371429, val:  51.25%, val_best:  51.25%, tr:  57.42%, tr_best:  57.42%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.262204/  1.333819, val:  51.25%, val_best:  51.25%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.226637/  1.302415, val:  52.08%, val_best:  52.08%, tr:  59.67%, tr_best:  59.67%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.198013/  1.278175, val:  52.08%, val_best:  52.08%, tr:  59.76%, tr_best:  59.76%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.171586/  1.253644, val:  53.75%, val_best:  53.75%, tr:  60.28%, tr_best:  60.28%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.146845/  1.233075, val:  54.17%, val_best:  54.17%, tr:  60.89%, tr_best:  60.89%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.124510/  1.214545, val:  54.58%, val_best:  54.58%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.105764/  1.193747, val:  55.83%, val_best:  55.83%, tr:  61.16%, tr_best:  61.18%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.089121/  1.178902, val:  54.17%, val_best:  55.83%, tr:  61.32%, tr_best:  61.32%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.072536/  1.162246, val:  53.75%, val_best:  55.83%, tr:  61.72%, tr_best:  61.72%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.056567/  1.145129, val:  56.25%, val_best:  56.25%, tr:  62.06%, tr_best:  62.06%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.042846/  1.132994, val:  56.67%, val_best:  56.67%, tr:  62.15%, tr_best:  62.15%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.029370/  1.117821, val:  57.08%, val_best:  57.08%, tr:  62.44%, tr_best:  62.44%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.016563/  1.109409, val:  56.25%, val_best:  57.08%, tr:  63.28%, tr_best:  63.28%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.007205/  1.097440, val:  56.25%, val_best:  57.08%, tr:  63.07%, tr_best:  63.28%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  0.995326/  1.088722, val:  57.08%, val_best:  57.08%, tr:  62.87%, tr_best:  63.28%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  0.984711/  1.077488, val:  57.08%, val_best:  57.08%, tr:  63.84%, tr_best:  63.84%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  0.975045/  1.068169, val:  59.58%, val_best:  59.58%, tr:  63.62%, tr_best:  63.84%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.968015/  1.061373, val:  58.33%, val_best:  59.58%, tr:  64.27%, tr_best:  64.27%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.956640/  1.049312, val:  59.58%, val_best:  59.58%, tr:  64.72%, tr_best:  64.72%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.949501/  1.039002, val:  60.00%, val_best:  60.00%, tr:  64.11%, tr_best:  64.72%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.941531/  1.032144, val:  59.17%, val_best:  60.00%, tr:  64.59%, tr_best:  64.72%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.934312/  1.024217, val:  60.00%, val_best:  60.00%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.925767/  1.017859, val:  58.75%, val_best:  60.00%, tr:  64.97%, tr_best:  64.97%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.919308/  1.008679, val:  60.00%, val_best:  60.00%, tr:  65.28%, tr_best:  65.28%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.912811/  1.001812, val:  60.00%, val_best:  60.00%, tr:  65.06%, tr_best:  65.28%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.906844/  0.995525, val:  60.42%, val_best:  60.42%, tr:  65.17%, tr_best:  65.28%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.898907/  0.988767, val:  60.42%, val_best:  60.42%, tr:  64.97%, tr_best:  65.28%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.892753/  0.983883, val:  60.83%, val_best:  60.83%, tr:  65.28%, tr_best:  65.28%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.886560/  0.978895, val:  62.08%, val_best:  62.08%, tr:  65.46%, tr_best:  65.46%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.882149/  0.975866, val:  60.83%, val_best:  62.08%, tr:  65.78%, tr_best:  65.78%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.874505/  0.972406, val:  61.25%, val_best:  62.08%, tr:  65.40%, tr_best:  65.78%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.868868/  0.968920, val:  60.00%, val_best:  62.08%, tr:  66.01%, tr_best:  66.01%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.863002/  0.960258, val:  59.58%, val_best:  62.08%, tr:  65.98%, tr_best:  66.01%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.857667/  0.956326, val:  60.42%, val_best:  62.08%, tr:  66.37%, tr_best:  66.37%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.852012/  0.951116, val:  59.58%, val_best:  62.08%, tr:  66.28%, tr_best:  66.37%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.847328/  0.946164, val:  60.83%, val_best:  62.08%, tr:  66.52%, tr_best:  66.52%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.841450/  0.944280, val:  60.42%, val_best:  62.08%, tr:  66.52%, tr_best:  66.52%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.837234/  0.942976, val:  59.58%, val_best:  62.08%, tr:  66.77%, tr_best:  66.77%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.833602/  0.936742, val:  60.83%, val_best:  62.08%, tr:  66.68%, tr_best:  66.77%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.827730/  0.935651, val:  60.83%, val_best:  62.08%, tr:  67.02%, tr_best:  67.02%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.822670/  0.932352, val:  60.42%, val_best:  62.08%, tr:  67.20%, tr_best:  67.20%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.819506/  0.928681, val:  60.42%, val_best:  62.08%, tr:  67.40%, tr_best:  67.40%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.814505/  0.924105, val:  61.25%, val_best:  62.08%, tr:  67.52%, tr_best:  67.52%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.808155/  0.922730, val:  61.67%, val_best:  62.08%, tr:  67.56%, tr_best:  67.56%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.805225/  0.918291, val:  62.08%, val_best:  62.08%, tr:  67.54%, tr_best:  67.56%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.800349/  0.914843, val:  60.83%, val_best:  62.08%, tr:  67.74%, tr_best:  67.74%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.796749/  0.912050, val:  62.08%, val_best:  62.08%, tr:  67.76%, tr_best:  67.76%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.792029/  0.906981, val:  62.92%, val_best:  62.92%, tr:  68.19%, tr_best:  68.19%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.789814/  0.903972, val:  62.92%, val_best:  62.92%, tr:  67.70%, tr_best:  68.19%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.784387/  0.901880, val:  62.92%, val_best:  62.92%, tr:  68.28%, tr_best:  68.28%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.780283/  0.894764, val:  63.33%, val_best:  63.33%, tr:  68.28%, tr_best:  68.28%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.776252/  0.893454, val:  62.92%, val_best:  63.33%, tr:  68.30%, tr_best:  68.30%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.773332/  0.890088, val:  62.92%, val_best:  63.33%, tr:  68.82%, tr_best:  68.82%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.769662/  0.886531, val:  63.33%, val_best:  63.33%, tr:  68.62%, tr_best:  68.82%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.764918/  0.882028, val:  63.75%, val_best:  63.75%, tr:  68.58%, tr_best:  68.82%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.760904/  0.879694, val:  64.17%, val_best:  64.17%, tr:  68.94%, tr_best:  68.94%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.757746/  0.877402, val:  63.75%, val_best:  64.17%, tr:  69.09%, tr_best:  69.09%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.753314/  0.871791, val:  63.75%, val_best:  64.17%, tr:  69.03%, tr_best:  69.09%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.751532/  0.870691, val:  63.33%, val_best:  64.17%, tr:  69.97%, tr_best:  69.97%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.747873/  0.868897, val:  63.75%, val_best:  64.17%, tr:  69.52%, tr_best:  69.97%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.743219/  0.865075, val:  64.17%, val_best:  64.17%, tr:  69.57%, tr_best:  69.97%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.738506/  0.864250, val:  65.00%, val_best:  65.00%, tr:  69.39%, tr_best:  69.97%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.735592/  0.858830, val:  65.00%, val_best:  65.00%, tr:  69.84%, tr_best:  69.97%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.733189/  0.857021, val:  65.42%, val_best:  65.42%, tr:  69.86%, tr_best:  69.97%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.731560/  0.854344, val:  65.00%, val_best:  65.42%, tr:  70.54%, tr_best:  70.54%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.728136/  0.851265, val:  65.83%, val_best:  65.83%, tr:  69.72%, tr_best:  70.54%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.723456/  0.848961, val:  65.42%, val_best:  65.83%, tr:  70.06%, tr_best:  70.54%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.721228/  0.844310, val:  65.83%, val_best:  65.83%, tr:  70.11%, tr_best:  70.54%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.718029/  0.844247, val:  65.83%, val_best:  65.83%, tr:  70.13%, tr_best:  70.54%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.715505/  0.841797, val:  65.00%, val_best:  65.83%, tr:  70.60%, tr_best:  70.60%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.712665/  0.840948, val:  64.17%, val_best:  65.83%, tr:  70.42%, tr_best:  70.60%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.709807/  0.838049, val:  65.42%, val_best:  65.83%, tr:  70.56%, tr_best:  70.60%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.706153/  0.834777, val:  65.83%, val_best:  65.83%, tr:  70.76%, tr_best:  70.76%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.703267/  0.833086, val:  65.42%, val_best:  65.83%, tr:  71.10%, tr_best:  71.10%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.700330/  0.832342, val:  65.00%, val_best:  65.83%, tr:  70.99%, tr_best:  71.10%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.698327/  0.828054, val:  64.58%, val_best:  65.83%, tr:  71.03%, tr_best:  71.10%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.695300/  0.825857, val:  65.00%, val_best:  65.83%, tr:  71.35%, tr_best:  71.35%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.691106/  0.826071, val:  65.00%, val_best:  65.83%, tr:  71.75%, tr_best:  71.75%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.691445/  0.822780, val:  65.83%, val_best:  65.83%, tr:  71.64%, tr_best:  71.75%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.686778/  0.822144, val:  65.83%, val_best:  65.83%, tr:  71.55%, tr_best:  71.75%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.683600/  0.820489, val:  66.67%, val_best:  66.67%, tr:  72.16%, tr_best:  72.16%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.681556/  0.820112, val:  67.08%, val_best:  67.08%, tr:  72.27%, tr_best:  72.27%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.678230/  0.817194, val:  66.25%, val_best:  67.08%, tr:  72.34%, tr_best:  72.34%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.677378/  0.816607, val:  67.08%, val_best:  67.08%, tr:  71.60%, tr_best:  72.34%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.673594/  0.812594, val:  65.83%, val_best:  67.08%, tr:  72.27%, tr_best:  72.34%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.672431/  0.811035, val:  67.08%, val_best:  67.08%, tr:  72.68%, tr_best:  72.68%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa965ce6928a4e76886e2867c24d3293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▅▅▆▃▆▂▅▆▄▇▆▅▆▇▇▆▆▆▇▅▅▅▇▇▅██▇▅▅▇▅▅▅▇▇▄█</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>tr_acc</td><td>▁▃▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▇▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_acc_best</td><td>▁▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.5</td></tr><tr><td>tr_acc</td><td>0.72678</td></tr><tr><td>tr_epoch_loss</td><td>0.67243</td></tr><tr><td>val_acc_best</td><td>0.67083</td></tr><tr><td>val_acc_now</td><td>0.67083</td></tr><tr><td>val_loss</td><td>0.81103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-sweep-26</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gii1gsyl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gii1gsyl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_015027-gii1gsyl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rpghm5l6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_020250-rpghm5l6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rpghm5l6' target=\"_blank\">dauntless-sweep-27</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rpghm5l6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rpghm5l6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.016439/  0.802613, val:  63.75%, val_best:  63.75%, tr:  58.97%, tr_best:  58.97%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.619332/  0.784150, val:  65.42%, val_best:  65.42%, tr:  70.90%, tr_best:  70.90%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.512534/  0.673537, val:  71.25%, val_best:  71.25%, tr:  75.20%, tr_best:  75.20%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.443509/  0.645136, val:  76.67%, val_best:  76.67%, tr:  80.23%, tr_best:  80.23%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.360663/  0.671696, val:  70.42%, val_best:  76.67%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.300247/  0.542142, val:  86.25%, val_best:  86.25%, tr:  89.02%, tr_best:  89.02%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.226133/  0.582908, val:  83.33%, val_best:  86.25%, tr:  93.01%, tr_best:  93.01%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.172619/  0.519774, val:  83.75%, val_best:  86.25%, tr:  94.91%, tr_best:  94.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.146155/  0.525797, val:  83.75%, val_best:  86.25%, tr:  95.76%, tr_best:  95.76%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.111876/  0.476352, val:  87.08%, val_best:  87.08%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.088034/  0.439414, val:  88.75%, val_best:  88.75%, tr:  97.77%, tr_best:  97.77%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.070093/  0.559634, val:  86.25%, val_best:  88.75%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.064685/  0.470421, val:  87.50%, val_best:  88.75%, tr:  98.40%, tr_best:  98.40%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.048402/  0.492895, val:  88.75%, val_best:  88.75%, tr:  98.94%, tr_best:  98.94%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.043668/  0.424231, val:  88.33%, val_best:  88.75%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.027770/  0.467451, val:  87.08%, val_best:  88.75%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.028315/  0.471460, val:  89.58%, val_best:  89.58%, tr:  99.50%, tr_best:  99.66%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.019457/  0.497331, val:  88.33%, val_best:  89.58%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.017521/  0.450368, val:  89.58%, val_best:  89.58%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.013650/  0.527639, val:  88.33%, val_best:  89.58%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.013265/  0.471538, val:  87.92%, val_best:  89.58%, tr:  99.84%, tr_best:  99.93%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.019884/  0.511899, val:  86.25%, val_best:  89.58%, tr:  99.55%, tr_best:  99.93%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.010028/  0.509362, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.008058/  0.505728, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.006743/  0.495967, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.006245/  0.514665, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.006001/  0.516073, val:  88.75%, val_best:  89.58%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.005316/  0.504043, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.005397/  0.496925, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.004691/  0.506097, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.004348/  0.505311, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.004014/  0.516863, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.003949/  0.507538, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.003690/  0.500978, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.003391/  0.519317, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.003273/  0.500563, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.003066/  0.537043, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.002839/  0.517191, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.002827/  0.526958, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.002683/  0.528168, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.002581/  0.539579, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.002441/  0.525429, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.002340/  0.516720, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.002205/  0.532099, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.002182/  0.530995, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.002126/  0.532458, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.002071/  0.536809, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.002015/  0.532496, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.001904/  0.544901, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.001935/  0.536808, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.001843/  0.545675, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.001798/  0.547875, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.001724/  0.550182, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.001675/  0.543766, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001692/  0.554826, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001622/  0.558257, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.001555/  0.553739, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.001516/  0.552958, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001471/  0.538751, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001422/  0.548527, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001419/  0.570303, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001383/  0.557049, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001339/  0.558825, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001301/  0.554096, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001257/  0.545094, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001239/  0.556040, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001217/  0.551612, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001196/  0.553041, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001167/  0.551956, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001133/  0.548603, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001090/  0.553970, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001106/  0.560683, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001079/  0.562011, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001062/  0.554194, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001020/  0.549501, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001020/  0.557347, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000995/  0.556085, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000994/  0.560942, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000982/  0.552280, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000951/  0.555776, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000937/  0.560985, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000905/  0.558889, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000896/  0.555279, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000896/  0.554836, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000889/  0.552994, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000855/  0.555377, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000857/  0.551204, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000849/  0.563190, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000831/  0.557899, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000817/  0.564256, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000808/  0.556876, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000800/  0.552630, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000776/  0.558452, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000772/  0.551879, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000770/  0.557560, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000757/  0.563324, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000747/  0.562344, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000745/  0.556856, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000725/  0.557226, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000731/  0.560762, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bf1923e86b4d0c85c8931b203379de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆▆▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▅▆▅▅▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00073</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.56076</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-27</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rpghm5l6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rpghm5l6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_020250-rpghm5l6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b6j3eg0q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_021356-b6j3eg0q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b6j3eg0q' target=\"_blank\">dutiful-sweep-28</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b6j3eg0q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b6j3eg0q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.75, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.044774/  1.408487, val:  41.67%, val_best:  41.67%, tr:  21.30%, tr_best:  21.30%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.957258/  0.897139, val:  64.58%, val_best:  64.58%, tr:  61.32%, tr_best:  61.32%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.706580/  0.765396, val:  66.67%, val_best:  66.67%, tr:  67.22%, tr_best:  67.22%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.605624/  0.741389, val:  67.92%, val_best:  67.92%, tr:  71.66%, tr_best:  71.66%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.540253/  0.727781, val:  66.67%, val_best:  67.92%, tr:  73.74%, tr_best:  73.74%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.494460/  0.712483, val:  72.08%, val_best:  72.08%, tr:  76.01%, tr_best:  76.01%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.452860/  0.708077, val:  71.25%, val_best:  72.08%, tr:  78.36%, tr_best:  78.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.418524/  0.716356, val:  74.17%, val_best:  74.17%, tr:  80.88%, tr_best:  80.88%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.379570/  0.705634, val:  73.75%, val_best:  74.17%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.345097/  0.669043, val:  77.92%, val_best:  77.92%, tr:  85.98%, tr_best:  85.98%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.305871/  0.728914, val:  75.00%, val_best:  77.92%, tr:  87.92%, tr_best:  87.92%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.278521/  0.701890, val:  78.75%, val_best:  78.75%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.242904/  0.710011, val:  79.58%, val_best:  79.58%, tr:  91.34%, tr_best:  91.34%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.203387/  0.596035, val:  84.58%, val_best:  84.58%, tr:  93.49%, tr_best:  93.49%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.168758/  0.622044, val:  82.50%, val_best:  84.58%, tr:  94.54%, tr_best:  94.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.139810/  0.563450, val:  83.75%, val_best:  84.58%, tr:  96.26%, tr_best:  96.26%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.121016/  0.649088, val:  83.33%, val_best:  84.58%, tr:  96.19%, tr_best:  96.26%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.103430/  0.635129, val:  80.00%, val_best:  84.58%, tr:  96.96%, tr_best:  96.96%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.089957/  0.564686, val:  84.17%, val_best:  84.58%, tr:  97.61%, tr_best:  97.61%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.073798/  0.625625, val:  84.17%, val_best:  84.58%, tr:  98.24%, tr_best:  98.24%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.068391/  0.631064, val:  83.33%, val_best:  84.58%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.051212/  0.610385, val:  83.75%, val_best:  84.58%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.045351/  0.635096, val:  82.50%, val_best:  84.58%, tr:  99.21%, tr_best:  99.21%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.045059/  0.596984, val:  85.00%, val_best:  85.00%, tr:  99.05%, tr_best:  99.21%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.037652/  0.586141, val:  83.75%, val_best:  85.00%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.030470/  0.640094, val:  84.58%, val_best:  85.00%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.028898/  0.618065, val:  85.42%, val_best:  85.42%, tr:  99.46%, tr_best:  99.59%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.020231/  0.584361, val:  85.42%, val_best:  85.42%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.019692/  0.584553, val:  85.00%, val_best:  85.42%, tr:  99.80%, tr_best:  99.91%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.016848/  0.586974, val:  85.42%, val_best:  85.42%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.016509/  0.642660, val:  85.00%, val_best:  85.42%, tr:  99.91%, tr_best:  99.93%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.013556/  0.612211, val:  85.42%, val_best:  85.42%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.012903/  0.608856, val:  86.25%, val_best:  86.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.011612/  0.613522, val:  85.42%, val_best:  86.25%, tr:  99.93%, tr_best:  99.95%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.010110/  0.644499, val:  85.00%, val_best:  86.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.010039/  0.610586, val:  85.83%, val_best:  86.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.008881/  0.643136, val:  85.83%, val_best:  86.25%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.008294/  0.626098, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.007781/  0.662554, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.008203/  0.628745, val:  85.00%, val_best:  86.25%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.006420/  0.644665, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.005788/  0.637558, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.005751/  0.633989, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.005408/  0.648989, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.005233/  0.643575, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.004916/  0.638859, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.004920/  0.657620, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.004717/  0.643387, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.004298/  0.665594, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.004103/  0.645736, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.003908/  0.666146, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.003811/  0.650641, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.003779/  0.625294, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.003649/  0.649899, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.003506/  0.670618, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.003402/  0.653561, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.003394/  0.657454, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.003249/  0.666059, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.003260/  0.663944, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.003060/  0.645423, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.002953/  0.700460, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.002921/  0.652716, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.002922/  0.657044, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.003007/  0.694025, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.002742/  0.642239, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002572/  0.668397, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002492/  0.660936, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.002468/  0.668376, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.002399/  0.679475, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002328/  0.677534, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002283/  0.674767, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.002223/  0.670024, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.002236/  0.654238, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002129/  0.680141, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002074/  0.661419, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002086/  0.655671, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002005/  0.672142, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001971/  0.664960, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001916/  0.651459, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001895/  0.663828, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001888/  0.679264, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001791/  0.672528, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001747/  0.657696, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001777/  0.662896, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001718/  0.662347, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001670/  0.671968, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001645/  0.674418, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001625/  0.663861, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001566/  0.658391, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001561/  0.659569, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001540/  0.651576, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001512/  0.674267, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001513/  0.677365, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001517/  0.662315, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001503/  0.671949, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001459/  0.665854, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001424/  0.671250, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001406/  0.673797, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001391/  0.678707, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001380/  0.670576, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507ddaaecea54f7f992af5e4bdec91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄▄▇█▇█▇▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇▇▇█▇████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▇▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▆▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇▇▇█▇████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00138</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>0.67058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-sweep-28</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b6j3eg0q' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b6j3eg0q</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_021356-b6j3eg0q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 78nnji4b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_022539-78nnji4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/78nnji4b' target=\"_blank\">mild-sweep-29</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/78nnji4b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/78nnji4b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.301286/  2.303460, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.300988/  2.303480, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.300646/  2.303511, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.300203/  2.303545, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.299970/  2.303592, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.299778/  2.303646, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.299497/  2.303704, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  2.299424/  2.303769, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  2.299265/  2.303828, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  2.298968/  2.303891, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  2.298871/  2.303962, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  2.298592/  2.304030, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  2.298459/  2.304115, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  2.298379/  2.304188, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  2.298295/  2.304265, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  2.298030/  2.304340, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  2.298017/  2.304422, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  2.297553/  2.304496, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  2.297601/  2.304589, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  2.297451/  2.304670, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  2.297352/  2.304762, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  2.297352/  2.304854, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  2.297287/  2.304944, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  2.297099/  2.305020, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  2.297077/  2.305111, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  2.296936/  2.305189, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  2.296890/  2.305276, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  2.296940/  2.305352, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  2.296811/  2.305445, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  2.296637/  2.305523, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  2.296700/  2.305607, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  2.296727/  2.305690, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  2.296513/  2.305765, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  2.296525/  2.305847, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  2.296349/  2.305921, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  2.296649/  2.305994, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  2.296420/  2.306073, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  2.296336/  2.306145, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  2.296126/  2.306217, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  2.296369/  2.306292, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  2.296394/  2.306368, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  2.296393/  2.306446, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  2.296189/  2.306497, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  2.296202/  2.306564, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  2.295956/  2.306634, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  2.296020/  2.306701, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  2.296277/  2.306759, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  2.296092/  2.306827, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  2.296033/  2.306889, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  2.296098/  2.306953, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  2.296002/  2.307009, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  2.295949/  2.307062, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  2.295844/  2.307120, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  2.295670/  2.307187, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  2.295594/  2.307243, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  2.295657/  2.307309, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  2.295711/  2.307380, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  2.295710/  2.307413, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  2.295845/  2.307484, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  2.295861/  2.307544, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  2.295661/  2.307580, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  2.295970/  2.307635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  2.295709/  2.307679, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  2.295809/  2.307726, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  2.295975/  2.307768, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  2.295654/  2.307801, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  2.295733/  2.307851, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  2.295884/  2.307886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  2.295734/  2.307938, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  2.295697/  2.307965, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  2.295595/  2.308010, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  2.295741/  2.308047, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  2.295842/  2.308087, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  2.295578/  2.308119, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  2.295573/  2.308160, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  2.295596/  2.308207, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  2.295764/  2.308246, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  2.295714/  2.308275, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  2.295440/  2.308289, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  2.295578/  2.308338, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  2.295703/  2.308366, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  2.295931/  2.308404, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  2.295539/  2.308422, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  2.295487/  2.308453, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  2.295888/  2.308490, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  2.295693/  2.308505, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  2.295612/  2.308537, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  2.295655/  2.308565, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  2.295583/  2.308585, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  2.295536/  2.308618, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  2.295739/  2.308648, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  2.295894/  2.308658, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  2.295656/  2.308684, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  2.295647/  2.308695, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  2.295885/  2.308713, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  2.295976/  2.308733, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  2.295692/  2.308754, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  2.295486/  2.308773, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  2.295690/  2.308799, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  2.295526/  2.308801, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620e80892d8b4695a057502c6f6dc0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▃██▆▃▅▅█▅▁▅▁▆▃▆▁▃▆▃█▃█▅▅█▃█▆▃▅▃▅▁▃▃▅▃▃▃</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.12647</td></tr><tr><td>tr_epoch_loss</td><td>2.29553</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.3088</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-sweep-29</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/78nnji4b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/78nnji4b</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_022539-78nnji4b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m3tdkoxa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_023708-m3tdkoxa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m3tdkoxa' target=\"_blank\">dulcet-sweep-30</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m3tdkoxa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m3tdkoxa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.955137/  1.007768, val:  57.92%, val_best:  57.92%, tr:  58.95%, tr_best:  58.95%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.633488/  0.861796, val:  64.17%, val_best:  64.17%, tr:  70.96%, tr_best:  70.96%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.483149/  0.760830, val:  70.83%, val_best:  70.83%, tr:  78.47%, tr_best:  78.47%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.366708/  0.676429, val:  83.33%, val_best:  83.33%, tr:  85.41%, tr_best:  85.41%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.260007/  0.596713, val:  80.00%, val_best:  83.33%, tr:  90.28%, tr_best:  90.28%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.224022/  0.558387, val:  85.00%, val_best:  85.00%, tr:  91.37%, tr_best:  91.37%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.159481/  0.809964, val:  77.50%, val_best:  85.00%, tr:  94.45%, tr_best:  94.45%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.134419/  0.605432, val:  86.67%, val_best:  86.67%, tr:  95.45%, tr_best:  95.45%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.085520/  0.883648, val:  80.42%, val_best:  86.67%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.122866/  0.538441, val:  85.42%, val_best:  86.67%, tr:  95.20%, tr_best:  97.23%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.085068/  0.548780, val:  85.42%, val_best:  86.67%, tr:  97.11%, tr_best:  97.23%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.040832/  0.769605, val:  81.25%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.032957/  0.642591, val:  88.33%, val_best:  88.33%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.024398/  0.678472, val:  85.83%, val_best:  88.33%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.045704/  0.707028, val:  85.00%, val_best:  88.33%, tr:  98.76%, tr_best:  99.28%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.027800/  0.584827, val:  85.83%, val_best:  88.33%, tr:  99.03%, tr_best:  99.28%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.027652/  0.651052, val:  85.83%, val_best:  88.33%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.011670/  0.650018, val:  86.25%, val_best:  88.33%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.011404/  0.546079, val:  86.67%, val_best:  88.33%, tr:  99.66%, tr_best:  99.77%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.003561/  0.633149, val:  86.25%, val_best:  88.33%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001558/  0.612924, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001024/  0.585231, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000845/  0.596241, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000755/  0.609148, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000651/  0.617620, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000602/  0.606601, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000560/  0.607063, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000503/  0.617558, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000471/  0.606844, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000452/  0.619361, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000412/  0.607277, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000389/  0.617022, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000367/  0.615817, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000357/  0.612885, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000341/  0.625152, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.633187, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000306/  0.639251, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000296/  0.646917, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000284/  0.646390, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000272/  0.652048, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.654455, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000254/  0.653792, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000253/  0.660993, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.659280, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000232/  0.649400, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000231/  0.647422, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000221/  0.648225, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000216/  0.648228, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000208/  0.652768, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000204/  0.647242, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000198/  0.645373, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000197/  0.656139, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.656302, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.658520, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000180/  0.658293, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000175/  0.658780, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000176/  0.657437, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000170/  0.651068, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000168/  0.654113, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000164/  0.656811, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000162/  0.667812, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.663543, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000161/  0.661248, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000151/  0.665512, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.663802, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.663569, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000140/  0.665893, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000138/  0.665396, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000135/  0.664289, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000135/  0.664218, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.664026, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.664901, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000127/  0.661522, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000126/  0.660365, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.658058, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000122/  0.661735, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.656486, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000119/  0.644730, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.645872, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000117/  0.649620, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000114/  0.651442, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000111/  0.653960, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.651691, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000110/  0.650589, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000107/  0.652803, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.655024, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000104/  0.653163, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000102/  0.652817, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000101/  0.654551, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000100/  0.654627, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.663356, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000097/  0.663640, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000098/  0.669786, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.665640, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.666797, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000095/  0.667991, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000093/  0.668086, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000092/  0.667181, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000091/  0.668421, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.672938, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986c3e7892ba49d8ae6a989583b7c219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁█▅█▅█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇██▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇██▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>0.67294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-sweep-30</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m3tdkoxa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m3tdkoxa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_023708-m3tdkoxa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hpm0rz9d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d180414db14eec84ecc86eb8f65e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113033759304218, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_024853-hpm0rz9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hpm0rz9d' target=\"_blank\">atomic-sweep-31</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hpm0rz9d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hpm0rz9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.928935/  0.881833, val:  59.58%, val_best:  59.58%, tr:  60.01%, tr_best:  60.01%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.622879/  0.854058, val:  62.92%, val_best:  62.92%, tr:  71.73%, tr_best:  71.73%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.415023/  0.815604, val:  70.00%, val_best:  70.00%, tr:  82.57%, tr_best:  82.57%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.301157/  0.537661, val:  85.42%, val_best:  85.42%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.169007/  0.494498, val:  85.42%, val_best:  85.42%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.108344/  0.441543, val:  89.17%, val_best:  89.17%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.078059/  0.622470, val:  88.33%, val_best:  89.17%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.058447/  0.424832, val:  88.75%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051343/  0.443390, val:  90.42%, val_best:  90.42%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019991/  0.473071, val:  88.75%, val_best:  90.42%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024913/  0.594811, val:  87.92%, val_best:  90.42%, tr:  99.23%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.011130/  0.510964, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003125/  0.552201, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001943/  0.502089, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001624/  0.521189, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001236/  0.545577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001065/  0.524273, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000955/  0.556683, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000850/  0.553951, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000777/  0.562979, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000711/  0.561448, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000666/  0.572040, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000628/  0.581079, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000576/  0.569785, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000538/  0.568354, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000500/  0.574372, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000475/  0.581407, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000450/  0.584895, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000429/  0.586062, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000411/  0.596621, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.597417, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000379/  0.608332, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000358/  0.602874, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000340/  0.604670, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000329/  0.601791, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.601425, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000308/  0.602356, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000292/  0.598435, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.597901, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000277/  0.604077, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.613960, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000256/  0.610192, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000249/  0.611147, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.614404, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000235/  0.615403, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.614356, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.623049, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000215/  0.624923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000211/  0.613693, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.619318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.622931, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000195/  0.624674, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.627025, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.628896, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.624894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.621996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000175/  0.622287, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.615900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617114, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.613777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000159/  0.619844, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.615970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000157/  0.617458, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.619992, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.616505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617499, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.623439, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.627106, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000138/  0.627911, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.630018, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000130/  0.633134, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.632592, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633326, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635861, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000123/  0.634962, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631111, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.628672, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629021, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.628650, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629489, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.628790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.628014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000109/  0.629268, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.627406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000109/  0.628916, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000104/  0.631419, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.634245, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.634045, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000103/  0.632370, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.629538, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.630059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.629965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.636612, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635149, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.633406, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000092/  0.634207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.632478, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5985044392be4b9abfb2de266de049dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.63248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-31</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hpm0rz9d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hpm0rz9d</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_024853-hpm0rz9d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2vqmu97w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_030018-2vqmu97w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2vqmu97w' target=\"_blank\">iconic-sweep-32</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2vqmu97w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2vqmu97w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.300406/  2.303886, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.298519/  2.304635, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.297336/  2.305443, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.296554/  2.306226, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.296175/  2.306992, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.296047/  2.307540, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.295769/  2.308052, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.295716/  2.307580, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.286639/  2.284575, val:  10.00%, val_best:  10.00%, tr:  12.65%, tr_best:  12.65%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.210781/  2.187126, val:  10.42%, val_best:  10.42%, tr:  12.67%, tr_best:  12.67%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.055854/  2.047327, val:  21.25%, val_best:  21.25%, tr:  21.91%, tr_best:  21.91%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.888593/  1.924559, val:  23.33%, val_best:  23.33%, tr:  27.91%, tr_best:  27.91%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.756232/  1.800813, val:  28.33%, val_best:  28.33%, tr:  28.99%, tr_best:  28.99%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.632423/  1.662951, val:  37.92%, val_best:  37.92%, tr:  35.41%, tr_best:  35.41%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.486217/  1.498987, val:  44.58%, val_best:  44.58%, tr:  47.07%, tr_best:  47.07%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.329806/  1.364398, val:  46.67%, val_best:  46.67%, tr:  49.75%, tr_best:  49.75%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  1.201907/  1.251006, val:  47.08%, val_best:  47.08%, tr:  53.54%, tr_best:  53.54%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  1.099221/  1.155850, val:  55.42%, val_best:  55.42%, tr:  58.86%, tr_best:  58.86%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  1.013993/  1.083575, val:  57.50%, val_best:  57.50%, tr:  62.15%, tr_best:  62.15%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.943627/  1.030956, val:  60.00%, val_best:  60.00%, tr:  63.98%, tr_best:  63.98%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.884884/  0.965105, val:  60.83%, val_best:  60.83%, tr:  66.05%, tr_best:  66.05%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.834076/  0.929678, val:  60.83%, val_best:  60.83%, tr:  66.73%, tr_best:  66.73%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.790373/  0.872868, val:  64.17%, val_best:  64.17%, tr:  67.85%, tr_best:  67.85%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.754646/  0.856220, val:  62.92%, val_best:  64.17%, tr:  68.69%, tr_best:  68.69%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.725661/  0.820305, val:  67.92%, val_best:  67.92%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.699536/  0.800306, val:  66.67%, val_best:  67.92%, tr:  70.02%, tr_best:  70.02%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.672997/  0.787155, val:  66.25%, val_best:  67.92%, tr:  71.03%, tr_best:  71.03%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.654784/  0.788151, val:  65.42%, val_best:  67.92%, tr:  71.42%, tr_best:  71.42%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.634913/  0.752084, val:  69.17%, val_best:  69.17%, tr:  72.07%, tr_best:  72.07%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.616509/  0.739715, val:  68.75%, val_best:  69.17%, tr:  72.84%, tr_best:  72.84%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.598245/  0.742256, val:  70.00%, val_best:  70.00%, tr:  73.33%, tr_best:  73.33%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.586344/  0.736012, val:  67.08%, val_best:  70.00%, tr:  73.33%, tr_best:  73.33%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.569655/  0.727496, val:  67.92%, val_best:  70.00%, tr:  74.26%, tr_best:  74.26%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.554029/  0.720097, val:  65.42%, val_best:  70.00%, tr:  75.11%, tr_best:  75.11%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.542608/  0.714616, val:  67.50%, val_best:  70.00%, tr:  75.47%, tr_best:  75.47%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.530415/  0.675297, val:  72.50%, val_best:  72.50%, tr:  75.68%, tr_best:  75.68%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.516579/  0.705482, val:  67.08%, val_best:  72.50%, tr:  77.75%, tr_best:  77.75%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.510587/  0.665940, val:  67.50%, val_best:  72.50%, tr:  77.80%, tr_best:  77.80%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.495987/  0.687417, val:  70.00%, val_best:  72.50%, tr:  78.56%, tr_best:  78.56%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.485748/  0.675939, val:  69.17%, val_best:  72.50%, tr:  79.01%, tr_best:  79.01%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.476930/  0.672229, val:  69.17%, val_best:  72.50%, tr:  79.13%, tr_best:  79.13%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.465591/  0.664228, val:  71.67%, val_best:  72.50%, tr:  79.71%, tr_best:  79.71%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.455940/  0.653307, val:  71.67%, val_best:  72.50%, tr:  80.73%, tr_best:  80.73%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.445975/  0.653888, val:  72.08%, val_best:  72.50%, tr:  80.68%, tr_best:  80.73%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.437114/  0.633182, val:  72.08%, val_best:  72.50%, tr:  81.42%, tr_best:  81.42%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.428438/  0.639205, val:  71.67%, val_best:  72.50%, tr:  82.35%, tr_best:  82.35%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.418372/  0.647948, val:  70.83%, val_best:  72.50%, tr:  83.30%, tr_best:  83.30%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.410759/  0.634315, val:  74.58%, val_best:  74.58%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.402738/  0.647983, val:  74.17%, val_best:  74.58%, tr:  83.88%, tr_best:  83.88%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.392987/  0.629990, val:  74.58%, val_best:  74.58%, tr:  85.10%, tr_best:  85.10%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.387320/  0.643571, val:  75.00%, val_best:  75.00%, tr:  85.55%, tr_best:  85.55%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.372428/  0.621160, val:  75.83%, val_best:  75.83%, tr:  86.36%, tr_best:  86.36%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.368249/  0.623131, val:  73.33%, val_best:  75.83%, tr:  86.59%, tr_best:  86.59%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.357867/  0.616768, val:  75.42%, val_best:  75.83%, tr:  87.06%, tr_best:  87.06%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.344141/  0.638411, val:  75.00%, val_best:  75.83%, tr:  88.26%, tr_best:  88.26%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.336417/  0.609385, val:  76.67%, val_best:  76.67%, tr:  88.73%, tr_best:  88.73%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.324665/  0.608671, val:  77.50%, val_best:  77.50%, tr:  89.16%, tr_best:  89.16%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.312431/  0.596314, val:  77.50%, val_best:  77.50%, tr:  90.37%, tr_best:  90.37%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.301767/  0.588205, val:  78.33%, val_best:  78.33%, tr:  90.46%, tr_best:  90.46%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.291194/  0.586336, val:  82.08%, val_best:  82.08%, tr:  90.87%, tr_best:  90.87%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.280254/  0.592773, val:  80.83%, val_best:  82.08%, tr:  91.79%, tr_best:  91.79%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.267893/  0.571595, val:  82.08%, val_best:  82.08%, tr:  92.70%, tr_best:  92.70%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.258846/  0.541779, val:  84.58%, val_best:  84.58%, tr:  92.99%, tr_best:  92.99%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.241837/  0.548851, val:  83.75%, val_best:  84.58%, tr:  94.07%, tr_best:  94.07%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.230854/  0.518431, val:  82.50%, val_best:  84.58%, tr:  94.30%, tr_best:  94.30%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.220500/  0.532576, val:  82.92%, val_best:  84.58%, tr:  94.70%, tr_best:  94.70%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.212342/  0.508849, val:  83.33%, val_best:  84.58%, tr:  94.72%, tr_best:  94.72%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.197979/  0.512256, val:  82.50%, val_best:  84.58%, tr:  95.78%, tr_best:  95.78%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.190662/  0.503488, val:  83.33%, val_best:  84.58%, tr:  95.83%, tr_best:  95.83%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.181264/  0.496318, val:  86.67%, val_best:  86.67%, tr:  96.19%, tr_best:  96.19%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.171551/  0.502652, val:  83.33%, val_best:  86.67%, tr:  96.37%, tr_best:  96.37%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.164319/  0.478454, val:  85.42%, val_best:  86.67%, tr:  96.62%, tr_best:  96.62%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.157034/  0.481855, val:  84.17%, val_best:  86.67%, tr:  96.89%, tr_best:  96.89%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.150415/  0.484920, val:  84.17%, val_best:  86.67%, tr:  97.11%, tr_best:  97.11%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.142368/  0.466235, val:  85.00%, val_best:  86.67%, tr:  97.25%, tr_best:  97.25%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.134922/  0.490631, val:  83.75%, val_best:  86.67%, tr:  97.61%, tr_best:  97.61%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.130027/  0.462683, val:  85.00%, val_best:  86.67%, tr:  97.48%, tr_best:  97.61%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.124908/  0.462860, val:  85.42%, val_best:  86.67%, tr:  97.59%, tr_best:  97.61%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.118826/  0.445825, val:  86.25%, val_best:  86.67%, tr:  97.93%, tr_best:  97.93%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.116321/  0.475755, val:  84.58%, val_best:  86.67%, tr:  97.84%, tr_best:  97.93%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.109021/  0.462296, val:  85.83%, val_best:  86.67%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.104011/  0.437773, val:  87.50%, val_best:  87.50%, tr:  98.42%, tr_best:  98.42%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.101236/  0.460356, val:  86.25%, val_best:  87.50%, tr:  98.38%, tr_best:  98.42%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.098287/  0.431341, val:  87.50%, val_best:  87.50%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.094320/  0.427158, val:  87.92%, val_best:  87.92%, tr:  98.69%, tr_best:  98.69%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.090215/  0.450059, val:  87.08%, val_best:  87.92%, tr:  98.67%, tr_best:  98.69%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.086014/  0.440793, val:  87.08%, val_best:  87.92%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.084647/  0.433451, val:  87.50%, val_best:  87.92%, tr:  98.90%, tr_best:  98.90%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.080230/  0.432670, val:  87.08%, val_best:  87.92%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.077785/  0.445505, val:  86.67%, val_best:  87.92%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.075635/  0.438836, val:  86.67%, val_best:  87.92%, tr:  99.17%, tr_best:  99.17%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.074171/  0.461432, val:  86.25%, val_best:  87.92%, tr:  99.03%, tr_best:  99.17%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.071202/  0.435016, val:  87.50%, val_best:  87.92%, tr:  99.12%, tr_best:  99.17%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.068745/  0.446797, val:  86.67%, val_best:  87.92%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.066456/  0.426423, val:  87.08%, val_best:  87.92%, tr:  99.21%, tr_best:  99.26%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.064579/  0.435704, val:  86.25%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.060955/  0.425112, val:  87.50%, val_best:  87.92%, tr:  99.35%, tr_best:  99.39%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.060229/  0.429612, val:  86.67%, val_best:  87.92%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.059045/  0.438678, val:  87.08%, val_best:  87.92%, tr:  99.30%, tr_best:  99.44%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.056281/  0.443608, val:  86.67%, val_best:  87.92%, tr:  99.57%, tr_best:  99.57%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f60572985c34dfd9f9ff729a564716b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▂▂▂▁▄▄▅▅▃▇▆▆▇▇▇▆▅█▆▇▇▇▇███████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▂▂▂▃▅▅▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>tr_acc</td><td>▁▂▂▂▂▃▄▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁████▇▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▂▂▂▃▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▂▂▂▃▅▅▆▆▆▆▇▆▆▆▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_loss</td><td>▁████▇▆▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99572</td></tr><tr><td>tr_epoch_loss</td><td>0.05628</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>0.44361</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-sweep-32</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2vqmu97w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2vqmu97w</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_030018-2vqmu97w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: se0lhjpy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_031249-se0lhjpy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/se0lhjpy' target=\"_blank\">logical-sweep-33</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/se0lhjpy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/se0lhjpy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  1.936443/  1.602167, val:  47.50%, val_best:  47.50%, tr:  35.57%, tr_best:  35.57%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.287427/  1.235234, val:  54.58%, val_best:  54.58%, tr:  56.67%, tr_best:  56.67%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.053464/  1.084104, val:  57.92%, val_best:  57.92%, tr:  60.98%, tr_best:  60.98%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.948965/  1.004113, val:  57.50%, val_best:  57.92%, tr:  63.53%, tr_best:  63.53%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.881292/  0.950644, val:  61.25%, val_best:  61.25%, tr:  64.70%, tr_best:  64.70%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.830867/  0.923531, val:  58.75%, val_best:  61.25%, tr:  66.68%, tr_best:  66.68%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.786368/  0.886352, val:  64.17%, val_best:  64.17%, tr:  67.49%, tr_best:  67.49%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.750863/  0.859378, val:  62.50%, val_best:  64.17%, tr:  68.49%, tr_best:  68.49%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.722141/  0.840573, val:  62.08%, val_best:  64.17%, tr:  69.57%, tr_best:  69.57%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.695065/  0.810127, val:  66.25%, val_best:  66.25%, tr:  70.78%, tr_best:  70.78%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.666600/  0.808836, val:  66.67%, val_best:  66.67%, tr:  71.69%, tr_best:  71.69%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.643896/  0.784723, val:  65.42%, val_best:  66.67%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.623417/  0.775203, val:  70.00%, val_best:  70.00%, tr:  73.04%, tr_best:  73.04%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.605490/  0.766464, val:  69.17%, val_best:  70.00%, tr:  73.83%, tr_best:  73.83%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.590348/  0.748499, val:  73.33%, val_best:  73.33%, tr:  74.39%, tr_best:  74.39%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.574486/  0.733263, val:  71.25%, val_best:  73.33%, tr:  75.32%, tr_best:  75.32%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.560561/  0.735162, val:  71.25%, val_best:  73.33%, tr:  75.11%, tr_best:  75.32%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.549634/  0.732972, val:  71.25%, val_best:  73.33%, tr:  77.07%, tr_best:  77.07%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.537416/  0.727236, val:  70.83%, val_best:  73.33%, tr:  77.03%, tr_best:  77.07%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.524410/  0.740618, val:  67.92%, val_best:  73.33%, tr:  77.43%, tr_best:  77.43%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.518824/  0.726085, val:  72.50%, val_best:  73.33%, tr:  77.75%, tr_best:  77.75%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.505569/  0.745640, val:  69.17%, val_best:  73.33%, tr:  78.11%, tr_best:  78.11%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.498410/  0.712377, val:  72.50%, val_best:  73.33%, tr:  78.61%, tr_best:  78.61%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.487371/  0.727017, val:  67.92%, val_best:  73.33%, tr:  79.98%, tr_best:  79.98%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.479365/  0.706523, val:  72.50%, val_best:  73.33%, tr:  80.12%, tr_best:  80.12%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.473831/  0.698130, val:  73.33%, val_best:  73.33%, tr:  80.05%, tr_best:  80.12%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.460505/  0.718584, val:  71.25%, val_best:  73.33%, tr:  81.65%, tr_best:  81.65%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.457400/  0.729693, val:  69.58%, val_best:  73.33%, tr:  81.13%, tr_best:  81.65%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.449366/  0.685875, val:  72.50%, val_best:  73.33%, tr:  81.94%, tr_best:  81.94%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.440565/  0.713852, val:  72.92%, val_best:  73.33%, tr:  82.10%, tr_best:  82.10%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.432999/  0.712453, val:  73.33%, val_best:  73.33%, tr:  82.73%, tr_best:  82.73%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.428704/  0.707581, val:  73.33%, val_best:  73.33%, tr:  83.07%, tr_best:  83.07%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.418931/  0.722811, val:  74.17%, val_best:  74.17%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.412735/  0.722121, val:  70.83%, val_best:  74.17%, tr:  83.77%, tr_best:  83.77%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.407570/  0.718733, val:  75.00%, val_best:  75.00%, tr:  84.63%, tr_best:  84.63%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.398587/  0.700079, val:  74.58%, val_best:  75.00%, tr:  84.47%, tr_best:  84.63%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.391687/  0.720815, val:  72.92%, val_best:  75.00%, tr:  85.62%, tr_best:  85.62%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.387276/  0.690699, val:  75.42%, val_best:  75.42%, tr:  84.81%, tr_best:  85.62%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.377487/  0.703085, val:  74.58%, val_best:  75.42%, tr:  85.93%, tr_best:  85.93%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.372158/  0.710452, val:  71.67%, val_best:  75.42%, tr:  86.18%, tr_best:  86.18%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.365472/  0.731589, val:  71.25%, val_best:  75.42%, tr:  86.81%, tr_best:  86.81%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.358355/  0.716283, val:  72.92%, val_best:  75.42%, tr:  87.22%, tr_best:  87.22%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.351261/  0.694005, val:  74.58%, val_best:  75.42%, tr:  87.31%, tr_best:  87.31%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.346911/  0.698816, val:  74.17%, val_best:  75.42%, tr:  87.40%, tr_best:  87.40%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.340320/  0.699873, val:  73.75%, val_best:  75.42%, tr:  88.19%, tr_best:  88.19%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.332772/  0.694046, val:  76.25%, val_best:  76.25%, tr:  88.37%, tr_best:  88.37%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.328419/  0.703641, val:  75.00%, val_best:  76.25%, tr:  89.34%, tr_best:  89.34%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.320102/  0.698138, val:  76.25%, val_best:  76.25%, tr:  89.16%, tr_best:  89.34%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.317623/  0.701935, val:  75.42%, val_best:  76.25%, tr:  89.22%, tr_best:  89.34%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.309358/  0.690951, val:  76.25%, val_best:  76.25%, tr:  90.10%, tr_best:  90.10%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.304900/  0.713645, val:  76.67%, val_best:  76.67%, tr:  90.08%, tr_best:  90.10%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.294370/  0.716704, val:  76.25%, val_best:  76.67%, tr:  90.64%, tr_best:  90.64%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.289971/  0.695851, val:  76.25%, val_best:  76.67%, tr:  91.03%, tr_best:  91.03%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.287208/  0.686533, val:  77.08%, val_best:  77.08%, tr:  90.49%, tr_best:  91.03%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.279829/  0.742900, val:  75.00%, val_best:  77.08%, tr:  91.37%, tr_best:  91.37%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.278966/  0.682496, val:  77.08%, val_best:  77.08%, tr:  90.94%, tr_best:  91.37%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.268921/  0.699521, val:  76.67%, val_best:  77.08%, tr:  91.91%, tr_best:  91.91%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.261767/  0.712590, val:  75.42%, val_best:  77.08%, tr:  92.22%, tr_best:  92.22%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.259276/  0.691247, val:  77.50%, val_best:  77.50%, tr:  92.11%, tr_best:  92.22%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.252987/  0.685523, val:  78.33%, val_best:  78.33%, tr:  92.06%, tr_best:  92.22%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.246171/  0.735654, val:  76.25%, val_best:  78.33%, tr:  92.49%, tr_best:  92.49%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.242999/  0.681811, val:  78.75%, val_best:  78.75%, tr:  92.76%, tr_best:  92.76%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.238603/  0.678945, val:  77.50%, val_best:  78.75%, tr:  93.42%, tr_best:  93.42%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.230903/  0.696003, val:  77.92%, val_best:  78.75%, tr:  93.49%, tr_best:  93.49%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.225320/  0.657770, val:  78.33%, val_best:  78.75%, tr:  93.64%, tr_best:  93.64%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.223032/  0.665972, val:  77.50%, val_best:  78.75%, tr:  93.94%, tr_best:  93.94%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.217124/  0.656246, val:  80.00%, val_best:  80.00%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.211856/  0.682749, val:  80.00%, val_best:  80.00%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.209157/  0.696131, val:  77.08%, val_best:  80.00%, tr:  94.23%, tr_best:  94.23%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.204198/  0.650549, val:  80.83%, val_best:  80.83%, tr:  94.43%, tr_best:  94.43%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.198743/  0.647879, val:  79.58%, val_best:  80.83%, tr:  94.75%, tr_best:  94.75%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.195123/  0.652185, val:  81.67%, val_best:  81.67%, tr:  94.82%, tr_best:  94.82%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.193767/  0.717721, val:  76.67%, val_best:  81.67%, tr:  94.72%, tr_best:  94.82%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.186016/  0.658445, val:  80.42%, val_best:  81.67%, tr:  95.31%, tr_best:  95.31%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.183299/  0.669467, val:  78.33%, val_best:  81.67%, tr:  95.31%, tr_best:  95.31%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.177934/  0.661473, val:  79.58%, val_best:  81.67%, tr:  95.58%, tr_best:  95.58%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.176983/  0.665848, val:  79.58%, val_best:  81.67%, tr:  95.49%, tr_best:  95.58%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.172056/  0.656670, val:  80.42%, val_best:  81.67%, tr:  95.47%, tr_best:  95.58%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.169912/  0.632907, val:  80.42%, val_best:  81.67%, tr:  95.65%, tr_best:  95.65%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.170020/  0.688769, val:  77.08%, val_best:  81.67%, tr:  95.69%, tr_best:  95.69%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.164647/  0.638374, val:  80.42%, val_best:  81.67%, tr:  95.67%, tr_best:  95.69%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.158302/  0.645911, val:  79.58%, val_best:  81.67%, tr:  96.06%, tr_best:  96.06%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.154256/  0.655693, val:  78.75%, val_best:  81.67%, tr:  96.28%, tr_best:  96.28%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.153421/  0.638798, val:  81.67%, val_best:  81.67%, tr:  96.28%, tr_best:  96.28%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.149328/  0.630367, val:  81.25%, val_best:  81.67%, tr:  96.33%, tr_best:  96.33%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.143937/  0.645803, val:  80.83%, val_best:  81.67%, tr:  96.60%, tr_best:  96.60%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.143029/  0.658940, val:  81.67%, val_best:  81.67%, tr:  96.57%, tr_best:  96.60%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.138467/  0.621058, val:  82.50%, val_best:  82.50%, tr:  97.07%, tr_best:  97.07%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.134775/  0.629489, val:  81.67%, val_best:  82.50%, tr:  96.98%, tr_best:  97.07%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.133361/  0.631973, val:  82.08%, val_best:  82.50%, tr:  97.00%, tr_best:  97.07%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.131023/  0.644259, val:  81.67%, val_best:  82.50%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.128660/  0.627309, val:  81.25%, val_best:  82.50%, tr:  96.93%, tr_best:  97.43%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.126064/  0.639665, val:  81.25%, val_best:  82.50%, tr:  97.23%, tr_best:  97.43%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.122132/  0.639401, val:  81.25%, val_best:  82.50%, tr:  97.63%, tr_best:  97.63%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.119478/  0.615016, val:  83.33%, val_best:  83.33%, tr:  97.59%, tr_best:  97.63%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.116973/  0.625129, val:  84.17%, val_best:  84.17%, tr:  97.41%, tr_best:  97.63%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.115091/  0.633582, val:  82.92%, val_best:  84.17%, tr:  97.63%, tr_best:  97.63%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.111957/  0.627088, val:  81.25%, val_best:  84.17%, tr:  97.84%, tr_best:  97.84%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.109436/  0.611161, val:  82.92%, val_best:  84.17%, tr:  97.88%, tr_best:  97.88%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.106219/  0.626641, val:  82.92%, val_best:  84.17%, tr:  97.84%, tr_best:  97.88%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45614edf7564222a39f98ae438e3ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▃▃▅▄▅▆▄▅▃▆▃▆█▇▆▆▆▇▆█▇▆▇█▆▆▇▇██▇▇▇█▇█▇█</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇██████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_acc_best</td><td>▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇██████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.97836</td></tr><tr><td>tr_epoch_loss</td><td>0.10622</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.82917</td></tr><tr><td>val_loss</td><td>0.62664</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-33</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/se0lhjpy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/se0lhjpy</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_031249-se0lhjpy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cjst53br with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_032516-cjst53br</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cjst53br' target=\"_blank\">misunderstood-sweep-34</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cjst53br' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cjst53br</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.085167/  0.785727, val:  65.42%, val_best:  65.42%, tr:  54.53%, tr_best:  54.53%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.621396/  0.801698, val:  64.58%, val_best:  65.42%, tr:  71.53%, tr_best:  71.53%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.463612/  0.618710, val:  75.42%, val_best:  75.42%, tr:  79.31%, tr_best:  79.31%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.342315/  0.604841, val:  83.33%, val_best:  83.33%, tr:  86.45%, tr_best:  86.45%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.219573/  0.512035, val:  84.17%, val_best:  84.17%, tr:  92.06%, tr_best:  92.06%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.157152/  0.467757, val:  88.75%, val_best:  88.75%, tr:  94.66%, tr_best:  94.66%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.117295/  0.516028, val:  88.33%, val_best:  88.75%, tr:  96.03%, tr_best:  96.03%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.099346/  0.421013, val:  88.75%, val_best:  88.75%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.075704/  0.523030, val:  90.42%, val_best:  90.42%, tr:  97.36%, tr_best:  97.36%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.048978/  0.484825, val:  88.33%, val_best:  90.42%, tr:  98.24%, tr_best:  98.24%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.051513/  0.486311, val:  89.58%, val_best:  90.42%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.035695/  0.594569, val:  87.92%, val_best:  90.42%, tr:  98.83%, tr_best:  98.83%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.023629/  0.441569, val:  90.42%, val_best:  90.42%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.008572/  0.556767, val:  88.75%, val_best:  90.42%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.008181/  0.544942, val:  88.33%, val_best:  90.42%, tr:  99.77%, tr_best:  99.84%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.008271/  0.539941, val:  89.58%, val_best:  90.42%, tr:  99.82%, tr_best:  99.84%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001825/  0.559389, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001474/  0.564481, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001191/  0.558705, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001017/  0.569585, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000879/  0.562857, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000811/  0.544972, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000745/  0.560770, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000686/  0.562189, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000619/  0.558667, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000581/  0.564796, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000558/  0.559934, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000507/  0.567349, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000495/  0.571856, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000470/  0.580337, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000427/  0.581093, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000428/  0.583625, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000395/  0.585497, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000392/  0.583313, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000371/  0.587296, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000357/  0.583335, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000349/  0.588050, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000332/  0.584965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000328/  0.580632, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000309/  0.575047, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000305/  0.574291, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000289/  0.574054, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000283/  0.570647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000273/  0.573516, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000267/  0.571196, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000256/  0.573602, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000255/  0.571524, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000244/  0.572920, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000243/  0.571731, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000230/  0.576653, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000227/  0.581683, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000223/  0.578789, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000220/  0.585534, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000215/  0.589652, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000213/  0.599041, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000205/  0.593821, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000202/  0.600942, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000197/  0.595613, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000194/  0.596134, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000190/  0.605304, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000188/  0.603768, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000185/  0.604790, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000181/  0.605555, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000176/  0.606843, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000173/  0.604265, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000174/  0.601349, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000167/  0.607487, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000164/  0.614975, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617606, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000160/  0.618143, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000159/  0.620064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000155/  0.621546, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000152/  0.621520, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000150/  0.622190, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000149/  0.624970, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000144/  0.624786, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000141/  0.625244, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000142/  0.627416, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000136/  0.626923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000137/  0.628699, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000132/  0.626448, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000133/  0.632959, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000129/  0.628137, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000129/  0.630492, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000126/  0.628050, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000127/  0.631110, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000123/  0.633177, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000121/  0.631200, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000120/  0.634572, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629252, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000116/  0.632401, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000116/  0.629099, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000114/  0.628696, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000112/  0.629311, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000112/  0.635544, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000109/  0.638340, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000107/  0.635403, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000106/  0.636043, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000105/  0.633818, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000102/  0.633381, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f79d22bb404177a010b991ebdcbb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▃▁▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.63338</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-sweep-34</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cjst53br' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cjst53br</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_032516-cjst53br/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r30x24d1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_033726-r30x24d1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r30x24d1' target=\"_blank\">devoted-sweep-35</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r30x24d1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r30x24d1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  1.764123/  1.385394, val:  51.67%, val_best:  51.67%, tr:  42.22%, tr_best:  42.22%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.125955/  1.119293, val:  56.67%, val_best:  56.67%, tr:  60.57%, tr_best:  60.57%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  0.944841/  0.996583, val:  60.00%, val_best:  60.00%, tr:  63.77%, tr_best:  63.77%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.849029/  0.924471, val:  61.67%, val_best:  61.67%, tr:  66.21%, tr_best:  66.21%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.780262/  0.873163, val:  65.00%, val_best:  65.00%, tr:  68.08%, tr_best:  68.08%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.725763/  0.840073, val:  62.08%, val_best:  65.00%, tr:  69.48%, tr_best:  69.48%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.680362/  0.808936, val:  67.08%, val_best:  67.08%, tr:  71.08%, tr_best:  71.08%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.644459/  0.786507, val:  68.33%, val_best:  68.33%, tr:  72.61%, tr_best:  72.61%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.616356/  0.780380, val:  67.92%, val_best:  68.33%, tr:  73.51%, tr_best:  73.51%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.590159/  0.736123, val:  71.67%, val_best:  71.67%, tr:  74.55%, tr_best:  74.55%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.566320/  0.749283, val:  68.75%, val_best:  71.67%, tr:  75.43%, tr_best:  75.43%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.545722/  0.728404, val:  70.00%, val_best:  71.67%, tr:  76.71%, tr_best:  76.71%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.530857/  0.712043, val:  73.33%, val_best:  73.33%, tr:  76.89%, tr_best:  76.89%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.516643/  0.701595, val:  72.92%, val_best:  73.33%, tr:  78.00%, tr_best:  78.00%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.503055/  0.691556, val:  74.58%, val_best:  74.58%, tr:  78.72%, tr_best:  78.72%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.486517/  0.680192, val:  72.50%, val_best:  74.58%, tr:  79.60%, tr_best:  79.60%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.474751/  0.691459, val:  74.58%, val_best:  74.58%, tr:  79.91%, tr_best:  79.91%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.463799/  0.687584, val:  75.42%, val_best:  75.42%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.451590/  0.687054, val:  74.58%, val_best:  75.42%, tr:  81.85%, tr_best:  81.85%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.438198/  0.701842, val:  71.25%, val_best:  75.42%, tr:  81.99%, tr_best:  81.99%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.432640/  0.699386, val:  73.33%, val_best:  75.42%, tr:  83.00%, tr_best:  83.00%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.416378/  0.707756, val:  72.08%, val_best:  75.42%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.408285/  0.673192, val:  77.50%, val_best:  77.50%, tr:  84.22%, tr_best:  84.22%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.398096/  0.691341, val:  71.25%, val_best:  77.50%, tr:  85.12%, tr_best:  85.12%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.387372/  0.676775, val:  75.42%, val_best:  77.50%, tr:  85.12%, tr_best:  85.12%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.381399/  0.656480, val:  76.25%, val_best:  77.50%, tr:  85.91%, tr_best:  85.91%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.366867/  0.691332, val:  75.00%, val_best:  77.50%, tr:  86.97%, tr_best:  86.97%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.361607/  0.679755, val:  72.08%, val_best:  77.50%, tr:  86.11%, tr_best:  86.97%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.351899/  0.647503, val:  77.92%, val_best:  77.92%, tr:  87.24%, tr_best:  87.24%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.340363/  0.655278, val:  77.08%, val_best:  77.92%, tr:  88.57%, tr_best:  88.57%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.329946/  0.676693, val:  73.33%, val_best:  77.92%, tr:  88.75%, tr_best:  88.75%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.325060/  0.649182, val:  80.42%, val_best:  80.42%, tr:  88.86%, tr_best:  88.86%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.313849/  0.660645, val:  78.33%, val_best:  80.42%, tr:  89.65%, tr_best:  89.65%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.303392/  0.657480, val:  77.92%, val_best:  80.42%, tr:  90.42%, tr_best:  90.42%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.296402/  0.656598, val:  76.67%, val_best:  80.42%, tr:  90.89%, tr_best:  90.89%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.285832/  0.635870, val:  79.17%, val_best:  80.42%, tr:  91.05%, tr_best:  91.05%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.276443/  0.661124, val:  77.50%, val_best:  80.42%, tr:  91.70%, tr_best:  91.70%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.270164/  0.629360, val:  79.58%, val_best:  80.42%, tr:  91.93%, tr_best:  91.93%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.259333/  0.645917, val:  80.00%, val_best:  80.42%, tr:  92.67%, tr_best:  92.67%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.252910/  0.630786, val:  79.17%, val_best:  80.42%, tr:  92.54%, tr_best:  92.67%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.241536/  0.667521, val:  78.33%, val_best:  80.42%, tr:  93.30%, tr_best:  93.30%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.233748/  0.658335, val:  77.08%, val_best:  80.42%, tr:  93.71%, tr_best:  93.71%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.226010/  0.626628, val:  81.67%, val_best:  81.67%, tr:  94.03%, tr_best:  94.03%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.217136/  0.623471, val:  79.17%, val_best:  81.67%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.210816/  0.633408, val:  77.92%, val_best:  81.67%, tr:  94.77%, tr_best:  94.77%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.203083/  0.598649, val:  80.00%, val_best:  81.67%, tr:  95.15%, tr_best:  95.15%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.197044/  0.629079, val:  80.83%, val_best:  81.67%, tr:  94.95%, tr_best:  95.15%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.189860/  0.609651, val:  82.92%, val_best:  82.92%, tr:  95.22%, tr_best:  95.22%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.182825/  0.617777, val:  82.50%, val_best:  82.92%, tr:  95.31%, tr_best:  95.31%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.178246/  0.591941, val:  81.67%, val_best:  82.92%, tr:  95.54%, tr_best:  95.54%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.170297/  0.609705, val:  81.25%, val_best:  82.92%, tr:  96.06%, tr_best:  96.06%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.160823/  0.621006, val:  82.08%, val_best:  82.92%, tr:  96.44%, tr_best:  96.44%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.155439/  0.613398, val:  82.08%, val_best:  82.92%, tr:  96.33%, tr_best:  96.44%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.152328/  0.593237, val:  82.50%, val_best:  82.92%, tr:  96.44%, tr_best:  96.44%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.144687/  0.662653, val:  80.00%, val_best:  82.92%, tr:  96.98%, tr_best:  96.98%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.144561/  0.586946, val:  81.67%, val_best:  82.92%, tr:  96.55%, tr_best:  96.98%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.135362/  0.591007, val:  81.67%, val_best:  82.92%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.130790/  0.628748, val:  80.83%, val_best:  82.92%, tr:  97.23%, tr_best:  97.34%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.127355/  0.612846, val:  82.08%, val_best:  82.92%, tr:  97.32%, tr_best:  97.34%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.120900/  0.605305, val:  81.25%, val_best:  82.92%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.118249/  0.635399, val:  82.08%, val_best:  82.92%, tr:  97.52%, tr_best:  97.52%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.117236/  0.625111, val:  80.42%, val_best:  82.92%, tr:  97.59%, tr_best:  97.59%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.111196/  0.593603, val:  81.25%, val_best:  82.92%, tr:  97.79%, tr_best:  97.79%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.105183/  0.599817, val:  81.67%, val_best:  82.92%, tr:  98.11%, tr_best:  98.11%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.102539/  0.578447, val:  82.92%, val_best:  82.92%, tr:  97.99%, tr_best:  98.11%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.100330/  0.593761, val:  82.08%, val_best:  82.92%, tr:  98.11%, tr_best:  98.11%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.095746/  0.570314, val:  82.92%, val_best:  82.92%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.092431/  0.604512, val:  82.92%, val_best:  82.92%, tr:  98.38%, tr_best:  98.38%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.089458/  0.628019, val:  80.42%, val_best:  82.92%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.084749/  0.595252, val:  83.33%, val_best:  83.33%, tr:  98.67%, tr_best:  98.81%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.082291/  0.598046, val:  83.33%, val_best:  83.33%, tr:  98.76%, tr_best:  98.81%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.081868/  0.593810, val:  82.50%, val_best:  83.33%, tr:  98.56%, tr_best:  98.81%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.078542/  0.632802, val:  81.25%, val_best:  83.33%, tr:  98.83%, tr_best:  98.83%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.074777/  0.598977, val:  84.17%, val_best:  84.17%, tr:  98.78%, tr_best:  98.83%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.073591/  0.609072, val:  82.92%, val_best:  84.17%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.070954/  0.580155, val:  82.08%, val_best:  84.17%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.068055/  0.605159, val:  82.92%, val_best:  84.17%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.065388/  0.611204, val:  82.50%, val_best:  84.17%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.065472/  0.607431, val:  84.17%, val_best:  84.17%, tr:  99.17%, tr_best:  99.28%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.063513/  0.637142, val:  83.33%, val_best:  84.17%, tr:  99.21%, tr_best:  99.28%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.061900/  0.614103, val:  83.75%, val_best:  84.17%, tr:  99.10%, tr_best:  99.28%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.057791/  0.623934, val:  83.75%, val_best:  84.17%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.056252/  0.623729, val:  82.92%, val_best:  84.17%, tr:  99.39%, tr_best:  99.53%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.055102/  0.643825, val:  83.75%, val_best:  84.17%, tr:  99.41%, tr_best:  99.53%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.057140/  0.608157, val:  84.58%, val_best:  84.58%, tr:  99.32%, tr_best:  99.53%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.053078/  0.625343, val:  82.50%, val_best:  84.58%, tr:  99.46%, tr_best:  99.53%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.052305/  0.632419, val:  84.17%, val_best:  84.58%, tr:  99.48%, tr_best:  99.53%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.050796/  0.610124, val:  85.00%, val_best:  85.00%, tr:  99.48%, tr_best:  99.53%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.050130/  0.633009, val:  84.58%, val_best:  85.00%, tr:  99.48%, tr_best:  99.53%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.049311/  0.623659, val:  85.00%, val_best:  85.00%, tr:  99.48%, tr_best:  99.53%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.047443/  0.643923, val:  82.92%, val_best:  85.00%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.046645/  0.630487, val:  84.58%, val_best:  85.00%, tr:  99.66%, tr_best:  99.71%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.046255/  0.643608, val:  85.42%, val_best:  85.42%, tr:  99.55%, tr_best:  99.71%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.044142/  0.640649, val:  85.00%, val_best:  85.42%, tr:  99.53%, tr_best:  99.71%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.043307/  0.620741, val:  85.83%, val_best:  85.83%, tr:  99.55%, tr_best:  99.71%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.041279/  0.625781, val:  84.58%, val_best:  85.83%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.040574/  0.636266, val:  85.42%, val_best:  85.83%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.039414/  0.637161, val:  86.25%, val_best:  86.25%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.038401/  0.644271, val:  85.00%, val_best:  86.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.038685/  0.636118, val:  85.00%, val_best:  86.25%, tr:  99.59%, tr_best:  99.89%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e151efd49c6e4bad9cc0a0d4b5dfd2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▂▃▅▄▄▅▇▅▄▆▅██▇▆█▅██▇█▇████▇███████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99594</td></tr><tr><td>tr_epoch_loss</td><td>0.03869</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>0.63612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-sweep-35</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r30x24d1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r30x24d1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_033726-r30x24d1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dx6p7hbt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_034906-dx6p7hbt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dx6p7hbt' target=\"_blank\">smooth-sweep-36</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dx6p7hbt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dx6p7hbt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.037776/  0.930170, val:  62.50%, val_best:  62.50%, tr:  55.93%, tr_best:  55.93%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.644646/  0.844027, val:  62.50%, val_best:  62.50%, tr:  70.67%, tr_best:  70.67%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.476248/  0.855774, val:  64.58%, val_best:  64.58%, tr:  78.07%, tr_best:  78.07%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.384761/  0.675969, val:  80.00%, val_best:  80.00%, tr:  84.63%, tr_best:  84.63%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.263506/  0.601087, val:  81.67%, val_best:  81.67%, tr:  90.44%, tr_best:  90.44%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.208464/  0.441930, val:  87.08%, val_best:  87.08%, tr:  92.52%, tr_best:  92.52%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.155287/  0.590035, val:  84.58%, val_best:  87.08%, tr:  94.57%, tr_best:  94.57%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.109181/  0.595699, val:  82.92%, val_best:  87.08%, tr:  96.37%, tr_best:  96.37%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.085645/  0.750979, val:  80.00%, val_best:  87.08%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.081104/  0.645356, val:  86.67%, val_best:  87.08%, tr:  97.27%, tr_best:  97.27%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.070403/  0.768777, val:  82.92%, val_best:  87.08%, tr:  97.41%, tr_best:  97.41%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.063203/  0.436842, val:  90.42%, val_best:  90.42%, tr:  97.84%, tr_best:  97.84%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.034813/  0.606368, val:  87.50%, val_best:  90.42%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.036210/  0.516024, val:  89.58%, val_best:  90.42%, tr:  98.85%, tr_best:  98.85%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.024244/  0.521420, val:  90.42%, val_best:  90.42%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.007764/  0.627813, val:  87.50%, val_best:  90.42%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.005078/  0.564286, val:  89.17%, val_best:  90.42%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.002034/  0.549682, val:  90.00%, val_best:  90.42%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001445/  0.567676, val:  89.58%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001046/  0.594097, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.003572/  0.578040, val:  89.58%, val_best:  90.42%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001323/  0.584345, val:  89.17%, val_best:  90.42%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001123/  0.597893, val:  88.33%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.003799/  0.552060, val:  88.75%, val_best:  90.42%, tr:  99.89%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.001201/  0.588459, val:  89.58%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000738/  0.586292, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000721/  0.587484, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000528/  0.585276, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000459/  0.591276, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000406/  0.587271, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000381/  0.581558, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000348/  0.587082, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000328/  0.588417, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000302/  0.587143, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000283/  0.594263, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000277/  0.595318, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000259/  0.592325, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000247/  0.595532, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000239/  0.591111, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000232/  0.597013, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000225/  0.596054, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000220/  0.597119, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000214/  0.600250, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000206/  0.605200, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000198/  0.606904, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000196/  0.608519, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000188/  0.609860, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000185/  0.602235, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000176/  0.606022, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000173/  0.607701, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000168/  0.612831, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000168/  0.612293, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000160/  0.615480, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000157/  0.612061, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000154/  0.613909, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000154/  0.617168, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000149/  0.615734, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000144/  0.617231, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000145/  0.626945, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000140/  0.628535, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000138/  0.630313, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000138/  0.631986, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000138/  0.629152, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000134/  0.633238, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000127/  0.631316, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000129/  0.627720, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000124/  0.631410, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000122/  0.632043, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000123/  0.632195, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000119/  0.634575, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000116/  0.633461, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000116/  0.636532, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000113/  0.632683, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000111/  0.636943, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000111/  0.637690, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000109/  0.635897, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000108/  0.639189, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000103/  0.636012, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000103/  0.639341, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000104/  0.637690, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000101/  0.640688, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000098/  0.641160, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000096/  0.637073, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635762, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632583, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000093/  0.634897, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000091/  0.638292, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000091/  0.637393, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000089/  0.637972, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000088/  0.638176, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000087/  0.637715, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000086/  0.637850, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000086/  0.637618, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000084/  0.638260, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000083/  0.636662, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000082/  0.635711, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000080/  0.634299, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000079/  0.639961, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000078/  0.643699, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000077/  0.643450, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d57ac146c34171b7b7681b723bba4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▃█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.64345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-36</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dx6p7hbt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/dx6p7hbt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_034906-dx6p7hbt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4d4s2y39 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_040027-4d4s2y39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4d4s2y39' target=\"_blank\">dashing-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4d4s2y39' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4d4s2y39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=1, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.311415/  2.337775, val:  10.00%, val_best:  10.00%, tr:  11.14%, tr_best:  11.14%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  1.901654/  1.170245, val:  56.25%, val_best:  56.25%, tr:  26.10%, tr_best:  26.10%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.769288/  0.792875, val:  63.75%, val_best:  63.75%, tr:  66.14%, tr_best:  66.14%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.611073/  0.737478, val:  67.92%, val_best:  67.92%, tr:  70.45%, tr_best:  70.45%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.510640/  0.783059, val:  70.00%, val_best:  70.00%, tr:  76.01%, tr_best:  76.01%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.395243/  0.629763, val:  79.58%, val_best:  79.58%, tr:  83.07%, tr_best:  83.07%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.293516/  0.598860, val:  84.17%, val_best:  84.17%, tr:  88.98%, tr_best:  88.98%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.206648/  0.885391, val:  71.25%, val_best:  84.17%, tr:  93.01%, tr_best:  93.01%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.201309/  0.492048, val:  80.83%, val_best:  84.17%, tr:  93.08%, tr_best:  93.08%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.122836/  0.489285, val:  86.67%, val_best:  86.67%, tr:  95.92%, tr_best:  95.92%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.105084/  0.397654, val:  86.25%, val_best:  86.67%, tr:  96.96%, tr_best:  96.96%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.091494/  0.530519, val:  85.00%, val_best:  86.67%, tr:  97.16%, tr_best:  97.16%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.053673/  0.451620, val:  89.17%, val_best:  89.17%, tr:  98.44%, tr_best:  98.44%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.051586/  0.503425, val:  87.92%, val_best:  89.17%, tr:  98.33%, tr_best:  98.44%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.042347/  0.441418, val:  86.25%, val_best:  89.17%, tr:  98.85%, tr_best:  98.85%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.021298/  0.338047, val:  91.25%, val_best:  91.25%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.019778/  0.457304, val:  90.00%, val_best:  91.25%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.012784/  0.415422, val:  87.08%, val_best:  91.25%, tr:  99.64%, tr_best:  99.64%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.009440/  0.405899, val:  88.75%, val_best:  91.25%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.004636/  0.474772, val:  88.33%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.002702/  0.439829, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001987/  0.478907, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001857/  0.493528, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.001488/  0.490246, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.001309/  0.481770, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.001188/  0.511207, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.001109/  0.499040, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.001044/  0.513212, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000967/  0.510912, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000942/  0.516086, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000846/  0.517015, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000846/  0.534453, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000788/  0.528849, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000745/  0.546104, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000715/  0.552616, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000669/  0.546757, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000637/  0.545434, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000609/  0.538337, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000583/  0.539713, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000564/  0.552580, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000539/  0.550693, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000507/  0.556906, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000488/  0.566868, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000480/  0.561826, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000464/  0.549151, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000443/  0.564335, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000429/  0.568042, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000416/  0.573399, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000407/  0.575506, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000391/  0.581563, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000383/  0.579526, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000372/  0.576146, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000359/  0.572549, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000358/  0.590321, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000342/  0.595250, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000338/  0.590537, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000334/  0.593483, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000319/  0.577848, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000311/  0.574995, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000307/  0.579682, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000301/  0.574150, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000297/  0.583822, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000290/  0.579163, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000280/  0.572052, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000273/  0.571297, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000267/  0.570793, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000266/  0.569705, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000257/  0.575665, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000251/  0.575247, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000251/  0.581486, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000247/  0.582580, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000242/  0.587261, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000238/  0.586294, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000236/  0.594542, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000230/  0.590927, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000230/  0.598913, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000222/  0.596591, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000222/  0.596068, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000213/  0.594959, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000216/  0.595601, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000206/  0.591940, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000203/  0.591701, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000202/  0.600555, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000200/  0.598834, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000193/  0.594632, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000189/  0.586884, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000191/  0.584662, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000189/  0.590651, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000184/  0.590272, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000185/  0.590381, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000184/  0.591972, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000176/  0.592384, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000176/  0.591918, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000172/  0.587885, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000171/  0.597510, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000170/  0.596926, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000164/  0.595879, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000165/  0.597238, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000161/  0.599291, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000159/  0.591212, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54b7ec6528a49368569a963b2879c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▇▇█▇▇▇█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▆█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▄▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00016</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.59121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-37</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4d4s2y39' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4d4s2y39</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_040027-4d4s2y39/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gr1gc2zb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_041226-gr1gc2zb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gr1gc2zb' target=\"_blank\">lemon-sweep-38</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gr1gc2zb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gr1gc2zb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.216649/  0.823063, val:  65.00%, val_best:  65.00%, tr:  52.01%, tr_best:  52.01%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.654083/  0.732902, val:  67.50%, val_best:  67.50%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.544799/  0.648881, val:  73.75%, val_best:  73.75%, tr:  73.67%, tr_best:  73.67%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.482453/  0.662840, val:  73.75%, val_best:  73.75%, tr:  78.07%, tr_best:  78.07%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.422023/  0.693348, val:  72.92%, val_best:  73.75%, tr:  81.58%, tr_best:  81.58%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.366986/  0.612835, val:  77.92%, val_best:  77.92%, tr:  84.45%, tr_best:  84.45%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.319570/  0.611406, val:  77.92%, val_best:  77.92%, tr:  87.58%, tr_best:  87.58%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.254660/  0.600702, val:  78.75%, val_best:  78.75%, tr:  91.16%, tr_best:  91.16%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.220731/  0.581712, val:  80.83%, val_best:  80.83%, tr:  91.50%, tr_best:  91.50%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.165866/  0.548842, val:  83.33%, val_best:  83.33%, tr:  95.13%, tr_best:  95.13%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.138979/  0.512631, val:  85.42%, val_best:  85.42%, tr:  95.83%, tr_best:  95.83%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.115500/  0.572394, val:  84.58%, val_best:  85.42%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.092573/  0.551637, val:  83.33%, val_best:  85.42%, tr:  97.39%, tr_best:  97.39%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.082891/  0.538529, val:  85.83%, val_best:  85.83%, tr:  97.50%, tr_best:  97.50%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.084397/  0.522106, val:  86.25%, val_best:  86.25%, tr:  97.20%, tr_best:  97.50%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.050127/  0.502272, val:  87.92%, val_best:  87.92%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.042786/  0.521616, val:  85.83%, val_best:  87.92%, tr:  99.19%, tr_best:  99.19%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.043000/  0.496938, val:  86.25%, val_best:  87.92%, tr:  99.12%, tr_best:  99.19%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.038647/  0.506369, val:  86.67%, val_best:  87.92%, tr:  99.10%, tr_best:  99.19%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.027889/  0.529019, val:  88.75%, val_best:  88.75%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.027482/  0.532485, val:  87.50%, val_best:  88.75%, tr:  99.46%, tr_best:  99.48%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.025527/  0.565435, val:  86.25%, val_best:  88.75%, tr:  99.37%, tr_best:  99.48%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.015870/  0.539317, val:  87.92%, val_best:  88.75%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.016221/  0.537559, val:  88.33%, val_best:  88.75%, tr:  99.80%, tr_best:  99.82%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.017573/  0.548694, val:  87.92%, val_best:  88.75%, tr:  99.68%, tr_best:  99.82%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.012747/  0.561575, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best:  99.82%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.011695/  0.550304, val:  89.17%, val_best:  89.17%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.008825/  0.547574, val:  88.33%, val_best:  89.17%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.008398/  0.520404, val:  87.08%, val_best:  89.17%, tr:  99.93%, tr_best:  99.95%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.007284/  0.549287, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.006737/  0.562395, val:  86.67%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.006164/  0.562573, val:  86.67%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.006040/  0.545423, val:  88.75%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.005549/  0.552726, val:  87.50%, val_best:  89.17%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.004908/  0.554999, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.004241/  0.582023, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.003927/  0.588517, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.003785/  0.557861, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.003686/  0.572069, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.003555/  0.545059, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.003375/  0.568464, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.003056/  0.561794, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.003000/  0.560983, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.002868/  0.567940, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.002765/  0.580873, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.002652/  0.577474, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.002631/  0.588134, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.002426/  0.575996, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.002377/  0.584464, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.002326/  0.586578, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.002270/  0.597934, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002185/  0.592803, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002091/  0.600042, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002090/  0.586445, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001985/  0.602157, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001942/  0.603438, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.001926/  0.603175, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.001827/  0.596684, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001801/  0.600542, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001733/  0.604565, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001680/  0.617713, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001651/  0.604209, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001653/  0.601992, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001578/  0.615203, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001553/  0.609432, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001503/  0.615433, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001488/  0.614768, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001417/  0.616782, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001423/  0.623082, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001379/  0.619741, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001346/  0.600276, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001330/  0.620227, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001303/  0.623376, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001267/  0.626467, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001236/  0.613326, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001257/  0.613650, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001199/  0.627966, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001199/  0.635332, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001203/  0.625795, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001163/  0.612397, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001136/  0.624274, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001122/  0.619046, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001084/  0.608591, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001072/  0.619661, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001058/  0.634930, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001036/  0.629401, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001019/  0.621623, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001002/  0.616075, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000988/  0.625516, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000996/  0.617547, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000963/  0.630186, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000960/  0.622909, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000955/  0.629719, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000936/  0.632145, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000942/  0.624683, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000904/  0.634403, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000907/  0.622231, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000894/  0.627954, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000874/  0.638550, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000849/  0.639657, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c1aa0b7b3d45ab9052428b0da13331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆█▆█▆█▆███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁██▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00085</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>0.63966</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-38</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gr1gc2zb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gr1gc2zb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_041226-gr1gc2zb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gbx1nqq1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_042431-gbx1nqq1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbx1nqq1' target=\"_blank\">amber-sweep-39</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbx1nqq1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbx1nqq1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.955144/  0.953731, val:  59.58%, val_best:  59.58%, tr:  60.03%, tr_best:  60.03%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.615838/  0.812181, val:  64.17%, val_best:  64.17%, tr:  71.51%, tr_best:  71.51%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.443413/  0.720790, val:  74.58%, val_best:  74.58%, tr:  81.13%, tr_best:  81.13%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.320028/  0.522215, val:  88.33%, val_best:  88.33%, tr:  87.74%, tr_best:  87.74%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.168445/  0.506562, val:  83.75%, val_best:  88.33%, tr:  94.39%, tr_best:  94.39%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.120168/  0.584831, val:  87.92%, val_best:  88.33%, tr:  95.94%, tr_best:  95.94%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.107285/  0.523794, val:  86.25%, val_best:  88.33%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.077238/  0.548015, val:  85.83%, val_best:  88.33%, tr:  97.50%, tr_best:  97.50%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.045438/  0.596055, val:  87.92%, val_best:  88.33%, tr:  98.65%, tr_best:  98.65%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.035971/  0.542101, val:  87.50%, val_best:  88.33%, tr:  98.78%, tr_best:  98.78%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.034351/  0.495694, val:  88.33%, val_best:  88.33%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.023680/  0.623505, val:  87.08%, val_best:  88.33%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.015138/  0.489443, val:  88.75%, val_best:  88.75%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.005412/  0.626974, val:  87.08%, val_best:  88.75%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.004998/  0.539013, val:  90.00%, val_best:  90.00%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001777/  0.554868, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001325/  0.546626, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001162/  0.550136, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000998/  0.553935, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000893/  0.575682, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000763/  0.565261, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000670/  0.566228, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000634/  0.567609, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000628/  0.567912, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000545/  0.586680, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000530/  0.585754, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000495/  0.590361, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000465/  0.585923, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000444/  0.596888, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000424/  0.594091, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000403/  0.590965, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000385/  0.598257, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000363/  0.601078, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000355/  0.608163, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000344/  0.611620, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000330/  0.616586, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000320/  0.616613, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000302/  0.621575, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000294/  0.624175, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000285/  0.625041, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000277/  0.627822, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000263/  0.629101, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000256/  0.631656, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000252/  0.640227, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000243/  0.635431, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000240/  0.626836, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000230/  0.629776, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000227/  0.631084, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000215/  0.631746, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000217/  0.630285, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000207/  0.632396, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000204/  0.637256, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000199/  0.633162, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000196/  0.633785, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000191/  0.633198, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000185/  0.633945, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000186/  0.637962, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000178/  0.639658, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000175/  0.646445, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000170/  0.643978, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000168/  0.646862, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000164/  0.650017, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000165/  0.652548, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000157/  0.657487, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000157/  0.654522, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000154/  0.655970, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000152/  0.655956, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000150/  0.657812, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000147/  0.655668, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000142/  0.659925, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000142/  0.666160, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000136/  0.664822, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000134/  0.664930, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000133/  0.666883, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000132/  0.668963, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000128/  0.668214, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000125/  0.667373, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000123/  0.666054, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.665353, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000122/  0.669234, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000118/  0.671358, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000117/  0.672646, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.681982, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000114/  0.684069, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000114/  0.683080, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000111/  0.676962, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000110/  0.676565, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000109/  0.679852, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000108/  0.676830, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.680211, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000104/  0.681280, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000103/  0.684070, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000103/  0.685299, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000101/  0.686881, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000101/  0.684797, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000098/  0.680254, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000099/  0.682217, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000097/  0.686295, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000095/  0.686866, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000094/  0.683607, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45af0244efdd4bd3a1ee23d4d746c3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.68361</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-39</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbx1nqq1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbx1nqq1</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_042431-gbx1nqq1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u96gjxsm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_043703-u96gjxsm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u96gjxsm' target=\"_blank\">golden-sweep-40</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u96gjxsm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u96gjxsm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.939514/  1.028554, val:  55.00%, val_best:  55.00%, tr:  59.69%, tr_best:  59.69%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.646140/  0.827309, val:  61.67%, val_best:  61.67%, tr:  70.33%, tr_best:  70.33%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.476885/  0.773788, val:  71.67%, val_best:  71.67%, tr:  78.49%, tr_best:  78.49%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.375719/  0.608161, val:  82.50%, val_best:  82.50%, tr:  84.76%, tr_best:  84.76%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.247399/  0.546215, val:  81.25%, val_best:  82.50%, tr:  90.71%, tr_best:  90.71%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.175273/  0.507187, val:  88.75%, val_best:  88.75%, tr:  93.89%, tr_best:  93.89%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.147925/  0.593087, val:  85.42%, val_best:  88.75%, tr:  94.82%, tr_best:  94.82%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.108920/  0.517451, val:  87.92%, val_best:  88.75%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.064577/  0.503652, val:  88.33%, val_best:  88.75%, tr:  98.04%, tr_best:  98.04%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.056149/  0.795677, val:  86.67%, val_best:  88.75%, tr:  98.17%, tr_best:  98.17%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.056086/  0.514786, val:  88.33%, val_best:  88.75%, tr:  98.15%, tr_best:  98.17%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.043433/  0.496530, val:  86.67%, val_best:  88.75%, tr:  98.42%, tr_best:  98.42%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.025369/  0.812013, val:  82.50%, val_best:  88.75%, tr:  99.14%, tr_best:  99.14%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.013728/  0.547490, val:  88.33%, val_best:  88.75%, tr:  99.68%, tr_best:  99.68%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.006006/  0.512758, val:  88.75%, val_best:  88.75%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.008749/  0.556425, val:  88.33%, val_best:  88.75%, tr:  99.80%, tr_best:  99.86%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.006876/  0.552079, val:  88.75%, val_best:  88.75%, tr:  99.84%, tr_best:  99.86%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.003101/  0.531228, val:  90.00%, val_best:  90.00%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.002852/  0.544350, val:  89.17%, val_best:  90.00%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001327/  0.568229, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001423/  0.555559, val:  89.58%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001433/  0.551334, val:  89.58%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001014/  0.571221, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000912/  0.559984, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000707/  0.573529, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000679/  0.560299, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000675/  0.571894, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000592/  0.560870, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000549/  0.563663, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000514/  0.568323, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000471/  0.559700, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000448/  0.569300, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000429/  0.579324, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000411/  0.583704, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000398/  0.589954, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000376/  0.589329, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000363/  0.589790, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000346/  0.588536, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000340/  0.583596, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000317/  0.586528, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000310/  0.586153, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000296/  0.585137, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000292/  0.586400, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000280/  0.594073, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000275/  0.588410, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000266/  0.588382, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000258/  0.587138, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000253/  0.592016, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000243/  0.585034, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000241/  0.587004, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000233/  0.589042, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000225/  0.589010, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000222/  0.591760, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000216/  0.592433, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000213/  0.591048, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000207/  0.593165, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000205/  0.607397, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000198/  0.606098, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000196/  0.603438, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000192/  0.606917, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000186/  0.613154, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000183/  0.609303, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000187/  0.611086, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000178/  0.610390, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000174/  0.613352, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000170/  0.602642, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000169/  0.607168, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000165/  0.607100, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000163/  0.611758, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000162/  0.612086, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000158/  0.616321, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000157/  0.618329, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000153/  0.626336, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000149/  0.622980, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000147/  0.621765, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000144/  0.623981, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000143/  0.626638, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000140/  0.633007, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000139/  0.630793, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000137/  0.628169, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000132/  0.639942, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000131/  0.636408, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000130/  0.638103, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000128/  0.640331, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000128/  0.644514, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000126/  0.643690, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000124/  0.650084, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000123/  0.649321, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000123/  0.647993, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000120/  0.646472, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000119/  0.647517, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000117/  0.646092, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000116/  0.642216, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000113/  0.643494, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000113/  0.639946, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000111/  0.638118, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000111/  0.639691, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000110/  0.644765, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000108/  0.647440, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000107/  0.649112, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1e366373794c0a9efd79cf95389e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅█▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>0.64911</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-sweep-40</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u96gjxsm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/u96gjxsm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_043703-u96gjxsm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8a0pwj4e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_044924-8a0pwj4e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8a0pwj4e' target=\"_blank\">jumping-sweep-41</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8a0pwj4e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8a0pwj4e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.940318/  0.855659, val:  62.50%, val_best:  62.50%, tr:  58.95%, tr_best:  58.95%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.640562/  0.916296, val:  60.83%, val_best:  62.50%, tr:  71.39%, tr_best:  71.39%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.486507/  0.797004, val:  68.33%, val_best:  68.33%, tr:  78.27%, tr_best:  78.27%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.387063/  0.719964, val:  76.25%, val_best:  76.25%, tr:  84.29%, tr_best:  84.29%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.265506/  0.603420, val:  82.92%, val_best:  82.92%, tr:  89.81%, tr_best:  89.81%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.187753/  0.609204, val:  83.33%, val_best:  83.33%, tr:  93.64%, tr_best:  93.64%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.146718/  0.658002, val:  83.75%, val_best:  83.75%, tr:  94.70%, tr_best:  94.70%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.123635/  0.685836, val:  81.67%, val_best:  83.75%, tr:  95.72%, tr_best:  95.72%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.121428/  0.528876, val:  85.83%, val_best:  85.83%, tr:  95.87%, tr_best:  95.87%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.084363/  0.581953, val:  83.33%, val_best:  85.83%, tr:  96.96%, tr_best:  96.96%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.059175/  0.552801, val:  87.08%, val_best:  87.08%, tr:  97.86%, tr_best:  97.86%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.058663/  0.603029, val:  87.50%, val_best:  87.50%, tr:  98.17%, tr_best:  98.17%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.075313/  0.634735, val:  85.83%, val_best:  87.50%, tr:  97.39%, tr_best:  98.17%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.049602/  0.648407, val:  85.42%, val_best:  87.50%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.031842/  0.592778, val:  88.75%, val_best:  88.75%, tr:  99.03%, tr_best:  99.03%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.020720/  0.670730, val:  85.83%, val_best:  88.75%, tr:  99.46%, tr_best:  99.46%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.047721/  0.613853, val:  88.33%, val_best:  88.75%, tr:  98.56%, tr_best:  99.46%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.023427/  0.700412, val:  86.25%, val_best:  88.75%, tr:  99.19%, tr_best:  99.46%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.022205/  0.464530, val:  90.00%, val_best:  90.00%, tr:  99.37%, tr_best:  99.46%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.019517/  0.647860, val:  87.92%, val_best:  90.00%, tr:  99.32%, tr_best:  99.46%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.018154/  0.672467, val:  85.83%, val_best:  90.00%, tr:  99.46%, tr_best:  99.46%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.025473/  0.586360, val:  86.67%, val_best:  90.00%, tr:  99.19%, tr_best:  99.46%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.036649/  0.551404, val:  88.33%, val_best:  90.00%, tr:  98.87%, tr_best:  99.46%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.022970/  0.559539, val:  88.75%, val_best:  90.00%, tr:  99.30%, tr_best:  99.46%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.017860/  0.556977, val:  91.67%, val_best:  91.67%, tr:  99.39%, tr_best:  99.46%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.013413/  0.604545, val:  87.08%, val_best:  91.67%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.003922/  0.553985, val:  89.17%, val_best:  91.67%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.003692/  0.620480, val:  88.75%, val_best:  91.67%, tr:  99.91%, tr_best:  99.98%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.003109/  0.646945, val:  87.50%, val_best:  91.67%, tr:  99.93%, tr_best:  99.98%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.020035/  0.632882, val:  89.58%, val_best:  91.67%, tr:  99.23%, tr_best:  99.98%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.008130/  0.607982, val:  87.50%, val_best:  91.67%, tr:  99.80%, tr_best:  99.98%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.029355/  0.510168, val:  88.75%, val_best:  91.67%, tr:  99.17%, tr_best:  99.98%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.087289/  0.483523, val:  86.25%, val_best:  91.67%, tr:  96.93%, tr_best:  99.98%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.052267/  0.570112, val:  86.67%, val_best:  91.67%, tr:  98.35%, tr_best:  99.98%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.023532/  0.531131, val:  90.00%, val_best:  91.67%, tr:  99.30%, tr_best:  99.98%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.019280/  0.463124, val:  88.33%, val_best:  91.67%, tr:  99.30%, tr_best:  99.98%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.012812/  0.539946, val:  87.08%, val_best:  91.67%, tr:  99.73%, tr_best:  99.98%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.005114/  0.475540, val:  89.58%, val_best:  91.67%, tr:  99.93%, tr_best:  99.98%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.001259/  0.489909, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000845/  0.497020, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000682/  0.509065, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000598/  0.497632, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000533/  0.501678, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000472/  0.508349, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000435/  0.501591, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000412/  0.509900, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000383/  0.517314, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000363/  0.533565, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000336/  0.540578, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000316/  0.532589, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000307/  0.529278, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000294/  0.531974, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000280/  0.521495, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000269/  0.522992, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000257/  0.530375, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000251/  0.533096, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000240/  0.533786, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000233/  0.533673, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000226/  0.538808, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000219/  0.534844, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000212/  0.538477, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000204/  0.538225, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000204/  0.544650, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000194/  0.549645, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000190/  0.542045, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000185/  0.539724, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000182/  0.545917, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000177/  0.539075, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000176/  0.538210, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000169/  0.541166, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000167/  0.538631, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000161/  0.541119, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000159/  0.538798, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000154/  0.538670, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000152/  0.536232, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000148/  0.544866, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000146/  0.538615, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000144/  0.538519, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000143/  0.540331, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000142/  0.533175, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000136/  0.538840, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000136/  0.540832, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000131/  0.543735, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000130/  0.540594, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000129/  0.542892, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000126/  0.538690, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000124/  0.538549, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000122/  0.542595, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000121/  0.540101, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000119/  0.540714, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000117/  0.539540, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000114/  0.541107, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000113/  0.543125, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000112/  0.543887, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000111/  0.546030, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000108/  0.544196, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000108/  0.540555, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000106/  0.541160, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000104/  0.542303, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000103/  0.540731, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa85fe856554d24b17f86851d15e4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁███▅███▆███▆██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▆▆▆▆▅▅▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.90417</td></tr><tr><td>val_loss</td><td>0.54073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-sweep-41</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8a0pwj4e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8a0pwj4e</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_044924-8a0pwj4e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j665n0po with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_050156-j665n0po</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j665n0po' target=\"_blank\">fast-sweep-42</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j665n0po' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j665n0po</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.925937/  0.962100, val:  58.75%, val_best:  58.75%, tr:  60.80%, tr_best:  60.80%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.620033/  0.796098, val:  66.67%, val_best:  66.67%, tr:  71.82%, tr_best:  71.82%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411212/  0.680588, val:  78.33%, val_best:  78.33%, tr:  83.23%, tr_best:  83.23%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.281519/  0.453874, val:  87.50%, val_best:  87.50%, tr:  89.50%, tr_best:  89.50%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.173730/  0.438690, val:  89.17%, val_best:  89.17%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.131084/  0.379213, val:  89.58%, val_best:  89.58%, tr:  95.60%, tr_best:  95.60%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.086907/  0.452013, val:  89.17%, val_best:  89.58%, tr:  97.11%, tr_best:  97.11%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.067697/  0.475806, val:  90.00%, val_best:  90.00%, tr:  97.88%, tr_best:  97.88%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036230/  0.564496, val:  87.08%, val_best:  90.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.027988/  0.473823, val:  88.75%, val_best:  90.00%, tr:  98.96%, tr_best:  99.08%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.022219/  0.579324, val:  88.33%, val_best:  90.00%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.010694/  0.597973, val:  87.50%, val_best:  90.00%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003942/  0.574832, val:  88.75%, val_best:  90.00%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002230/  0.577591, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001721/  0.549777, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001340/  0.551486, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001115/  0.559681, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000964/  0.557628, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000855/  0.562009, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000767/  0.586662, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000701/  0.575527, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000667/  0.573591, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000630/  0.574261, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000584/  0.575940, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000550/  0.584995, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000510/  0.581284, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000499/  0.586610, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000466/  0.591101, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000446/  0.584309, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000426/  0.595009, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000420/  0.584428, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000397/  0.586886, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000375/  0.589818, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000359/  0.596350, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000352/  0.605685, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000336/  0.604453, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000324/  0.607680, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000315/  0.602724, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000308/  0.608555, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000296/  0.611728, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000282/  0.606344, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000274/  0.607052, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000268/  0.609053, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000258/  0.606337, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000256/  0.604765, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000248/  0.603075, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000243/  0.603179, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000235/  0.607299, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000234/  0.605558, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000228/  0.603638, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000222/  0.606064, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000216/  0.601828, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000211/  0.605405, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000206/  0.601419, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000200/  0.601284, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000196/  0.609681, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000192/  0.613942, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000188/  0.610085, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000185/  0.609123, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000178/  0.608805, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000177/  0.610920, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000172/  0.605248, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000176/  0.613553, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614912, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614062, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000161/  0.612927, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000162/  0.615787, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000156/  0.617892, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000153/  0.615856, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000151/  0.620698, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000147/  0.627239, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000146/  0.630415, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000144/  0.633122, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000143/  0.638761, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000140/  0.647128, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000138/  0.649175, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000135/  0.653331, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000135/  0.653878, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000133/  0.657584, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000132/  0.654067, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.654005, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000126/  0.653358, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000125/  0.655715, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000122/  0.651898, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000120/  0.655880, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000120/  0.657423, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000117/  0.653922, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000116/  0.655184, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000114/  0.653668, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000112/  0.658471, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000110/  0.661146, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000111/  0.662841, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.660637, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.660053, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.660780, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000105/  0.662403, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000103/  0.665917, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000103/  0.668271, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000101/  0.670042, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.668199, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca7b689ec544da389fe59de18019615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.6682</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-sweep-42</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j665n0po' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j665n0po</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_050156-j665n0po/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iuf006t3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_051341-iuf006t3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iuf006t3' target=\"_blank\">worldly-sweep-43</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iuf006t3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iuf006t3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.981943/  0.892993, val:  60.00%, val_best:  60.00%, tr:  57.78%, tr_best:  57.78%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.696266/  0.935377, val:  56.67%, val_best:  60.00%, tr:  68.06%, tr_best:  68.06%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.558763/  0.958767, val:  66.25%, val_best:  66.25%, tr:  72.72%, tr_best:  72.72%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.509517/  0.831245, val:  64.17%, val_best:  66.25%, tr:  75.92%, tr_best:  75.92%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.449646/  0.810540, val:  70.00%, val_best:  70.00%, tr:  79.69%, tr_best:  79.69%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.419794/  0.769037, val:  72.50%, val_best:  72.50%, tr:  80.79%, tr_best:  80.79%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.374418/  0.908083, val:  70.83%, val_best:  72.50%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.310214/  0.871396, val:  72.50%, val_best:  72.50%, tr:  87.58%, tr_best:  87.58%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.312826/  0.883149, val:  70.83%, val_best:  72.50%, tr:  87.51%, tr_best:  87.58%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.348061/  0.682835, val:  78.75%, val_best:  78.75%, tr:  85.75%, tr_best:  87.58%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.278359/  0.812161, val:  77.50%, val_best:  78.75%, tr:  89.54%, tr_best:  89.54%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.246628/  0.635296, val:  78.33%, val_best:  78.75%, tr:  90.33%, tr_best:  90.33%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.209201/  0.703218, val:  78.75%, val_best:  78.75%, tr:  91.84%, tr_best:  91.84%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.192189/  0.479127, val:  85.42%, val_best:  85.42%, tr:  92.92%, tr_best:  92.92%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.202751/  0.626534, val:  80.83%, val_best:  85.42%, tr:  92.45%, tr_best:  92.92%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.218078/  0.733008, val:  83.33%, val_best:  85.42%, tr:  91.97%, tr_best:  92.92%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.204568/  0.622620, val:  79.58%, val_best:  85.42%, tr:  92.40%, tr_best:  92.92%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.204567/  0.753108, val:  80.00%, val_best:  85.42%, tr:  92.18%, tr_best:  92.92%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.154855/  0.697135, val:  85.42%, val_best:  85.42%, tr:  94.43%, tr_best:  94.43%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.157874/  0.723641, val:  81.67%, val_best:  85.42%, tr:  94.32%, tr_best:  94.43%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.162084/  0.659315, val:  82.08%, val_best:  85.42%, tr:  93.96%, tr_best:  94.43%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.142269/  0.741596, val:  82.92%, val_best:  85.42%, tr:  95.33%, tr_best:  95.33%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.152780/  0.681179, val:  83.33%, val_best:  85.42%, tr:  94.50%, tr_best:  95.33%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.111217/  0.728096, val:  82.50%, val_best:  85.42%, tr:  96.03%, tr_best:  96.03%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.094955/  0.944064, val:  77.92%, val_best:  85.42%, tr:  96.84%, tr_best:  96.84%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.120317/  1.017576, val:  75.83%, val_best:  85.42%, tr:  95.54%, tr_best:  96.84%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.163596/  0.875391, val:  80.42%, val_best:  85.42%, tr:  94.09%, tr_best:  96.84%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.104610/  0.768640, val:  79.17%, val_best:  85.42%, tr:  96.53%, tr_best:  96.84%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.071998/  0.779827, val:  83.75%, val_best:  85.42%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.115176/  0.786363, val:  83.33%, val_best:  85.42%, tr:  95.76%, tr_best:  97.45%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.112058/  1.163768, val:  76.25%, val_best:  85.42%, tr:  95.94%, tr_best:  97.45%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.152758/  0.907466, val:  80.42%, val_best:  85.42%, tr:  94.30%, tr_best:  97.45%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.150314/  0.862032, val:  77.92%, val_best:  85.42%, tr:  94.41%, tr_best:  97.45%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.091533/  0.828295, val:  80.83%, val_best:  85.42%, tr:  96.48%, tr_best:  97.45%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.090874/  1.022558, val:  79.17%, val_best:  85.42%, tr:  96.87%, tr_best:  97.45%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.121429/  0.906779, val:  82.08%, val_best:  85.42%, tr:  96.12%, tr_best:  97.45%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.074094/  0.844846, val:  80.42%, val_best:  85.42%, tr:  97.25%, tr_best:  97.45%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.087394/  0.878516, val:  80.42%, val_best:  85.42%, tr:  97.16%, tr_best:  97.45%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.109606/  0.847801, val:  81.67%, val_best:  85.42%, tr:  96.08%, tr_best:  97.45%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.113250/  0.801398, val:  80.42%, val_best:  85.42%, tr:  96.17%, tr_best:  97.45%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.100530/  0.737109, val:  82.50%, val_best:  85.42%, tr:  96.57%, tr_best:  97.45%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.078212/  1.103107, val:  77.50%, val_best:  85.42%, tr:  97.50%, tr_best:  97.50%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.091035/  0.807671, val:  79.58%, val_best:  85.42%, tr:  96.73%, tr_best:  97.50%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.102638/  0.817032, val:  80.00%, val_best:  85.42%, tr:  96.35%, tr_best:  97.50%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.141529/  0.807128, val:  80.00%, val_best:  85.42%, tr:  95.00%, tr_best:  97.50%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.096729/  0.768766, val:  82.08%, val_best:  85.42%, tr:  96.48%, tr_best:  97.50%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.105234/  0.963404, val:  80.42%, val_best:  85.42%, tr:  96.33%, tr_best:  97.50%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.074912/  0.670052, val:  84.17%, val_best:  85.42%, tr:  97.32%, tr_best:  97.50%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.051014/  0.780523, val:  83.33%, val_best:  85.42%, tr:  98.38%, tr_best:  98.38%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.037658/  0.791724, val:  81.67%, val_best:  85.42%, tr:  98.65%, tr_best:  98.65%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.052869/  0.787009, val:  82.08%, val_best:  85.42%, tr:  98.24%, tr_best:  98.65%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.052488/  0.770851, val:  83.33%, val_best:  85.42%, tr:  98.20%, tr_best:  98.65%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.033021/  0.987885, val:  82.92%, val_best:  85.42%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.053870/  0.852207, val:  82.08%, val_best:  85.42%, tr:  97.77%, tr_best:  99.05%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.047534/  0.945616, val:  79.58%, val_best:  85.42%, tr:  98.29%, tr_best:  99.05%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.041113/  0.791494, val:  83.75%, val_best:  85.42%, tr:  98.53%, tr_best:  99.05%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.053666/  0.848142, val:  82.50%, val_best:  85.42%, tr:  98.22%, tr_best:  99.05%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.040170/  0.844309, val:  83.75%, val_best:  85.42%, tr:  98.42%, tr_best:  99.05%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.033372/  0.954615, val:  82.08%, val_best:  85.42%, tr:  98.99%, tr_best:  99.05%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.046579/  0.980308, val:  81.25%, val_best:  85.42%, tr:  98.26%, tr_best:  99.05%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.071531/  0.978744, val:  82.50%, val_best:  85.42%, tr:  97.63%, tr_best:  99.05%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.066686/  0.716437, val:  85.42%, val_best:  85.42%, tr:  97.63%, tr_best:  99.05%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.027960/  0.826975, val:  85.83%, val_best:  85.83%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.033988/  0.874508, val:  83.75%, val_best:  85.83%, tr:  98.83%, tr_best:  99.12%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.023231/  0.899276, val:  81.67%, val_best:  85.83%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.044624/  0.857854, val:  84.17%, val_best:  85.83%, tr:  98.42%, tr_best:  99.28%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.028212/  0.845636, val:  85.42%, val_best:  85.83%, tr:  99.03%, tr_best:  99.28%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.040831/  0.861804, val:  82.92%, val_best:  85.83%, tr:  98.56%, tr_best:  99.28%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.054025/  0.861690, val:  82.50%, val_best:  85.83%, tr:  98.08%, tr_best:  99.28%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.078931/  0.874240, val:  84.17%, val_best:  85.83%, tr:  97.14%, tr_best:  99.28%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.062255/  0.779366, val:  82.92%, val_best:  85.83%, tr:  97.88%, tr_best:  99.28%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.091067/  0.985065, val:  79.58%, val_best:  85.83%, tr:  96.75%, tr_best:  99.28%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.078812/  0.915851, val:  82.50%, val_best:  85.83%, tr:  97.34%, tr_best:  99.28%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.045482/  0.894444, val:  82.50%, val_best:  85.83%, tr:  98.47%, tr_best:  99.28%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.032402/  0.814880, val:  83.75%, val_best:  85.83%, tr:  98.87%, tr_best:  99.28%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.048716/  0.751673, val:  84.17%, val_best:  85.83%, tr:  98.42%, tr_best:  99.28%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.037856/  0.778761, val:  84.17%, val_best:  85.83%, tr:  98.60%, tr_best:  99.28%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.031783/  0.907177, val:  85.00%, val_best:  85.83%, tr:  98.83%, tr_best:  99.28%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.017893/  0.778136, val:  83.33%, val_best:  85.83%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.032527/  0.823335, val:  85.42%, val_best:  85.83%, tr:  98.78%, tr_best:  99.44%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.044949/  0.999992, val:  80.42%, val_best:  85.83%, tr:  98.53%, tr_best:  99.44%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.042336/  0.979934, val:  80.83%, val_best:  85.83%, tr:  98.58%, tr_best:  99.44%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.030666/  1.000318, val:  80.42%, val_best:  85.83%, tr:  98.92%, tr_best:  99.44%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.018093/  1.065266, val:  80.83%, val_best:  85.83%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.013045/  0.951498, val:  82.92%, val_best:  85.83%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.020296/  0.878190, val:  85.83%, val_best:  85.83%, tr:  99.37%, tr_best:  99.55%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.016669/  1.019549, val:  82.08%, val_best:  85.83%, tr:  99.44%, tr_best:  99.55%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.023715/  0.963749, val:  84.17%, val_best:  85.83%, tr:  99.28%, tr_best:  99.55%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.025455/  0.942052, val:  84.17%, val_best:  85.83%, tr:  99.19%, tr_best:  99.55%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.039896/  0.734154, val:  87.08%, val_best:  87.08%, tr:  98.78%, tr_best:  99.55%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.046068/  1.082888, val:  83.33%, val_best:  87.08%, tr:  98.20%, tr_best:  99.55%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.045247/  1.076298, val:  81.25%, val_best:  87.08%, tr:  98.31%, tr_best:  99.55%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.062165/  1.078491, val:  82.50%, val_best:  87.08%, tr:  97.81%, tr_best:  99.55%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.035839/  1.058428, val:  82.50%, val_best:  87.08%, tr:  98.78%, tr_best:  99.55%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.036733/  0.933813, val:  80.83%, val_best:  87.08%, tr:  98.74%, tr_best:  99.55%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.032810/  0.963387, val:  82.92%, val_best:  87.08%, tr:  99.03%, tr_best:  99.55%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.068798/  0.803040, val:  83.33%, val_best:  87.08%, tr:  97.23%, tr_best:  99.55%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.031748/  0.798040, val:  83.33%, val_best:  87.08%, tr:  98.94%, tr_best:  99.55%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.027627/  0.941072, val:  83.33%, val_best:  87.08%, tr:  99.03%, tr_best:  99.55%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.033116/  0.895307, val:  85.42%, val_best:  87.08%, tr:  99.03%, tr_best:  99.55%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72678ee175ae416f9af5f9a7c6860a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▃▅▃▁█▅█▆▆▆█▆▅▆▆█▆█████▅████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇████▇▇█▇▇██▇██████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇▇▇█▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▄▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇████▇▇█▇▇██▇██████████████████████</td></tr><tr><td>val_loss</td><td>▁▇▆▇▅▅▅▆▆▅▇▆▆▇█▇▆▆▆▅▆▇▆▆▇▆▇▇▇▇▆▆▇▇▇▇██▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99031</td></tr><tr><td>tr_epoch_loss</td><td>0.03312</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>0.89531</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-43</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iuf006t3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/iuf006t3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_051341-iuf006t3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yhyl2lfb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_052619-yhyl2lfb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yhyl2lfb' target=\"_blank\">pleasant-sweep-44</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yhyl2lfb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yhyl2lfb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.951686/  1.025397, val:  55.42%, val_best:  55.42%, tr:  59.22%, tr_best:  59.22%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.645860/  0.897405, val:  60.83%, val_best:  60.83%, tr:  70.18%, tr_best:  70.18%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.522693/  0.812382, val:  67.92%, val_best:  67.92%, tr:  76.19%, tr_best:  76.19%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.441000/  0.615929, val:  79.17%, val_best:  79.17%, tr:  80.97%, tr_best:  80.97%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.287387/  0.634769, val:  80.00%, val_best:  80.00%, tr:  88.30%, tr_best:  88.30%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.246179/  0.553312, val:  84.58%, val_best:  84.58%, tr:  91.07%, tr_best:  91.07%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.179010/  0.696105, val:  82.92%, val_best:  84.58%, tr:  93.73%, tr_best:  93.73%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.160591/  0.671182, val:  80.42%, val_best:  84.58%, tr:  93.98%, tr_best:  93.98%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.110746/  0.530803, val:  87.50%, val_best:  87.50%, tr:  96.24%, tr_best:  96.24%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.083320/  0.565763, val:  85.83%, val_best:  87.50%, tr:  97.20%, tr_best:  97.20%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.081690/  0.559897, val:  85.42%, val_best:  87.50%, tr:  97.11%, tr_best:  97.20%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.072882/  0.755249, val:  83.75%, val_best:  87.50%, tr:  97.52%, tr_best:  97.52%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.078347/  0.560587, val:  86.25%, val_best:  87.50%, tr:  97.00%, tr_best:  97.52%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.074588/  0.565343, val:  86.25%, val_best:  87.50%, tr:  97.14%, tr_best:  97.52%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.037913/  0.557928, val:  85.42%, val_best:  87.50%, tr:  98.83%, tr_best:  98.83%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.033076/  0.545931, val:  89.17%, val_best:  89.17%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.031951/  0.431775, val:  90.83%, val_best:  90.83%, tr:  99.14%, tr_best:  99.14%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.021453/  0.439322, val:  88.75%, val_best:  90.83%, tr:  99.19%, tr_best:  99.19%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.008767/  0.431811, val:  90.00%, val_best:  90.83%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.028777/  0.643223, val:  85.42%, val_best:  90.83%, tr:  99.21%, tr_best:  99.84%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.023827/  0.722754, val:  85.42%, val_best:  90.83%, tr:  99.26%, tr_best:  99.84%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.024565/  0.462364, val:  88.75%, val_best:  90.83%, tr:  99.17%, tr_best:  99.84%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.007651/  0.504434, val:  88.75%, val_best:  90.83%, tr:  99.80%, tr_best:  99.84%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.002182/  0.576545, val:  87.50%, val_best:  90.83%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.001085/  0.577423, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000974/  0.579776, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000851/  0.582227, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000631/  0.587716, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000538/  0.587124, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000497/  0.591733, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000452/  0.596504, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000436/  0.604846, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000398/  0.615468, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000379/  0.626004, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000359/  0.624873, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000348/  0.618163, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000334/  0.621966, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000323/  0.619748, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000311/  0.617342, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000296/  0.609559, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000287/  0.616535, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000276/  0.619197, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000265/  0.620222, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000255/  0.624315, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000245/  0.631606, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000241/  0.633899, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000235/  0.630158, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000226/  0.636244, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.636540, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000212/  0.635090, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000211/  0.635824, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000203/  0.633432, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000198/  0.630581, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000195/  0.628777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000186/  0.631405, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000180/  0.628617, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000182/  0.628932, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000174/  0.633480, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000171/  0.633970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000165/  0.631230, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000160/  0.634817, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.630238, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000159/  0.630842, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000152/  0.630696, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000149/  0.633682, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.633862, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000144/  0.635011, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.641019, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000136/  0.642139, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000134/  0.638909, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000133/  0.639605, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000130/  0.638448, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000129/  0.639212, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000125/  0.638032, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000127/  0.634510, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000125/  0.642955, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000122/  0.640561, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000119/  0.644412, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000119/  0.645502, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000117/  0.642985, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.644341, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000113/  0.647870, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000113/  0.643086, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.640622, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000108/  0.642478, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000106/  0.640399, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000106/  0.637305, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000105/  0.642005, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.636793, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000101/  0.633565, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000101/  0.635142, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000099/  0.633970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000099/  0.638948, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000096/  0.639033, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000096/  0.640772, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000094/  0.641351, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000092/  0.644410, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000092/  0.641242, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.639663, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.643640, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd470bf331a4ab19e470ee11f3fe218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▅▅▇████▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▇▅▄▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>0.64364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-sweep-44</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yhyl2lfb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yhyl2lfb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_052619-yhyl2lfb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1sqrspue with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_053803-1sqrspue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1sqrspue' target=\"_blank\">dainty-sweep-45</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1sqrspue' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1sqrspue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.949064/  0.895685, val:  59.58%, val_best:  59.58%, tr:  57.80%, tr_best:  57.80%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.640163/  0.835294, val:  58.75%, val_best:  59.58%, tr:  70.00%, tr_best:  70.00%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.516165/  0.872722, val:  66.67%, val_best:  66.67%, tr:  75.02%, tr_best:  75.02%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.456428/  0.906665, val:  73.33%, val_best:  73.33%, tr:  80.00%, tr_best:  80.00%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.374808/  0.925944, val:  70.83%, val_best:  73.33%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.316111/  0.816887, val:  75.83%, val_best:  75.83%, tr:  87.76%, tr_best:  87.76%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.294957/  0.721442, val:  82.50%, val_best:  82.50%, tr:  89.16%, tr_best:  89.16%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.203373/  0.789883, val:  78.33%, val_best:  82.50%, tr:  92.63%, tr_best:  92.63%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.168184/  0.593088, val:  85.42%, val_best:  85.42%, tr:  94.09%, tr_best:  94.09%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.136414/  0.538086, val:  85.83%, val_best:  85.83%, tr:  95.24%, tr_best:  95.24%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.143883/  0.717735, val:  80.83%, val_best:  85.83%, tr:  94.86%, tr_best:  95.24%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.111236/  0.694964, val:  79.58%, val_best:  85.83%, tr:  96.37%, tr_best:  96.37%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.104926/  0.655966, val:  84.58%, val_best:  85.83%, tr:  96.55%, tr_best:  96.55%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.082335/  0.498794, val:  87.50%, val_best:  87.50%, tr:  97.20%, tr_best:  97.20%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.076798/  0.571115, val:  87.08%, val_best:  87.50%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.061656/  0.465572, val:  85.42%, val_best:  87.50%, tr:  97.90%, tr_best:  97.90%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.080133/  0.768955, val:  84.17%, val_best:  87.50%, tr:  97.29%, tr_best:  97.90%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.069883/  0.778914, val:  83.33%, val_best:  87.50%, tr:  97.68%, tr_best:  97.90%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.047172/  0.500700, val:  87.08%, val_best:  87.50%, tr:  98.31%, tr_best:  98.31%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.033161/  0.649103, val:  85.00%, val_best:  87.50%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.044963/  0.796841, val:  83.33%, val_best:  87.50%, tr:  98.69%, tr_best:  99.05%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.023351/  0.632537, val:  84.17%, val_best:  87.50%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.019359/  0.698113, val:  85.83%, val_best:  87.50%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.039110/  0.754414, val:  85.00%, val_best:  87.50%, tr:  98.67%, tr_best:  99.41%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.022788/  0.775233, val:  82.50%, val_best:  87.50%, tr:  99.28%, tr_best:  99.41%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.015019/  0.854414, val:  83.75%, val_best:  87.50%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.023431/  0.785984, val:  85.83%, val_best:  87.50%, tr:  99.28%, tr_best:  99.57%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.043957/  0.750248, val:  81.67%, val_best:  87.50%, tr:  98.58%, tr_best:  99.57%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.034686/  0.675525, val:  86.25%, val_best:  87.50%, tr:  99.10%, tr_best:  99.57%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.025963/  0.768090, val:  85.00%, val_best:  87.50%, tr:  99.21%, tr_best:  99.57%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.007983/  0.667176, val:  86.25%, val_best:  87.50%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.009298/  0.639135, val:  85.00%, val_best:  87.50%, tr:  99.75%, tr_best:  99.86%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.016254/  0.721272, val:  85.42%, val_best:  87.50%, tr:  99.39%, tr_best:  99.86%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.029297/  0.591850, val:  87.50%, val_best:  87.50%, tr:  98.99%, tr_best:  99.86%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.005346/  0.766216, val:  86.25%, val_best:  87.50%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.001999/  0.755534, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.001074/  0.765628, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000856/  0.765999, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000691/  0.775945, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000633/  0.780868, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000574/  0.775787, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000513/  0.776145, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000465/  0.782235, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000438/  0.792291, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000411/  0.784383, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000378/  0.774453, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000362/  0.782481, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000348/  0.784029, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000330/  0.782945, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000316/  0.783693, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000306/  0.781755, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000295/  0.779068, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000288/  0.785913, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000273/  0.785106, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000264/  0.788720, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000255/  0.797702, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000254/  0.797603, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000239/  0.797536, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000232/  0.795696, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000225/  0.798821, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000220/  0.798971, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000215/  0.808698, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000217/  0.815114, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000203/  0.811970, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000198/  0.810731, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000192/  0.811595, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000188/  0.809812, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000186/  0.813127, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000181/  0.811500, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000175/  0.809974, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000173/  0.819961, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000167/  0.826033, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000166/  0.826232, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000163/  0.830099, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000159/  0.833316, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000157/  0.828356, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000155/  0.834282, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000153/  0.827388, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000150/  0.833876, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000146/  0.832980, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000144/  0.835779, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000141/  0.828923, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000138/  0.834083, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000137/  0.833685, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000132/  0.839824, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000130/  0.841093, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000129/  0.845832, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000126/  0.847454, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000125/  0.852318, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000123/  0.853212, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000121/  0.852771, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000120/  0.853106, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000117/  0.853383, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000115/  0.854295, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000115/  0.855967, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000114/  0.856400, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000111/  0.855222, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000111/  0.859254, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000108/  0.858980, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000108/  0.860923, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3162b7e79442529ea4e705902199ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁██▆█▅▆█▆██▆▆██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁▇█▇▅▆▅▇▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>0.86092</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-sweep-45</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1sqrspue' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1sqrspue</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_053803-1sqrspue/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kg3wis1h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_055023-kg3wis1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kg3wis1h' target=\"_blank\">iconic-sweep-46</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kg3wis1h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kg3wis1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.035453/  1.029950, val:  58.33%, val_best:  58.33%, tr:  56.54%, tr_best:  56.54%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.666802/  0.855884, val:  67.08%, val_best:  67.08%, tr:  69.48%, tr_best:  69.48%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.509979/  0.843827, val:  65.83%, val_best:  67.08%, tr:  77.07%, tr_best:  77.07%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.407098/  0.686871, val:  76.25%, val_best:  76.25%, tr:  83.32%, tr_best:  83.32%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.303036/  0.643259, val:  78.33%, val_best:  78.33%, tr:  88.48%, tr_best:  88.48%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.236765/  0.510076, val:  84.17%, val_best:  84.17%, tr:  91.32%, tr_best:  91.32%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.185520/  0.581392, val:  83.33%, val_best:  84.17%, tr:  93.53%, tr_best:  93.53%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.144215/  0.608712, val:  82.50%, val_best:  84.17%, tr:  94.84%, tr_best:  94.84%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.136494/  0.626809, val:  82.92%, val_best:  84.17%, tr:  95.24%, tr_best:  95.24%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.096812/  0.550497, val:  85.83%, val_best:  85.83%, tr:  96.64%, tr_best:  96.64%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.096373/  0.508922, val:  87.92%, val_best:  87.92%, tr:  96.46%, tr_best:  96.64%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.075243/  0.538114, val:  87.50%, val_best:  87.92%, tr:  97.57%, tr_best:  97.57%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.077376/  0.651389, val:  85.83%, val_best:  87.92%, tr:  97.20%, tr_best:  97.57%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.072989/  0.559711, val:  87.92%, val_best:  87.92%, tr:  97.79%, tr_best:  97.79%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.073443/  0.511660, val:  86.25%, val_best:  87.92%, tr:  97.36%, tr_best:  97.79%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.037172/  0.570033, val:  86.67%, val_best:  87.92%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.031851/  0.461435, val:  90.83%, val_best:  90.83%, tr:  98.90%, tr_best:  98.90%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.029136/  0.563660, val:  86.25%, val_best:  90.83%, tr:  98.87%, tr_best:  98.90%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.008117/  0.622118, val:  86.25%, val_best:  90.83%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.029266/  0.655201, val:  87.08%, val_best:  90.83%, tr:  98.99%, tr_best:  99.86%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.018910/  0.782123, val:  86.25%, val_best:  90.83%, tr:  99.41%, tr_best:  99.86%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.033993/  0.777282, val:  84.58%, val_best:  90.83%, tr:  98.85%, tr_best:  99.86%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.037841/  0.702647, val:  87.50%, val_best:  90.83%, tr:  98.74%, tr_best:  99.86%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.014073/  0.726397, val:  86.67%, val_best:  90.83%, tr:  99.55%, tr_best:  99.86%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.018715/  0.683305, val:  87.08%, val_best:  90.83%, tr:  99.50%, tr_best:  99.86%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.019935/  0.793891, val:  89.17%, val_best:  90.83%, tr:  99.26%, tr_best:  99.86%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.027386/  0.677126, val:  88.75%, val_best:  90.83%, tr:  99.12%, tr_best:  99.86%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.019789/  0.703160, val:  84.58%, val_best:  90.83%, tr:  99.28%, tr_best:  99.86%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.010716/  0.690764, val:  87.08%, val_best:  90.83%, tr:  99.71%, tr_best:  99.86%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.012565/  0.503551, val:  87.92%, val_best:  90.83%, tr:  99.66%, tr_best:  99.86%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.013305/  0.619050, val:  89.17%, val_best:  90.83%, tr:  99.59%, tr_best:  99.86%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.002055/  0.640612, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000727/  0.659283, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000519/  0.656789, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000442/  0.662954, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000390/  0.660451, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000353/  0.673450, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000327/  0.670658, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000299/  0.661822, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000281/  0.672531, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000255/  0.674894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000247/  0.672385, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000234/  0.696654, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000224/  0.694398, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000215/  0.701670, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000201/  0.705817, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000200/  0.709354, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000186/  0.709227, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000178/  0.715687, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000172/  0.720720, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000168/  0.722599, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000159/  0.725977, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000155/  0.722880, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000149/  0.727829, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000145/  0.734076, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000143/  0.736225, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000137/  0.738914, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000134/  0.741398, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000129/  0.741865, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000127/  0.748946, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000124/  0.750369, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000122/  0.752059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000124/  0.755285, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000115/  0.754757, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000113/  0.753276, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000111/  0.752323, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000110/  0.753504, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000106/  0.752998, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000105/  0.754489, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000101/  0.747992, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000101/  0.743479, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000100/  0.745909, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000097/  0.739554, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000096/  0.740109, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000094/  0.741633, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000092/  0.742164, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000092/  0.742648, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000091/  0.747258, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000089/  0.746934, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000088/  0.752438, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000086/  0.752227, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000084/  0.752789, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000084/  0.752647, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000081/  0.752056, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000080/  0.756638, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000080/  0.758498, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000079/  0.762590, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000077/  0.763454, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000075/  0.768858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000076/  0.773721, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000075/  0.770226, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000073/  0.766972, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000073/  0.770851, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000071/  0.767608, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000070/  0.766505, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000069/  0.766825, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000069/  0.767686, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000067/  0.775423, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000066/  0.778063, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000065/  0.778253, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56b213a48524d97806e675a2120fb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▆▆▆█▆█▆▅██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▅▆▆▇▇▇▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>7e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.77825</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-sweep-46</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kg3wis1h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kg3wis1h</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_055023-kg3wis1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s4o8fyql with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_060208-s4o8fyql</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s4o8fyql' target=\"_blank\">helpful-sweep-47</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s4o8fyql' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s4o8fyql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.406844/  0.958219, val:  59.17%, val_best:  59.17%, tr:  45.47%, tr_best:  45.47%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.723980/  0.780645, val:  69.17%, val_best:  69.17%, tr:  66.75%, tr_best:  66.75%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.595533/  0.689062, val:  70.42%, val_best:  70.42%, tr:  71.62%, tr_best:  71.62%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.532839/  0.690661, val:  71.67%, val_best:  71.67%, tr:  75.32%, tr_best:  75.32%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.474772/  0.676391, val:  72.08%, val_best:  72.08%, tr:  78.49%, tr_best:  78.49%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.430450/  0.662064, val:  75.00%, val_best:  75.00%, tr:  81.04%, tr_best:  81.04%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.394135/  0.610545, val:  77.08%, val_best:  77.08%, tr:  83.00%, tr_best:  83.00%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.340648/  0.619833, val:  77.50%, val_best:  77.50%, tr:  86.52%, tr_best:  86.52%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.311277/  0.632571, val:  79.17%, val_best:  79.17%, tr:  87.67%, tr_best:  87.67%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.269769/  0.652907, val:  80.00%, val_best:  80.00%, tr:  89.63%, tr_best:  89.63%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.238243/  0.571554, val:  82.50%, val_best:  82.50%, tr:  91.59%, tr_best:  91.59%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.206252/  0.573569, val:  83.75%, val_best:  83.75%, tr:  93.06%, tr_best:  93.06%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.180926/  0.549008, val:  84.17%, val_best:  84.17%, tr:  93.78%, tr_best:  93.78%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.153464/  0.524603, val:  85.83%, val_best:  85.83%, tr:  95.15%, tr_best:  95.15%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.139332/  0.545647, val:  84.58%, val_best:  85.83%, tr:  95.29%, tr_best:  95.29%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.109422/  0.515474, val:  84.58%, val_best:  85.83%, tr:  96.89%, tr_best:  96.89%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.103712/  0.489009, val:  87.92%, val_best:  87.92%, tr:  96.62%, tr_best:  96.89%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.086067/  0.607610, val:  83.33%, val_best:  87.92%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.085048/  0.511707, val:  84.58%, val_best:  87.92%, tr:  97.36%, tr_best:  97.36%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.069078/  0.575243, val:  84.17%, val_best:  87.92%, tr:  97.99%, tr_best:  97.99%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.065334/  0.522077, val:  85.83%, val_best:  87.92%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.049795/  0.523818, val:  86.67%, val_best:  87.92%, tr:  98.76%, tr_best:  98.76%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.055332/  0.553821, val:  85.42%, val_best:  87.92%, tr:  98.38%, tr_best:  98.76%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.051328/  0.532477, val:  84.58%, val_best:  87.92%, tr:  98.62%, tr_best:  98.76%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.040799/  0.549924, val:  87.50%, val_best:  87.92%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.037340/  0.557823, val:  87.08%, val_best:  87.92%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.032842/  0.526354, val:  86.67%, val_best:  87.92%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.027453/  0.558586, val:  85.83%, val_best:  87.92%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.026304/  0.533662, val:  87.50%, val_best:  87.92%, tr:  99.39%, tr_best:  99.41%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.023107/  0.573622, val:  86.67%, val_best:  87.92%, tr:  99.64%, tr_best:  99.64%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.018787/  0.593004, val:  85.83%, val_best:  87.92%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.017773/  0.550491, val:  87.08%, val_best:  87.92%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.015158/  0.545993, val:  87.50%, val_best:  87.92%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.017828/  0.590282, val:  87.08%, val_best:  87.92%, tr:  99.71%, tr_best:  99.75%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.014665/  0.579965, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.014213/  0.596327, val:  86.67%, val_best:  87.92%, tr:  99.77%, tr_best:  99.80%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.016180/  0.586905, val:  87.08%, val_best:  87.92%, tr:  99.73%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.013524/  0.572464, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.010606/  0.596438, val:  87.92%, val_best:  87.92%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.009866/  0.573601, val:  87.50%, val_best:  87.92%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.008773/  0.594846, val:  86.67%, val_best:  87.92%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.006975/  0.569506, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.006633/  0.553995, val:  87.92%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.006266/  0.599122, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.006300/  0.574804, val:  87.92%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.005797/  0.587227, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.005592/  0.581082, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.005327/  0.590073, val:  87.92%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.005866/  0.600795, val:  87.50%, val_best:  87.92%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.013877/  0.583855, val:  88.33%, val_best:  88.33%, tr:  99.66%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.005411/  0.608503, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.005954/  0.581653, val:  88.75%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.004097/  0.590245, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.004301/  0.574839, val:  88.33%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.003978/  0.601863, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.003871/  0.589340, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.003694/  0.607726, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.004127/  0.617398, val:  86.25%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.003618/  0.600928, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.003617/  0.602800, val:  88.75%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.003411/  0.610985, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.003306/  0.606200, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.003196/  0.612432, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002898/  0.610701, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.002703/  0.594651, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002805/  0.610929, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002695/  0.612000, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.002541/  0.608633, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.002499/  0.607915, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002359/  0.607496, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002298/  0.628959, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.002366/  0.611112, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.002254/  0.618752, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002212/  0.632760, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002152/  0.622030, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002157/  0.604860, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002107/  0.627361, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002056/  0.645251, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002023/  0.628835, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001993/  0.632103, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001931/  0.638454, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001932/  0.625771, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001844/  0.626585, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001821/  0.627709, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001813/  0.621450, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001746/  0.640118, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001732/  0.644198, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001744/  0.644604, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001658/  0.650794, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001703/  0.640181, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001692/  0.641139, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001604/  0.642714, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001595/  0.647977, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001612/  0.652086, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001524/  0.635167, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001539/  0.638963, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001483/  0.649551, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001432/  0.640280, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001452/  0.655275, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001403/  0.646363, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506713cfd634409fb7d04207532d804d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▆▆█▆▆▅▆██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▆▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0014</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>0.64636</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-sweep-47</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s4o8fyql' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s4o8fyql</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_060208-s4o8fyql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 60zbq3eh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_061328-60zbq3eh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/60zbq3eh' target=\"_blank\">exalted-sweep-48</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/60zbq3eh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/60zbq3eh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.100505/  0.808528, val:  64.17%, val_best:  64.17%, tr:  53.45%, tr_best:  53.45%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.642298/  0.813037, val:  62.50%, val_best:  64.17%, tr:  71.24%, tr_best:  71.24%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.493790/  0.749422, val:  70.00%, val_best:  70.00%, tr:  78.47%, tr_best:  78.47%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.394734/  0.562889, val:  81.25%, val_best:  81.25%, tr:  84.63%, tr_best:  84.63%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.281326/  0.620747, val:  83.33%, val_best:  83.33%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.220942/  0.521294, val:  87.50%, val_best:  87.50%, tr:  92.02%, tr_best:  92.02%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.175607/  0.578194, val:  84.17%, val_best:  87.50%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.134290/  0.532760, val:  84.17%, val_best:  87.50%, tr:  95.56%, tr_best:  95.56%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.116384/  0.523376, val:  85.83%, val_best:  87.50%, tr:  95.92%, tr_best:  95.92%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.090998/  0.534723, val:  85.83%, val_best:  87.50%, tr:  96.78%, tr_best:  96.78%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.094984/  0.617773, val:  85.00%, val_best:  87.50%, tr:  96.51%, tr_best:  96.78%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.086062/  0.488322, val:  88.33%, val_best:  88.33%, tr:  97.00%, tr_best:  97.00%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.085221/  0.499560, val:  88.75%, val_best:  88.75%, tr:  96.73%, tr_best:  97.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.091697/  0.556736, val:  87.92%, val_best:  88.75%, tr:  96.89%, tr_best:  97.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.061939/  0.553562, val:  87.08%, val_best:  88.75%, tr:  97.88%, tr_best:  97.88%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.054225/  0.630321, val:  87.50%, val_best:  88.75%, tr:  98.15%, tr_best:  98.15%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.055602/  0.699448, val:  86.25%, val_best:  88.75%, tr:  98.06%, tr_best:  98.15%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.027473/  0.505449, val:  90.00%, val_best:  90.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.016052/  0.636046, val:  88.33%, val_best:  90.00%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.010194/  0.577962, val:  88.75%, val_best:  90.00%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.002434/  0.569490, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001490/  0.572876, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001210/  0.593887, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.001012/  0.591675, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000847/  0.594092, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000769/  0.588328, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000713/  0.604219, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000649/  0.600328, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000601/  0.596442, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000554/  0.593820, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000521/  0.590108, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000515/  0.604925, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000472/  0.596571, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000448/  0.602751, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000430/  0.612289, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000412/  0.618431, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000398/  0.614911, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000376/  0.618112, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000369/  0.623524, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000348/  0.625995, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000338/  0.627236, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000323/  0.626526, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000309/  0.631469, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000308/  0.623785, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000293/  0.632527, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000285/  0.629793, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000282/  0.629813, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000275/  0.634663, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000265/  0.640470, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000257/  0.639408, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000249/  0.640418, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000242/  0.641200, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000236/  0.638007, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000232/  0.642180, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000219/  0.645668, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000220/  0.641448, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000218/  0.636065, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000213/  0.635561, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000208/  0.633668, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000205/  0.636243, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000198/  0.638779, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000194/  0.638570, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000195/  0.640879, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000187/  0.643707, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000187/  0.644351, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000185/  0.634727, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000182/  0.636308, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000174/  0.635818, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000171/  0.634720, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000168/  0.636879, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000167/  0.641171, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000162/  0.632774, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000159/  0.632238, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000156/  0.634901, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000157/  0.640459, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000152/  0.632266, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000149/  0.634533, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000149/  0.634156, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000147/  0.628359, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000146/  0.627483, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000142/  0.630423, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631503, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000134/  0.627109, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000133/  0.629811, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000131/  0.628284, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000129/  0.631311, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000127/  0.629332, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633218, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000125/  0.631407, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635196, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000122/  0.635720, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000121/  0.637245, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000119/  0.632590, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000119/  0.642267, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000118/  0.639717, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000116/  0.640287, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000116/  0.640395, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000115/  0.637996, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000113/  0.638854, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000110/  0.636839, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f01f5933eb4493930a6f1f9f6e5591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆█▆▆██████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▆▅▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.63684</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-48</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/60zbq3eh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/60zbq3eh</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_061328-60zbq3eh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ry0dq0ux with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_062548-ry0dq0ux</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ry0dq0ux' target=\"_blank\">devout-sweep-49</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ry0dq0ux' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ry0dq0ux</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.955137/  1.007768, val:  57.92%, val_best:  57.92%, tr:  58.95%, tr_best:  58.95%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.633488/  0.861796, val:  64.17%, val_best:  64.17%, tr:  70.96%, tr_best:  70.96%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.483149/  0.760830, val:  70.83%, val_best:  70.83%, tr:  78.47%, tr_best:  78.47%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.366708/  0.676429, val:  83.33%, val_best:  83.33%, tr:  85.41%, tr_best:  85.41%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.260007/  0.596713, val:  80.00%, val_best:  83.33%, tr:  90.28%, tr_best:  90.28%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.224022/  0.558387, val:  85.00%, val_best:  85.00%, tr:  91.37%, tr_best:  91.37%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.159481/  0.809964, val:  77.50%, val_best:  85.00%, tr:  94.45%, tr_best:  94.45%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.134419/  0.605432, val:  86.67%, val_best:  86.67%, tr:  95.45%, tr_best:  95.45%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.085520/  0.883648, val:  80.42%, val_best:  86.67%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.122866/  0.538441, val:  85.42%, val_best:  86.67%, tr:  95.20%, tr_best:  97.23%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.085068/  0.548780, val:  85.42%, val_best:  86.67%, tr:  97.11%, tr_best:  97.23%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.040832/  0.769605, val:  81.25%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.032957/  0.642591, val:  88.33%, val_best:  88.33%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.024398/  0.678472, val:  85.83%, val_best:  88.33%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.045704/  0.707028, val:  85.00%, val_best:  88.33%, tr:  98.76%, tr_best:  99.28%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.027800/  0.584827, val:  85.83%, val_best:  88.33%, tr:  99.03%, tr_best:  99.28%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.027652/  0.651052, val:  85.83%, val_best:  88.33%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.011670/  0.650018, val:  86.25%, val_best:  88.33%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.011404/  0.546079, val:  86.67%, val_best:  88.33%, tr:  99.66%, tr_best:  99.77%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.003561/  0.633149, val:  86.25%, val_best:  88.33%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001558/  0.612924, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001024/  0.585231, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000845/  0.596241, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000755/  0.609148, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000651/  0.617620, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000602/  0.606601, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000560/  0.607063, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000503/  0.617558, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000471/  0.606844, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000452/  0.619361, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000412/  0.607277, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000389/  0.617022, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000367/  0.615817, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000357/  0.612885, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000341/  0.625152, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.633187, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000306/  0.639251, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000296/  0.646917, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000284/  0.646390, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000272/  0.652048, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.654455, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000254/  0.653792, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000253/  0.660993, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.659280, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000232/  0.649400, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000231/  0.647422, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000221/  0.648225, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000216/  0.648228, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000208/  0.652768, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000204/  0.647242, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000198/  0.645373, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000197/  0.656139, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.656302, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.658520, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000180/  0.658293, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000175/  0.658780, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000176/  0.657437, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000170/  0.651068, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000168/  0.654113, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000164/  0.656811, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000162/  0.667812, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.663543, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000161/  0.661248, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000151/  0.665512, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.663802, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.663569, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000140/  0.665893, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000138/  0.665396, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000135/  0.664289, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000135/  0.664218, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.664026, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.664901, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000127/  0.661522, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000126/  0.660365, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.658058, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000122/  0.661735, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.656486, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000119/  0.644730, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.645872, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000117/  0.649620, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000114/  0.651442, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000111/  0.653960, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.651691, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000110/  0.650589, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000107/  0.652803, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.655024, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000104/  0.653163, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000102/  0.652817, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000101/  0.654551, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000100/  0.654627, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.663356, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000097/  0.663640, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000098/  0.669786, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.665640, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.666797, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000095/  0.667991, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000093/  0.668086, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000092/  0.667181, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000091/  0.668421, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.672938, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817682f46dfa49488b07fe4abe7d91cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁█▅█▅█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇██▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇██▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>0.67294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-49</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ry0dq0ux' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ry0dq0ux</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_062548-ry0dq0ux/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0o4m6vi4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_063728-0o4m6vi4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0o4m6vi4' target=\"_blank\">smart-sweep-50</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0o4m6vi4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0o4m6vi4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.283327/  0.859835, val:  64.17%, val_best:  64.17%, tr:  49.86%, tr_best:  49.86%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.667487/  0.742630, val:  67.92%, val_best:  67.92%, tr:  69.03%, tr_best:  69.03%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.550434/  0.659926, val:  71.67%, val_best:  71.67%, tr:  73.31%, tr_best:  73.31%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.486257/  0.665857, val:  71.67%, val_best:  71.67%, tr:  77.61%, tr_best:  77.61%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.424559/  0.661036, val:  74.17%, val_best:  74.17%, tr:  81.24%, tr_best:  81.24%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.365786/  0.588115, val:  78.75%, val_best:  78.75%, tr:  84.74%, tr_best:  84.74%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.310152/  0.619527, val:  79.58%, val_best:  79.58%, tr:  87.83%, tr_best:  87.83%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.243017/  0.615357, val:  77.92%, val_best:  79.58%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.220379/  0.551849, val:  81.67%, val_best:  81.67%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.162025/  0.499324, val:  85.42%, val_best:  85.42%, tr:  94.91%, tr_best:  94.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.134706/  0.491673, val:  86.25%, val_best:  86.25%, tr:  96.17%, tr_best:  96.17%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.113059/  0.520883, val:  85.83%, val_best:  86.25%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.090843/  0.535819, val:  85.00%, val_best:  86.25%, tr:  97.29%, tr_best:  97.29%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.082358/  0.512585, val:  87.50%, val_best:  87.50%, tr:  97.86%, tr_best:  97.86%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.084413/  0.441487, val:  87.50%, val_best:  87.50%, tr:  97.25%, tr_best:  97.86%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.054818/  0.444199, val:  89.17%, val_best:  89.17%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.049575/  0.484767, val:  90.42%, val_best:  90.42%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.042595/  0.473062, val:  87.50%, val_best:  90.42%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.041355/  0.455359, val:  88.75%, val_best:  90.42%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.034640/  0.537417, val:  88.33%, val_best:  90.42%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.031402/  0.508172, val:  87.08%, val_best:  90.42%, tr:  99.32%, tr_best:  99.32%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.021901/  0.500831, val:  87.08%, val_best:  90.42%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.018540/  0.496287, val:  87.50%, val_best:  90.42%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.026824/  0.494493, val:  87.92%, val_best:  90.42%, tr:  99.39%, tr_best:  99.73%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.013900/  0.496134, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.013900/  0.551243, val:  87.50%, val_best:  90.42%, tr:  99.75%, tr_best:  99.86%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.012358/  0.527075, val:  87.50%, val_best:  90.42%, tr:  99.84%, tr_best:  99.86%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.013886/  0.483620, val:  88.75%, val_best:  90.42%, tr:  99.73%, tr_best:  99.86%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.009035/  0.481363, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.007542/  0.508968, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.007088/  0.501466, val:  88.33%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.006327/  0.497382, val:  86.67%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.006673/  0.499093, val:  88.75%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.005489/  0.518741, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.005253/  0.513355, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.004806/  0.504352, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.004525/  0.535062, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.004553/  0.531822, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.004327/  0.519834, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.004139/  0.530511, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.003675/  0.549868, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.003465/  0.538864, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.003351/  0.535427, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.003158/  0.545481, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.003067/  0.545538, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.003102/  0.547944, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.003056/  0.548016, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.002866/  0.557133, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.002736/  0.568427, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.002587/  0.557685, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.002439/  0.548920, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002379/  0.573048, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002345/  0.570280, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002212/  0.559222, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.002193/  0.570949, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.002122/  0.566643, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002108/  0.575095, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.002066/  0.577081, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.002039/  0.564888, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001930/  0.566400, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001875/  0.581408, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001913/  0.571594, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001798/  0.570857, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001738/  0.572239, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001724/  0.569646, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001692/  0.571327, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001624/  0.567090, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001573/  0.567384, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001537/  0.578447, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001502/  0.571166, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001474/  0.577440, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001445/  0.577552, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001380/  0.579979, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001378/  0.577513, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001354/  0.566407, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001346/  0.568680, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001287/  0.580834, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001295/  0.579458, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001293/  0.570859, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001235/  0.572740, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001212/  0.577683, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001176/  0.582499, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001156/  0.579661, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001119/  0.578453, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001145/  0.575280, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001094/  0.589982, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001110/  0.572782, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001076/  0.587013, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001051/  0.590334, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001037/  0.583845, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001030/  0.587182, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001030/  0.590625, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000992/  0.591808, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001019/  0.595206, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000987/  0.593471, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000961/  0.594508, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000983/  0.588360, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000960/  0.601137, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000966/  0.594395, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000912/  0.604068, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903d5e9d887845f99972ae8122d6e3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃██▆▅█▆███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00091</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>0.60407</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-sweep-50</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0o4m6vi4' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0o4m6vi4</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_063728-0o4m6vi4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qk64w94m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_064922-qk64w94m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qk64w94m' target=\"_blank\">playful-sweep-51</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qk64w94m' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qk64w94m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.953619/  0.928395, val:  61.25%, val_best:  61.25%, tr:  59.42%, tr_best:  59.42%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.590829/  0.797877, val:  67.50%, val_best:  67.50%, tr:  73.58%, tr_best:  73.58%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.370028/  0.568619, val:  80.83%, val_best:  80.83%, tr:  85.80%, tr_best:  85.80%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.232828/  0.435068, val:  87.50%, val_best:  87.50%, tr:  91.86%, tr_best:  91.86%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.142929/  0.417608, val:  89.17%, val_best:  89.17%, tr:  95.06%, tr_best:  95.06%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.112096/  0.441171, val:  90.00%, val_best:  90.00%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.067803/  0.543741, val:  89.17%, val_best:  90.00%, tr:  97.66%, tr_best:  97.66%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.054566/  0.452827, val:  88.33%, val_best:  90.00%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.041852/  0.497237, val:  89.58%, val_best:  90.00%, tr:  98.83%, tr_best:  98.83%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.031380/  0.457256, val:  89.17%, val_best:  90.00%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.041246/  0.456711, val:  90.00%, val_best:  90.00%, tr:  98.49%, tr_best:  98.96%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.025125/  0.500376, val:  90.42%, val_best:  90.42%, tr:  99.21%, tr_best:  99.21%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.008109/  0.447308, val:  91.25%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002911/  0.420444, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001833/  0.435004, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001389/  0.438457, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001144/  0.444814, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001024/  0.436305, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000914/  0.450622, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000780/  0.464463, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000713/  0.464241, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000659/  0.465784, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000620/  0.469834, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000580/  0.478736, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000529/  0.479853, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000513/  0.488066, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000481/  0.498390, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000451/  0.496718, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000425/  0.494863, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000420/  0.491761, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.479870, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000378/  0.490601, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000352/  0.492989, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000338/  0.496638, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000333/  0.493996, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000320/  0.495415, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000304/  0.487261, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000296/  0.491152, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000287/  0.492506, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000279/  0.502984, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000265/  0.500845, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000258/  0.501892, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000252/  0.498551, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000241/  0.498092, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000236/  0.494158, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.498780, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000226/  0.499818, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000220/  0.496906, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000215/  0.507075, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000208/  0.508423, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000204/  0.511655, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000197/  0.506157, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000193/  0.513221, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000190/  0.513570, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000186/  0.512391, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000185/  0.514633, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000178/  0.522651, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000173/  0.531043, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000169/  0.533854, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000166/  0.527164, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000161/  0.527482, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000159/  0.535251, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000160/  0.533687, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000157/  0.534075, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000152/  0.533162, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000148/  0.531826, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000147/  0.531213, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000142/  0.533488, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.529259, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000137/  0.530435, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.531024, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.533250, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000132/  0.533517, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.534700, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000127/  0.542032, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000125/  0.536595, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000122/  0.538523, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000123/  0.529235, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000121/  0.533247, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000120/  0.531858, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000117/  0.531258, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000116/  0.526251, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000113/  0.528444, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000112/  0.530644, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000111/  0.534326, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000108/  0.538056, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000107/  0.537308, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000106/  0.534974, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000105/  0.532631, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000104/  0.533755, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000101/  0.534923, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.536573, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.535592, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.534255, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000098/  0.539131, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000097/  0.542759, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.542476, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000096/  0.542263, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000094/  0.541641, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000093/  0.546454, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb508e229fdc4d80bb7cdf0bfb9fe29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.54645</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-sweep-51</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qk64w94m' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qk64w94m</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_064922-qk64w94m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mt92ynnz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_070101-mt92ynnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt92ynnz' target=\"_blank\">gentle-sweep-52</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt92ynnz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt92ynnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.942593/  0.903914, val:  60.83%, val_best:  60.83%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.642465/  0.798651, val:  62.92%, val_best:  62.92%, tr:  70.40%, tr_best:  70.40%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.477290/  0.861618, val:  67.92%, val_best:  67.92%, tr:  78.13%, tr_best:  78.13%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.413692/  0.644274, val:  80.42%, val_best:  80.42%, tr:  83.03%, tr_best:  83.03%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.256515/  0.636068, val:  77.50%, val_best:  80.42%, tr:  90.92%, tr_best:  90.92%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.209704/  0.579852, val:  83.33%, val_best:  83.33%, tr:  92.63%, tr_best:  92.63%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.133868/  0.593811, val:  84.58%, val_best:  84.58%, tr:  95.54%, tr_best:  95.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.123163/  0.505620, val:  85.00%, val_best:  85.00%, tr:  95.60%, tr_best:  95.60%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.089735/  0.646998, val:  82.50%, val_best:  85.00%, tr:  97.16%, tr_best:  97.16%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.079484/  0.484891, val:  87.08%, val_best:  87.08%, tr:  97.25%, tr_best:  97.25%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.049500/  0.508999, val:  88.75%, val_best:  88.75%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.052914/  0.584016, val:  90.00%, val_best:  90.00%, tr:  98.06%, tr_best:  98.53%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.036708/  0.558366, val:  88.33%, val_best:  90.00%, tr:  99.03%, tr_best:  99.03%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.024783/  0.449408, val:  89.17%, val_best:  90.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.017506/  0.514594, val:  90.42%, val_best:  90.42%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.018535/  0.578328, val:  87.08%, val_best:  90.42%, tr:  99.48%, tr_best:  99.55%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.028552/  0.466015, val:  88.33%, val_best:  90.42%, tr:  99.08%, tr_best:  99.55%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.010837/  0.526453, val:  86.25%, val_best:  90.42%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.002893/  0.502844, val:  89.17%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001605/  0.543820, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001220/  0.526082, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001033/  0.503936, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000911/  0.510902, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000827/  0.513808, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000721/  0.509712, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000669/  0.519355, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000617/  0.527798, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000574/  0.531724, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000538/  0.527529, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000513/  0.528394, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000485/  0.536235, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000463/  0.539451, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000434/  0.539548, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000421/  0.539682, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000403/  0.541043, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000382/  0.544384, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000367/  0.544030, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000357/  0.542826, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000345/  0.543629, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000329/  0.540483, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000317/  0.544864, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000308/  0.546379, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000297/  0.544604, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000287/  0.549397, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000278/  0.547389, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000267/  0.539323, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000263/  0.549534, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000256/  0.546456, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000250/  0.541858, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000240/  0.539060, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000235/  0.537314, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000231/  0.536353, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000229/  0.537821, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000221/  0.532671, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.531339, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000214/  0.534797, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000208/  0.533404, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000205/  0.531805, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000202/  0.535660, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000197/  0.538491, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000194/  0.536451, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000187/  0.539274, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000189/  0.543742, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000181/  0.543535, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000177/  0.542593, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000173/  0.539531, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000171/  0.539073, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000166/  0.542758, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000162/  0.545818, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000161/  0.538167, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000157/  0.535568, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000156/  0.539287, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000153/  0.527536, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000150/  0.539376, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000148/  0.538311, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000146/  0.538875, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000143/  0.536463, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000142/  0.539625, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000141/  0.542708, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000141/  0.546230, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000136/  0.545079, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000134/  0.542630, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000132/  0.541944, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000129/  0.541629, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000128/  0.541077, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000127/  0.535782, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000124/  0.535124, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000122/  0.533963, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000122/  0.536804, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000120/  0.537249, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000119/  0.539175, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000117/  0.544608, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000117/  0.545868, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000114/  0.548575, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000115/  0.544528, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000112/  0.543917, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000112/  0.549869, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000110/  0.550867, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000109/  0.551440, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000108/  0.548797, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b577976ce0e0487d95bc4cbdc9fef645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▅▅▆▆▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.5488</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-52</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt92ynnz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mt92ynnz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_070101-mt92ynnz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ubk7qjs5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_071251-ubk7qjs5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ubk7qjs5' target=\"_blank\">distinctive-sweep-53</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ubk7qjs5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ubk7qjs5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.974480/  0.867463, val:  64.17%, val_best:  64.17%, tr:  58.93%, tr_best:  58.93%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619049/  0.779950, val:  68.75%, val_best:  68.75%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.410511/  0.901403, val:  70.83%, val_best:  70.83%, tr:  83.41%, tr_best:  83.41%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.292086/  0.534297, val:  87.08%, val_best:  87.08%, tr:  89.13%, tr_best:  89.13%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.162379/  0.612390, val:  82.50%, val_best:  87.08%, tr:  94.36%, tr_best:  94.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.140578/  0.442822, val:  89.58%, val_best:  89.58%, tr:  95.36%, tr_best:  95.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.082165/  0.784311, val:  85.00%, val_best:  89.58%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.059931/  0.539399, val:  87.50%, val_best:  89.58%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036210/  0.485832, val:  90.83%, val_best:  90.83%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019819/  0.519804, val:  88.75%, val_best:  90.83%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032321/  0.569617, val:  87.50%, val_best:  90.83%, tr:  98.90%, tr_best:  99.48%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.030033/  0.488638, val:  88.75%, val_best:  90.83%, tr:  98.96%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.012969/  0.511075, val:  91.25%, val_best:  91.25%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006165/  0.697958, val:  86.25%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.003339/  0.576901, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002147/  0.547625, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001528/  0.554330, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001201/  0.556022, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001124/  0.551401, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000866/  0.574368, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000799/  0.591013, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000714/  0.581928, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000671/  0.589273, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000652/  0.598257, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000555/  0.602714, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000540/  0.597360, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000500/  0.592729, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000474/  0.601346, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000439/  0.602200, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000428/  0.595900, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000411/  0.599700, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000382/  0.607436, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000371/  0.604986, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000348/  0.610807, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000338/  0.618313, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000328/  0.620386, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000313/  0.618911, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000303/  0.616548, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000305/  0.621521, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000285/  0.625960, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000283/  0.630643, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000271/  0.629686, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000262/  0.628815, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000250/  0.627120, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000250/  0.628091, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000242/  0.628134, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000232/  0.633806, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000230/  0.632606, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.635183, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000215/  0.640931, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000209/  0.649058, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000209/  0.644091, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000201/  0.644871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000195/  0.649851, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000196/  0.657061, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000189/  0.662943, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000186/  0.667632, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000181/  0.671509, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000180/  0.673545, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000177/  0.670262, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000173/  0.666736, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000167/  0.664875, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000169/  0.669161, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000165/  0.666249, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000162/  0.662686, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000158/  0.662513, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000158/  0.666680, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000155/  0.672476, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000152/  0.672495, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000147/  0.665847, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000146/  0.670442, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000141/  0.670678, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000143/  0.671315, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000136/  0.674515, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000138/  0.676093, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000137/  0.676866, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000134/  0.684349, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000131/  0.688230, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000130/  0.687458, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000129/  0.690846, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.691173, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000127/  0.693805, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000127/  0.689193, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000123/  0.694343, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000121/  0.694647, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000118/  0.695344, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000121/  0.699655, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000118/  0.693519, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000116/  0.685618, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000113/  0.693628, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000112/  0.695912, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000110/  0.687126, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.686712, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.686534, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.696083, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696329, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696316, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000102/  0.702537, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000100/  0.702209, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.701244, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67cf678b6494bc29a78a9e089d85147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▅█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.70124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-53</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ubk7qjs5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ubk7qjs5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_071251-ubk7qjs5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0wbw4vqk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_072451-0wbw4vqk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0wbw4vqk' target=\"_blank\">swift-sweep-54</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0wbw4vqk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0wbw4vqk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.75, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.311415/  2.337775, val:  10.00%, val_best:  10.00%, tr:  11.14%, tr_best:  11.14%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.315665/  2.322051, val:  10.00%, val_best:  10.00%, tr:  11.25%, tr_best:  11.25%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.309714/  2.322027, val:  10.00%, val_best:  10.00%, tr:  11.70%, tr_best:  11.70%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  2.313065/  2.316025, val:  10.00%, val_best:  10.00%, tr:  11.34%, tr_best:  11.70%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.311174/  2.323059, val:  10.00%, val_best:  10.00%, tr:  11.50%, tr_best:  11.70%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  2.311678/  2.334571, val:  10.00%, val_best:  10.00%, tr:   9.94%, tr_best:  11.70%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.310203/  2.325367, val:  10.00%, val_best:  10.00%, tr:  10.35%, tr_best:  11.70%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  2.308333/  2.317604, val:  10.00%, val_best:  10.00%, tr:  12.04%, tr_best:  12.04%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  2.309830/  2.321623, val:  10.00%, val_best:  10.00%, tr:  11.56%, tr_best:  12.04%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  2.310548/  2.355829, val:  10.00%, val_best:  10.00%, tr:  11.00%, tr_best:  12.04%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  2.310342/  2.327464, val:  10.00%, val_best:  10.00%, tr:  11.41%, tr_best:  12.04%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  2.311117/  2.326993, val:  10.00%, val_best:  10.00%, tr:  11.86%, tr_best:  12.04%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  2.310355/  2.313930, val:  10.00%, val_best:  10.00%, tr:  11.38%, tr_best:  12.04%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  2.311205/  2.325738, val:  10.00%, val_best:  10.00%, tr:  10.75%, tr_best:  12.04%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  2.308423/  2.310772, val:  10.00%, val_best:  10.00%, tr:  11.20%, tr_best:  12.04%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  2.308753/  2.325564, val:  10.00%, val_best:  10.00%, tr:  11.77%, tr_best:  12.04%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.309486/  2.334863, val:  10.00%, val_best:  10.00%, tr:  10.87%, tr_best:  12.04%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.311709/  2.322237, val:  10.00%, val_best:  10.00%, tr:  11.52%, tr_best:  12.04%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.311193/  2.323013, val:  10.00%, val_best:  10.00%, tr:  11.43%, tr_best:  12.04%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.310013/  2.342222, val:  10.00%, val_best:  10.00%, tr:  11.36%, tr_best:  12.04%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.309715/  2.318968, val:  10.00%, val_best:  10.00%, tr:  11.00%, tr_best:  12.04%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.308726/  2.338284, val:  10.00%, val_best:  10.00%, tr:  11.72%, tr_best:  12.04%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.309965/  2.314572, val:  10.00%, val_best:  10.00%, tr:  11.54%, tr_best:  12.04%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.310320/  2.337421, val:  10.00%, val_best:  10.00%, tr:  11.09%, tr_best:  12.04%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  2.311857/  2.322965, val:  10.00%, val_best:  10.00%, tr:  10.46%, tr_best:  12.04%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.310583/  2.319780, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  12.04%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.308661/  2.314351, val:  10.00%, val_best:  10.00%, tr:  11.59%, tr_best:  12.04%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.312303/  2.332501, val:  10.00%, val_best:  10.00%, tr:  11.20%, tr_best:  12.04%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.309097/  2.320082, val:  10.00%, val_best:  10.00%, tr:  11.79%, tr_best:  12.04%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.310119/  2.315516, val:  10.00%, val_best:  10.00%, tr:  10.89%, tr_best:  12.04%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.308915/  2.331731, val:  10.00%, val_best:  10.00%, tr:  11.27%, tr_best:  12.04%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.313266/  2.314847, val:  10.00%, val_best:  10.00%, tr:  10.75%, tr_best:  12.04%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.307828/  2.315791, val:  10.00%, val_best:  10.00%, tr:  10.98%, tr_best:  12.04%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.307024/  2.327160, val:  10.00%, val_best:  10.00%, tr:  11.81%, tr_best:  12.04%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  2.311325/  2.327400, val:  10.00%, val_best:  10.00%, tr:  11.18%, tr_best:  12.04%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.311329/  2.332263, val:  10.00%, val_best:  10.00%, tr:  10.78%, tr_best:  12.04%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.314122/  2.325414, val:  10.00%, val_best:  10.00%, tr:  10.98%, tr_best:  12.04%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.311021/  2.314025, val:  10.00%, val_best:  10.00%, tr:  10.71%, tr_best:  12.04%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.306807/  2.317743, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.307983/  2.339308, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.307885/  2.330388, val:  10.00%, val_best:  10.00%, tr:  11.56%, tr_best:  12.04%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.308486/  2.312471, val:  10.00%, val_best:  10.00%, tr:  12.02%, tr_best:  12.04%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  2.312144/  2.316271, val:  10.00%, val_best:  10.00%, tr:  11.43%, tr_best:  12.04%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.307326/  2.315969, val:  10.00%, val_best:  10.00%, tr:  11.61%, tr_best:  12.04%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.311460/  2.337556, val:  10.00%, val_best:  10.00%, tr:  11.18%, tr_best:  12.04%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.309283/  2.316134, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.309570/  2.359777, val:  10.00%, val_best:  10.00%, tr:  11.86%, tr_best:  12.04%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.314904/  2.314409, val:  10.00%, val_best:  10.00%, tr:  10.75%, tr_best:  12.04%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.310363/  2.318036, val:  10.00%, val_best:  10.00%, tr:  11.23%, tr_best:  12.04%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.309212/  2.310428, val:  10.00%, val_best:  10.00%, tr:  11.14%, tr_best:  12.04%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.313053/  2.320248, val:  10.00%, val_best:  10.00%, tr:  10.60%, tr_best:  12.04%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.311198/  2.323355, val:  10.00%, val_best:  10.00%, tr:  11.09%, tr_best:  12.04%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  2.314126/  2.314701, val:  10.00%, val_best:  10.00%, tr:  11.34%, tr_best:  12.04%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  2.310761/  2.329582, val:  10.00%, val_best:  10.00%, tr:  10.84%, tr_best:  12.04%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.311206/  2.336532, val:  10.00%, val_best:  10.00%, tr:  11.65%, tr_best:  12.04%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.311120/  2.328647, val:  10.00%, val_best:  10.00%, tr:  11.27%, tr_best:  12.04%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.311873/  2.307673, val:  10.00%, val_best:  10.00%, tr:  10.91%, tr_best:  12.04%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.308321/  2.355703, val:  10.00%, val_best:  10.00%, tr:  11.07%, tr_best:  12.04%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  2.312194/  2.317735, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  2.310633/  2.312312, val:  10.00%, val_best:  10.00%, tr:  11.02%, tr_best:  12.04%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  2.307499/  2.327483, val:  10.00%, val_best:  10.00%, tr:  11.20%, tr_best:  12.04%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.309154/  2.328090, val:  10.00%, val_best:  10.00%, tr:  11.54%, tr_best:  12.04%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.310326/  2.313795, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.04%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  2.308818/  2.310107, val:  10.00%, val_best:  10.00%, tr:  10.91%, tr_best:  12.04%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  2.313966/  2.326728, val:  10.00%, val_best:  10.00%, tr:  10.98%, tr_best:  12.04%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  2.308399/  2.325821, val:  10.00%, val_best:  10.00%, tr:  11.18%, tr_best:  12.04%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.307315/  2.315957, val:  10.00%, val_best:  10.00%, tr:  10.89%, tr_best:  12.04%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  2.310255/  2.315364, val:  10.00%, val_best:  10.00%, tr:  10.53%, tr_best:  12.04%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  2.309570/  2.325358, val:  10.00%, val_best:  10.00%, tr:  11.18%, tr_best:  12.04%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  2.309862/  2.336876, val:  10.00%, val_best:  10.00%, tr:  11.54%, tr_best:  12.04%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  2.310051/  2.329872, val:  10.00%, val_best:  10.00%, tr:  11.86%, tr_best:  12.04%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.309705/  2.332129, val:  10.00%, val_best:  10.00%, tr:  11.63%, tr_best:  12.04%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.312120/  2.326112, val:  10.00%, val_best:  10.00%, tr:  11.25%, tr_best:  12.04%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.308165/  2.324929, val:  10.00%, val_best:  10.00%, tr:  11.56%, tr_best:  12.04%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  2.310988/  2.326405, val:  10.00%, val_best:  10.00%, tr:  11.16%, tr_best:  12.04%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  2.310163/  2.314960, val:  10.00%, val_best:  10.00%, tr:  11.61%, tr_best:  12.04%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  2.312999/  2.322777, val:  10.00%, val_best:  10.00%, tr:  11.43%, tr_best:  12.04%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  2.312900/  2.317387, val:  10.00%, val_best:  10.00%, tr:  11.41%, tr_best:  12.04%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  2.311446/  2.335660, val:  10.00%, val_best:  10.00%, tr:  10.80%, tr_best:  12.04%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  2.313660/  2.325460, val:  10.00%, val_best:  10.00%, tr:  10.69%, tr_best:  12.04%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  2.313882/  2.334766, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  2.305652/  2.319712, val:  10.00%, val_best:  10.00%, tr:  11.63%, tr_best:  12.04%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  2.309252/  2.310574, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  2.309480/  2.330408, val:  10.00%, val_best:  10.00%, tr:  11.43%, tr_best:  12.04%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.314740/  2.316997, val:  10.00%, val_best:  10.00%, tr:  10.91%, tr_best:  12.04%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  2.310700/  2.319560, val:  10.00%, val_best:  10.00%, tr:  11.29%, tr_best:  12.04%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  2.313793/  2.310966, val:  10.00%, val_best:  10.00%, tr:  10.82%, tr_best:  12.04%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  2.310887/  2.317144, val:  10.00%, val_best:  10.00%, tr:  10.98%, tr_best:  12.04%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  2.310653/  2.326299, val:  10.00%, val_best:  10.00%, tr:  11.70%, tr_best:  12.04%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.310845/  2.325767, val:  10.00%, val_best:  10.00%, tr:  10.98%, tr_best:  12.04%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  2.311365/  2.334502, val:  10.00%, val_best:  10.00%, tr:  11.23%, tr_best:  12.04%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  2.310180/  2.335211, val:  10.00%, val_best:  10.00%, tr:  11.18%, tr_best:  12.04%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  2.312549/  2.315858, val:  10.00%, val_best:  10.00%, tr:  11.86%, tr_best:  12.04%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  2.310207/  2.323152, val:  10.00%, val_best:  10.00%, tr:  11.02%, tr_best:  12.04%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  2.311802/  2.325397, val:  10.00%, val_best:  10.00%, tr:  11.14%, tr_best:  12.04%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  2.313088/  2.316913, val:  10.00%, val_best:  10.00%, tr:  11.41%, tr_best:  12.04%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  2.310720/  2.318682, val:  10.00%, val_best:  10.00%, tr:  11.88%, tr_best:  12.04%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  2.309914/  2.313889, val:  10.00%, val_best:  10.00%, tr:  10.66%, tr_best:  12.04%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  2.312624/  2.325377, val:  10.00%, val_best:  10.00%, tr:  11.47%, tr_best:  12.04%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  2.314283/  2.321078, val:  10.00%, val_best:  10.00%, tr:  11.09%, tr_best:  12.04%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692194ebee5c47e887ea525e74b2644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>█▂▄▅▅▅▄▄▇▄▁▄▁▅▅▂▁▄▅▂▂▅▇▄▁▇▅▂▄▄▁▂▄▂▄▇▂▄▂▂</td></tr><tr><td>summary_val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁███▇█████▇█▇▇█▇███▇▇██▇▇▇█████▇████████</td></tr><tr><td>tr_epoch_loss</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_best</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.11091</td></tr><tr><td>tr_epoch_loss</td><td>2.31428</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.32108</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sweep-54</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0wbw4vqk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0wbw4vqk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_072451-0wbw4vqk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: biheomqn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_073727-biheomqn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/biheomqn' target=\"_blank\">cosmic-sweep-55</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/biheomqn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/biheomqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.925937/  0.962100, val:  58.75%, val_best:  58.75%, tr:  60.80%, tr_best:  60.80%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.620033/  0.796098, val:  66.67%, val_best:  66.67%, tr:  71.82%, tr_best:  71.82%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411212/  0.680588, val:  78.33%, val_best:  78.33%, tr:  83.23%, tr_best:  83.23%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.281519/  0.453874, val:  87.50%, val_best:  87.50%, tr:  89.50%, tr_best:  89.50%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.173730/  0.438690, val:  89.17%, val_best:  89.17%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.131084/  0.379213, val:  89.58%, val_best:  89.58%, tr:  95.60%, tr_best:  95.60%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.086907/  0.452013, val:  89.17%, val_best:  89.58%, tr:  97.11%, tr_best:  97.11%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.067697/  0.475806, val:  90.00%, val_best:  90.00%, tr:  97.88%, tr_best:  97.88%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036230/  0.564496, val:  87.08%, val_best:  90.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.027988/  0.473823, val:  88.75%, val_best:  90.00%, tr:  98.96%, tr_best:  99.08%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.022219/  0.579324, val:  88.33%, val_best:  90.00%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.010694/  0.597973, val:  87.50%, val_best:  90.00%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003942/  0.574832, val:  88.75%, val_best:  90.00%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002230/  0.577591, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001721/  0.549777, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001340/  0.551486, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001115/  0.559681, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000964/  0.557628, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000855/  0.562009, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000767/  0.586662, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000701/  0.575527, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000667/  0.573591, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000630/  0.574261, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000584/  0.575940, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000550/  0.584995, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000510/  0.581284, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000499/  0.586610, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000466/  0.591101, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000446/  0.584309, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000426/  0.595009, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000420/  0.584428, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000397/  0.586886, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000375/  0.589818, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000359/  0.596350, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000352/  0.605685, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000336/  0.604453, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000324/  0.607680, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000315/  0.602724, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000308/  0.608555, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000296/  0.611728, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000282/  0.606344, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000274/  0.607052, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000268/  0.609053, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000258/  0.606337, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000256/  0.604765, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000248/  0.603075, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000243/  0.603179, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000235/  0.607299, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000234/  0.605558, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000228/  0.603638, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000222/  0.606064, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000216/  0.601828, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000211/  0.605405, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000206/  0.601419, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000200/  0.601284, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000196/  0.609681, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000192/  0.613942, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000188/  0.610085, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000185/  0.609123, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000178/  0.608805, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000177/  0.610920, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000172/  0.605248, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000176/  0.613553, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614912, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614062, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000161/  0.612927, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000162/  0.615787, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000156/  0.617892, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000153/  0.615856, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000151/  0.620698, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000147/  0.627239, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000146/  0.630415, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000144/  0.633122, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000143/  0.638761, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000140/  0.647128, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000138/  0.649175, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000135/  0.653331, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000135/  0.653878, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000133/  0.657584, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000132/  0.654067, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.654005, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000126/  0.653358, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000125/  0.655715, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000122/  0.651898, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000120/  0.655880, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000120/  0.657423, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000117/  0.653922, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000116/  0.655184, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000114/  0.653668, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000112/  0.658471, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000110/  0.661146, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000111/  0.662841, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.660637, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.660053, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.660780, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000105/  0.662403, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000103/  0.665917, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000103/  0.668271, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000101/  0.670042, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.668199, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a705372a29240a98979ec4fed5b111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.6682</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-sweep-55</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/biheomqn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/biheomqn</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_073727-biheomqn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ei1xu13s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_075020-ei1xu13s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ei1xu13s' target=\"_blank\">icy-sweep-56</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ei1xu13s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ei1xu13s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.953882/  0.999939, val:  60.83%, val_best:  60.83%, tr:  58.88%, tr_best:  58.88%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.662181/  0.907265, val:  57.08%, val_best:  60.83%, tr:  69.34%, tr_best:  69.34%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.537935/  0.886468, val:  63.75%, val_best:  63.75%, tr:  74.98%, tr_best:  74.98%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.451322/  0.985177, val:  69.17%, val_best:  69.17%, tr:  79.31%, tr_best:  79.31%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.336750/  0.766279, val:  75.42%, val_best:  75.42%, tr:  86.52%, tr_best:  86.52%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.291317/  0.627707, val:  78.75%, val_best:  78.75%, tr:  88.17%, tr_best:  88.17%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.232897/  0.727227, val:  84.17%, val_best:  84.17%, tr:  90.98%, tr_best:  90.98%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.200366/  0.881923, val:  75.83%, val_best:  84.17%, tr:  92.90%, tr_best:  92.90%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.182439/  0.572845, val:  80.42%, val_best:  84.17%, tr:  93.39%, tr_best:  93.39%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.152172/  0.735089, val:  79.17%, val_best:  84.17%, tr:  94.86%, tr_best:  94.86%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.130957/  0.671627, val:  83.75%, val_best:  84.17%, tr:  95.33%, tr_best:  95.33%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.113581/  0.710056, val:  79.17%, val_best:  84.17%, tr:  96.06%, tr_best:  96.06%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.091088/  0.670401, val:  83.33%, val_best:  84.17%, tr:  96.96%, tr_best:  96.96%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.099186/  0.729208, val:  81.67%, val_best:  84.17%, tr:  96.62%, tr_best:  96.96%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.114317/  0.560195, val:  85.42%, val_best:  85.42%, tr:  95.87%, tr_best:  96.96%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.066710/  0.575347, val:  83.33%, val_best:  85.42%, tr:  97.66%, tr_best:  97.66%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.070153/  0.642833, val:  83.33%, val_best:  85.42%, tr:  97.57%, tr_best:  97.66%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.062837/  0.672072, val:  82.92%, val_best:  85.42%, tr:  98.02%, tr_best:  98.02%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.040127/  0.677156, val:  82.92%, val_best:  85.42%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.034533/  0.663207, val:  85.83%, val_best:  85.83%, tr:  99.01%, tr_best:  99.01%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.025628/  0.712360, val:  84.17%, val_best:  85.83%, tr:  99.19%, tr_best:  99.19%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.018541/  0.622228, val:  85.00%, val_best:  85.83%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.015296/  0.597883, val:  85.83%, val_best:  85.83%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.009045/  0.588058, val:  86.25%, val_best:  86.25%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.005321/  0.618681, val:  87.08%, val_best:  87.08%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.004175/  0.627268, val:  86.25%, val_best:  87.08%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.001606/  0.664020, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.001198/  0.638397, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000927/  0.649230, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000815/  0.655273, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000725/  0.638243, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000657/  0.646109, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000617/  0.653141, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000565/  0.655642, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000541/  0.669791, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000520/  0.682242, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000481/  0.674883, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000463/  0.675863, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000446/  0.677783, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000422/  0.683026, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000405/  0.679317, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000387/  0.675316, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000375/  0.680088, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000367/  0.671304, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000347/  0.678886, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000335/  0.674517, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000321/  0.681586, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000313/  0.683949, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000302/  0.682245, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000295/  0.681244, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000288/  0.678184, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000286/  0.676484, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000277/  0.679238, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000270/  0.674995, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000261/  0.673572, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000254/  0.678580, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000246/  0.679748, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000239/  0.686899, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000237/  0.695745, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000230/  0.692951, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000228/  0.694661, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000221/  0.693099, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000222/  0.698329, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000215/  0.701996, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000209/  0.698739, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000204/  0.698843, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000203/  0.704135, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000199/  0.701027, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000193/  0.704201, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000191/  0.707913, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000187/  0.703786, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000183/  0.701564, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000181/  0.703639, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000179/  0.702744, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000178/  0.706544, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000172/  0.706477, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000172/  0.706202, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000170/  0.707124, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000166/  0.707308, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000164/  0.701616, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000160/  0.700644, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000160/  0.698461, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000157/  0.701218, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000154/  0.701721, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000153/  0.704683, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000149/  0.706419, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000147/  0.703452, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000146/  0.707661, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000145/  0.707997, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000143/  0.708960, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000141/  0.710686, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000138/  0.708252, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000136/  0.708994, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000134/  0.704195, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000134/  0.705087, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000132/  0.703984, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000131/  0.707999, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000128/  0.707574, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000127/  0.709638, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000126/  0.713173, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b29d944d874250a98b1eeeab8aff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▅▆▆█▅█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇█▇▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00013</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>0.71317</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-56</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ei1xu13s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ei1xu13s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_075020-ei1xu13s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ksl3996i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_080302-ksl3996i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksl3996i' target=\"_blank\">blooming-sweep-57</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksl3996i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksl3996i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.175600/  1.349039, val:  52.08%, val_best:  52.08%, tr:  52.50%, tr_best:  52.50%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  1.000961/  1.092820, val:  55.00%, val_best:  55.00%, tr:  57.89%, tr_best:  57.89%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.979460/  1.318723, val:  49.17%, val_best:  55.00%, tr:  60.23%, tr_best:  60.23%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  1.085589/  1.849903, val:  55.42%, val_best:  55.42%, tr:  58.43%, tr_best:  60.23%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  1.003224/  1.397968, val:  50.83%, val_best:  55.42%, tr:  61.29%, tr_best:  61.29%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.959481/  1.177873, val:  59.58%, val_best:  59.58%, tr:  61.77%, tr_best:  61.77%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.994009/  1.214665, val:  53.75%, val_best:  59.58%, tr:  62.83%, tr_best:  62.83%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.938743/  1.452128, val:  53.33%, val_best:  59.58%, tr:  61.86%, tr_best:  62.83%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.961061/  1.148716, val:  61.25%, val_best:  61.25%, tr:  61.97%, tr_best:  62.83%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.889786/  1.116736, val:  61.25%, val_best:  61.25%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.892625/  1.130965, val:  54.58%, val_best:  61.25%, tr:  61.70%, tr_best:  65.17%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.851540/  1.391323, val:  55.42%, val_best:  61.25%, tr:  61.97%, tr_best:  65.17%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.880412/  1.076968, val:  55.00%, val_best:  61.25%, tr:  63.44%, tr_best:  65.17%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.840294/  1.016574, val:  55.42%, val_best:  61.25%, tr:  62.83%, tr_best:  65.17%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.967999/  1.520702, val:  49.58%, val_best:  61.25%, tr:  63.03%, tr_best:  65.17%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.838694/  1.089829, val:  52.92%, val_best:  61.25%, tr:  63.03%, tr_best:  65.17%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.923798/  1.091317, val:  62.50%, val_best:  62.50%, tr:  63.59%, tr_best:  65.17%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.942989/  1.249938, val:  57.92%, val_best:  62.50%, tr:  62.87%, tr_best:  65.17%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.856022/  1.396354, val:  57.08%, val_best:  62.50%, tr:  63.57%, tr_best:  65.17%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.901121/  1.395710, val:  47.92%, val_best:  62.50%, tr:  62.40%, tr_best:  65.17%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  1.032200/  2.100169, val:  48.33%, val_best:  62.50%, tr:  60.44%, tr_best:  65.17%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.992810/  1.115658, val:  54.58%, val_best:  62.50%, tr:  59.42%, tr_best:  65.17%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.982544/  1.241211, val:  59.17%, val_best:  62.50%, tr:  61.72%, tr_best:  65.17%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.988255/  1.533320, val:  59.58%, val_best:  62.50%, tr:  63.48%, tr_best:  65.17%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.045107/  1.946558, val:  50.83%, val_best:  62.50%, tr:  62.92%, tr_best:  65.17%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  1.168271/  1.991099, val:  50.00%, val_best:  62.50%, tr:  60.53%, tr_best:  65.17%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  1.022266/  1.558622, val:  58.33%, val_best:  62.50%, tr:  61.02%, tr_best:  65.17%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.921471/  1.549147, val:  54.17%, val_best:  62.50%, tr:  63.64%, tr_best:  65.17%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.959613/  1.229558, val:  54.17%, val_best:  62.50%, tr:  63.05%, tr_best:  65.17%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.921106/  1.232634, val:  52.50%, val_best:  62.50%, tr:  62.44%, tr_best:  65.17%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.915350/  1.251215, val:  58.33%, val_best:  62.50%, tr:  64.38%, tr_best:  65.17%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.955582/  1.479481, val:  55.42%, val_best:  62.50%, tr:  64.65%, tr_best:  65.17%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.919665/  1.416317, val:  57.08%, val_best:  62.50%, tr:  63.93%, tr_best:  65.17%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.908072/  1.517583, val:  54.58%, val_best:  62.50%, tr:  64.31%, tr_best:  65.17%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.854206/  1.519726, val:  54.58%, val_best:  62.50%, tr:  64.95%, tr_best:  65.17%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.900380/  1.426506, val:  54.58%, val_best:  62.50%, tr:  64.90%, tr_best:  65.17%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.992140/  1.719996, val:  56.67%, val_best:  62.50%, tr:  64.68%, tr_best:  65.17%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  1.021266/  1.254186, val:  57.50%, val_best:  62.50%, tr:  62.51%, tr_best:  65.17%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.967088/  1.515051, val:  55.42%, val_best:  62.50%, tr:  63.71%, tr_best:  65.17%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  1.014891/  1.981053, val:  56.25%, val_best:  62.50%, tr:  63.01%, tr_best:  65.17%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  1.032664/  1.376068, val:  54.17%, val_best:  62.50%, tr:  62.89%, tr_best:  65.17%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  1.003821/  1.238969, val:  61.25%, val_best:  62.50%, tr:  64.20%, tr_best:  65.17%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.943986/  1.897063, val:  64.58%, val_best:  64.58%, tr:  66.07%, tr_best:  66.07%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  1.017900/  1.862774, val:  57.08%, val_best:  64.58%, tr:  64.22%, tr_best:  66.07%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  1.006741/  1.802865, val:  59.58%, val_best:  64.58%, tr:  64.88%, tr_best:  66.07%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  1.004975/  1.425431, val:  57.08%, val_best:  64.58%, tr:  64.00%, tr_best:  66.07%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  1.095780/  1.492360, val:  57.08%, val_best:  64.58%, tr:  62.26%, tr_best:  66.07%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.972562/  1.342596, val:  54.17%, val_best:  64.58%, tr:  65.06%, tr_best:  66.07%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  1.076115/  1.635140, val:  54.58%, val_best:  64.58%, tr:  64.02%, tr_best:  66.07%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.914423/  1.629127, val:  59.58%, val_best:  64.58%, tr:  65.78%, tr_best:  66.07%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  1.097911/  1.437627, val:  60.83%, val_best:  64.58%, tr:  66.48%, tr_best:  66.48%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  1.122469/  1.330883, val:  57.92%, val_best:  64.58%, tr:  61.74%, tr_best:  66.48%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.972636/  1.300555, val:  59.58%, val_best:  64.58%, tr:  61.86%, tr_best:  66.48%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.939828/  1.366539, val:  55.83%, val_best:  64.58%, tr:  63.93%, tr_best:  66.48%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  1.060416/  1.735907, val:  53.33%, val_best:  64.58%, tr:  63.50%, tr_best:  66.48%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.969345/  1.172281, val:  60.00%, val_best:  64.58%, tr:  64.25%, tr_best:  66.48%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.939154/  1.337426, val:  53.33%, val_best:  64.58%, tr:  64.63%, tr_best:  66.48%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.990261/  1.844305, val:  50.00%, val_best:  64.58%, tr:  63.03%, tr_best:  66.48%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.990727/  1.271849, val:  58.33%, val_best:  64.58%, tr:  63.39%, tr_best:  66.48%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  1.052145/  1.576621, val:  48.75%, val_best:  64.58%, tr:  62.92%, tr_best:  66.48%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  1.025825/  1.968405, val:  50.00%, val_best:  64.58%, tr:  62.08%, tr_best:  66.48%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  1.098383/  2.255517, val:  45.42%, val_best:  64.58%, tr:  58.99%, tr_best:  66.48%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  1.084093/  1.749835, val:  52.92%, val_best:  64.58%, tr:  62.78%, tr_best:  66.48%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.881184/  1.485853, val:  53.75%, val_best:  64.58%, tr:  65.06%, tr_best:  66.48%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  1.023950/  1.419950, val:  53.33%, val_best:  64.58%, tr:  63.35%, tr_best:  66.48%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.971002/  1.497411, val:  56.25%, val_best:  64.58%, tr:  62.51%, tr_best:  66.48%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  1.095985/  1.559712, val:  59.58%, val_best:  64.58%, tr:  61.59%, tr_best:  66.48%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  1.044764/  1.412035, val:  52.50%, val_best:  64.58%, tr:  60.60%, tr_best:  66.48%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  1.057603/  1.436621, val:  55.00%, val_best:  64.58%, tr:  58.95%, tr_best:  66.48%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  1.114458/  1.024552, val:  61.25%, val_best:  64.58%, tr:  62.22%, tr_best:  66.48%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.984767/  1.647768, val:  58.75%, val_best:  64.58%, tr:  62.98%, tr_best:  66.48%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.973090/  1.068424, val:  62.50%, val_best:  64.58%, tr:  63.03%, tr_best:  66.48%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.973251/  1.624642, val:  59.17%, val_best:  64.58%, tr:  63.84%, tr_best:  66.48%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.921568/  1.265385, val:  56.67%, val_best:  64.58%, tr:  65.10%, tr_best:  66.48%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.949813/  1.175986, val:  60.00%, val_best:  64.58%, tr:  64.81%, tr_best:  66.48%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.890238/  1.293143, val:  60.83%, val_best:  64.58%, tr:  67.27%, tr_best:  67.27%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.994800/  1.299374, val:  62.08%, val_best:  64.58%, tr:  65.49%, tr_best:  67.27%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  1.041193/  1.156240, val:  58.75%, val_best:  64.58%, tr:  61.05%, tr_best:  67.27%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.949307/  1.121242, val:  63.75%, val_best:  64.58%, tr:  64.59%, tr_best:  67.27%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.909844/  1.369279, val:  57.08%, val_best:  64.58%, tr:  68.10%, tr_best:  68.10%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.951870/  1.625325, val:  55.00%, val_best:  64.58%, tr:  67.47%, tr_best:  68.10%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.908102/  1.014379, val:  62.08%, val_best:  64.58%, tr:  68.39%, tr_best:  68.39%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.887639/  1.450872, val:  61.67%, val_best:  64.58%, tr:  68.01%, tr_best:  68.39%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.911653/  1.275031, val:  62.08%, val_best:  64.58%, tr:  66.19%, tr_best:  68.39%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.873161/  0.977625, val:  63.33%, val_best:  64.58%, tr:  66.61%, tr_best:  68.39%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.949023/  1.073046, val:  66.25%, val_best:  66.25%, tr:  67.67%, tr_best:  68.39%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.960594/  1.529342, val:  59.17%, val_best:  66.25%, tr:  66.70%, tr_best:  68.39%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  1.031457/  1.385123, val:  55.42%, val_best:  66.25%, tr:  65.58%, tr_best:  68.39%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.862973/  1.328339, val:  57.92%, val_best:  66.25%, tr:  67.43%, tr_best:  68.39%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.924058/  1.839856, val:  57.08%, val_best:  66.25%, tr:  67.58%, tr_best:  68.39%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.919219/  1.182956, val:  64.17%, val_best:  66.25%, tr:  66.95%, tr_best:  68.39%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  1.170976/  1.109751, val:  57.92%, val_best:  66.25%, tr:  64.00%, tr_best:  68.39%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  1.078056/  1.962499, val:  50.00%, val_best:  66.25%, tr:  61.86%, tr_best:  68.39%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  1.138774/  1.382622, val:  60.00%, val_best:  66.25%, tr:  60.26%, tr_best:  68.39%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.936857/  1.776072, val:  55.00%, val_best:  66.25%, tr:  65.42%, tr_best:  68.39%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  1.041933/  1.715148, val:  52.50%, val_best:  66.25%, tr:  64.70%, tr_best:  68.39%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  1.021336/  1.212483, val:  60.83%, val_best:  66.25%, tr:  63.75%, tr_best:  68.39%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.966463/  1.487292, val:  59.58%, val_best:  66.25%, tr:  65.58%, tr_best:  68.39%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.879697/  1.722976, val:  57.08%, val_best:  66.25%, tr:  66.28%, tr_best:  68.39%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.966498/  1.472302, val:  54.58%, val_best:  66.25%, tr:  66.82%, tr_best:  68.39%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7300bbd8cba3454f85683c5b90bd8da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▆▅▄▄▄▅▂▃▆▁▄▇▅▇▅▂▄▃▅▃▃▃▃▃█▅▇▅▆▃▅▅▅▅▆▅▃▁▅</td></tr><tr><td>summary_val_acc</td><td>▁▇▆▇▇▇▆▇▆▇▆▇▇▇▇▇▇█▇▇▇▇▇▆▆▇▇▇▇▇▇█▇██▇█▆▇▇</td></tr><tr><td>tr_acc</td><td>▁▇▇▇█▇▇▇▇▇▇█▇██▇▇████▇█▇▇▇▇▇▇████████▇██</td></tr><tr><td>tr_epoch_loss</td><td>▁▇▇▇▇▆▇▇▇▇█▇▇▇▆▇▇▇▇▇▇▇▇▇██▇██▇▇▇▇▇▇▆▇██▇</td></tr><tr><td>val_acc_best</td><td>▁▇▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▇▆▇▇▇▆▇▆▇▆▇▇▇▇▇▇█▇▇▇▇▇▆▆▇▇▇▇▇▇█▇██▇█▆▇▇</td></tr><tr><td>val_loss</td><td>▁▅▆▆▅▆▆▅▆▅█▆▅▆▆▅██▆▆▇▆▅█▇▇▆▆▅▇▆▅▇▆▅▆▅█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.25</td></tr><tr><td>tr_acc</td><td>0.66817</td></tr><tr><td>tr_epoch_loss</td><td>0.9665</td></tr><tr><td>val_acc_best</td><td>0.6625</td></tr><tr><td>val_acc_now</td><td>0.54583</td></tr><tr><td>val_loss</td><td>1.4723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-57</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksl3996i' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ksl3996i</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_080302-ksl3996i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6l489s3j with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_081452-6l489s3j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l489s3j' target=\"_blank\">crisp-sweep-58</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l489s3j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l489s3j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  0.985705/  0.782255, val:  65.42%, val_best:  65.42%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.615856/  0.784570, val:  64.17%, val_best:  65.42%, tr:  70.54%, tr_best:  70.54%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.509993/  0.681711, val:  70.83%, val_best:  70.83%, tr:  74.98%, tr_best:  74.98%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.464675/  0.649360, val:  75.42%, val_best:  75.42%, tr:  78.92%, tr_best:  78.92%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.390787/  0.697770, val:  70.83%, val_best:  75.42%, tr:  83.27%, tr_best:  83.27%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.345549/  0.646499, val:  81.25%, val_best:  81.25%, tr:  85.55%, tr_best:  85.55%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.282133/  0.651854, val:  81.25%, val_best:  81.25%, tr:  89.63%, tr_best:  89.63%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.228802/  0.711891, val:  74.17%, val_best:  81.25%, tr:  91.88%, tr_best:  91.88%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.209925/  0.662762, val:  77.92%, val_best:  81.25%, tr:  92.25%, tr_best:  92.25%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.150566/  0.685238, val:  81.25%, val_best:  81.25%, tr:  95.00%, tr_best:  95.00%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.124842/  0.573499, val:  83.75%, val_best:  83.75%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.097971/  0.706033, val:  84.58%, val_best:  84.58%, tr:  96.98%, tr_best:  96.98%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.082384/  0.664600, val:  81.67%, val_best:  84.58%, tr:  97.61%, tr_best:  97.61%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.067073/  0.612353, val:  86.25%, val_best:  86.25%, tr:  98.08%, tr_best:  98.08%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.056860/  0.543375, val:  86.67%, val_best:  86.67%, tr:  98.56%, tr_best:  98.56%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.040798/  0.532972, val:  85.00%, val_best:  86.67%, tr:  99.14%, tr_best:  99.14%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.045382/  0.579348, val:  84.58%, val_best:  86.67%, tr:  98.78%, tr_best:  99.14%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.032580/  0.619796, val:  83.33%, val_best:  86.67%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.028627/  0.555056, val:  87.08%, val_best:  87.08%, tr:  99.46%, tr_best:  99.46%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.019181/  0.648560, val:  87.08%, val_best:  87.08%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.047629/  0.635213, val:  83.75%, val_best:  87.08%, tr:  98.60%, tr_best:  99.66%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.019110/  0.655823, val:  86.25%, val_best:  87.08%, tr:  99.59%, tr_best:  99.66%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.011064/  0.632191, val:  85.83%, val_best:  87.08%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.010496/  0.616402, val:  87.08%, val_best:  87.08%, tr:  99.89%, tr_best:  99.98%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.007118/  0.615980, val:  87.08%, val_best:  87.08%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.006287/  0.631548, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.005809/  0.646367, val:  87.50%, val_best:  87.50%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.004992/  0.624668, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.005340/  0.617366, val:  86.67%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.004699/  0.636784, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.003912/  0.656598, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.003622/  0.668038, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.003407/  0.672636, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.003131/  0.656540, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.002947/  0.677408, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.002637/  0.661715, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.002511/  0.692154, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.002414/  0.664669, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.002375/  0.690582, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.002231/  0.676650, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.002067/  0.683044, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.002073/  0.660023, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.001893/  0.671774, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.001838/  0.698415, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.001804/  0.678083, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001699/  0.683938, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.001674/  0.687292, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.001566/  0.698591, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.001497/  0.698392, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.001463/  0.690200, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.001448/  0.697903, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.001401/  0.698691, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.001353/  0.696239, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.001368/  0.688043, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001277/  0.699879, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001269/  0.701713, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.001229/  0.696421, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.001227/  0.706137, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001177/  0.701729, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001151/  0.705252, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001111/  0.713169, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001104/  0.717781, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001080/  0.722257, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001040/  0.708791, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001019/  0.706369, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001014/  0.713010, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000972/  0.716647, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000967/  0.718391, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000941/  0.724153, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000907/  0.715103, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000890/  0.719870, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000879/  0.728077, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000889/  0.734333, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000851/  0.731584, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000834/  0.731722, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000813/  0.722001, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000824/  0.737560, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000812/  0.737427, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000789/  0.739512, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000793/  0.735096, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000760/  0.741475, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000732/  0.732986, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000728/  0.733703, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000735/  0.741048, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000711/  0.738336, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000700/  0.735548, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000697/  0.739852, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000686/  0.740820, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000674/  0.738638, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000655/  0.738051, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000656/  0.743842, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000637/  0.735162, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000630/  0.742656, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000624/  0.741511, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000627/  0.741955, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000614/  0.753111, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000597/  0.749942, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000600/  0.757262, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000581/  0.756338, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000574/  0.754070, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6602fabd4e8a4358be3d7a74ea960f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▆▅▆███▆███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00057</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>0.75407</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-sweep-58</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l489s3j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6l489s3j</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_081452-6l489s3j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q6xxsjyq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_082615-q6xxsjyq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q6xxsjyq' target=\"_blank\">likely-sweep-59</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q6xxsjyq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q6xxsjyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.5, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.202746/  1.798478, val:  29.17%, val_best:  29.17%, tr:  16.10%, tr_best:  16.10%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.125508/  0.966350, val:  58.75%, val_best:  58.75%, tr:  54.78%, tr_best:  54.78%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.757269/  0.828212, val:  64.17%, val_best:  64.17%, tr:  65.89%, tr_best:  65.89%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.643623/  0.765136, val:  66.67%, val_best:  66.67%, tr:  70.83%, tr_best:  70.83%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.576463/  0.710469, val:  70.42%, val_best:  70.42%, tr:  72.70%, tr_best:  72.70%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.520954/  0.698983, val:  70.83%, val_best:  70.83%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.477465/  0.645683, val:  73.75%, val_best:  73.75%, tr:  77.41%, tr_best:  77.41%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.431198/  0.615632, val:  79.17%, val_best:  79.17%, tr:  80.55%, tr_best:  80.55%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.388018/  0.619418, val:  78.33%, val_best:  79.17%, tr:  83.48%, tr_best:  83.48%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.348291/  0.611260, val:  80.83%, val_best:  80.83%, tr:  85.62%, tr_best:  85.62%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.300321/  0.598641, val:  79.58%, val_best:  80.83%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.274315/  0.585077, val:  82.08%, val_best:  82.08%, tr:  90.24%, tr_best:  90.24%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.236376/  0.579386, val:  82.08%, val_best:  82.08%, tr:  91.70%, tr_best:  91.70%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.191412/  0.518070, val:  85.83%, val_best:  85.83%, tr:  93.85%, tr_best:  93.85%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.169229/  0.539953, val:  83.33%, val_best:  85.83%, tr:  94.32%, tr_best:  94.32%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.131815/  0.499962, val:  85.83%, val_best:  85.83%, tr:  96.06%, tr_best:  96.06%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.120403/  0.518095, val:  86.25%, val_best:  86.25%, tr:  96.46%, tr_best:  96.46%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.116719/  0.487488, val:  85.00%, val_best:  86.25%, tr:  96.15%, tr_best:  96.46%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.094241/  0.501193, val:  87.50%, val_best:  87.50%, tr:  97.36%, tr_best:  97.36%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.080511/  0.532754, val:  86.67%, val_best:  87.50%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.075127/  0.510739, val:  85.83%, val_best:  87.50%, tr:  97.86%, tr_best:  97.86%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.056791/  0.535446, val:  86.67%, val_best:  87.50%, tr:  98.65%, tr_best:  98.65%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.053040/  0.540913, val:  85.83%, val_best:  87.50%, tr:  98.74%, tr_best:  98.74%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.059799/  0.472415, val:  87.50%, val_best:  87.50%, tr:  98.53%, tr_best:  98.74%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.043972/  0.541843, val:  87.08%, val_best:  87.50%, tr:  99.03%, tr_best:  99.03%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.046446/  0.523152, val:  87.08%, val_best:  87.50%, tr:  98.74%, tr_best:  99.03%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.035565/  0.525287, val:  85.83%, val_best:  87.50%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.029378/  0.512246, val:  86.67%, val_best:  87.50%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.028495/  0.515376, val:  87.50%, val_best:  87.50%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.021770/  0.560524, val:  86.67%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.019713/  0.514089, val:  86.25%, val_best:  87.50%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.025480/  0.528573, val:  87.92%, val_best:  87.92%, tr:  99.55%, tr_best:  99.82%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.018690/  0.511203, val:  87.92%, val_best:  87.92%, tr:  99.77%, tr_best:  99.82%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.016983/  0.548591, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best:  99.82%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.016039/  0.555409, val:  87.50%, val_best:  87.92%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.012657/  0.567716, val:  87.50%, val_best:  87.92%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.012298/  0.561209, val:  87.50%, val_best:  87.92%, tr:  99.91%, tr_best:  99.98%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.011149/  0.588910, val:  86.67%, val_best:  87.92%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.010797/  0.590741, val:  86.25%, val_best:  87.92%, tr:  99.93%, tr_best:  99.98%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.010192/  0.594106, val:  87.92%, val_best:  87.92%, tr:  99.95%, tr_best:  99.98%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.012534/  0.617692, val:  86.67%, val_best:  87.92%, tr:  99.84%, tr_best:  99.98%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.009443/  0.594256, val:  86.25%, val_best:  87.92%, tr:  99.95%, tr_best:  99.98%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.008475/  0.610080, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.007317/  0.641489, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.007425/  0.614087, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.007214/  0.649907, val:  86.25%, val_best:  87.92%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.006638/  0.644961, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.007722/  0.668935, val:  87.08%, val_best:  87.92%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.006479/  0.637665, val:  87.08%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.005684/  0.661400, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.005503/  0.659988, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.005235/  0.627741, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.005321/  0.634615, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.004975/  0.655775, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.004413/  0.651949, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.004404/  0.662896, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.004374/  0.656867, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.004286/  0.660117, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.004023/  0.671895, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.003896/  0.658950, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.004008/  0.669243, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.003998/  0.685546, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.004571/  0.681542, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.003501/  0.694036, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.003416/  0.661397, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.003482/  0.703491, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.003222/  0.681887, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.003195/  0.704701, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.003245/  0.718943, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002898/  0.695313, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002912/  0.697585, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.002742/  0.707335, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.002656/  0.706169, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002670/  0.711897, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002681/  0.722315, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002472/  0.734703, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002392/  0.737848, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002358/  0.730115, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002299/  0.729204, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.002263/  0.725239, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.002165/  0.723305, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.002241/  0.722719, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.002098/  0.726283, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.002122/  0.729367, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.002072/  0.718180, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.002074/  0.720822, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002000/  0.721291, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001884/  0.736242, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001889/  0.729820, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001911/  0.734628, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001869/  0.733071, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001815/  0.743241, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001817/  0.733024, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001773/  0.740852, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001903/  0.733369, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001725/  0.749434, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001675/  0.735632, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001670/  0.739966, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001668/  0.748967, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001649/  0.737960, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ccb32a3128418d9b18bb3f507ef521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▆▇▇█▇▇▇▇██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▇▇▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00165</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.73796</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-sweep-59</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q6xxsjyq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/q6xxsjyq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_082615-q6xxsjyq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1jjsbnjl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_083749-1jjsbnjl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jjsbnjl' target=\"_blank\">feasible-sweep-60</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jjsbnjl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jjsbnjl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  1.856112/  1.491345, val:  50.42%, val_best:  50.42%, tr:  38.59%, tr_best:  38.59%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.202294/  1.174747, val:  54.58%, val_best:  54.58%, tr:  59.11%, tr_best:  59.11%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  0.997037/  1.037527, val:  60.00%, val_best:  60.00%, tr:  62.06%, tr_best:  62.06%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.897586/  0.967297, val:  62.08%, val_best:  62.08%, tr:  65.01%, tr_best:  65.01%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.829361/  0.912051, val:  61.25%, val_best:  62.08%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.776033/  0.885050, val:  61.25%, val_best:  62.08%, tr:  67.81%, tr_best:  67.81%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.728813/  0.845825, val:  66.67%, val_best:  66.67%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.691608/  0.817815, val:  65.00%, val_best:  66.67%, tr:  70.76%, tr_best:  70.76%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.662835/  0.804911, val:  66.67%, val_best:  66.67%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.635876/  0.773927, val:  69.58%, val_best:  69.58%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.608052/  0.777371, val:  67.50%, val_best:  69.58%, tr:  73.85%, tr_best:  73.85%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.587428/  0.761556, val:  68.75%, val_best:  69.58%, tr:  74.91%, tr_best:  74.91%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.570001/  0.743184, val:  69.17%, val_best:  69.58%, tr:  75.63%, tr_best:  75.63%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.552858/  0.735115, val:  69.58%, val_best:  69.58%, tr:  76.58%, tr_best:  76.58%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.538979/  0.723393, val:  72.92%, val_best:  72.92%, tr:  76.49%, tr_best:  76.58%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.522642/  0.708651, val:  69.17%, val_best:  72.92%, tr:  77.52%, tr_best:  77.52%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.509737/  0.715418, val:  68.75%, val_best:  72.92%, tr:  77.68%, tr_best:  77.68%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.498531/  0.711622, val:  69.58%, val_best:  72.92%, tr:  79.04%, tr_best:  79.04%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.486902/  0.713708, val:  68.75%, val_best:  72.92%, tr:  79.44%, tr_best:  79.44%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.473176/  0.729976, val:  67.50%, val_best:  72.92%, tr:  80.21%, tr_best:  80.21%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.466311/  0.717615, val:  70.83%, val_best:  72.92%, tr:  80.66%, tr_best:  80.66%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.452369/  0.721134, val:  67.92%, val_best:  72.92%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.444057/  0.703068, val:  70.83%, val_best:  72.92%, tr:  82.03%, tr_best:  82.03%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.431237/  0.703953, val:  67.50%, val_best:  72.92%, tr:  83.16%, tr_best:  83.16%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.423299/  0.687447, val:  71.67%, val_best:  72.92%, tr:  83.12%, tr_best:  83.16%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.416932/  0.674550, val:  73.33%, val_best:  73.33%, tr:  83.36%, tr_best:  83.36%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.402817/  0.701463, val:  74.17%, val_best:  74.17%, tr:  84.85%, tr_best:  84.85%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.398179/  0.704275, val:  67.92%, val_best:  74.17%, tr:  84.54%, tr_best:  84.85%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.390220/  0.678831, val:  75.42%, val_best:  75.42%, tr:  85.23%, tr_best:  85.23%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.381020/  0.685462, val:  74.17%, val_best:  75.42%, tr:  85.57%, tr_best:  85.57%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.370118/  0.696932, val:  74.58%, val_best:  75.42%, tr:  86.83%, tr_best:  86.83%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.362894/  0.676036, val:  75.42%, val_best:  75.42%, tr:  86.88%, tr_best:  86.88%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.354389/  0.691771, val:  76.25%, val_best:  76.25%, tr:  87.71%, tr_best:  87.71%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.343654/  0.694912, val:  75.42%, val_best:  76.25%, tr:  88.35%, tr_best:  88.35%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.337879/  0.705807, val:  74.17%, val_best:  76.25%, tr:  88.57%, tr_best:  88.57%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.327254/  0.684273, val:  77.92%, val_best:  77.92%, tr:  89.11%, tr_best:  89.11%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.319249/  0.696887, val:  74.17%, val_best:  77.92%, tr:  89.74%, tr_best:  89.74%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.311989/  0.671100, val:  75.42%, val_best:  77.92%, tr:  89.90%, tr_best:  89.90%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.299947/  0.679341, val:  76.67%, val_best:  77.92%, tr:  90.24%, tr_best:  90.24%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.295292/  0.672406, val:  75.42%, val_best:  77.92%, tr:  90.69%, tr_best:  90.69%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.284169/  0.681041, val:  77.92%, val_best:  77.92%, tr:  91.59%, tr_best:  91.59%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.276016/  0.686425, val:  77.92%, val_best:  77.92%, tr:  92.22%, tr_best:  92.22%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.267570/  0.668115, val:  77.50%, val_best:  77.92%, tr:  92.54%, tr_best:  92.54%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.260428/  0.659166, val:  77.92%, val_best:  77.92%, tr:  92.56%, tr_best:  92.56%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.252093/  0.658711, val:  77.92%, val_best:  77.92%, tr:  93.30%, tr_best:  93.30%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.243903/  0.636512, val:  78.33%, val_best:  78.33%, tr:  93.51%, tr_best:  93.51%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.236146/  0.652293, val:  79.17%, val_best:  79.17%, tr:  93.91%, tr_best:  93.91%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.229147/  0.648243, val:  81.25%, val_best:  81.25%, tr:  93.89%, tr_best:  93.91%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.222721/  0.645578, val:  78.33%, val_best:  81.25%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.215526/  0.628548, val:  79.17%, val_best:  81.25%, tr:  94.93%, tr_best:  94.93%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.208795/  0.646754, val:  78.33%, val_best:  81.25%, tr:  95.06%, tr_best:  95.06%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.198069/  0.647101, val:  78.75%, val_best:  81.25%, tr:  95.47%, tr_best:  95.47%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.194150/  0.642072, val:  79.58%, val_best:  81.25%, tr:  95.33%, tr_best:  95.47%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.189635/  0.626083, val:  80.42%, val_best:  81.25%, tr:  95.49%, tr_best:  95.49%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.180697/  0.693574, val:  77.92%, val_best:  81.25%, tr:  96.03%, tr_best:  96.03%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.180263/  0.625434, val:  80.42%, val_best:  81.25%, tr:  95.56%, tr_best:  96.03%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.171307/  0.649277, val:  79.17%, val_best:  81.25%, tr:  95.99%, tr_best:  96.03%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.165115/  0.650407, val:  80.00%, val_best:  81.25%, tr:  96.26%, tr_best:  96.26%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.162130/  0.636105, val:  81.25%, val_best:  81.25%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.156787/  0.645951, val:  80.00%, val_best:  81.25%, tr:  96.69%, tr_best:  96.69%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.151422/  0.701978, val:  79.58%, val_best:  81.25%, tr:  96.78%, tr_best:  96.78%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.147857/  0.650326, val:  81.25%, val_best:  81.25%, tr:  96.78%, tr_best:  96.78%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.143467/  0.622027, val:  81.67%, val_best:  81.67%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.135379/  0.638869, val:  80.42%, val_best:  81.67%, tr:  97.32%, tr_best:  97.32%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.130897/  0.614704, val:  80.83%, val_best:  81.67%, tr:  97.50%, tr_best:  97.50%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.129928/  0.626606, val:  81.25%, val_best:  81.67%, tr:  97.48%, tr_best:  97.50%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.125114/  0.622709, val:  82.50%, val_best:  82.50%, tr:  97.39%, tr_best:  97.50%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.119594/  0.637136, val:  83.75%, val_best:  83.75%, tr:  97.81%, tr_best:  97.81%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.117734/  0.669311, val:  80.83%, val_best:  83.75%, tr:  97.68%, tr_best:  97.81%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.112647/  0.605753, val:  83.75%, val_best:  83.75%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.107426/  0.616845, val:  82.92%, val_best:  83.75%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.105655/  0.603097, val:  82.92%, val_best:  83.75%, tr:  97.97%, tr_best:  98.20%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.102415/  0.644372, val:  81.67%, val_best:  83.75%, tr:  98.24%, tr_best:  98.24%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.097442/  0.602912, val:  84.17%, val_best:  84.17%, tr:  98.33%, tr_best:  98.33%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.095762/  0.636508, val:  82.92%, val_best:  84.17%, tr:  98.49%, tr_best:  98.49%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.092667/  0.609196, val:  82.92%, val_best:  84.17%, tr:  98.38%, tr_best:  98.49%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.090001/  0.620340, val:  82.92%, val_best:  84.17%, tr:  98.42%, tr_best:  98.49%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.088411/  0.629162, val:  82.50%, val_best:  84.17%, tr:  98.60%, tr_best:  98.60%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.086361/  0.602270, val:  84.17%, val_best:  84.17%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.085231/  0.654095, val:  81.67%, val_best:  84.17%, tr:  98.42%, tr_best:  98.81%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.082776/  0.605202, val:  83.33%, val_best:  84.17%, tr:  98.69%, tr_best:  98.81%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.077579/  0.617854, val:  83.75%, val_best:  84.17%, tr:  98.83%, tr_best:  98.83%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.077129/  0.639388, val:  83.33%, val_best:  84.17%, tr:  98.85%, tr_best:  98.85%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.074945/  0.612749, val:  83.75%, val_best:  84.17%, tr:  98.96%, tr_best:  98.96%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.075924/  0.600965, val:  85.00%, val_best:  85.00%, tr:  98.83%, tr_best:  98.96%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.070853/  0.610060, val:  83.33%, val_best:  85.00%, tr:  98.94%, tr_best:  98.96%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.069744/  0.609177, val:  83.33%, val_best:  85.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.069590/  0.582751, val:  85.83%, val_best:  85.83%, tr:  98.99%, tr_best:  99.08%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.065810/  0.604225, val:  84.17%, val_best:  85.83%, tr:  99.14%, tr_best:  99.14%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.065771/  0.590931, val:  84.17%, val_best:  85.83%, tr:  99.05%, tr_best:  99.14%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.062925/  0.631442, val:  84.58%, val_best:  85.83%, tr:  99.23%, tr_best:  99.23%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.061484/  0.591949, val:  85.00%, val_best:  85.83%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.059524/  0.600481, val:  84.58%, val_best:  85.83%, tr:  99.19%, tr_best:  99.35%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.059219/  0.606774, val:  82.92%, val_best:  85.83%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.058269/  0.597253, val:  85.42%, val_best:  85.83%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.055505/  0.586962, val:  87.08%, val_best:  87.08%, tr:  99.46%, tr_best:  99.46%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.055214/  0.594938, val:  85.83%, val_best:  87.08%, tr:  99.35%, tr_best:  99.46%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.054738/  0.601659, val:  83.75%, val_best:  87.08%, tr:  99.35%, tr_best:  99.46%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.051105/  0.593260, val:  84.58%, val_best:  87.08%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.051473/  0.599735, val:  85.42%, val_best:  87.08%, tr:  99.41%, tr_best:  99.53%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03206435c8643fcbac7f48c0869421c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▁▃▅▄▅▅▅▅▃▆▄█▇▆▇▇▅██▇█▇███▇▇▇██████████</td></tr><tr><td>summary_val_acc</td><td>▁▅▆▆▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇██▇████████████</td></tr><tr><td>tr_acc</td><td>▁▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▅▆▆▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇██▇████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99414</td></tr><tr><td>tr_epoch_loss</td><td>0.05147</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>0.59974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-60</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jjsbnjl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1jjsbnjl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_083749-1jjsbnjl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bh53bkfk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_085010-bh53bkfk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bh53bkfk' target=\"_blank\">clear-sweep-61</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bh53bkfk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bh53bkfk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.924084/  0.900167, val:  62.50%, val_best:  62.50%, tr:  60.17%, tr_best:  60.17%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.608945/  0.868340, val:  60.00%, val_best:  62.50%, tr:  72.86%, tr_best:  72.86%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411019/  0.862224, val:  72.08%, val_best:  72.08%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.290071/  0.516031, val:  84.17%, val_best:  84.17%, tr:  89.34%, tr_best:  89.34%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.148748/  0.462285, val:  87.92%, val_best:  87.92%, tr:  95.38%, tr_best:  95.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.110478/  0.453902, val:  89.17%, val_best:  89.17%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.087798/  0.488559, val:  88.75%, val_best:  89.17%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.056540/  0.460554, val:  87.92%, val_best:  89.17%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.032420/  0.453790, val:  90.00%, val_best:  90.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.044882/  0.377394, val:  90.00%, val_best:  90.00%, tr:  98.67%, tr_best:  99.12%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.031081/  0.458891, val:  90.00%, val_best:  90.00%, tr:  99.01%, tr_best:  99.12%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.012163/  0.410768, val:  90.83%, val_best:  90.83%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003469/  0.424667, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002129/  0.401361, val:  91.25%, val_best:  91.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001459/  0.412334, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001118/  0.409213, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000979/  0.411735, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000870/  0.414670, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000784/  0.411883, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000699/  0.427088, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000645/  0.428068, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000597/  0.433039, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000562/  0.453180, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000520/  0.458498, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000493/  0.451457, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000468/  0.455886, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000441/  0.445366, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000417/  0.445858, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000400/  0.447511, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000372/  0.454448, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000364/  0.447946, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000352/  0.454611, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000334/  0.458291, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000324/  0.462057, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000310/  0.460060, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000302/  0.459708, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000290/  0.464959, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000283/  0.463789, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000270/  0.462140, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000260/  0.459259, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000252/  0.460512, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000247/  0.460966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000241/  0.461501, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000232/  0.467219, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000227/  0.465373, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000218/  0.464732, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000214/  0.462301, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000206/  0.463863, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000203/  0.462686, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000201/  0.461293, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000193/  0.463994, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000188/  0.463246, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000183/  0.461719, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000177/  0.464404, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000174/  0.462007, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000172/  0.465646, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000167/  0.467702, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000163/  0.466615, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000162/  0.465064, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000155/  0.466431, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000156/  0.470764, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000151/  0.474162, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000149/  0.475791, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000144/  0.477446, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000143/  0.473104, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000139/  0.477203, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000137/  0.482258, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000136/  0.479527, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000132/  0.478409, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000131/  0.478923, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.479966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.481726, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000125/  0.477696, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000124/  0.479414, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.479214, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000121/  0.478879, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.476968, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.477993, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.480774, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000115/  0.478548, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000113/  0.480919, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000112/  0.478824, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.480684, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000109/  0.478337, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000106/  0.473828, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000105/  0.479483, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000105/  0.480563, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000103/  0.480323, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000103/  0.481059, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000102/  0.479574, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.478145, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000098/  0.482602, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000097/  0.483561, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.486765, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.482070, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000093/  0.486749, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000091/  0.483553, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000091/  0.492586, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.491570, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.493886, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371b95443f40434ea2196b9a8827756e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.49389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-sweep-61</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bh53bkfk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bh53bkfk</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_085010-bh53bkfk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 87dolm5s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_090241-87dolm5s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/87dolm5s' target=\"_blank\">astral-sweep-62</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/87dolm5s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/87dolm5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.940631/  0.971694, val:  60.00%, val_best:  60.00%, tr:  60.14%, tr_best:  60.14%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.630883/  0.897482, val:  62.92%, val_best:  62.92%, tr:  71.03%, tr_best:  71.03%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.446882/  0.741105, val:  74.17%, val_best:  74.17%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.291870/  0.508247, val:  86.67%, val_best:  86.67%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.166100/  0.532877, val:  87.08%, val_best:  87.08%, tr:  94.32%, tr_best:  94.32%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.097045/  0.471449, val:  87.92%, val_best:  87.92%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.079279/  0.542684, val:  85.42%, val_best:  87.92%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.053816/  0.385868, val:  89.17%, val_best:  89.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.029766/  0.461558, val:  89.17%, val_best:  89.17%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.043129/  0.407184, val:  90.00%, val_best:  90.00%, tr:  98.56%, tr_best:  99.26%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024331/  0.485701, val:  89.17%, val_best:  90.00%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.009858/  0.397074, val:  90.83%, val_best:  90.83%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.005635/  0.472394, val:  91.67%, val_best:  91.67%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.003003/  0.427595, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001792/  0.446107, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001347/  0.457066, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001128/  0.463881, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001065/  0.469466, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000992/  0.469521, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000860/  0.476817, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000816/  0.467585, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000712/  0.471026, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000669/  0.485179, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000636/  0.480550, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000572/  0.488351, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000537/  0.498910, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000507/  0.499225, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000470/  0.504784, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000457/  0.497928, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000424/  0.497091, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000418/  0.509973, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000394/  0.521349, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000378/  0.521303, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000358/  0.526090, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000344/  0.516998, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000339/  0.518939, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000323/  0.523821, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000310/  0.523994, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000301/  0.531876, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000288/  0.535207, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000284/  0.532543, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000273/  0.537515, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000264/  0.528881, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000258/  0.528008, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000254/  0.533494, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000244/  0.530134, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000239/  0.544103, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000229/  0.536179, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000226/  0.536386, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000219/  0.534645, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000213/  0.541545, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000210/  0.541155, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000208/  0.543737, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000200/  0.538721, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000198/  0.543003, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000196/  0.545501, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000192/  0.545955, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000186/  0.549556, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000182/  0.548140, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000179/  0.547507, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000177/  0.547195, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000172/  0.545982, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000173/  0.546602, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000166/  0.553394, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000162/  0.549833, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000161/  0.554355, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000160/  0.558481, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000153/  0.563451, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000149/  0.561897, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000148/  0.561056, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000149/  0.566028, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000142/  0.568494, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000139/  0.565895, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000137/  0.572022, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000135/  0.575145, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000133/  0.569638, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000132/  0.563469, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000132/  0.555015, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000127/  0.554992, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000125/  0.552628, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000123/  0.552222, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000121/  0.554661, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000121/  0.551329, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000117/  0.560474, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000117/  0.558354, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000114/  0.558434, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000115/  0.555474, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000112/  0.556597, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000111/  0.557073, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000109/  0.559865, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000107/  0.561674, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000105/  0.562137, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000105/  0.563710, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000105/  0.562709, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000103/  0.568955, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000102/  0.569531, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000100/  0.564454, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000101/  0.562444, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000097/  0.568291, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000097/  0.567012, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aba3fc3cc7040e5a2decab491112391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.56701</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-sweep-62</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/87dolm5s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/87dolm5s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_090241-87dolm5s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xc0vquzc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_091436-xc0vquzc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xc0vquzc' target=\"_blank\">toasty-sweep-63</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xc0vquzc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xc0vquzc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.948241/  0.810933, val:  60.00%, val_best:  60.00%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.584885/  0.808915, val:  67.50%, val_best:  67.50%, tr:  73.08%, tr_best:  73.08%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.399293/  0.645334, val:  80.83%, val_best:  80.83%, tr:  83.00%, tr_best:  83.00%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.259728/  0.521861, val:  86.67%, val_best:  86.67%, tr:  90.69%, tr_best:  90.69%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.137906/  0.453284, val:  85.83%, val_best:  86.67%, tr:  95.47%, tr_best:  95.47%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.116877/  0.433451, val:  90.00%, val_best:  90.00%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.063803/  0.606309, val:  86.25%, val_best:  90.00%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.055475/  0.473878, val:  87.92%, val_best:  90.00%, tr:  98.44%, tr_best:  98.44%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.029867/  0.433133, val:  88.33%, val_best:  90.00%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.017870/  0.454203, val:  88.75%, val_best:  90.00%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.008982/  0.488475, val:  90.83%, val_best:  90.83%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.006984/  0.494711, val:  88.75%, val_best:  90.83%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003424/  0.511547, val:  89.58%, val_best:  90.83%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001866/  0.521343, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001192/  0.529747, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.000995/  0.516564, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000863/  0.507904, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000783/  0.511017, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000708/  0.514485, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000655/  0.507083, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000602/  0.517106, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000546/  0.506922, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000511/  0.513302, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000485/  0.524377, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000439/  0.527266, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000418/  0.529247, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000399/  0.527515, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000374/  0.526822, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000364/  0.527172, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000348/  0.533110, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000342/  0.534845, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000327/  0.534818, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000312/  0.537681, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000303/  0.548502, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000288/  0.543104, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000277/  0.530877, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000265/  0.535054, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000254/  0.538853, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000251/  0.540546, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000242/  0.547666, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000240/  0.553670, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000229/  0.555047, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000224/  0.552312, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000220/  0.549097, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000212/  0.542288, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000208/  0.544394, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000205/  0.543587, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000199/  0.546550, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000193/  0.548627, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000188/  0.546433, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000185/  0.546571, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000181/  0.547747, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000175/  0.549658, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000173/  0.548468, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000166/  0.550029, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000165/  0.552771, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000164/  0.555906, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000159/  0.552982, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000156/  0.564276, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000150/  0.565147, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000150/  0.559848, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000146/  0.562109, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000150/  0.564647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000139/  0.569064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000138/  0.568714, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000134/  0.563484, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000132/  0.564067, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000131/  0.574878, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000129/  0.567569, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000128/  0.567597, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000125/  0.569186, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000123/  0.574120, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000121/  0.574806, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000120/  0.577079, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000117/  0.577886, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000115/  0.583823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000115/  0.586276, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000112/  0.584207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000112/  0.583698, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000110/  0.580823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000109/  0.584858, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000106/  0.585372, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000105/  0.584947, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000103/  0.581064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000101/  0.579744, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000100/  0.578086, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000100/  0.580465, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000098/  0.582796, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000097/  0.579961, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000095/  0.577584, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000095/  0.578770, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000094/  0.577921, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000093/  0.579972, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000092/  0.577036, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000091/  0.578224, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000088/  0.577647, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000087/  0.579864, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000087/  0.579691, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000086/  0.579581, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000084/  0.579485, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5425fe01e2a3472bacac1e37da0a7488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.57949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-63</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xc0vquzc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xc0vquzc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_091436-xc0vquzc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b87vo6xs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_092609-b87vo6xs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b87vo6xs' target=\"_blank\">jumping-sweep-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b87vo6xs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b87vo6xs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.928935/  0.881833, val:  59.58%, val_best:  59.58%, tr:  60.01%, tr_best:  60.01%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.622879/  0.854058, val:  62.92%, val_best:  62.92%, tr:  71.73%, tr_best:  71.73%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.415023/  0.815604, val:  70.00%, val_best:  70.00%, tr:  82.57%, tr_best:  82.57%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.301157/  0.537661, val:  85.42%, val_best:  85.42%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.169007/  0.494498, val:  85.42%, val_best:  85.42%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.108344/  0.441543, val:  89.17%, val_best:  89.17%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.078059/  0.622470, val:  88.33%, val_best:  89.17%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.058447/  0.424832, val:  88.75%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051343/  0.443390, val:  90.42%, val_best:  90.42%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019991/  0.473071, val:  88.75%, val_best:  90.42%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024913/  0.594811, val:  87.92%, val_best:  90.42%, tr:  99.23%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.011130/  0.510964, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003125/  0.552201, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001943/  0.502089, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001624/  0.521189, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001236/  0.545577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001065/  0.524273, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000955/  0.556683, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000850/  0.553951, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000777/  0.562979, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000711/  0.561448, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000666/  0.572040, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000628/  0.581079, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000576/  0.569785, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000538/  0.568354, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000500/  0.574372, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000475/  0.581407, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000450/  0.584895, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000429/  0.586062, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000411/  0.596621, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.597417, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000379/  0.608332, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000358/  0.602874, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000340/  0.604670, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000329/  0.601791, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.601425, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000308/  0.602356, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000292/  0.598435, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.597901, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000277/  0.604077, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.613960, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000256/  0.610192, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000249/  0.611147, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.614404, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000235/  0.615403, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.614356, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.623049, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000215/  0.624923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000211/  0.613693, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.619318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.622931, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000195/  0.624674, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.627025, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.628896, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.624894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.621996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000175/  0.622287, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.615900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617114, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.613777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000159/  0.619844, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.615970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000157/  0.617458, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.619992, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.616505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617499, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.623439, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.627106, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000138/  0.627911, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.630018, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000130/  0.633134, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.632592, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633326, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635861, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000123/  0.634962, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631111, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.628672, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629021, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.628650, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629489, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.628790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.628014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000109/  0.629268, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.627406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000109/  0.628916, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000104/  0.631419, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.634245, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.634045, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000103/  0.632370, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.629538, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.630059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.629965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.636612, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635149, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.633406, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000092/  0.634207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.632478, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb41faa1a0e4f0dac9b0135ba12d13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.63248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-sweep-64</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b87vo6xs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b87vo6xs</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_092609-b87vo6xs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j0t35i9c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_093816-j0t35i9c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0t35i9c' target=\"_blank\">swept-sweep-65</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0t35i9c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0t35i9c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.974480/  0.867463, val:  64.17%, val_best:  64.17%, tr:  58.93%, tr_best:  58.93%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619049/  0.779950, val:  68.75%, val_best:  68.75%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.410511/  0.901403, val:  70.83%, val_best:  70.83%, tr:  83.41%, tr_best:  83.41%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.292086/  0.534297, val:  87.08%, val_best:  87.08%, tr:  89.13%, tr_best:  89.13%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.162379/  0.612390, val:  82.50%, val_best:  87.08%, tr:  94.36%, tr_best:  94.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.140578/  0.442822, val:  89.58%, val_best:  89.58%, tr:  95.36%, tr_best:  95.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.082165/  0.784311, val:  85.00%, val_best:  89.58%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.059931/  0.539399, val:  87.50%, val_best:  89.58%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036210/  0.485832, val:  90.83%, val_best:  90.83%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019819/  0.519804, val:  88.75%, val_best:  90.83%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032321/  0.569617, val:  87.50%, val_best:  90.83%, tr:  98.90%, tr_best:  99.48%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.030033/  0.488638, val:  88.75%, val_best:  90.83%, tr:  98.96%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.012969/  0.511075, val:  91.25%, val_best:  91.25%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006165/  0.697958, val:  86.25%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.003339/  0.576901, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002147/  0.547625, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001528/  0.554330, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001201/  0.556022, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001124/  0.551401, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000866/  0.574368, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000799/  0.591013, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000714/  0.581928, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000671/  0.589273, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000652/  0.598257, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000555/  0.602714, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000540/  0.597360, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000500/  0.592729, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000474/  0.601346, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000439/  0.602200, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000428/  0.595900, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000411/  0.599700, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000382/  0.607436, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000371/  0.604986, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000348/  0.610807, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000338/  0.618313, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000328/  0.620386, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000313/  0.618911, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000303/  0.616548, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000305/  0.621521, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000285/  0.625960, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000283/  0.630643, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000271/  0.629686, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000262/  0.628815, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000250/  0.627120, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000250/  0.628091, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000242/  0.628134, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000232/  0.633806, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000230/  0.632606, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.635183, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000215/  0.640931, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000209/  0.649058, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000209/  0.644091, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000201/  0.644871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000195/  0.649851, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000196/  0.657061, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000189/  0.662943, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000186/  0.667632, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000181/  0.671509, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000180/  0.673545, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000177/  0.670262, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000173/  0.666736, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000167/  0.664875, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000169/  0.669161, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000165/  0.666249, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000162/  0.662686, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000158/  0.662513, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000158/  0.666680, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000155/  0.672476, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000152/  0.672495, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000147/  0.665847, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000146/  0.670442, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000141/  0.670678, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000143/  0.671315, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000136/  0.674515, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000138/  0.676093, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000137/  0.676866, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000134/  0.684349, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000131/  0.688230, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000130/  0.687458, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000129/  0.690846, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.691173, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000127/  0.693805, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000127/  0.689193, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000123/  0.694343, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000121/  0.694647, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000118/  0.695344, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000121/  0.699655, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000118/  0.693519, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000116/  0.685618, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000113/  0.693628, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000112/  0.695912, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000110/  0.687126, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.686712, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.686534, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.696083, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696329, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696316, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000102/  0.702537, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000100/  0.702209, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.701244, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeafe40be374c17927e193e193f00da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▅█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.70124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-sweep-65</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0t35i9c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j0t35i9c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_093816-j0t35i9c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rsehav5c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_094942-rsehav5c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rsehav5c' target=\"_blank\">earnest-sweep-66</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rsehav5c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rsehav5c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.050137/  0.981823, val:  58.33%, val_best:  58.33%, tr:  56.36%, tr_best:  56.36%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.806784/  1.007279, val:  55.00%, val_best:  58.33%, tr:  64.97%, tr_best:  64.97%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.740694/  1.058702, val:  57.08%, val_best:  58.33%, tr:  66.25%, tr_best:  66.25%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.667456/  0.910731, val:  60.42%, val_best:  60.42%, tr:  67.83%, tr_best:  67.83%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.579484/  0.974838, val:  61.67%, val_best:  61.67%, tr:  71.26%, tr_best:  71.26%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.579505/  0.934786, val:  65.42%, val_best:  65.42%, tr:  71.82%, tr_best:  71.82%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.593398/  0.986796, val:  63.75%, val_best:  65.42%, tr:  71.39%, tr_best:  71.82%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.528394/  0.881913, val:  67.92%, val_best:  67.92%, tr:  72.88%, tr_best:  72.88%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.492339/  0.868640, val:  66.25%, val_best:  67.92%, tr:  75.50%, tr_best:  75.50%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.485833/  0.924298, val:  64.58%, val_best:  67.92%, tr:  75.47%, tr_best:  75.50%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.473120/  1.300118, val:  59.58%, val_best:  67.92%, tr:  77.30%, tr_best:  77.30%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.472998/  1.135979, val:  65.42%, val_best:  67.92%, tr:  76.80%, tr_best:  77.30%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.444941/  1.033916, val:  64.58%, val_best:  67.92%, tr:  77.98%, tr_best:  77.98%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.442458/  0.868266, val:  69.17%, val_best:  69.17%, tr:  78.97%, tr_best:  78.97%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.406801/  0.913231, val:  67.08%, val_best:  69.17%, tr:  80.93%, tr_best:  80.93%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.388797/  0.941937, val:  68.75%, val_best:  69.17%, tr:  81.42%, tr_best:  81.42%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.376967/  1.110108, val:  64.17%, val_best:  69.17%, tr:  82.10%, tr_best:  82.10%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.378890/  0.914394, val:  67.08%, val_best:  69.17%, tr:  82.62%, tr_best:  82.62%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.355477/  0.905381, val:  72.50%, val_best:  72.50%, tr:  83.88%, tr_best:  83.88%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.321826/  1.032542, val:  71.25%, val_best:  72.50%, tr:  86.25%, tr_best:  86.25%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.312284/  0.875228, val:  73.75%, val_best:  73.75%, tr:  86.99%, tr_best:  86.99%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.279855/  1.055938, val:  71.25%, val_best:  73.75%, tr:  88.19%, tr_best:  88.19%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.302000/  1.041747, val:  69.58%, val_best:  73.75%, tr:  87.13%, tr_best:  88.19%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.313545/  1.014833, val:  72.92%, val_best:  73.75%, tr:  86.79%, tr_best:  88.19%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.273883/  1.183650, val:  69.17%, val_best:  73.75%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.250388/  0.984572, val:  73.33%, val_best:  73.75%, tr:  90.08%, tr_best:  90.08%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.228251/  1.029804, val:  72.92%, val_best:  73.75%, tr:  90.83%, tr_best:  90.83%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.178464/  1.108674, val:  75.00%, val_best:  75.00%, tr:  93.17%, tr_best:  93.17%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.187971/  1.228601, val:  72.50%, val_best:  75.00%, tr:  92.58%, tr_best:  93.17%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.194540/  1.011968, val:  79.17%, val_best:  79.17%, tr:  92.18%, tr_best:  93.17%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.172865/  1.285016, val:  73.33%, val_best:  79.17%, tr:  93.30%, tr_best:  93.30%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.184281/  1.045931, val:  77.92%, val_best:  79.17%, tr:  92.83%, tr_best:  93.30%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.208244/  1.013679, val:  72.08%, val_best:  79.17%, tr:  92.16%, tr_best:  93.30%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.172176/  0.915687, val:  80.42%, val_best:  80.42%, tr:  93.44%, tr_best:  93.44%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.180763/  1.010136, val:  75.00%, val_best:  80.42%, tr:  92.76%, tr_best:  93.44%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.162864/  1.013100, val:  72.92%, val_best:  80.42%, tr:  93.51%, tr_best:  93.51%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.169922/  0.952735, val:  74.17%, val_best:  80.42%, tr:  93.19%, tr_best:  93.51%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.161194/  0.962189, val:  78.75%, val_best:  80.42%, tr:  94.03%, tr_best:  94.03%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.133452/  1.147799, val:  75.83%, val_best:  80.42%, tr:  94.93%, tr_best:  94.93%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.144045/  1.049232, val:  76.67%, val_best:  80.42%, tr:  94.61%, tr_best:  94.93%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.111351/  1.447990, val:  71.25%, val_best:  80.42%, tr:  95.92%, tr_best:  95.92%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.106350/  1.309730, val:  75.00%, val_best:  80.42%, tr:  96.24%, tr_best:  96.24%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.108132/  1.165064, val:  78.33%, val_best:  80.42%, tr:  96.10%, tr_best:  96.24%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.100696/  1.133112, val:  76.67%, val_best:  80.42%, tr:  96.15%, tr_best:  96.24%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.092652/  1.176427, val:  77.50%, val_best:  80.42%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.120375/  1.350463, val:  73.75%, val_best:  80.42%, tr:  95.47%, tr_best:  96.53%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.063751/  1.262477, val:  78.33%, val_best:  80.42%, tr:  97.68%, tr_best:  97.68%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.088736/  1.337401, val:  77.50%, val_best:  80.42%, tr:  96.51%, tr_best:  97.68%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.102040/  1.307807, val:  75.00%, val_best:  80.42%, tr:  96.21%, tr_best:  97.68%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.113552/  1.302999, val:  76.25%, val_best:  80.42%, tr:  96.21%, tr_best:  97.68%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.123886/  1.473730, val:  73.33%, val_best:  80.42%, tr:  95.38%, tr_best:  97.68%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.109509/  1.230151, val:  76.67%, val_best:  80.42%, tr:  95.78%, tr_best:  97.68%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.061715/  1.179445, val:  77.92%, val_best:  80.42%, tr:  98.08%, tr_best:  98.08%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.076430/  1.131911, val:  79.58%, val_best:  80.42%, tr:  97.05%, tr_best:  98.08%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.069602/  1.244805, val:  72.92%, val_best:  80.42%, tr:  97.29%, tr_best:  98.08%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.080681/  0.999628, val:  78.33%, val_best:  80.42%, tr:  97.07%, tr_best:  98.08%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.041917/  1.120283, val:  77.08%, val_best:  80.42%, tr:  98.33%, tr_best:  98.33%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.034148/  1.000534, val:  82.08%, val_best:  82.08%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.047356/  1.206803, val:  77.92%, val_best:  82.08%, tr:  98.62%, tr_best:  98.81%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.057154/  1.360007, val:  75.83%, val_best:  82.08%, tr:  97.90%, tr_best:  98.81%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.044763/  1.390068, val:  77.50%, val_best:  82.08%, tr:  98.44%, tr_best:  98.81%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.042891/  1.463676, val:  77.08%, val_best:  82.08%, tr:  98.56%, tr_best:  98.81%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.052594/  1.238117, val:  76.25%, val_best:  82.08%, tr:  98.35%, tr_best:  98.81%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.050124/  1.280731, val:  75.00%, val_best:  82.08%, tr:  98.06%, tr_best:  98.81%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.072233/  1.449077, val:  76.25%, val_best:  82.08%, tr:  97.43%, tr_best:  98.81%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.070876/  1.566341, val:  74.17%, val_best:  82.08%, tr:  97.81%, tr_best:  98.81%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.081042/  1.273191, val:  76.67%, val_best:  82.08%, tr:  97.16%, tr_best:  98.81%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.079130/  1.142115, val:  77.92%, val_best:  82.08%, tr:  97.39%, tr_best:  98.81%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.080234/  1.214637, val:  79.58%, val_best:  82.08%, tr:  97.18%, tr_best:  98.81%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.069865/  1.415674, val:  76.67%, val_best:  82.08%, tr:  97.68%, tr_best:  98.81%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.055676/  1.313728, val:  78.75%, val_best:  82.08%, tr:  97.99%, tr_best:  98.81%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.027688/  1.426440, val:  77.50%, val_best:  82.08%, tr:  99.17%, tr_best:  99.17%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.017943/  1.264461, val:  78.33%, val_best:  82.08%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.026946/  1.312594, val:  80.83%, val_best:  82.08%, tr:  99.12%, tr_best:  99.44%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.022171/  1.418302, val:  78.33%, val_best:  82.08%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.008248/  1.472948, val:  78.33%, val_best:  82.08%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.004434/  1.416536, val:  78.75%, val_best:  82.08%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.002280/  1.452716, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.001312/  1.503111, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000944/  1.505127, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000779/  1.503889, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000684/  1.494959, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000618/  1.482728, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000571/  1.492095, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000526/  1.493479, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000500/  1.498799, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000485/  1.487708, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000455/  1.488613, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000427/  1.500197, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000412/  1.500489, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000400/  1.501735, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000370/  1.511776, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000361/  1.507507, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000351/  1.518058, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000333/  1.525383, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000320/  1.525198, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000309/  1.531227, val:  79.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000297/  1.535165, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000291/  1.541820, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000280/  1.548625, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a334c7754844f18a3f668927858e85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▇▁▄▁▁▅▅▅▇▄▅▅▇█▇██▇██████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇▇▇▇▇▇▇▇▇█▇▇███▇█████▇█▇█████████████</td></tr><tr><td>tr_acc</td><td>▁▆▆▆▆▆▇▇▇▇▇█▇▇▇█████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▆▇▇▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇▇▇▇▇▇▇▇▇█▇▇███▇█████▇█▇█████████████</td></tr><tr><td>val_loss</td><td>▁▆▅▅▅▆▅▅▆▆▆▆▆▆▆▅▆▆▇▇▇▆▅▅▇▇█▆▇▇██████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00028</td></tr><tr><td>val_acc_best</td><td>0.82083</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>1.54862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-sweep-66</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rsehav5c' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rsehav5c</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_094942-rsehav5c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vio03lg3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_100108-vio03lg3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vio03lg3' target=\"_blank\">hearty-sweep-67</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vio03lg3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vio03lg3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.062041/  0.869062, val:  62.92%, val_best:  62.92%, tr:  57.91%, tr_best:  57.91%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.658935/  0.823880, val:  64.58%, val_best:  64.58%, tr:  69.21%, tr_best:  69.21%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.560856/  0.702597, val:  70.83%, val_best:  70.83%, tr:  72.79%, tr_best:  72.79%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.517809/  0.707583, val:  70.42%, val_best:  70.83%, tr:  76.01%, tr_best:  76.01%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.456385/  0.728018, val:  69.17%, val_best:  70.83%, tr:  79.64%, tr_best:  79.64%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.423090/  0.713493, val:  72.92%, val_best:  72.92%, tr:  81.49%, tr_best:  81.49%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.395765/  0.704023, val:  76.25%, val_best:  76.25%, tr:  83.07%, tr_best:  83.07%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.336045/  0.681862, val:  77.50%, val_best:  77.50%, tr:  86.97%, tr_best:  86.97%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.317118/  0.700529, val:  75.83%, val_best:  77.50%, tr:  87.69%, tr_best:  87.69%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.266152/  0.694668, val:  78.75%, val_best:  78.75%, tr:  90.13%, tr_best:  90.13%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.222888/  0.640425, val:  82.50%, val_best:  82.50%, tr:  92.31%, tr_best:  92.31%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.198137/  0.650001, val:  81.67%, val_best:  82.50%, tr:  92.92%, tr_best:  92.92%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.194773/  0.651505, val:  81.67%, val_best:  82.50%, tr:  93.35%, tr_best:  93.35%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.152788/  0.571083, val:  85.00%, val_best:  85.00%, tr:  95.00%, tr_best:  95.00%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.142873/  0.604206, val:  85.00%, val_best:  85.00%, tr:  95.47%, tr_best:  95.47%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.112365/  0.584848, val:  84.58%, val_best:  85.00%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.097724/  0.573654, val:  84.58%, val_best:  85.00%, tr:  97.02%, tr_best:  97.02%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.083079/  0.649306, val:  81.67%, val_best:  85.00%, tr:  97.45%, tr_best:  97.45%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.075012/  0.605031, val:  82.50%, val_best:  85.00%, tr:  97.93%, tr_best:  97.93%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.058547/  0.685422, val:  84.17%, val_best:  85.00%, tr:  98.33%, tr_best:  98.33%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.063232/  0.621555, val:  84.58%, val_best:  85.00%, tr:  98.22%, tr_best:  98.33%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.048516/  0.673935, val:  81.25%, val_best:  85.00%, tr:  98.74%, tr_best:  98.74%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.045466/  0.689761, val:  85.00%, val_best:  85.00%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.050195/  0.675964, val:  82.50%, val_best:  85.00%, tr:  98.74%, tr_best:  98.92%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.033152/  0.705435, val:  82.50%, val_best:  85.00%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.026751/  0.622838, val:  85.42%, val_best:  85.42%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.027010/  0.694138, val:  85.00%, val_best:  85.42%, tr:  99.37%, tr_best:  99.55%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.027093/  0.634829, val:  83.75%, val_best:  85.42%, tr:  99.37%, tr_best:  99.55%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.021510/  0.656212, val:  84.17%, val_best:  85.42%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.021926/  0.635961, val:  86.67%, val_best:  86.67%, tr:  99.59%, tr_best:  99.62%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.015944/  0.653666, val:  86.67%, val_best:  86.67%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.015106/  0.638701, val:  86.25%, val_best:  86.67%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.013955/  0.614460, val:  86.25%, val_best:  86.67%, tr:  99.89%, tr_best:  99.91%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.011880/  0.654341, val:  87.50%, val_best:  87.50%, tr:  99.86%, tr_best:  99.91%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.009522/  0.681700, val:  89.17%, val_best:  89.17%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.008699/  0.643645, val:  87.50%, val_best:  89.17%, tr:  99.95%, tr_best:  99.98%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.007754/  0.660359, val:  87.08%, val_best:  89.17%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.008990/  0.672461, val:  87.50%, val_best:  89.17%, tr:  99.91%, tr_best:  99.98%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.006443/  0.678324, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.006164/  0.655412, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.006074/  0.728746, val:  86.25%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.010741/  0.677268, val:  86.67%, val_best:  89.17%, tr:  99.75%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.005673/  0.670703, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.004969/  0.687382, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.005080/  0.675137, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.004648/  0.658860, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.004377/  0.671665, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.004107/  0.651717, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.003692/  0.679905, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.003658/  0.672072, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.003589/  0.665307, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.003111/  0.674013, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.003001/  0.680464, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.003001/  0.672392, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.002844/  0.679395, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.002768/  0.695793, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002709/  0.684754, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.002605/  0.692849, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.002576/  0.694326, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.002524/  0.693158, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.002385/  0.722715, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.002344/  0.696640, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.002348/  0.704314, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002251/  0.707844, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.002139/  0.674696, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002157/  0.703757, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002007/  0.698042, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001990/  0.694530, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001899/  0.720839, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001895/  0.699143, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001804/  0.706337, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001811/  0.725325, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001745/  0.726149, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001785/  0.713948, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001694/  0.710123, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001632/  0.715531, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001636/  0.729772, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001586/  0.728585, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001563/  0.718026, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001576/  0.728761, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001529/  0.735126, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001471/  0.736391, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001443/  0.732586, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001438/  0.734761, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001431/  0.734568, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001353/  0.744138, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001366/  0.742506, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001324/  0.739318, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001326/  0.740594, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001333/  0.744427, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001291/  0.754230, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001248/  0.747527, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001239/  0.752269, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001223/  0.749826, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001206/  0.739258, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001153/  0.742405, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001178/  0.746707, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001123/  0.766270, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001116/  0.757594, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001104/  0.744461, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a934a29590406b894036f4c7701763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▃▃▆█▃█▆▆██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇▇▇█▇██▇█████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇▇▇█▇██▇█████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▇▇▆▇▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0011</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>0.74446</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-67</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vio03lg3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vio03lg3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_100108-vio03lg3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ojf4hthv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_101328-ojf4hthv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ojf4hthv' target=\"_blank\">fresh-sweep-68</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ojf4hthv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ojf4hthv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.75, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.75, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.75, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.75, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  1.154462/  1.137793, val:  52.08%, val_best:  52.08%, tr:  54.53%, tr_best:  54.53%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.965802/  1.023569, val:  51.67%, val_best:  52.08%, tr:  60.80%, tr_best:  60.80%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.848988/  1.021865, val:  58.33%, val_best:  58.33%, tr:  64.34%, tr_best:  64.34%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.818455/  1.261111, val:  52.50%, val_best:  58.33%, tr:  65.06%, tr_best:  65.06%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.740142/  1.120102, val:  58.75%, val_best:  58.75%, tr:  66.55%, tr_best:  66.55%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.693081/  0.898179, val:  59.58%, val_best:  59.58%, tr:  67.90%, tr_best:  67.90%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.667071/  0.858037, val:  58.33%, val_best:  59.58%, tr:  68.71%, tr_best:  68.71%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.648173/  1.191715, val:  61.25%, val_best:  61.25%, tr:  69.07%, tr_best:  69.07%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.651635/  0.986445, val:  60.00%, val_best:  61.25%, tr:  70.20%, tr_best:  70.20%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.693163/  1.038879, val:  62.08%, val_best:  62.08%, tr:  69.09%, tr_best:  70.20%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.656009/  1.187735, val:  62.08%, val_best:  62.08%, tr:  70.00%, tr_best:  70.20%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.653032/  1.243011, val:  59.58%, val_best:  62.08%, tr:  69.36%, tr_best:  70.20%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.641223/  1.047125, val:  64.17%, val_best:  64.17%, tr:  71.87%, tr_best:  71.87%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.630036/  1.093125, val:  60.00%, val_best:  64.17%, tr:  71.51%, tr_best:  71.87%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.633032/  1.076934, val:  62.92%, val_best:  64.17%, tr:  70.31%, tr_best:  71.87%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.557828/  1.063560, val:  63.33%, val_best:  64.17%, tr:  73.49%, tr_best:  73.49%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.615850/  1.060591, val:  65.83%, val_best:  65.83%, tr:  71.96%, tr_best:  73.49%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.630446/  1.279666, val:  58.33%, val_best:  65.83%, tr:  71.60%, tr_best:  73.49%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.635732/  1.015292, val:  63.33%, val_best:  65.83%, tr:  71.64%, tr_best:  73.49%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.621432/  1.014316, val:  67.50%, val_best:  67.50%, tr:  72.48%, tr_best:  73.49%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.583554/  1.248528, val:  62.50%, val_best:  67.50%, tr:  73.90%, tr_best:  73.90%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.578399/  1.350734, val:  61.25%, val_best:  67.50%, tr:  73.53%, tr_best:  73.90%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.563761/  1.084831, val:  65.83%, val_best:  67.50%, tr:  75.45%, tr_best:  75.45%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.550577/  1.151979, val:  69.17%, val_best:  69.17%, tr:  75.88%, tr_best:  75.88%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.593672/  1.475819, val:  65.00%, val_best:  69.17%, tr:  75.09%, tr_best:  75.88%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.620617/  1.123202, val:  64.17%, val_best:  69.17%, tr:  73.78%, tr_best:  75.88%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.507885/  1.183928, val:  61.25%, val_best:  69.17%, tr:  76.87%, tr_best:  76.87%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.502966/  1.183508, val:  62.92%, val_best:  69.17%, tr:  77.23%, tr_best:  77.23%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.509370/  1.147018, val:  67.08%, val_best:  69.17%, tr:  77.19%, tr_best:  77.23%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.525883/  1.270471, val:  60.83%, val_best:  69.17%, tr:  76.76%, tr_best:  77.23%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.487936/  1.326353, val:  62.92%, val_best:  69.17%, tr:  78.52%, tr_best:  78.52%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.529104/  1.091729, val:  66.67%, val_best:  69.17%, tr:  78.09%, tr_best:  78.52%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.489064/  1.270560, val:  61.25%, val_best:  69.17%, tr:  79.24%, tr_best:  79.24%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.475634/  1.144491, val:  64.17%, val_best:  69.17%, tr:  79.31%, tr_best:  79.31%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.506839/  1.195843, val:  63.75%, val_best:  69.17%, tr:  78.29%, tr_best:  79.31%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.433492/  1.058111, val:  64.58%, val_best:  69.17%, tr:  80.55%, tr_best:  80.55%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.463864/  1.216885, val:  66.25%, val_best:  69.17%, tr:  80.28%, tr_best:  80.55%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.455302/  1.201390, val:  65.83%, val_best:  69.17%, tr:  81.51%, tr_best:  81.51%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.455946/  0.973772, val:  70.83%, val_best:  70.83%, tr:  80.09%, tr_best:  81.51%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.468944/  1.020715, val:  70.00%, val_best:  70.83%, tr:  80.18%, tr_best:  81.51%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.504590/  1.854169, val:  58.75%, val_best:  70.83%, tr:  79.06%, tr_best:  81.51%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.500264/  1.215702, val:  67.92%, val_best:  70.83%, tr:  79.17%, tr_best:  81.51%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.446904/  1.184605, val:  69.58%, val_best:  70.83%, tr:  81.45%, tr_best:  81.51%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.439620/  1.111671, val:  69.58%, val_best:  70.83%, tr:  81.88%, tr_best:  81.88%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.440478/  1.083678, val:  68.75%, val_best:  70.83%, tr:  81.88%, tr_best:  81.88%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.443863/  1.204726, val:  67.50%, val_best:  70.83%, tr:  82.15%, tr_best:  82.15%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.449830/  1.274608, val:  63.75%, val_best:  70.83%, tr:  81.90%, tr_best:  82.15%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.393801/  1.247095, val:  71.67%, val_best:  71.67%, tr:  84.38%, tr_best:  84.38%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.391894/  1.182820, val:  70.42%, val_best:  71.67%, tr:  84.29%, tr_best:  84.38%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.365149/  0.875422, val:  75.00%, val_best:  75.00%, tr:  85.71%, tr_best:  85.71%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.360012/  1.126033, val:  72.50%, val_best:  75.00%, tr:  85.37%, tr_best:  85.71%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.350246/  1.324275, val:  69.58%, val_best:  75.00%, tr:  86.63%, tr_best:  86.63%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.327344/  1.426590, val:  64.58%, val_best:  75.00%, tr:  86.90%, tr_best:  86.90%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.346183/  1.060183, val:  70.42%, val_best:  75.00%, tr:  86.18%, tr_best:  86.90%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.347674/  1.102178, val:  70.42%, val_best:  75.00%, tr:  85.78%, tr_best:  86.90%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.307235/  1.184613, val:  67.50%, val_best:  75.00%, tr:  87.17%, tr_best:  87.17%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.344497/  1.288487, val:  70.00%, val_best:  75.00%, tr:  86.23%, tr_best:  87.17%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.371840/  1.455364, val:  65.83%, val_best:  75.00%, tr:  85.21%, tr_best:  87.17%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.336301/  1.168421, val:  70.00%, val_best:  75.00%, tr:  86.52%, tr_best:  87.17%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.323718/  1.262508, val:  67.92%, val_best:  75.00%, tr:  87.15%, tr_best:  87.17%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.312174/  1.227882, val:  70.00%, val_best:  75.00%, tr:  86.95%, tr_best:  87.17%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.387686/  1.520001, val:  68.75%, val_best:  75.00%, tr:  86.16%, tr_best:  87.17%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.339521/  1.127333, val:  72.92%, val_best:  75.00%, tr:  86.50%, tr_best:  87.17%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.295313/  1.051564, val:  73.33%, val_best:  75.00%, tr:  88.84%, tr_best:  88.84%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.344423/  1.233692, val:  67.92%, val_best:  75.00%, tr:  86.68%, tr_best:  88.84%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.344703/  1.207827, val:  68.75%, val_best:  75.00%, tr:  86.79%, tr_best:  88.84%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.326465/  1.058281, val:  71.25%, val_best:  75.00%, tr:  87.13%, tr_best:  88.84%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.299199/  1.423685, val:  69.17%, val_best:  75.00%, tr:  88.44%, tr_best:  88.84%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.349443/  1.303577, val:  69.17%, val_best:  75.00%, tr:  86.74%, tr_best:  88.84%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.351647/  1.119907, val:  72.92%, val_best:  75.00%, tr:  86.11%, tr_best:  88.84%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.351686/  1.216063, val:  67.08%, val_best:  75.00%, tr:  86.52%, tr_best:  88.84%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.361879/  0.936130, val:  71.67%, val_best:  75.00%, tr:  86.20%, tr_best:  88.84%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.329010/  1.137360, val:  72.08%, val_best:  75.00%, tr:  86.83%, tr_best:  88.84%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.296945/  1.171583, val:  72.08%, val_best:  75.00%, tr:  88.66%, tr_best:  88.84%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.295571/  1.012460, val:  74.58%, val_best:  75.00%, tr:  88.71%, tr_best:  88.84%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.263035/  1.135702, val:  72.50%, val_best:  75.00%, tr:  89.74%, tr_best:  89.74%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.235702/  1.224778, val:  70.00%, val_best:  75.00%, tr:  90.94%, tr_best:  90.94%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.240118/  1.629642, val:  68.33%, val_best:  75.00%, tr:  91.34%, tr_best:  91.34%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.274578/  1.337828, val:  73.75%, val_best:  75.00%, tr:  89.65%, tr_best:  91.34%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.301405/  1.121827, val:  71.25%, val_best:  75.00%, tr:  89.36%, tr_best:  91.34%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.280918/  1.133698, val:  72.50%, val_best:  75.00%, tr:  89.52%, tr_best:  91.34%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.241999/  1.149745, val:  74.58%, val_best:  75.00%, tr:  91.25%, tr_best:  91.34%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.264509/  1.291039, val:  70.83%, val_best:  75.00%, tr:  90.35%, tr_best:  91.34%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.265057/  1.162579, val:  71.67%, val_best:  75.00%, tr:  89.68%, tr_best:  91.34%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.230949/  1.136136, val:  72.92%, val_best:  75.00%, tr:  91.61%, tr_best:  91.61%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.252209/  1.019260, val:  73.75%, val_best:  75.00%, tr:  90.24%, tr_best:  91.61%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.257736/  1.248872, val:  72.50%, val_best:  75.00%, tr:  91.03%, tr_best:  91.61%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.295173/  1.019947, val:  72.50%, val_best:  75.00%, tr:  89.72%, tr_best:  91.61%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.248904/  1.308456, val:  66.67%, val_best:  75.00%, tr:  90.24%, tr_best:  91.61%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.293896/  1.339177, val:  72.50%, val_best:  75.00%, tr:  89.02%, tr_best:  91.61%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.278057/  1.283003, val:  70.00%, val_best:  75.00%, tr:  90.42%, tr_best:  91.61%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.214428/  1.412125, val:  69.17%, val_best:  75.00%, tr:  91.52%, tr_best:  91.61%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.234235/  1.180795, val:  71.67%, val_best:  75.00%, tr:  91.34%, tr_best:  91.61%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.211217/  1.237771, val:  70.00%, val_best:  75.00%, tr:  92.20%, tr_best:  92.20%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.223163/  1.337669, val:  71.25%, val_best:  75.00%, tr:  91.30%, tr_best:  92.20%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.201275/  0.941738, val:  77.08%, val_best:  77.08%, tr:  92.67%, tr_best:  92.67%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.207002/  0.999219, val:  75.42%, val_best:  77.08%, tr:  92.45%, tr_best:  92.67%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.175565/  1.397610, val:  71.25%, val_best:  77.08%, tr:  93.76%, tr_best:  93.76%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.201600/  1.198300, val:  76.25%, val_best:  77.08%, tr:  92.99%, tr_best:  93.76%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.207616/  1.342483, val:  68.75%, val_best:  77.08%, tr:  91.75%, tr_best:  93.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddee61fa545d4fbba445e49f47115a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▅▃▄▂█▄▄▄▃▂▂▇▅▅▆▁▅▃▅▆▅▄▆▅▆█▇▇▄██▄▇▅▇▇▆▆▇</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇█▇▇█████▇█▇▇███</td></tr><tr><td>tr_acc</td><td>▁▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇████▇███████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▄▃▃▃▄▄▃▃▃▃▃▃▃▃▃▂▂</td></tr><tr><td>val_acc_best</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇█▇▇█████▇█▇▇███</td></tr><tr><td>val_loss</td><td>▁▆▆▇▆▇▆▇▆▆█▇▇▇▇▇▆▇▇▇▅█▇█▇▆▇▇▆▆▆▇▆▇▆▇▇▇▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.91749</td></tr><tr><td>tr_epoch_loss</td><td>0.20762</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.6875</td></tr><tr><td>val_loss</td><td>1.34248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-68</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ojf4hthv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ojf4hthv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_101328-ojf4hthv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l17mctyo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_102539-l17mctyo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l17mctyo' target=\"_blank\">morning-sweep-69</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l17mctyo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l17mctyo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.975030/  0.991757, val:  57.50%, val_best:  57.50%, tr:  57.94%, tr_best:  57.94%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.672616/  0.926743, val:  57.08%, val_best:  57.50%, tr:  69.23%, tr_best:  69.23%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.547830/  0.755480, val:  68.33%, val_best:  68.33%, tr:  73.20%, tr_best:  73.20%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.496304/  0.722631, val:  71.25%, val_best:  71.25%, tr:  75.86%, tr_best:  75.86%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.418420/  0.771463, val:  70.83%, val_best:  71.25%, tr:  80.88%, tr_best:  80.88%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.351621/  0.676711, val:  77.92%, val_best:  77.92%, tr:  84.17%, tr_best:  84.17%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.288541/  0.837767, val:  79.58%, val_best:  79.58%, tr:  88.77%, tr_best:  88.77%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.256158/  0.749531, val:  76.67%, val_best:  79.58%, tr:  89.77%, tr_best:  89.77%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.217468/  0.660032, val:  80.00%, val_best:  80.00%, tr:  91.88%, tr_best:  91.88%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.174874/  0.706169, val:  81.67%, val_best:  81.67%, tr:  93.26%, tr_best:  93.26%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.158395/  0.741135, val:  78.33%, val_best:  81.67%, tr:  94.21%, tr_best:  94.21%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.133108/  0.787329, val:  77.50%, val_best:  81.67%, tr:  95.18%, tr_best:  95.18%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.114216/  1.010283, val:  79.17%, val_best:  81.67%, tr:  95.47%, tr_best:  95.47%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.146818/  0.743989, val:  82.08%, val_best:  82.08%, tr:  94.91%, tr_best:  95.47%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.110709/  0.738224, val:  83.33%, val_best:  83.33%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.057628/  0.670519, val:  86.25%, val_best:  86.25%, tr:  97.99%, tr_best:  97.99%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.066968/  0.695840, val:  86.67%, val_best:  86.67%, tr:  97.95%, tr_best:  97.99%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.029700/  0.890738, val:  83.75%, val_best:  86.67%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.060779/  0.677849, val:  85.83%, val_best:  86.67%, tr:  97.95%, tr_best:  99.12%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.062885/  0.837760, val:  81.67%, val_best:  86.67%, tr:  97.90%, tr_best:  99.12%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.022083/  0.763618, val:  84.58%, val_best:  86.67%, tr:  99.44%, tr_best:  99.44%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.014384/  0.752073, val:  85.83%, val_best:  86.67%, tr:  99.64%, tr_best:  99.64%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.004246/  0.751865, val:  86.25%, val_best:  86.67%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.003830/  0.808237, val:  86.25%, val_best:  86.67%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.001289/  0.794606, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.001040/  0.812577, val:  85.83%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000864/  0.796382, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000713/  0.794556, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000638/  0.804900, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000562/  0.809277, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000526/  0.808429, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000470/  0.825555, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000444/  0.821981, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000415/  0.821700, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000397/  0.827193, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000371/  0.826848, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000358/  0.813747, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000337/  0.831884, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000325/  0.831817, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000309/  0.826039, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000294/  0.832907, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000284/  0.833797, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000272/  0.827883, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000263/  0.831690, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000251/  0.831927, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000242/  0.830139, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000235/  0.828615, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000226/  0.826167, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.832099, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000212/  0.837962, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000207/  0.842537, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000201/  0.844948, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000196/  0.845348, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000188/  0.834060, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000184/  0.835479, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000180/  0.837751, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000176/  0.835122, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.832945, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000167/  0.826619, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.830393, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000161/  0.829417, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000157/  0.826624, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000155/  0.831964, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000151/  0.835342, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000148/  0.836757, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000144/  0.839362, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000141/  0.836985, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000138/  0.839648, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000134/  0.839897, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000132/  0.848404, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.850224, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.852088, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000125/  0.855238, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000124/  0.856294, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.860183, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000121/  0.861020, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000118/  0.862218, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000116/  0.859933, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.865012, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000111/  0.865852, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000110/  0.865836, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000107/  0.865894, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000106/  0.871937, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000106/  0.870413, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000105/  0.874529, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000101/  0.875993, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000101/  0.876836, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000098/  0.873239, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000096/  0.873634, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000095/  0.883274, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000095/  0.887032, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000093/  0.886375, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000092/  0.889039, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000090/  0.886991, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000089/  0.889117, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000090/  0.891596, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000087/  0.889295, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000086/  0.888970, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000085/  0.889035, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000084/  0.891712, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e80d36e3f342078f9bd3b8eb3b0a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▅██▆█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▄▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▆▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>0.89171</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-sweep-69</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l17mctyo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l17mctyo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_102539-l17mctyo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b9ww3vet with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_103724-b9ww3vet</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9ww3vet' target=\"_blank\">grateful-sweep-70</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9ww3vet' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9ww3vet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.922245/  0.893013, val:  62.50%, val_best:  62.50%, tr:  59.38%, tr_best:  59.38%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619733/  0.880989, val:  60.42%, val_best:  62.50%, tr:  72.18%, tr_best:  72.18%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.435798/  0.683104, val:  77.08%, val_best:  77.08%, tr:  81.02%, tr_best:  81.02%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.299372/  0.505556, val:  85.00%, val_best:  85.00%, tr:  89.02%, tr_best:  89.02%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.197358/  0.403630, val:  85.42%, val_best:  85.42%, tr:  93.10%, tr_best:  93.10%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.129766/  0.439173, val:  87.08%, val_best:  87.08%, tr:  95.56%, tr_best:  95.56%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.104668/  0.714281, val:  84.58%, val_best:  87.08%, tr:  96.44%, tr_best:  96.44%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.090463/  0.478552, val:  87.08%, val_best:  87.08%, tr:  97.00%, tr_best:  97.00%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.049652/  0.436157, val:  88.33%, val_best:  88.33%, tr:  98.60%, tr_best:  98.60%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.023756/  0.413970, val:  90.00%, val_best:  90.00%, tr:  99.50%, tr_best:  99.50%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032942/  0.472970, val:  88.75%, val_best:  90.00%, tr:  99.03%, tr_best:  99.50%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.027201/  0.430632, val:  89.17%, val_best:  90.00%, tr:  99.12%, tr_best:  99.50%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.013121/  0.514948, val:  89.17%, val_best:  90.00%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.010347/  0.563254, val:  87.92%, val_best:  90.00%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.009572/  0.435153, val:  90.00%, val_best:  90.00%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.003371/  0.482731, val:  90.83%, val_best:  90.83%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001488/  0.473772, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001112/  0.486397, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000995/  0.495737, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000847/  0.505289, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000754/  0.504387, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000669/  0.500575, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000629/  0.510733, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000573/  0.509627, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000528/  0.522962, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000497/  0.518812, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000478/  0.511635, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000442/  0.520021, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000434/  0.512689, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000405/  0.525894, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000383/  0.520380, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000363/  0.527806, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000349/  0.518233, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000336/  0.520770, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000322/  0.527361, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000309/  0.520290, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000296/  0.522064, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000285/  0.526341, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.531222, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000272/  0.532444, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000261/  0.533822, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000254/  0.536716, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000246/  0.538109, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000241/  0.536533, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000233/  0.537335, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000232/  0.527503, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000220/  0.538040, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000214/  0.531304, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000209/  0.538361, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.543586, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.541322, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000196/  0.542786, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000192/  0.547309, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000189/  0.547139, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000184/  0.542990, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000183/  0.544180, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000179/  0.548399, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000175/  0.548830, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000173/  0.553346, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000169/  0.551210, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000168/  0.558970, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000162/  0.561837, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000162/  0.560709, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000157/  0.561463, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000154/  0.560693, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000149/  0.560770, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000150/  0.559654, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000146/  0.561553, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000144/  0.562505, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000142/  0.564370, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000137/  0.567566, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000136/  0.571191, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000135/  0.564148, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000132/  0.569437, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000131/  0.568583, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000128/  0.572920, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000126/  0.570853, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000124/  0.573105, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.572809, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000122/  0.576635, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000119/  0.575801, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000117/  0.575580, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.577640, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000114/  0.579572, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000112/  0.580623, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000110/  0.577019, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000110/  0.579865, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000108/  0.580845, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000107/  0.577829, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000106/  0.577943, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000105/  0.577916, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000102/  0.581132, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000102/  0.583284, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000101/  0.579166, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.579297, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000097/  0.579135, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000096/  0.582260, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000095/  0.580457, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000094/  0.579609, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000093/  0.581171, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beac1049c3024269b65bdb6f0326f7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▄▅▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.91667</td></tr><tr><td>val_loss</td><td>0.58117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-70</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9ww3vet' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/b9ww3vet</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_103724-b9ww3vet/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l5la51qs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_104901-l5la51qs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5la51qs' target=\"_blank\">swept-sweep-71</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5la51qs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5la51qs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.935111/  0.930279, val:  60.00%, val_best:  60.00%, tr:  60.10%, tr_best:  60.10%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.645711/  0.861115, val:  64.17%, val_best:  64.17%, tr:  70.06%, tr_best:  70.06%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.464411/  0.776048, val:  69.17%, val_best:  69.17%, tr:  79.46%, tr_best:  79.46%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.317624/  0.596377, val:  87.08%, val_best:  87.08%, tr:  87.51%, tr_best:  87.51%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.193335/  0.598724, val:  81.67%, val_best:  87.08%, tr:  92.72%, tr_best:  92.72%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.149819/  0.455364, val:  88.33%, val_best:  88.33%, tr:  95.00%, tr_best:  95.00%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.088756/  0.554473, val:  86.67%, val_best:  88.33%, tr:  97.00%, tr_best:  97.00%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.075862/  0.510597, val:  89.17%, val_best:  89.17%, tr:  97.57%, tr_best:  97.57%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.049052/  0.497094, val:  88.33%, val_best:  89.17%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.031018/  0.415403, val:  89.58%, val_best:  89.58%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.029693/  0.542248, val:  86.67%, val_best:  89.58%, tr:  98.96%, tr_best:  99.10%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.010855/  0.526078, val:  88.33%, val_best:  89.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.008798/  0.547291, val:  87.50%, val_best:  89.58%, tr:  99.77%, tr_best:  99.80%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.015032/  0.560051, val:  88.33%, val_best:  89.58%, tr:  99.64%, tr_best:  99.80%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.005708/  0.569632, val:  89.17%, val_best:  89.58%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002184/  0.554556, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001564/  0.569374, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001318/  0.565142, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001147/  0.569481, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000997/  0.579033, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000912/  0.574463, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000790/  0.574381, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000757/  0.581305, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000706/  0.581143, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000645/  0.587124, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000605/  0.589668, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000579/  0.599829, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000535/  0.593670, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000503/  0.602743, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000494/  0.604577, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000449/  0.597533, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000450/  0.594905, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000420/  0.601580, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000406/  0.607032, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000387/  0.603703, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000381/  0.612077, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000358/  0.613359, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000350/  0.614901, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000338/  0.609380, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000331/  0.605119, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000315/  0.605948, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000306/  0.607426, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000292/  0.604346, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000282/  0.608507, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000278/  0.615299, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000268/  0.611451, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000262/  0.617933, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000255/  0.621305, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000249/  0.622792, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000244/  0.623876, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000243/  0.625618, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000231/  0.624153, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000230/  0.629497, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000221/  0.634032, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.631043, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000215/  0.625982, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000212/  0.626839, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000207/  0.630988, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000205/  0.633899, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000203/  0.632317, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000195/  0.636105, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000192/  0.634124, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000189/  0.636216, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000186/  0.631300, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000181/  0.634030, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000178/  0.631975, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000177/  0.631937, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000174/  0.632074, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000171/  0.627129, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000169/  0.628457, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000164/  0.625540, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000162/  0.629360, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000161/  0.627331, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000157/  0.625633, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000157/  0.624692, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000151/  0.627005, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000150/  0.625528, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000147/  0.623157, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000143/  0.622877, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000142/  0.625825, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000140/  0.628313, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000136/  0.623228, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000135/  0.621479, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000134/  0.626184, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000130/  0.624385, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000131/  0.625518, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000129/  0.626592, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000126/  0.630556, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000126/  0.633178, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000125/  0.636248, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000123/  0.639624, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000121/  0.644317, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000119/  0.646648, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000116/  0.643217, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000114/  0.644149, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000114/  0.641993, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644623, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644130, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000110/  0.644194, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000109/  0.649310, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257fdeeb4a8443f5b0ec4c861d44337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.64931</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-sweep-71</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5la51qs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/l5la51qs</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_104901-l5la51qs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6d8pw50p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_110124-6d8pw50p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6d8pw50p' target=\"_blank\">summer-sweep-72</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6d8pw50p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6d8pw50p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.317791/  0.901137, val:  62.08%, val_best:  62.08%, tr:  48.20%, tr_best:  48.20%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.697365/  0.754347, val:  68.75%, val_best:  68.75%, tr:  67.97%, tr_best:  67.97%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.579029/  0.662889, val:  71.25%, val_best:  71.25%, tr:  72.43%, tr_best:  72.43%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.523778/  0.672136, val:  70.83%, val_best:  71.25%, tr:  75.88%, tr_best:  75.88%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.471563/  0.673381, val:  72.08%, val_best:  72.08%, tr:  78.85%, tr_best:  78.85%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.426075/  0.658884, val:  74.58%, val_best:  74.58%, tr:  81.33%, tr_best:  81.33%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.391394/  0.629092, val:  76.25%, val_best:  76.25%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.337095/  0.602105, val:  78.75%, val_best:  78.75%, tr:  86.72%, tr_best:  86.72%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.299739/  0.603002, val:  79.58%, val_best:  79.58%, tr:  88.57%, tr_best:  88.57%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.253928/  0.588129, val:  82.50%, val_best:  82.50%, tr:  90.62%, tr_best:  90.62%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.219572/  0.547260, val:  83.33%, val_best:  83.33%, tr:  92.61%, tr_best:  92.61%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.185195/  0.530068, val:  84.17%, val_best:  84.17%, tr:  93.76%, tr_best:  93.76%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.158359/  0.604478, val:  82.92%, val_best:  84.17%, tr:  95.22%, tr_best:  95.22%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.138643/  0.474686, val:  87.92%, val_best:  87.92%, tr:  95.54%, tr_best:  95.54%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.134354/  0.455562, val:  86.67%, val_best:  87.92%, tr:  95.40%, tr_best:  95.54%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.100770/  0.451988, val:  83.75%, val_best:  87.92%, tr:  96.84%, tr_best:  96.84%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.097351/  0.465656, val:  85.83%, val_best:  87.92%, tr:  96.91%, tr_best:  96.91%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.081862/  0.518851, val:  84.58%, val_best:  87.92%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.078367/  0.510588, val:  85.83%, val_best:  87.92%, tr:  97.77%, tr_best:  97.77%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.062339/  0.507325, val:  87.50%, val_best:  87.92%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.053821/  0.556106, val:  85.00%, val_best:  87.92%, tr:  98.49%, tr_best:  98.49%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.045657/  0.576654, val:  84.58%, val_best:  87.92%, tr:  98.90%, tr_best:  98.90%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.038553/  0.540663, val:  87.08%, val_best:  87.92%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.048074/  0.486234, val:  86.25%, val_best:  87.92%, tr:  98.78%, tr_best:  99.10%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.035494/  0.522165, val:  88.33%, val_best:  88.33%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.025166/  0.556271, val:  86.25%, val_best:  88.33%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.027491/  0.548536, val:  85.83%, val_best:  88.33%, tr:  99.35%, tr_best:  99.57%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.023966/  0.507675, val:  86.25%, val_best:  88.33%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.020183/  0.527415, val:  86.67%, val_best:  88.33%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.018089/  0.504388, val:  87.92%, val_best:  88.33%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.016306/  0.544496, val:  86.67%, val_best:  88.33%, tr:  99.68%, tr_best:  99.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.018347/  0.476669, val:  87.50%, val_best:  88.33%, tr:  99.64%, tr_best:  99.77%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.020790/  0.488015, val:  88.33%, val_best:  88.33%, tr:  99.53%, tr_best:  99.77%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.011808/  0.525052, val:  88.33%, val_best:  88.33%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.009261/  0.563480, val:  87.92%, val_best:  88.33%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.008718/  0.527995, val:  88.33%, val_best:  88.33%, tr:  99.95%, tr_best:  99.98%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.008121/  0.545174, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.008503/  0.540562, val:  88.33%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.007886/  0.535225, val:  88.75%, val_best:  88.75%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.006398/  0.524422, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.006381/  0.586545, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.005717/  0.542992, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.005559/  0.554159, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.005245/  0.581738, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.005179/  0.575151, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.004693/  0.564398, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.004464/  0.578246, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.004477/  0.552343, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.004252/  0.561890, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.004082/  0.572994, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.004100/  0.578424, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.004650/  0.574422, val:  87.92%, val_best:  88.75%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.003689/  0.591131, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.003655/  0.575070, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.003317/  0.602189, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.003254/  0.596487, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.003107/  0.581150, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.003063/  0.583380, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.002974/  0.597823, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.002742/  0.582209, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.002705/  0.599091, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.002742/  0.576127, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.002853/  0.580030, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002665/  0.588701, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.002405/  0.584536, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002367/  0.577682, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002394/  0.580000, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.002186/  0.592077, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.002148/  0.596995, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002193/  0.585010, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002147/  0.577568, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.002071/  0.583679, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.002059/  0.591282, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002012/  0.592153, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001937/  0.583132, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001926/  0.581731, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001855/  0.589253, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001814/  0.597953, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001818/  0.595003, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001807/  0.599416, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001701/  0.598140, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001667/  0.576592, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001667/  0.582907, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001632/  0.586557, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001614/  0.581471, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001562/  0.584048, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001503/  0.591823, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001565/  0.593122, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001497/  0.587503, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001484/  0.586232, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001484/  0.597951, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001392/  0.595420, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001379/  0.592659, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001383/  0.603263, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001354/  0.590934, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001329/  0.602207, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001318/  0.609845, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001300/  0.606236, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001276/  0.600390, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001235/  0.621174, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f4eca715d94f538b8b1090dae048e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▅█▆▆▅█▆▆██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▇▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▇▆▆▆▇▇▆▆▆▇▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00123</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>0.62117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-72</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6d8pw50p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6d8pw50p</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_110124-6d8pw50p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llt51w0s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_111413-llt51w0s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llt51w0s' target=\"_blank\">efficient-sweep-73</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llt51w0s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llt51w0s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.953107/  0.938305, val:  59.58%, val_best:  59.58%, tr:  58.90%, tr_best:  58.90%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.651507/  0.910724, val:  60.42%, val_best:  60.42%, tr:  70.40%, tr_best:  70.40%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.517081/  0.951042, val:  62.92%, val_best:  62.92%, tr:  75.27%, tr_best:  75.27%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.458615/  0.724900, val:  74.17%, val_best:  74.17%, tr:  80.05%, tr_best:  80.05%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.332505/  0.783821, val:  72.50%, val_best:  74.17%, tr:  86.81%, tr_best:  86.81%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.259260/  0.493379, val:  84.17%, val_best:  84.17%, tr:  90.58%, tr_best:  90.58%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.207894/  0.633929, val:  81.67%, val_best:  84.17%, tr:  92.34%, tr_best:  92.34%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.189358/  0.870315, val:  75.42%, val_best:  84.17%, tr:  92.79%, tr_best:  92.79%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.158651/  0.547789, val:  86.25%, val_best:  86.25%, tr:  94.57%, tr_best:  94.57%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.130329/  0.489861, val:  87.92%, val_best:  87.92%, tr:  95.63%, tr_best:  95.63%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.092848/  0.539160, val:  85.42%, val_best:  87.92%, tr:  96.57%, tr_best:  96.57%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.080580/  0.621149, val:  85.83%, val_best:  87.92%, tr:  97.32%, tr_best:  97.32%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.068924/  0.655764, val:  87.08%, val_best:  87.92%, tr:  97.79%, tr_best:  97.79%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.058601/  0.542894, val:  88.75%, val_best:  88.75%, tr:  97.86%, tr_best:  97.86%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.052573/  0.436005, val:  89.58%, val_best:  89.58%, tr:  98.33%, tr_best:  98.33%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.038269/  0.567150, val:  87.08%, val_best:  89.58%, tr:  98.62%, tr_best:  98.62%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.022370/  0.536501, val:  90.42%, val_best:  90.42%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.020773/  0.508552, val:  88.33%, val_best:  90.42%, tr:  99.44%, tr_best:  99.48%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.008879/  0.474313, val:  90.00%, val_best:  90.42%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.015513/  0.482745, val:  88.33%, val_best:  90.42%, tr:  99.39%, tr_best:  99.77%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.024721/  0.620573, val:  87.08%, val_best:  90.42%, tr:  99.19%, tr_best:  99.77%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.012816/  0.541863, val:  90.00%, val_best:  90.42%, tr:  99.64%, tr_best:  99.77%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.004148/  0.585074, val:  88.75%, val_best:  90.42%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.002682/  0.600688, val:  89.58%, val_best:  90.42%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.001377/  0.612101, val:  90.00%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.001070/  0.588539, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.001783/  0.589997, val:  90.00%, val_best:  90.42%, tr:  99.91%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000754/  0.584652, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000665/  0.568595, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000545/  0.577976, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000501/  0.588613, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000470/  0.591859, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000420/  0.598476, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000402/  0.601267, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000376/  0.599196, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000350/  0.600601, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000330/  0.605089, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000306/  0.603576, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000294/  0.606167, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000289/  0.602550, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000270/  0.603690, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000265/  0.599183, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000257/  0.599092, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.602820, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000238/  0.611417, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.617158, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.614822, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000216/  0.615568, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000206/  0.615049, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000204/  0.623142, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.618530, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000193/  0.618779, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000186/  0.620038, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000182/  0.622304, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000179/  0.632756, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000174/  0.627310, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000172/  0.626363, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000164/  0.624982, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000163/  0.626915, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000157/  0.629669, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000153/  0.627658, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000151/  0.631203, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000151/  0.632573, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000145/  0.634002, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000142/  0.640684, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000138/  0.637273, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000136/  0.632981, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634008, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000131/  0.638018, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000128/  0.635527, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000126/  0.638568, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000124/  0.638642, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000122/  0.642753, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000119/  0.643266, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000118/  0.646550, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000115/  0.648682, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000113/  0.650380, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644513, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000110/  0.648480, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000108/  0.646552, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000106/  0.647111, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000106/  0.647037, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000105/  0.650413, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000102/  0.648884, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000101/  0.646713, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000098/  0.648400, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000098/  0.646902, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000097/  0.644486, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000095/  0.647113, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000094/  0.641932, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000092/  0.642901, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000092/  0.639776, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000091/  0.643213, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000089/  0.644660, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000086/  0.644021, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000086/  0.641443, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000086/  0.643930, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000083/  0.644086, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000082/  0.645352, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000080/  0.643082, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25725b0fe1964d9eb207b625643a19f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▁█▅█▅█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇█▅▆▄▅▅▅▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.64308</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-73</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llt51w0s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llt51w0s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_111413-llt51w0s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kk3r9wqo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_112543-kk3r9wqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kk3r9wqo' target=\"_blank\">spring-sweep-74</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kk3r9wqo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kk3r9wqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.952857/  0.825421, val:  63.33%, val_best:  63.33%, tr:  59.04%, tr_best:  59.04%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.648967/  0.852933, val:  61.67%, val_best:  63.33%, tr:  70.49%, tr_best:  70.49%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.485335/  0.839675, val:  68.33%, val_best:  68.33%, tr:  78.25%, tr_best:  78.25%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.387051/  0.616729, val:  79.17%, val_best:  79.17%, tr:  84.42%, tr_best:  84.42%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.289949/  0.655673, val:  74.58%, val_best:  79.17%, tr:  88.98%, tr_best:  88.98%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.214469/  0.599094, val:  85.42%, val_best:  85.42%, tr:  92.47%, tr_best:  92.47%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.202393/  0.761820, val:  79.58%, val_best:  85.42%, tr:  92.83%, tr_best:  92.83%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.148434/  0.551951, val:  84.17%, val_best:  85.42%, tr:  94.66%, tr_best:  94.66%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.112114/  0.565984, val:  85.42%, val_best:  85.42%, tr:  95.96%, tr_best:  95.96%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.077534/  0.497425, val:  86.25%, val_best:  86.25%, tr:  97.48%, tr_best:  97.48%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.067240/  0.514828, val:  87.92%, val_best:  87.92%, tr:  97.68%, tr_best:  97.68%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.080662/  0.615251, val:  83.33%, val_best:  87.92%, tr:  97.18%, tr_best:  97.68%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.082407/  0.562441, val:  84.58%, val_best:  87.92%, tr:  96.91%, tr_best:  97.68%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.047357/  0.553840, val:  88.75%, val_best:  88.75%, tr:  98.38%, tr_best:  98.38%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.032547/  0.605889, val:  87.08%, val_best:  88.75%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.018779/  0.450784, val:  90.00%, val_best:  90.00%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.014937/  0.591963, val:  86.25%, val_best:  90.00%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.035282/  0.566745, val:  86.25%, val_best:  90.00%, tr:  98.78%, tr_best:  99.48%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.012285/  0.652689, val:  85.00%, val_best:  90.00%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.023110/  0.494165, val:  89.17%, val_best:  90.00%, tr:  99.28%, tr_best:  99.66%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.007880/  0.654567, val:  88.33%, val_best:  90.00%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.004243/  0.483552, val:  87.92%, val_best:  90.00%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001281/  0.502068, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000843/  0.485636, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000689/  0.497797, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000617/  0.509015, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000548/  0.504469, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000487/  0.510740, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000450/  0.509562, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000419/  0.512152, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000388/  0.519887, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000367/  0.524113, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000346/  0.522739, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000327/  0.523189, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000318/  0.526886, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000302/  0.520204, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000280/  0.526231, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000274/  0.523908, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000266/  0.530208, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000248/  0.530043, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000239/  0.531338, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000230/  0.535591, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000221/  0.535722, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000217/  0.532000, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000212/  0.538493, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000203/  0.541831, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000201/  0.545361, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000193/  0.543755, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000190/  0.546959, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000182/  0.547381, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000177/  0.551038, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000174/  0.549149, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000168/  0.547630, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000165/  0.554588, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000160/  0.553629, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000156/  0.553140, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000153/  0.556958, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000150/  0.554029, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000148/  0.556476, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000144/  0.554858, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000140/  0.553949, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000137/  0.557498, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000138/  0.561489, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000133/  0.557124, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000130/  0.563232, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000128/  0.567658, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000127/  0.565291, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000125/  0.567231, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000121/  0.569663, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000119/  0.571161, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000119/  0.571654, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000117/  0.569339, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000116/  0.570370, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000112/  0.571861, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000110/  0.572053, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000109/  0.577088, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000106/  0.572590, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000105/  0.573096, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000103/  0.571871, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000103/  0.574234, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000101/  0.571711, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000100/  0.572074, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000097/  0.574149, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000097/  0.573621, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000096/  0.571233, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000094/  0.569758, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000092/  0.571481, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000093/  0.568655, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000091/  0.569005, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000090/  0.572812, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000088/  0.578155, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000088/  0.584114, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000086/  0.581101, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000083/  0.574578, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000083/  0.571765, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000081/  0.568677, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000080/  0.562610, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000079/  0.565715, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000079/  0.559325, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000078/  0.555617, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4f99332e9a485886ed5bf1d036bf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▅▅████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.55562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-sweep-74</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kk3r9wqo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kk3r9wqo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_112543-kk3r9wqo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7zisr7kw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_113711-7zisr7kw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7zisr7kw' target=\"_blank\">tough-sweep-75</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7zisr7kw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7zisr7kw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.928935/  0.881833, val:  59.58%, val_best:  59.58%, tr:  60.01%, tr_best:  60.01%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.622879/  0.854058, val:  62.92%, val_best:  62.92%, tr:  71.73%, tr_best:  71.73%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.415023/  0.815604, val:  70.00%, val_best:  70.00%, tr:  82.57%, tr_best:  82.57%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.301157/  0.537661, val:  85.42%, val_best:  85.42%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.169007/  0.494498, val:  85.42%, val_best:  85.42%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.108344/  0.441543, val:  89.17%, val_best:  89.17%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.078059/  0.622470, val:  88.33%, val_best:  89.17%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.058447/  0.424832, val:  88.75%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051343/  0.443390, val:  90.42%, val_best:  90.42%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019991/  0.473071, val:  88.75%, val_best:  90.42%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024913/  0.594811, val:  87.92%, val_best:  90.42%, tr:  99.23%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.011130/  0.510964, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003125/  0.552201, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001943/  0.502089, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001624/  0.521189, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001236/  0.545577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001065/  0.524273, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000955/  0.556683, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000850/  0.553951, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000777/  0.562979, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000711/  0.561448, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000666/  0.572040, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000628/  0.581079, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000576/  0.569785, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000538/  0.568354, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000500/  0.574372, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000475/  0.581407, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000450/  0.584895, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000429/  0.586062, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000411/  0.596621, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.597417, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000379/  0.608332, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000358/  0.602874, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000340/  0.604670, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000329/  0.601791, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.601425, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000308/  0.602356, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000292/  0.598435, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.597901, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000277/  0.604077, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.613960, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000256/  0.610192, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000249/  0.611147, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.614404, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000235/  0.615403, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.614356, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.623049, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000215/  0.624923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000211/  0.613693, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.619318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.622931, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000195/  0.624674, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.627025, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.628896, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.624894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.621996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000175/  0.622287, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.615900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617114, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.613777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000159/  0.619844, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.615970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000157/  0.617458, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.619992, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.616505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617499, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.623439, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.627106, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000138/  0.627911, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.630018, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000130/  0.633134, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.632592, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633326, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635861, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000123/  0.634962, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631111, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.628672, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629021, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.628650, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629489, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.628790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.628014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000109/  0.629268, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.627406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000109/  0.628916, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000104/  0.631419, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.634245, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.634045, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000103/  0.632370, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.629538, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.630059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.629965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.636612, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635149, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.633406, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000092/  0.634207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.632478, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11026d386b044fd599551357a92ff1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.63248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-75</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7zisr7kw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7zisr7kw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_113711-7zisr7kw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5r1z3l1z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_114919-5r1z3l1z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5r1z3l1z' target=\"_blank\">resilient-sweep-76</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5r1z3l1z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5r1z3l1z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.974480/  0.867463, val:  64.17%, val_best:  64.17%, tr:  58.93%, tr_best:  58.93%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619049/  0.779950, val:  68.75%, val_best:  68.75%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.410511/  0.901403, val:  70.83%, val_best:  70.83%, tr:  83.41%, tr_best:  83.41%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.292086/  0.534297, val:  87.08%, val_best:  87.08%, tr:  89.13%, tr_best:  89.13%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.162379/  0.612390, val:  82.50%, val_best:  87.08%, tr:  94.36%, tr_best:  94.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.140578/  0.442822, val:  89.58%, val_best:  89.58%, tr:  95.36%, tr_best:  95.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.082165/  0.784311, val:  85.00%, val_best:  89.58%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.059931/  0.539399, val:  87.50%, val_best:  89.58%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036210/  0.485832, val:  90.83%, val_best:  90.83%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019819/  0.519804, val:  88.75%, val_best:  90.83%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032321/  0.569617, val:  87.50%, val_best:  90.83%, tr:  98.90%, tr_best:  99.48%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.030033/  0.488638, val:  88.75%, val_best:  90.83%, tr:  98.96%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.012969/  0.511075, val:  91.25%, val_best:  91.25%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006165/  0.697958, val:  86.25%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.003339/  0.576901, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002147/  0.547625, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001528/  0.554330, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001201/  0.556022, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001124/  0.551401, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000866/  0.574368, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000799/  0.591013, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000714/  0.581928, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000671/  0.589273, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000652/  0.598257, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000555/  0.602714, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000540/  0.597360, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000500/  0.592729, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000474/  0.601346, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000439/  0.602200, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000428/  0.595900, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000411/  0.599700, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000382/  0.607436, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000371/  0.604986, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000348/  0.610807, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000338/  0.618313, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000328/  0.620386, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000313/  0.618911, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000303/  0.616548, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000305/  0.621521, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000285/  0.625960, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000283/  0.630643, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000271/  0.629686, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000262/  0.628815, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000250/  0.627120, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000250/  0.628091, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000242/  0.628134, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000232/  0.633806, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000230/  0.632606, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.635183, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000215/  0.640931, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000209/  0.649058, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000209/  0.644091, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000201/  0.644871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000195/  0.649851, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000196/  0.657061, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000189/  0.662943, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000186/  0.667632, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000181/  0.671509, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000180/  0.673545, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000177/  0.670262, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000173/  0.666736, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000167/  0.664875, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000169/  0.669161, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000165/  0.666249, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000162/  0.662686, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000158/  0.662513, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000158/  0.666680, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000155/  0.672476, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000152/  0.672495, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000147/  0.665847, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000146/  0.670442, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000141/  0.670678, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000143/  0.671315, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000136/  0.674515, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000138/  0.676093, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000137/  0.676866, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000134/  0.684349, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000131/  0.688230, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000130/  0.687458, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000129/  0.690846, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.691173, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000127/  0.693805, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000127/  0.689193, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000123/  0.694343, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000121/  0.694647, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000118/  0.695344, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000121/  0.699655, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000118/  0.693519, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000116/  0.685618, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000113/  0.693628, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000112/  0.695912, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000110/  0.687126, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.686712, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.686534, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.696083, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696329, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696316, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000102/  0.702537, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000100/  0.702209, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.701244, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f6f0516da240b4a270700762375468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▅█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.70124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-sweep-76</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5r1z3l1z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5r1z3l1z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_114919-5r1z3l1z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0yuq0typ with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48e64d9da11483ebf1ee4c4f02fb210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112726065847608, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_120055-0yuq0typ</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0yuq0typ' target=\"_blank\">comic-sweep-77</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0yuq0typ' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0yuq0typ</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.953018/  0.967212, val:  60.83%, val_best:  60.83%, tr:  59.58%, tr_best:  59.58%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.644991/  0.810506, val:  63.75%, val_best:  63.75%, tr:  70.24%, tr_best:  70.24%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.466322/  0.875111, val:  68.75%, val_best:  68.75%, tr:  79.55%, tr_best:  79.55%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.357321/  0.583519, val:  82.92%, val_best:  82.92%, tr:  85.57%, tr_best:  85.57%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.193410/  0.607979, val:  83.75%, val_best:  83.75%, tr:  93.06%, tr_best:  93.06%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.168225/  0.567896, val:  83.75%, val_best:  83.75%, tr:  94.09%, tr_best:  94.09%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.119751/  0.648352, val:  85.00%, val_best:  85.00%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.081161/  0.448524, val:  88.75%, val_best:  88.75%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.055093/  0.636578, val:  86.25%, val_best:  88.75%, tr:  98.31%, tr_best:  98.31%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.049443/  0.760179, val:  85.83%, val_best:  88.75%, tr:  98.38%, tr_best:  98.38%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.047290/  0.592549, val:  86.67%, val_best:  88.75%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.032362/  0.539072, val:  88.75%, val_best:  88.75%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.026094/  0.773426, val:  86.25%, val_best:  88.75%, tr:  99.05%, tr_best:  99.12%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.023321/  0.628924, val:  85.42%, val_best:  88.75%, tr:  99.17%, tr_best:  99.17%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.007880/  0.655442, val:  87.92%, val_best:  88.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.003872/  0.691944, val:  87.08%, val_best:  88.75%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.002026/  0.695137, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001267/  0.683992, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000982/  0.698778, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000810/  0.700535, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000720/  0.688468, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000657/  0.701845, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000576/  0.705274, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000550/  0.703979, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000507/  0.709431, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000477/  0.712022, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000460/  0.709761, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000420/  0.715845, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000395/  0.718305, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000382/  0.725989, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000365/  0.721288, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000353/  0.729642, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000336/  0.730154, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000320/  0.729320, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000312/  0.743510, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000299/  0.739568, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000283/  0.742285, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000273/  0.742441, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000266/  0.749720, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000258/  0.744158, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000245/  0.743758, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000238/  0.747056, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000240/  0.749259, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000226/  0.746813, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000217/  0.745975, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000214/  0.747276, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000205/  0.748278, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000200/  0.747442, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000192/  0.745683, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000189/  0.747415, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000185/  0.745606, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000183/  0.750998, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000177/  0.750856, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000170/  0.753888, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000166/  0.752202, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000164/  0.753845, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000162/  0.749103, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000155/  0.750237, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000153/  0.750249, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000149/  0.752792, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000148/  0.756515, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000145/  0.752566, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000144/  0.751532, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000139/  0.754518, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000137/  0.756308, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000136/  0.758358, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000135/  0.756996, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000132/  0.757289, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000129/  0.757536, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000127/  0.759651, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000124/  0.766466, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000122/  0.769054, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000121/  0.764523, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000119/  0.767961, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000116/  0.768126, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000116/  0.765853, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000113/  0.769080, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000112/  0.769370, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000109/  0.771659, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000110/  0.768780, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000107/  0.768745, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000104/  0.771277, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000104/  0.773000, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000103/  0.770488, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000102/  0.772085, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000101/  0.773752, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000100/  0.778005, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000098/  0.778321, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000098/  0.777972, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000097/  0.777386, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000095/  0.779767, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000093/  0.779112, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000094/  0.779982, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000091/  0.784529, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000091/  0.784094, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000089/  0.783213, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000088/  0.783486, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000088/  0.783510, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000085/  0.786149, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000085/  0.787972, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fe002ad9cb4af59096b6a73700f50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▇▁▇█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅█▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.78797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-sweep-77</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0yuq0typ' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0yuq0typ</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_120055-0yuq0typ/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zeqwqdpg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_121241-zeqwqdpg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zeqwqdpg' target=\"_blank\">vocal-sweep-78</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zeqwqdpg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zeqwqdpg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.948241/  0.810933, val:  60.00%, val_best:  60.00%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.584885/  0.808915, val:  67.50%, val_best:  67.50%, tr:  73.08%, tr_best:  73.08%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.399293/  0.645334, val:  80.83%, val_best:  80.83%, tr:  83.00%, tr_best:  83.00%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.259728/  0.521861, val:  86.67%, val_best:  86.67%, tr:  90.69%, tr_best:  90.69%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.137906/  0.453284, val:  85.83%, val_best:  86.67%, tr:  95.47%, tr_best:  95.47%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.116877/  0.433451, val:  90.00%, val_best:  90.00%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.063803/  0.606309, val:  86.25%, val_best:  90.00%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.055475/  0.473878, val:  87.92%, val_best:  90.00%, tr:  98.44%, tr_best:  98.44%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.029867/  0.433133, val:  88.33%, val_best:  90.00%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.017870/  0.454203, val:  88.75%, val_best:  90.00%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.008982/  0.488475, val:  90.83%, val_best:  90.83%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.006984/  0.494711, val:  88.75%, val_best:  90.83%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003424/  0.511547, val:  89.58%, val_best:  90.83%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001866/  0.521343, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001192/  0.529747, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.000995/  0.516564, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000863/  0.507904, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000783/  0.511017, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000708/  0.514485, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000655/  0.507083, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000602/  0.517106, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000546/  0.506922, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000511/  0.513302, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000485/  0.524377, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000439/  0.527266, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000418/  0.529247, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000399/  0.527515, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000374/  0.526822, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000364/  0.527172, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000348/  0.533110, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000342/  0.534845, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000327/  0.534818, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000312/  0.537681, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000303/  0.548502, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000288/  0.543104, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000277/  0.530877, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000265/  0.535054, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000254/  0.538853, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000251/  0.540546, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000242/  0.547666, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000240/  0.553670, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000229/  0.555047, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000224/  0.552312, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000220/  0.549097, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000212/  0.542288, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000208/  0.544394, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000205/  0.543587, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000199/  0.546550, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000193/  0.548627, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000188/  0.546433, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000185/  0.546571, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000181/  0.547747, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000175/  0.549658, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000173/  0.548468, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000166/  0.550029, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000165/  0.552771, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000164/  0.555906, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000159/  0.552982, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000156/  0.564276, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000150/  0.565147, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000150/  0.559848, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000146/  0.562109, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000150/  0.564647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000139/  0.569064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000138/  0.568714, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000134/  0.563484, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000132/  0.564067, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000131/  0.574878, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000129/  0.567569, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000128/  0.567597, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000125/  0.569186, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000123/  0.574120, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000121/  0.574806, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000120/  0.577079, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000117/  0.577886, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000115/  0.583823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000115/  0.586276, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000112/  0.584207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000112/  0.583698, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000110/  0.580823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000109/  0.584858, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000106/  0.585372, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000105/  0.584947, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000103/  0.581064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000101/  0.579744, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000100/  0.578086, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000100/  0.580465, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000098/  0.582796, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000097/  0.579961, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000095/  0.577584, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000095/  0.578770, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000094/  0.577921, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000093/  0.579972, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000092/  0.577036, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000091/  0.578224, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000088/  0.577647, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000087/  0.579864, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000087/  0.579691, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000086/  0.579581, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000084/  0.579485, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84684b02e8cc44428d9d4fc0052a4cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.57949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-78</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zeqwqdpg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zeqwqdpg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_121241-zeqwqdpg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gvzjr8qg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_122519-gvzjr8qg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvzjr8qg' target=\"_blank\">rose-sweep-79</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvzjr8qg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvzjr8qg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.054398/  0.845250, val:  60.83%, val_best:  60.83%, tr:  58.16%, tr_best:  58.16%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.642555/  0.804240, val:  65.83%, val_best:  65.83%, tr:  69.82%, tr_best:  69.82%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.539616/  0.683456, val:  71.67%, val_best:  71.67%, tr:  74.28%, tr_best:  74.28%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.481948/  0.672955, val:  74.17%, val_best:  74.17%, tr:  78.31%, tr_best:  78.31%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.419047/  0.679024, val:  70.42%, val_best:  74.17%, tr:  81.54%, tr_best:  81.54%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.367928/  0.620206, val:  80.83%, val_best:  80.83%, tr:  84.96%, tr_best:  84.96%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.317076/  0.607215, val:  81.67%, val_best:  81.67%, tr:  88.53%, tr_best:  88.53%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.245474/  0.599089, val:  81.25%, val_best:  81.67%, tr:  91.86%, tr_best:  91.86%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.222528/  0.619093, val:  80.42%, val_best:  81.67%, tr:  92.63%, tr_best:  92.63%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.174688/  0.567952, val:  85.42%, val_best:  85.42%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.138282/  0.525131, val:  86.67%, val_best:  86.67%, tr:  96.10%, tr_best:  96.10%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.116055/  0.614448, val:  85.83%, val_best:  86.67%, tr:  96.64%, tr_best:  96.64%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.107556/  0.546330, val:  85.83%, val_best:  86.67%, tr:  96.87%, tr_best:  96.87%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.086905/  0.506234, val:  88.75%, val_best:  88.75%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.073153/  0.498686, val:  87.92%, val_best:  88.75%, tr:  97.99%, tr_best:  97.99%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.048064/  0.470222, val:  88.75%, val_best:  88.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.043851/  0.505202, val:  87.92%, val_best:  88.75%, tr:  99.01%, tr_best:  99.08%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.039075/  0.524233, val:  84.17%, val_best:  88.75%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.032440/  0.495406, val:  87.92%, val_best:  88.75%, tr:  99.46%, tr_best:  99.46%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.029090/  0.576671, val:  85.83%, val_best:  88.75%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.023320/  0.557595, val:  87.50%, val_best:  88.75%, tr:  99.68%, tr_best:  99.68%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.020551/  0.550772, val:  86.25%, val_best:  88.75%, tr:  99.75%, tr_best:  99.75%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.016258/  0.553590, val:  87.08%, val_best:  88.75%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.018138/  0.519690, val:  89.17%, val_best:  89.17%, tr:  99.73%, tr_best:  99.86%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.013756/  0.547469, val:  87.92%, val_best:  89.17%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.012196/  0.559514, val:  87.92%, val_best:  89.17%, tr:  99.86%, tr_best:  99.89%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.011197/  0.540402, val:  89.17%, val_best:  89.17%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.009768/  0.528961, val:  89.17%, val_best:  89.17%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.008724/  0.528106, val:  88.33%, val_best:  89.17%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.007677/  0.544971, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.007271/  0.549115, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.006452/  0.562293, val:  89.17%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.005944/  0.526501, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.005340/  0.546472, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.004791/  0.568122, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.004946/  0.527123, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.004896/  0.579089, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.004234/  0.565700, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.004066/  0.579822, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.004246/  0.564652, val:  89.17%, val_best:  89.58%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.003757/  0.575512, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.003504/  0.572312, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.003379/  0.567295, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.003236/  0.592957, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.003076/  0.587408, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.002934/  0.566150, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.002968/  0.586898, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.002963/  0.585340, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.002843/  0.597399, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.002652/  0.584561, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.002520/  0.594850, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002417/  0.594808, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002362/  0.608089, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002310/  0.587455, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.002211/  0.602888, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.002159/  0.599122, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002044/  0.605103, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.002054/  0.609727, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001985/  0.621104, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001924/  0.617051, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001810/  0.638400, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001779/  0.614654, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001784/  0.612248, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001658/  0.634098, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001643/  0.592921, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001621/  0.623176, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001583/  0.607978, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001542/  0.606638, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001497/  0.618163, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001481/  0.610782, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001440/  0.627330, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001449/  0.626133, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001413/  0.621176, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001416/  0.624880, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001375/  0.622793, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001354/  0.626910, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001326/  0.629193, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001345/  0.632977, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001272/  0.618342, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001255/  0.620719, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001246/  0.634610, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001185/  0.621834, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001173/  0.620002, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001157/  0.625323, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001122/  0.625039, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001139/  0.617400, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001136/  0.631569, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001106/  0.631226, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001074/  0.626623, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001053/  0.631609, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001054/  0.635439, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001046/  0.627789, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001020/  0.637868, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001005/  0.637024, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000982/  0.636081, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000973/  0.643082, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000983/  0.639578, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000934/  0.638934, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000928/  0.643825, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000929/  0.627172, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03deda0422ac489c9bf014a5f5dc4883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆▆▆█▆█▆███████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00093</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.62717</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-sweep-79</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvzjr8qg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gvzjr8qg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_122519-gvzjr8qg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4yy4fmqo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_123749-4yy4fmqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4yy4fmqo' target=\"_blank\">gentle-sweep-80</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4yy4fmqo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4yy4fmqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.946897/  1.005342, val:  57.50%, val_best:  57.50%, tr:  59.40%, tr_best:  59.40%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.638867/  0.947721, val:  60.42%, val_best:  60.42%, tr:  70.63%, tr_best:  70.63%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.482423/  0.974106, val:  68.75%, val_best:  68.75%, tr:  78.25%, tr_best:  78.25%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.358548/  0.654887, val:  80.83%, val_best:  80.83%, tr:  85.96%, tr_best:  85.96%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.230987/  0.708833, val:  78.75%, val_best:  80.83%, tr:  91.82%, tr_best:  91.82%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.171760/  0.508556, val:  86.67%, val_best:  86.67%, tr:  93.85%, tr_best:  93.85%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.111449/  0.596217, val:  85.42%, val_best:  86.67%, tr:  96.24%, tr_best:  96.24%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.105808/  0.552687, val:  86.25%, val_best:  86.67%, tr:  96.30%, tr_best:  96.30%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.066457/  0.588494, val:  85.83%, val_best:  86.67%, tr:  97.90%, tr_best:  97.90%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.031007/  0.782376, val:  82.50%, val_best:  86.67%, tr:  99.14%, tr_best:  99.14%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.040941/  0.712870, val:  87.92%, val_best:  87.92%, tr:  98.62%, tr_best:  99.14%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.044566/  0.660289, val:  88.33%, val_best:  88.33%, tr:  98.60%, tr_best:  99.14%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.045855/  0.573114, val:  88.75%, val_best:  88.75%, tr:  98.62%, tr_best:  99.14%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.018127/  0.539127, val:  87.92%, val_best:  88.75%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.014949/  0.497973, val:  88.33%, val_best:  88.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.003572/  0.503426, val:  89.58%, val_best:  89.58%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001584/  0.525607, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001237/  0.526300, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001037/  0.546366, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000900/  0.557835, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000883/  0.561494, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000748/  0.562390, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000675/  0.593544, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000640/  0.590112, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000560/  0.604037, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000517/  0.593326, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000512/  0.598808, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000475/  0.602808, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000451/  0.597148, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000418/  0.606973, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000405/  0.609709, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000386/  0.615387, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000371/  0.619293, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000360/  0.608198, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000339/  0.598502, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000330/  0.607023, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000312/  0.600084, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000305/  0.590301, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000295/  0.586308, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000281/  0.580739, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000280/  0.587841, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000267/  0.586761, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000260/  0.586290, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000251/  0.588642, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000243/  0.593710, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000236/  0.598619, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000229/  0.600514, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000225/  0.599632, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000215/  0.600448, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000211/  0.600039, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000202/  0.602876, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000200/  0.606815, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000193/  0.611941, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000186/  0.609037, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000183/  0.606633, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000181/  0.609279, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000178/  0.613849, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000170/  0.609211, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000171/  0.610669, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000165/  0.614838, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000162/  0.616915, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000159/  0.616605, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000160/  0.616688, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.622439, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000151/  0.626507, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000148/  0.629446, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000145/  0.633146, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000143/  0.633386, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000140/  0.633380, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000139/  0.635025, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000136/  0.636411, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000136/  0.638248, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000132/  0.642497, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000130/  0.644120, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000128/  0.644192, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000127/  0.641420, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000127/  0.642588, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000122/  0.637521, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.649135, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000120/  0.648418, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000117/  0.648904, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.645005, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000114/  0.646377, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000112/  0.651643, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000110/  0.656006, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000110/  0.649986, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000107/  0.645650, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000105/  0.645131, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.647527, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000102/  0.651567, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000100/  0.651197, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000100/  0.650466, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000098/  0.656600, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000097/  0.655690, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000096/  0.657455, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000095/  0.655312, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000093/  0.654694, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.658173, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000093/  0.655830, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.657380, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af8473c4858470fa09a5f21dc89ced9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▅█▅███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅▇▆▅▅▅▅▅▅▅▆▅▅▅▅▅▅▅▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>0.65738</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-80</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4yy4fmqo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4yy4fmqo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_123749-4yy4fmqo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vo1tntyj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_124923-vo1tntyj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vo1tntyj' target=\"_blank\">grateful-sweep-81</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vo1tntyj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vo1tntyj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.948241/  0.810933, val:  60.00%, val_best:  60.00%, tr:  59.96%, tr_best:  59.96%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.584885/  0.808915, val:  67.50%, val_best:  67.50%, tr:  73.08%, tr_best:  73.08%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.399293/  0.645334, val:  80.83%, val_best:  80.83%, tr:  83.00%, tr_best:  83.00%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.259728/  0.521861, val:  86.67%, val_best:  86.67%, tr:  90.69%, tr_best:  90.69%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.137906/  0.453284, val:  85.83%, val_best:  86.67%, tr:  95.47%, tr_best:  95.47%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.116877/  0.433451, val:  90.00%, val_best:  90.00%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.063803/  0.606309, val:  86.25%, val_best:  90.00%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.055475/  0.473878, val:  87.92%, val_best:  90.00%, tr:  98.44%, tr_best:  98.44%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.029867/  0.433133, val:  88.33%, val_best:  90.00%, tr:  99.05%, tr_best:  99.05%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.017870/  0.454203, val:  88.75%, val_best:  90.00%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.008982/  0.488475, val:  90.83%, val_best:  90.83%, tr:  99.82%, tr_best:  99.82%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.006984/  0.494711, val:  88.75%, val_best:  90.83%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003424/  0.511547, val:  89.58%, val_best:  90.83%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001866/  0.521343, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001192/  0.529747, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.000995/  0.516564, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000863/  0.507904, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000783/  0.511017, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000708/  0.514485, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000655/  0.507083, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000602/  0.517106, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000546/  0.506922, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000511/  0.513302, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000485/  0.524377, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000439/  0.527266, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000418/  0.529247, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000399/  0.527515, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000374/  0.526822, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000364/  0.527172, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000348/  0.533110, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000342/  0.534845, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000327/  0.534818, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000312/  0.537681, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000303/  0.548502, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000288/  0.543104, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000277/  0.530877, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000265/  0.535054, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000254/  0.538853, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000251/  0.540546, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000242/  0.547666, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000240/  0.553670, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000229/  0.555047, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000224/  0.552312, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000220/  0.549097, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000212/  0.542288, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000208/  0.544394, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000205/  0.543587, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000199/  0.546550, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000193/  0.548627, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000188/  0.546433, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000185/  0.546571, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000181/  0.547747, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000175/  0.549658, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000173/  0.548468, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000166/  0.550029, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000165/  0.552771, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000164/  0.555906, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000159/  0.552982, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000156/  0.564276, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000150/  0.565147, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000150/  0.559848, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000146/  0.562109, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000150/  0.564647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000139/  0.569064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000138/  0.568714, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000134/  0.563484, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000132/  0.564067, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000131/  0.574878, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000129/  0.567569, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000128/  0.567597, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000125/  0.569186, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000123/  0.574120, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000121/  0.574806, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000120/  0.577079, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000117/  0.577886, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000115/  0.583823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000115/  0.586276, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000112/  0.584207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000112/  0.583698, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000110/  0.580823, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000109/  0.584858, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000106/  0.585372, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000105/  0.584947, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000103/  0.581064, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000101/  0.579744, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000100/  0.578086, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000100/  0.580465, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000098/  0.582796, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000097/  0.579961, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000095/  0.577584, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000095/  0.578770, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000094/  0.577921, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000093/  0.579972, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000092/  0.577036, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000091/  0.578224, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000088/  0.577647, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000087/  0.579864, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000087/  0.579691, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000086/  0.579581, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000084/  0.579485, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25832e34fa148868d243d15a3b10a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▅▆▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.57949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-81</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vo1tntyj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vo1tntyj</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_124923-vo1tntyj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s77ab24t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_130149-s77ab24t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s77ab24t' target=\"_blank\">neat-sweep-82</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s77ab24t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s77ab24t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.026099/  0.817240, val:  64.17%, val_best:  64.17%, tr:  59.36%, tr_best:  59.36%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.631402/  0.801544, val:  65.42%, val_best:  65.42%, tr:  70.29%, tr_best:  70.29%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.529497/  0.680173, val:  71.67%, val_best:  71.67%, tr:  74.14%, tr_best:  74.14%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.467685/  0.657269, val:  74.58%, val_best:  74.58%, tr:  79.28%, tr_best:  79.28%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.396492/  0.698810, val:  71.67%, val_best:  74.58%, tr:  83.50%, tr_best:  83.50%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.340464/  0.613662, val:  81.67%, val_best:  81.67%, tr:  86.88%, tr_best:  86.88%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.274033/  0.582481, val:  82.50%, val_best:  82.50%, tr:  90.64%, tr_best:  90.64%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.212354/  0.615413, val:  78.33%, val_best:  82.50%, tr:  93.19%, tr_best:  93.19%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.193748/  0.572952, val:  82.92%, val_best:  82.92%, tr:  93.51%, tr_best:  93.51%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.138997/  0.543581, val:  84.17%, val_best:  84.17%, tr:  96.01%, tr_best:  96.01%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.114032/  0.460420, val:  86.67%, val_best:  86.67%, tr:  96.96%, tr_best:  96.96%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.096840/  0.593573, val:  86.25%, val_best:  86.67%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.083182/  0.559759, val:  85.42%, val_best:  86.67%, tr:  97.84%, tr_best:  97.84%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.066905/  0.506272, val:  86.67%, val_best:  86.67%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.058730/  0.458238, val:  86.67%, val_best:  86.67%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.043808/  0.458626, val:  87.50%, val_best:  87.50%, tr:  99.19%, tr_best:  99.19%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.043499/  0.552141, val:  86.67%, val_best:  87.50%, tr:  98.94%, tr_best:  99.19%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.035982/  0.542688, val:  85.83%, val_best:  87.50%, tr:  99.21%, tr_best:  99.21%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.030559/  0.484708, val:  88.33%, val_best:  88.33%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.021821/  0.547258, val:  87.92%, val_best:  88.33%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.019143/  0.556267, val:  88.75%, val_best:  88.75%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.019828/  0.522859, val:  88.75%, val_best:  88.75%, tr:  99.64%, tr_best:  99.84%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.014074/  0.532696, val:  88.75%, val_best:  88.75%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.014202/  0.515308, val:  88.75%, val_best:  88.75%, tr:  99.86%, tr_best:  99.93%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.010632/  0.539331, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.009180/  0.563237, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.010182/  0.559953, val:  88.75%, val_best:  88.75%, tr:  99.84%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.007912/  0.538845, val:  87.50%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.007692/  0.542758, val:  89.17%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.009377/  0.521028, val:  88.75%, val_best:  89.17%, tr:  99.89%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.006298/  0.539640, val:  87.92%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.006119/  0.540603, val:  88.33%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.005585/  0.550032, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.005144/  0.541797, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.004392/  0.563237, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.004422/  0.551182, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.004181/  0.589753, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.003938/  0.560805, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.003701/  0.570428, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.003530/  0.570258, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.003333/  0.592966, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.003144/  0.576484, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.003032/  0.576765, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.002968/  0.600586, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.002896/  0.581285, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.002717/  0.583376, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.002610/  0.588517, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.002510/  0.593924, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.002426/  0.575955, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.002363/  0.592786, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.002243/  0.594698, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002176/  0.589945, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002134/  0.601228, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.002279/  0.601840, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.002137/  0.615732, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.002104/  0.617723, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.002010/  0.614328, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.001897/  0.616488, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001866/  0.613056, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001732/  0.607011, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001734/  0.630025, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001724/  0.604060, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.001746/  0.598095, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.001671/  0.616067, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.001584/  0.588691, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001536/  0.610672, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.001560/  0.603470, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.001463/  0.608079, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.001479/  0.604501, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.001400/  0.603993, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.001397/  0.598589, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001401/  0.611205, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.001333/  0.619507, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001331/  0.623827, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001261/  0.608209, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.001256/  0.616959, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.001252/  0.616937, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.001199/  0.621942, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.001189/  0.621043, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001194/  0.616073, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001136/  0.617210, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001118/  0.628958, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001100/  0.630586, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001072/  0.619867, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001079/  0.620767, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001018/  0.624122, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001014/  0.617615, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000992/  0.628528, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000987/  0.622992, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000969/  0.621914, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000959/  0.625908, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000937/  0.624709, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000936/  0.632001, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000930/  0.632310, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000907/  0.627791, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000891/  0.636464, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000873/  0.621835, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000853/  0.640317, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000865/  0.638624, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000850/  0.641290, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b18d1ef956484d9ba936c14543b1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆█▆▆▆█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00085</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>0.64129</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-sweep-82</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s77ab24t' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s77ab24t</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_130149-s77ab24t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k355mx8w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_131434-k355mx8w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k355mx8w' target=\"_blank\">lyric-sweep-83</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k355mx8w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k355mx8w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.974480/  0.867463, val:  64.17%, val_best:  64.17%, tr:  58.93%, tr_best:  58.93%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619049/  0.779950, val:  68.75%, val_best:  68.75%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.410511/  0.901403, val:  70.83%, val_best:  70.83%, tr:  83.41%, tr_best:  83.41%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.292086/  0.534297, val:  87.08%, val_best:  87.08%, tr:  89.13%, tr_best:  89.13%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.162379/  0.612390, val:  82.50%, val_best:  87.08%, tr:  94.36%, tr_best:  94.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.140578/  0.442822, val:  89.58%, val_best:  89.58%, tr:  95.36%, tr_best:  95.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.082165/  0.784311, val:  85.00%, val_best:  89.58%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.059931/  0.539399, val:  87.50%, val_best:  89.58%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036210/  0.485832, val:  90.83%, val_best:  90.83%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019819/  0.519804, val:  88.75%, val_best:  90.83%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032321/  0.569617, val:  87.50%, val_best:  90.83%, tr:  98.90%, tr_best:  99.48%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.030033/  0.488638, val:  88.75%, val_best:  90.83%, tr:  98.96%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.012969/  0.511075, val:  91.25%, val_best:  91.25%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006165/  0.697958, val:  86.25%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.003339/  0.576901, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002147/  0.547625, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001528/  0.554330, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001201/  0.556022, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001124/  0.551401, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000866/  0.574368, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000799/  0.591013, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000714/  0.581928, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000671/  0.589273, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000652/  0.598257, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000555/  0.602714, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000540/  0.597360, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000500/  0.592729, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000474/  0.601346, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000439/  0.602200, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000428/  0.595900, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000411/  0.599700, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000382/  0.607436, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000371/  0.604986, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000348/  0.610807, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000338/  0.618313, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000328/  0.620386, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000313/  0.618911, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000303/  0.616548, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000305/  0.621521, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000285/  0.625960, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000283/  0.630643, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000271/  0.629686, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000262/  0.628815, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000250/  0.627120, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000250/  0.628091, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000242/  0.628134, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000232/  0.633806, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000230/  0.632606, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.635183, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000215/  0.640931, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000209/  0.649058, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000209/  0.644091, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000201/  0.644871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000195/  0.649851, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000196/  0.657061, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000189/  0.662943, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000186/  0.667632, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000181/  0.671509, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000180/  0.673545, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000177/  0.670262, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000173/  0.666736, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000167/  0.664875, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000169/  0.669161, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000165/  0.666249, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000162/  0.662686, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000158/  0.662513, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000158/  0.666680, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000155/  0.672476, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000152/  0.672495, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000147/  0.665847, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000146/  0.670442, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000141/  0.670678, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000143/  0.671315, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000136/  0.674515, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000138/  0.676093, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000137/  0.676866, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000134/  0.684349, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000131/  0.688230, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000130/  0.687458, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000129/  0.690846, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.691173, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000127/  0.693805, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000127/  0.689193, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000123/  0.694343, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000121/  0.694647, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000118/  0.695344, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000121/  0.699655, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000118/  0.693519, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000116/  0.685618, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000113/  0.693628, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000112/  0.695912, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000110/  0.687126, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.686712, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.686534, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.696083, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696329, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000104/  0.696316, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000102/  0.702537, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000100/  0.702209, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.701244, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d8d491fdb0424f9725bf007a14c95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▅█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>0.70124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-sweep-83</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k355mx8w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k355mx8w</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_131434-k355mx8w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: if5cbv9f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_132650-if5cbv9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5cbv9f' target=\"_blank\">scarlet-sweep-84</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5cbv9f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5cbv9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.964566/  0.939101, val:  60.00%, val_best:  60.00%, tr:  58.84%, tr_best:  58.84%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.633053/  0.838297, val:  64.58%, val_best:  64.58%, tr:  70.42%, tr_best:  70.42%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.486333/  0.904320, val:  67.50%, val_best:  67.50%, tr:  78.31%, tr_best:  78.31%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.378800/  0.649719, val:  80.00%, val_best:  80.00%, tr:  84.51%, tr_best:  84.51%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.238374/  0.541910, val:  82.50%, val_best:  82.50%, tr:  91.32%, tr_best:  91.32%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.175726/  0.531432, val:  86.25%, val_best:  86.25%, tr:  93.94%, tr_best:  93.94%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.145951/  0.776129, val:  79.17%, val_best:  86.25%, tr:  94.82%, tr_best:  94.82%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.121052/  0.588239, val:  87.50%, val_best:  87.50%, tr:  95.38%, tr_best:  95.38%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.065080/  0.554806, val:  87.92%, val_best:  87.92%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.041710/  0.594638, val:  86.25%, val_best:  87.92%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.040240/  0.657138, val:  85.42%, val_best:  87.92%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.058937/  0.650077, val:  84.17%, val_best:  87.92%, tr:  97.95%, tr_best:  98.87%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.040377/  0.552323, val:  88.75%, val_best:  88.75%, tr:  98.72%, tr_best:  98.87%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.016479/  0.575499, val:  89.58%, val_best:  89.58%, tr:  99.53%, tr_best:  99.53%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.008113/  0.576995, val:  88.75%, val_best:  89.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.004328/  0.604224, val:  87.08%, val_best:  89.58%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.003959/  0.598487, val:  87.08%, val_best:  89.58%, tr:  99.86%, tr_best:  99.95%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.002372/  0.577962, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.002448/  0.582163, val:  88.75%, val_best:  89.58%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001246/  0.602246, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001181/  0.604908, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000894/  0.608412, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000795/  0.630708, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000753/  0.612931, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000656/  0.625491, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000627/  0.630863, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000608/  0.632617, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000542/  0.621524, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000531/  0.625664, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000512/  0.629640, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000482/  0.636499, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000446/  0.630994, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000433/  0.644267, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000407/  0.650205, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000391/  0.650853, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000378/  0.646827, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000367/  0.649288, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000356/  0.656600, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000339/  0.653127, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000325/  0.656766, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000318/  0.655782, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000302/  0.656882, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000294/  0.660882, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000287/  0.662734, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000284/  0.663005, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000274/  0.666743, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000266/  0.671194, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000259/  0.674776, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000254/  0.677374, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000246/  0.678524, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000238/  0.682508, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000231/  0.678742, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000228/  0.680829, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000226/  0.684338, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.685412, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000216/  0.689505, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000210/  0.688812, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000202/  0.689522, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000198/  0.686904, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000194/  0.686965, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000187/  0.690195, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000187/  0.693632, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000186/  0.697945, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000179/  0.694530, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000176/  0.687222, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000171/  0.689766, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000170/  0.688745, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000167/  0.692302, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000165/  0.689592, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000162/  0.695058, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000161/  0.694595, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000158/  0.697084, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000158/  0.704454, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000155/  0.706438, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000151/  0.707821, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000149/  0.707012, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000149/  0.711141, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000145/  0.714078, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000146/  0.714119, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000142/  0.711444, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000140/  0.709206, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000137/  0.711638, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000135/  0.715979, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000132/  0.713483, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000131/  0.715645, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000128/  0.711662, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000127/  0.718235, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000126/  0.712707, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000124/  0.714610, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000124/  0.716897, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000123/  0.720294, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000121/  0.717908, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000119/  0.715027, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000119/  0.719597, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000116/  0.721738, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000116/  0.722389, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000114/  0.718758, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000112/  0.722320, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000112/  0.719813, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000110/  0.719403, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1714a7c40548b39c945e81cf976a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.7194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-sweep-84</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5cbv9f' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/if5cbv9f</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_132650-if5cbv9f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e7fakyy9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266529336afb4574b9bc372444eacaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113198748272326, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_133918-e7fakyy9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e7fakyy9' target=\"_blank\">daily-sweep-85</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e7fakyy9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e7fakyy9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.954592/  0.916575, val:  55.00%, val_best:  55.00%, tr:  58.86%, tr_best:  58.86%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.635527/  0.856837, val:  63.75%, val_best:  63.75%, tr:  70.81%, tr_best:  70.81%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.481085/  0.752895, val:  70.83%, val_best:  70.83%, tr:  78.27%, tr_best:  78.27%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.407791/  0.664284, val:  80.42%, val_best:  80.42%, tr:  83.25%, tr_best:  83.25%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.275552/  0.559521, val:  81.67%, val_best:  81.67%, tr:  89.38%, tr_best:  89.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.249519/  0.498845, val:  86.25%, val_best:  86.25%, tr:  91.16%, tr_best:  91.16%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.161066/  0.635387, val:  84.58%, val_best:  86.25%, tr:  94.23%, tr_best:  94.23%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.114636/  0.580847, val:  84.17%, val_best:  86.25%, tr:  96.19%, tr_best:  96.19%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.083485/  0.653674, val:  86.25%, val_best:  86.25%, tr:  97.32%, tr_best:  97.32%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.079508/  0.513826, val:  87.50%, val_best:  87.50%, tr:  97.52%, tr_best:  97.52%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.068733/  0.626169, val:  84.17%, val_best:  87.50%, tr:  97.57%, tr_best:  97.57%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.067631/  0.675714, val:  83.75%, val_best:  87.50%, tr:  97.93%, tr_best:  97.93%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.088753/  0.641135, val:  80.83%, val_best:  87.50%, tr:  97.16%, tr_best:  97.93%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.054723/  0.678678, val:  85.42%, val_best:  87.50%, tr:  98.13%, tr_best:  98.13%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.041304/  0.498544, val:  87.92%, val_best:  87.92%, tr:  98.87%, tr_best:  98.87%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.025029/  0.484980, val:  86.67%, val_best:  87.92%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.027734/  0.563155, val:  87.50%, val_best:  87.92%, tr:  99.32%, tr_best:  99.32%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.020841/  0.511919, val:  90.83%, val_best:  90.83%, tr:  99.32%, tr_best:  99.32%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.008019/  0.555831, val:  88.33%, val_best:  90.83%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.013761/  0.606562, val:  87.08%, val_best:  90.83%, tr:  99.53%, tr_best:  99.77%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.007446/  0.558806, val:  88.33%, val_best:  90.83%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001949/  0.537360, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.001250/  0.554018, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000841/  0.569551, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000715/  0.579825, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000611/  0.585676, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000550/  0.595837, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000498/  0.593208, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000461/  0.598899, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000434/  0.586899, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000398/  0.585080, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000384/  0.584870, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000365/  0.586801, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000339/  0.606708, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000328/  0.600754, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000312/  0.603034, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000298/  0.608677, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000287/  0.604142, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000279/  0.607206, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000268/  0.602603, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000256/  0.601423, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000246/  0.602014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000242/  0.601248, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000231/  0.602078, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000225/  0.597340, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000216/  0.598513, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000210/  0.601620, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000207/  0.599398, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000197/  0.603548, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000193/  0.606923, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000189/  0.609332, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000183/  0.606086, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000178/  0.612889, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000175/  0.617204, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000172/  0.617220, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000168/  0.610617, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000161/  0.614784, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000156/  0.615845, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000153/  0.608381, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000148/  0.613991, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000147/  0.613903, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000142/  0.620106, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000143/  0.620756, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000138/  0.619210, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000135/  0.623544, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000132/  0.624875, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000130/  0.624689, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000128/  0.622206, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000123/  0.623131, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000121/  0.622837, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000121/  0.618320, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000118/  0.620942, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000117/  0.620946, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000114/  0.620507, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000113/  0.624813, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000112/  0.614007, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000111/  0.613159, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000108/  0.613136, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000107/  0.611603, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000106/  0.610465, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000103/  0.609833, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000103/  0.608083, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000101/  0.608955, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000100/  0.605305, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000099/  0.603577, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000098/  0.606409, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000096/  0.604290, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000096/  0.605292, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000094/  0.612231, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000094/  0.604724, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000093/  0.610642, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000091/  0.606527, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000091/  0.607648, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000089/  0.609513, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000088/  0.606126, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000088/  0.607279, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000087/  0.606428, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000086/  0.608622, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000085/  0.609014, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000083/  0.610417, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7ab9db1a142ed887614b0d7ba55bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆██▆▆█████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▆▅▇▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>8e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.61042</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-85</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e7fakyy9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e7fakyy9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_133918-e7fakyy9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 484fe7r7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_135054-484fe7r7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/484fe7r7' target=\"_blank\">different-sweep-86</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/484fe7r7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/484fe7r7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.935111/  0.930279, val:  60.00%, val_best:  60.00%, tr:  60.10%, tr_best:  60.10%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.645711/  0.861115, val:  64.17%, val_best:  64.17%, tr:  70.06%, tr_best:  70.06%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.464411/  0.776048, val:  69.17%, val_best:  69.17%, tr:  79.46%, tr_best:  79.46%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.317624/  0.596377, val:  87.08%, val_best:  87.08%, tr:  87.51%, tr_best:  87.51%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.193335/  0.598724, val:  81.67%, val_best:  87.08%, tr:  92.72%, tr_best:  92.72%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.149819/  0.455364, val:  88.33%, val_best:  88.33%, tr:  95.00%, tr_best:  95.00%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.088756/  0.554473, val:  86.67%, val_best:  88.33%, tr:  97.00%, tr_best:  97.00%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.075862/  0.510597, val:  89.17%, val_best:  89.17%, tr:  97.57%, tr_best:  97.57%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.049052/  0.497094, val:  88.33%, val_best:  89.17%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.031018/  0.415403, val:  89.58%, val_best:  89.58%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.029693/  0.542248, val:  86.67%, val_best:  89.58%, tr:  98.96%, tr_best:  99.10%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.010855/  0.526078, val:  88.33%, val_best:  89.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.008798/  0.547291, val:  87.50%, val_best:  89.58%, tr:  99.77%, tr_best:  99.80%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.015032/  0.560051, val:  88.33%, val_best:  89.58%, tr:  99.64%, tr_best:  99.80%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.005708/  0.569632, val:  89.17%, val_best:  89.58%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002184/  0.554556, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001564/  0.569374, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001318/  0.565142, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001147/  0.569481, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000997/  0.579033, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000912/  0.574463, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000790/  0.574381, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000757/  0.581305, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000706/  0.581143, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000645/  0.587124, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000605/  0.589668, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000579/  0.599829, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000535/  0.593670, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000503/  0.602743, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000494/  0.604577, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000449/  0.597533, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000450/  0.594905, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000420/  0.601580, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000406/  0.607032, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000387/  0.603703, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000381/  0.612077, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000358/  0.613359, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000350/  0.614901, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000338/  0.609380, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000331/  0.605119, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000315/  0.605948, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000306/  0.607426, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000292/  0.604346, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000282/  0.608507, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000278/  0.615299, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000268/  0.611451, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000262/  0.617933, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000255/  0.621305, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000249/  0.622792, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000244/  0.623876, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000243/  0.625618, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000231/  0.624153, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000230/  0.629497, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000221/  0.634032, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.631043, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000215/  0.625982, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000212/  0.626839, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000207/  0.630988, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000205/  0.633899, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000203/  0.632317, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000195/  0.636105, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000192/  0.634124, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000189/  0.636216, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000186/  0.631300, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000181/  0.634030, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000178/  0.631975, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000177/  0.631937, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000174/  0.632074, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000171/  0.627129, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000169/  0.628457, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000164/  0.625540, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000162/  0.629360, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000161/  0.627331, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000157/  0.625633, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000157/  0.624692, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000151/  0.627005, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000150/  0.625528, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000147/  0.623157, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000143/  0.622877, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000142/  0.625825, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000140/  0.628313, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000136/  0.623228, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000135/  0.621479, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000134/  0.626184, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000130/  0.624385, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000131/  0.625518, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000129/  0.626592, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000126/  0.630556, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000126/  0.633178, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000125/  0.636248, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000123/  0.639624, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000121/  0.644317, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000119/  0.646648, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000116/  0.643217, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000114/  0.644149, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000114/  0.641993, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644623, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644130, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000110/  0.644194, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000109/  0.649310, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beca2cb0ae4e4b0ea981ee823b27b278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.64931</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-sweep-86</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/484fe7r7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/484fe7r7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_135054-484fe7r7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: utc73pk9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c9a5e466fa432e97fe230d44baa2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114114988595247, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_140226-utc73pk9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/utc73pk9' target=\"_blank\">unique-sweep-87</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/utc73pk9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/utc73pk9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.924084/  0.900167, val:  62.50%, val_best:  62.50%, tr:  60.17%, tr_best:  60.17%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.608945/  0.868340, val:  60.00%, val_best:  62.50%, tr:  72.86%, tr_best:  72.86%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411019/  0.862224, val:  72.08%, val_best:  72.08%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.290071/  0.516031, val:  84.17%, val_best:  84.17%, tr:  89.34%, tr_best:  89.34%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.148748/  0.462285, val:  87.92%, val_best:  87.92%, tr:  95.38%, tr_best:  95.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.110478/  0.453902, val:  89.17%, val_best:  89.17%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.087798/  0.488559, val:  88.75%, val_best:  89.17%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.056540/  0.460554, val:  87.92%, val_best:  89.17%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.032420/  0.453790, val:  90.00%, val_best:  90.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.044882/  0.377394, val:  90.00%, val_best:  90.00%, tr:  98.67%, tr_best:  99.12%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.031081/  0.458891, val:  90.00%, val_best:  90.00%, tr:  99.01%, tr_best:  99.12%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.012163/  0.410768, val:  90.83%, val_best:  90.83%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003469/  0.424667, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002129/  0.401361, val:  91.25%, val_best:  91.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001459/  0.412334, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001118/  0.409213, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000979/  0.411735, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000870/  0.414670, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000784/  0.411883, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000699/  0.427088, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000645/  0.428068, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000597/  0.433039, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000562/  0.453180, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000520/  0.458498, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000493/  0.451457, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000468/  0.455886, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000441/  0.445366, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000417/  0.445858, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000400/  0.447511, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000372/  0.454448, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000364/  0.447946, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000352/  0.454611, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000334/  0.458291, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000324/  0.462057, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000310/  0.460060, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000302/  0.459708, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000290/  0.464959, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000283/  0.463789, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000270/  0.462140, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000260/  0.459259, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000252/  0.460512, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000247/  0.460966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000241/  0.461501, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000232/  0.467219, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000227/  0.465373, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000218/  0.464732, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000214/  0.462301, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000206/  0.463863, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000203/  0.462686, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000201/  0.461293, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000193/  0.463994, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000188/  0.463246, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000183/  0.461719, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000177/  0.464404, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000174/  0.462007, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000172/  0.465646, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000167/  0.467702, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000163/  0.466615, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000162/  0.465064, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000155/  0.466431, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000156/  0.470764, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000151/  0.474162, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000149/  0.475791, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000144/  0.477446, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000143/  0.473104, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000139/  0.477203, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000137/  0.482258, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000136/  0.479527, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000132/  0.478409, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000131/  0.478923, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.479966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.481726, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000125/  0.477696, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000124/  0.479414, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.479214, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000121/  0.478879, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.476968, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.477993, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.480774, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000115/  0.478548, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000113/  0.480919, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000112/  0.478824, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.480684, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000109/  0.478337, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000106/  0.473828, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000105/  0.479483, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000105/  0.480563, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000103/  0.480323, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000103/  0.481059, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000102/  0.479574, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.478145, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000098/  0.482602, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000097/  0.483561, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.486765, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.482070, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000093/  0.486749, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000091/  0.483553, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000091/  0.492586, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.491570, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.493886, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b33fbc501d4fef85e5b7f02e581e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.49389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-87</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/utc73pk9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/utc73pk9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_140226-utc73pk9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kskwom8j with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_141457-kskwom8j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kskwom8j' target=\"_blank\">snowy-sweep-88</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kskwom8j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kskwom8j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.925937/  0.962100, val:  58.75%, val_best:  58.75%, tr:  60.80%, tr_best:  60.80%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.620033/  0.796098, val:  66.67%, val_best:  66.67%, tr:  71.82%, tr_best:  71.82%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411212/  0.680588, val:  78.33%, val_best:  78.33%, tr:  83.23%, tr_best:  83.23%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.281519/  0.453874, val:  87.50%, val_best:  87.50%, tr:  89.50%, tr_best:  89.50%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.173730/  0.438690, val:  89.17%, val_best:  89.17%, tr:  94.18%, tr_best:  94.18%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.131084/  0.379213, val:  89.58%, val_best:  89.58%, tr:  95.60%, tr_best:  95.60%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.086907/  0.452013, val:  89.17%, val_best:  89.58%, tr:  97.11%, tr_best:  97.11%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.067697/  0.475806, val:  90.00%, val_best:  90.00%, tr:  97.88%, tr_best:  97.88%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036230/  0.564496, val:  87.08%, val_best:  90.00%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.027988/  0.473823, val:  88.75%, val_best:  90.00%, tr:  98.96%, tr_best:  99.08%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.022219/  0.579324, val:  88.33%, val_best:  90.00%, tr:  99.30%, tr_best:  99.30%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.010694/  0.597973, val:  87.50%, val_best:  90.00%, tr:  99.73%, tr_best:  99.73%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003942/  0.574832, val:  88.75%, val_best:  90.00%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002230/  0.577591, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001721/  0.549777, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001340/  0.551486, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001115/  0.559681, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000964/  0.557628, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000855/  0.562009, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000767/  0.586662, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000701/  0.575527, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000667/  0.573591, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000630/  0.574261, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000584/  0.575940, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000550/  0.584995, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000510/  0.581284, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000499/  0.586610, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000466/  0.591101, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000446/  0.584309, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000426/  0.595009, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000420/  0.584428, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000397/  0.586886, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000375/  0.589818, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000359/  0.596350, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000352/  0.605685, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000336/  0.604453, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000324/  0.607680, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000315/  0.602724, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000308/  0.608555, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000296/  0.611728, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000282/  0.606344, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000274/  0.607052, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000268/  0.609053, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000258/  0.606337, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000256/  0.604765, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000248/  0.603075, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000243/  0.603179, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000235/  0.607299, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000234/  0.605558, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000228/  0.603638, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000222/  0.606064, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000216/  0.601828, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000211/  0.605405, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000206/  0.601419, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000200/  0.601284, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000196/  0.609681, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000192/  0.613942, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000188/  0.610085, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000185/  0.609123, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000178/  0.608805, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000177/  0.610920, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000172/  0.605248, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000176/  0.613553, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614912, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614062, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000161/  0.612927, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000162/  0.615787, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000156/  0.617892, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000153/  0.615856, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000151/  0.620698, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000147/  0.627239, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000146/  0.630415, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000144/  0.633122, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000143/  0.638761, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000140/  0.647128, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000138/  0.649175, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000135/  0.653331, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000135/  0.653878, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000133/  0.657584, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000132/  0.654067, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000128/  0.654005, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000126/  0.653358, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000125/  0.655715, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000122/  0.651898, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000120/  0.655880, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000120/  0.657423, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000117/  0.653922, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000116/  0.655184, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000114/  0.653668, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000112/  0.658471, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000110/  0.661146, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000111/  0.662841, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000109/  0.660637, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000107/  0.660053, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000107/  0.660780, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000105/  0.662403, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000103/  0.665917, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000103/  0.668271, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000101/  0.670042, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000099/  0.668199, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af972572919b4042886a7fc693c6cc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.6682</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-88</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kskwom8j' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kskwom8j</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_141457-kskwom8j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0b196au5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_142804-0b196au5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0b196au5' target=\"_blank\">super-sweep-89</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0b196au5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0b196au5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.943642/  0.938946, val:  59.58%, val_best:  59.58%, tr:  58.81%, tr_best:  58.81%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.632470/  0.824024, val:  63.33%, val_best:  63.33%, tr:  71.06%, tr_best:  71.06%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.454604/  0.698644, val:  73.75%, val_best:  73.75%, tr:  79.60%, tr_best:  79.60%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.370547/  0.547234, val:  81.67%, val_best:  81.67%, tr:  85.05%, tr_best:  85.05%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.232067/  0.565105, val:  78.75%, val_best:  81.67%, tr:  91.68%, tr_best:  91.68%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.183293/  0.507258, val:  85.00%, val_best:  85.00%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.130160/  0.622391, val:  85.00%, val_best:  85.00%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.116128/  0.689524, val:  81.67%, val_best:  85.00%, tr:  96.28%, tr_best:  96.28%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.081709/  0.456600, val:  88.33%, val_best:  88.33%, tr:  97.09%, tr_best:  97.09%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.040249/  0.453722, val:  88.33%, val_best:  88.33%, tr:  98.81%, tr_best:  98.81%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.049947/  0.479559, val:  87.08%, val_best:  88.33%, tr:  98.42%, tr_best:  98.81%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.032516/  0.501117, val:  86.67%, val_best:  88.33%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.037313/  0.568651, val:  87.50%, val_best:  88.33%, tr:  98.78%, tr_best:  98.92%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.017896/  0.522650, val:  88.75%, val_best:  88.75%, tr:  99.57%, tr_best:  99.57%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.011006/  0.561810, val:  88.75%, val_best:  88.75%, tr:  99.66%, tr_best:  99.66%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.020407/  0.533172, val:  87.92%, val_best:  88.75%, tr:  99.32%, tr_best:  99.66%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.005115/  0.560657, val:  88.75%, val_best:  88.75%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.002927/  0.553337, val:  89.58%, val_best:  89.58%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001720/  0.571720, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001230/  0.578826, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001154/  0.577622, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000869/  0.573300, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000771/  0.585810, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000699/  0.576284, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000630/  0.589262, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000601/  0.592010, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000559/  0.598223, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000524/  0.589991, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000491/  0.581665, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000465/  0.587633, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000448/  0.583953, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000424/  0.592860, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000401/  0.601159, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000383/  0.597530, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000377/  0.594501, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000358/  0.605778, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000345/  0.603125, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000327/  0.598943, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000320/  0.607294, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000311/  0.608121, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000299/  0.606639, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000288/  0.605758, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000278/  0.597642, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000270/  0.602392, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000266/  0.604703, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000258/  0.606441, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000253/  0.606223, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000248/  0.607460, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000239/  0.605299, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000232/  0.608534, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000229/  0.602382, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000223/  0.599983, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000218/  0.606486, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000215/  0.608798, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000212/  0.608538, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000205/  0.610573, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000204/  0.612358, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000198/  0.615875, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000194/  0.610868, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000190/  0.613705, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000187/  0.617222, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000182/  0.618169, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000185/  0.623947, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000176/  0.628703, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000172/  0.628241, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000171/  0.630439, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000167/  0.626262, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000166/  0.628198, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000161/  0.624809, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000159/  0.624819, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000157/  0.622147, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000153/  0.626005, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000152/  0.624928, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000147/  0.624302, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000146/  0.624295, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000142/  0.624435, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000142/  0.619070, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000138/  0.617594, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000136/  0.620262, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000136/  0.619234, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000132/  0.623694, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000131/  0.622780, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000129/  0.624223, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000127/  0.623179, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000126/  0.621937, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000123/  0.619523, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000122/  0.617034, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000120/  0.617570, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000119/  0.614704, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000119/  0.617924, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000117/  0.614631, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000116/  0.618753, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000116/  0.618850, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000113/  0.618639, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000111/  0.616248, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000111/  0.612686, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000110/  0.606712, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000108/  0.601429, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000106/  0.599054, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000105/  0.599432, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0e655073834f109f0e016b8dcdc227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▇▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.59943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-sweep-89</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0b196au5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0b196au5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_142804-0b196au5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nr0q46ce with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_144018-nr0q46ce</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr0q46ce' target=\"_blank\">dauntless-sweep-90</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr0q46ce' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr0q46ce</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.937756/  0.813246, val:  64.17%, val_best:  64.17%, tr:  59.51%, tr_best:  59.51%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.604349/  0.756226, val:  66.67%, val_best:  66.67%, tr:  73.04%, tr_best:  73.04%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.381829/  0.480738, val:  84.58%, val_best:  84.58%, tr:  84.76%, tr_best:  84.76%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.280212/  0.505069, val:  87.50%, val_best:  87.50%, tr:  89.61%, tr_best:  89.61%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.136583/  0.485057, val:  84.58%, val_best:  87.50%, tr:  95.33%, tr_best:  95.33%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.123431/  0.460068, val:  87.08%, val_best:  87.50%, tr:  95.54%, tr_best:  95.54%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.085928/  0.622408, val:  87.50%, val_best:  87.50%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.071865/  0.415367, val:  90.00%, val_best:  90.00%, tr:  97.93%, tr_best:  97.93%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.053337/  0.414044, val:  90.00%, val_best:  90.00%, tr:  98.49%, tr_best:  98.49%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.022885/  0.428588, val:  89.17%, val_best:  90.00%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.052629/  0.569018, val:  87.08%, val_best:  90.00%, tr:  98.26%, tr_best:  99.39%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.022401/  0.639776, val:  83.75%, val_best:  90.00%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.022681/  0.482846, val:  90.83%, val_best:  90.83%, tr:  99.35%, tr_best:  99.41%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.004641/  0.478649, val:  89.17%, val_best:  90.83%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.002375/  0.489021, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001662/  0.514604, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001300/  0.520146, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001095/  0.507004, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000983/  0.518947, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000846/  0.533512, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000764/  0.529286, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000702/  0.535361, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000658/  0.541591, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000599/  0.553161, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000561/  0.562380, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000537/  0.553626, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000500/  0.557461, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000467/  0.555658, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000445/  0.556212, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000429/  0.555734, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000413/  0.557695, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000396/  0.568284, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000375/  0.571909, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000363/  0.572512, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000349/  0.569447, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000331/  0.575575, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000327/  0.577624, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000321/  0.574349, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000308/  0.581374, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000296/  0.580406, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000286/  0.586095, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000278/  0.585821, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000267/  0.587048, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000261/  0.579426, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000251/  0.579604, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000244/  0.578412, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000237/  0.583254, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000231/  0.586104, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.588093, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000218/  0.582374, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000210/  0.583818, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000208/  0.579435, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000201/  0.583519, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000194/  0.582978, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000189/  0.582016, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000187/  0.587727, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000185/  0.588957, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000179/  0.587078, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000175/  0.600465, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000171/  0.605275, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000167/  0.606686, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000166/  0.610842, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000166/  0.611131, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000162/  0.605616, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000158/  0.602830, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000151/  0.604928, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000151/  0.602059, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000147/  0.604342, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000144/  0.598097, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000140/  0.597972, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000139/  0.598225, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000136/  0.599292, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000135/  0.598328, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000133/  0.601004, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000131/  0.599117, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000128/  0.600900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000126/  0.600223, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000125/  0.600312, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000123/  0.602308, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000123/  0.596979, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000120/  0.598618, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000119/  0.602765, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000116/  0.603654, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000114/  0.606989, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000113/  0.607332, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000111/  0.610777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000110/  0.611940, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000109/  0.615097, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000107/  0.616857, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000106/  0.619686, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000106/  0.622297, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000103/  0.624575, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000103/  0.625682, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000102/  0.628589, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000100/  0.623862, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000099/  0.624992, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000098/  0.624880, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000097/  0.628218, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000096/  0.630238, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000094/  0.630088, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dac7bb863e472ab56dc26cb7f04eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>0.63009</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-sweep-90</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr0q46ce' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nr0q46ce</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_144018-nr0q46ce/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5t88eppm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_145251-5t88eppm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5t88eppm' target=\"_blank\">fresh-sweep-91</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5t88eppm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5t88eppm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.952000/  1.231085, val:  50.00%, val_best:  50.00%, tr:  59.76%, tr_best:  59.76%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.639533/  0.897232, val:  62.92%, val_best:  62.92%, tr:  69.88%, tr_best:  69.88%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.486535/  0.709101, val:  72.08%, val_best:  72.08%, tr:  78.07%, tr_best:  78.07%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.347768/  0.518460, val:  86.25%, val_best:  86.25%, tr:  85.98%, tr_best:  85.98%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.223573/  0.536972, val:  83.33%, val_best:  86.25%, tr:  92.09%, tr_best:  92.09%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.168641/  0.403525, val:  88.33%, val_best:  88.33%, tr:  94.07%, tr_best:  94.07%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.141347/  0.604021, val:  85.42%, val_best:  88.33%, tr:  95.04%, tr_best:  95.04%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.092805/  0.500604, val:  85.83%, val_best:  88.33%, tr:  96.91%, tr_best:  96.91%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051965/  0.499128, val:  87.92%, val_best:  88.33%, tr:  98.40%, tr_best:  98.40%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.040949/  0.453119, val:  89.17%, val_best:  89.17%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.041244/  0.505729, val:  87.50%, val_best:  89.17%, tr:  98.76%, tr_best:  98.76%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.018430/  0.434031, val:  90.42%, val_best:  90.42%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.007792/  0.504335, val:  88.75%, val_best:  90.42%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006341/  0.583799, val:  88.75%, val_best:  90.42%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.036784/  0.550127, val:  86.25%, val_best:  90.42%, tr:  98.76%, tr_best:  99.89%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.013292/  0.565080, val:  87.50%, val_best:  90.42%, tr:  99.75%, tr_best:  99.89%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.004776/  0.500452, val:  88.33%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.002036/  0.494497, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001467/  0.512010, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001220/  0.535980, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001144/  0.526493, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000904/  0.513010, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000830/  0.536531, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000789/  0.527892, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000674/  0.550877, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000633/  0.542718, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000597/  0.539697, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000560/  0.545034, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000531/  0.557570, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000504/  0.548600, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000476/  0.554127, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000453/  0.571670, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000427/  0.570341, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000417/  0.560136, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000389/  0.570263, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000380/  0.577522, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000362/  0.592854, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000344/  0.591334, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000339/  0.599032, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000325/  0.592993, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000311/  0.605017, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000302/  0.610800, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000296/  0.606672, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000285/  0.606705, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000274/  0.607071, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000273/  0.601730, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000260/  0.605861, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000255/  0.610262, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000245/  0.612981, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000242/  0.613714, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000236/  0.610248, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000229/  0.611894, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000229/  0.610417, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000219/  0.607538, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.611668, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000210/  0.605743, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000205/  0.612771, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000196/  0.614337, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000194/  0.618518, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000187/  0.607529, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000186/  0.612210, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000182/  0.612613, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000176/  0.613569, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000174/  0.614897, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000170/  0.617899, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614508, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000166/  0.615614, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000164/  0.618730, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000161/  0.620691, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000158/  0.621241, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000155/  0.610036, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000152/  0.615297, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000150/  0.619101, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000150/  0.620152, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000148/  0.620037, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000146/  0.622376, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000142/  0.626638, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000141/  0.627804, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000141/  0.627893, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000140/  0.629202, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000135/  0.628153, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000134/  0.629958, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000132/  0.627862, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000128/  0.621927, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000130/  0.624140, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000126/  0.626055, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000124/  0.629551, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000123/  0.627544, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000122/  0.624273, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000121/  0.624468, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000118/  0.626473, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000118/  0.621595, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000115/  0.624036, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000115/  0.624101, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000114/  0.622380, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000111/  0.622888, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000110/  0.623640, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000109/  0.627101, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000109/  0.620720, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000106/  0.621448, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f640bd2c9440b1b7d78d946f448915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.62145</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-91</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5t88eppm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/5t88eppm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_145251-5t88eppm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1hwzbomq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cfd29c11654cd5bd1ab21ccd30a464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112782224598858, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_150457-1hwzbomq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1hwzbomq' target=\"_blank\">worthy-sweep-92</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1hwzbomq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1hwzbomq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.965213/  0.982956, val:  61.25%, val_best:  61.25%, tr:  58.52%, tr_best:  58.52%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.634353/  0.851469, val:  60.83%, val_best:  61.25%, tr:  70.18%, tr_best:  70.18%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.484476/  0.876349, val:  68.33%, val_best:  68.33%, tr:  77.59%, tr_best:  77.59%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.407166/  0.771534, val:  75.42%, val_best:  75.42%, tr:  82.01%, tr_best:  82.01%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.295740/  0.727161, val:  75.42%, val_best:  75.42%, tr:  87.80%, tr_best:  87.80%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.222004/  0.703895, val:  77.92%, val_best:  77.92%, tr:  91.32%, tr_best:  91.32%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.183451/  0.757637, val:  80.00%, val_best:  80.00%, tr:  93.08%, tr_best:  93.08%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.134116/  0.588351, val:  84.58%, val_best:  84.58%, tr:  95.04%, tr_best:  95.04%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.114549/  0.578167, val:  81.25%, val_best:  84.58%, tr:  95.99%, tr_best:  95.99%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.088238/  0.731127, val:  82.92%, val_best:  84.58%, tr:  96.84%, tr_best:  96.84%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.067073/  0.797514, val:  83.33%, val_best:  84.58%, tr:  97.66%, tr_best:  97.66%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.087675/  0.681632, val:  81.25%, val_best:  84.58%, tr:  97.41%, tr_best:  97.66%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.135388/  0.959785, val:  80.42%, val_best:  84.58%, tr:  95.13%, tr_best:  97.66%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.053462/  0.673809, val:  84.58%, val_best:  84.58%, tr:  98.29%, tr_best:  98.29%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.030154/  0.625854, val:  87.08%, val_best:  87.08%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.023307/  0.700090, val:  85.83%, val_best:  87.08%, tr:  99.19%, tr_best:  99.19%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.015944/  0.700490, val:  85.42%, val_best:  87.08%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.005656/  0.657265, val:  86.67%, val_best:  87.08%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.002844/  0.682463, val:  87.50%, val_best:  87.50%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001641/  0.699170, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001893/  0.684671, val:  87.08%, val_best:  87.50%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001204/  0.697734, val:  87.50%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000932/  0.705689, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000960/  0.730585, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000928/  0.749649, val:  86.25%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000645/  0.740823, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000604/  0.760179, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000555/  0.764770, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000518/  0.773795, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000478/  0.784415, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000450/  0.790145, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000446/  0.802830, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000411/  0.797034, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000395/  0.803825, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000383/  0.807036, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000362/  0.803672, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000353/  0.808211, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000332/  0.800878, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000334/  0.805913, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000309/  0.814929, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000294/  0.818515, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000286/  0.828992, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000278/  0.826027, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000267/  0.829192, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000260/  0.823501, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000253/  0.840107, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000248/  0.834746, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000243/  0.833350, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000231/  0.828519, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000226/  0.828586, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000222/  0.836546, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000215/  0.834151, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000206/  0.837218, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000205/  0.836805, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000198/  0.838008, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000195/  0.838094, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000187/  0.843093, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000184/  0.845214, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000180/  0.842872, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000176/  0.839661, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000173/  0.843241, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000170/  0.852883, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000171/  0.858508, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000164/  0.860773, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000164/  0.869013, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000157/  0.874235, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000156/  0.869767, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000152/  0.875525, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000151/  0.876547, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000145/  0.874244, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000145/  0.872054, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000141/  0.871534, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000141/  0.874056, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000136/  0.874656, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000131/  0.873098, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000131/  0.879277, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000130/  0.879590, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000127/  0.879208, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000126/  0.880393, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000123/  0.879448, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000122/  0.878924, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000120/  0.881129, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000118/  0.883720, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000117/  0.880876, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000115/  0.883899, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000116/  0.883654, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000113/  0.887773, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000113/  0.888774, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000110/  0.887084, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000109/  0.892222, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000109/  0.893278, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000106/  0.894564, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000104/  0.894575, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000103/  0.895536, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000102/  0.893947, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000101/  0.898218, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000099/  0.897113, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000099/  0.898901, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000097/  0.902968, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000095/  0.899851, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d6d957438e49849825486a14f9d0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁█▆▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▇▆▆▆▆▆▇▇▇▇▇▇▇▇█▇▇▇▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.875</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>0.89985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-sweep-92</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1hwzbomq' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1hwzbomq</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_150457-1hwzbomq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 388own7g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_151658-388own7g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/388own7g' target=\"_blank\">stellar-sweep-93</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/388own7g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/388own7g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.924084/  0.900167, val:  62.50%, val_best:  62.50%, tr:  60.17%, tr_best:  60.17%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.608945/  0.868340, val:  60.00%, val_best:  62.50%, tr:  72.86%, tr_best:  72.86%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411019/  0.862224, val:  72.08%, val_best:  72.08%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.290071/  0.516031, val:  84.17%, val_best:  84.17%, tr:  89.34%, tr_best:  89.34%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.148748/  0.462285, val:  87.92%, val_best:  87.92%, tr:  95.38%, tr_best:  95.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.110478/  0.453902, val:  89.17%, val_best:  89.17%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.087798/  0.488559, val:  88.75%, val_best:  89.17%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.056540/  0.460554, val:  87.92%, val_best:  89.17%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.032420/  0.453790, val:  90.00%, val_best:  90.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.044882/  0.377394, val:  90.00%, val_best:  90.00%, tr:  98.67%, tr_best:  99.12%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.031081/  0.458891, val:  90.00%, val_best:  90.00%, tr:  99.01%, tr_best:  99.12%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.012163/  0.410768, val:  90.83%, val_best:  90.83%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003469/  0.424667, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002129/  0.401361, val:  91.25%, val_best:  91.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001459/  0.412334, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001118/  0.409213, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000979/  0.411735, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000870/  0.414670, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000784/  0.411883, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000699/  0.427088, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000645/  0.428068, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000597/  0.433039, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000562/  0.453180, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000520/  0.458498, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000493/  0.451457, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000468/  0.455886, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000441/  0.445366, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000417/  0.445858, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000400/  0.447511, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000372/  0.454448, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000364/  0.447946, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000352/  0.454611, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000334/  0.458291, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000324/  0.462057, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000310/  0.460060, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000302/  0.459708, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000290/  0.464959, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000283/  0.463789, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000270/  0.462140, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000260/  0.459259, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000252/  0.460512, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000247/  0.460966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000241/  0.461501, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000232/  0.467219, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000227/  0.465373, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000218/  0.464732, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000214/  0.462301, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000206/  0.463863, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000203/  0.462686, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000201/  0.461293, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000193/  0.463994, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000188/  0.463246, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000183/  0.461719, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000177/  0.464404, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000174/  0.462007, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000172/  0.465646, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000167/  0.467702, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000163/  0.466615, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000162/  0.465064, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000155/  0.466431, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000156/  0.470764, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000151/  0.474162, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000149/  0.475791, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000144/  0.477446, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000143/  0.473104, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000139/  0.477203, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000137/  0.482258, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000136/  0.479527, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000132/  0.478409, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000131/  0.478923, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.479966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.481726, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000125/  0.477696, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000124/  0.479414, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.479214, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000121/  0.478879, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.476968, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.477993, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.480774, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000115/  0.478548, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000113/  0.480919, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000112/  0.478824, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.480684, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000109/  0.478337, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000106/  0.473828, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000105/  0.479483, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000105/  0.480563, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000103/  0.480323, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000103/  0.481059, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000102/  0.479574, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.478145, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000098/  0.482602, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000097/  0.483561, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.486765, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.482070, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000093/  0.486749, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000091/  0.483553, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000091/  0.492586, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.491570, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.493886, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1229188358d541498971a44bf361738c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.49389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-93</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/388own7g' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/388own7g</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_151658-388own7g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jjmhwkrg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0358778f066a4fa4beab419879bef234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112650313104192, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_152909-jjmhwkrg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jjmhwkrg' target=\"_blank\">rosy-sweep-94</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jjmhwkrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jjmhwkrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.935822/  0.884066, val:  58.75%, val_best:  58.75%, tr:  60.30%, tr_best:  60.30%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.634335/  0.928297, val:  59.58%, val_best:  59.58%, tr:  70.99%, tr_best:  70.99%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.454417/  0.893505, val:  70.83%, val_best:  70.83%, tr:  79.87%, tr_best:  79.87%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.334512/  0.619390, val:  81.25%, val_best:  81.25%, tr:  86.90%, tr_best:  86.90%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.201674/  0.547677, val:  83.75%, val_best:  83.75%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.161253/  0.629578, val:  82.92%, val_best:  83.75%, tr:  94.57%, tr_best:  94.57%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.109431/  0.587903, val:  85.42%, val_best:  85.42%, tr:  95.87%, tr_best:  95.87%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.078868/  0.506035, val:  87.08%, val_best:  87.08%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.063650/  0.511400, val:  87.92%, val_best:  87.92%, tr:  98.08%, tr_best:  98.08%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.040023/  0.448062, val:  87.92%, val_best:  87.92%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.025498/  0.537524, val:  88.75%, val_best:  88.75%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.019807/  0.510825, val:  90.00%, val_best:  90.00%, tr:  99.41%, tr_best:  99.41%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.007747/  0.542261, val:  88.75%, val_best:  90.00%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.004113/  0.575099, val:  87.50%, val_best:  90.00%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.002798/  0.536892, val:  89.17%, val_best:  90.00%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001746/  0.545750, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001453/  0.545179, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001223/  0.537655, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001056/  0.543303, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000957/  0.563960, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000847/  0.549770, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000778/  0.534907, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000725/  0.558076, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000661/  0.553829, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000617/  0.577428, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000578/  0.568890, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000555/  0.577025, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000518/  0.581257, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000501/  0.573303, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000480/  0.573048, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000448/  0.571081, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000430/  0.574907, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000415/  0.582571, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000401/  0.578870, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000390/  0.580617, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000372/  0.574361, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000360/  0.584179, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000347/  0.584902, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000337/  0.592033, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000326/  0.594923, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000310/  0.592841, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000302/  0.591019, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000291/  0.584744, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000285/  0.585339, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000281/  0.586709, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000268/  0.584832, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000266/  0.595049, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000255/  0.601554, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000252/  0.606897, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000247/  0.600995, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000246/  0.603389, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000241/  0.601912, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000229/  0.602281, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000229/  0.605566, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000223/  0.602500, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000218/  0.614330, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000213/  0.609339, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000207/  0.610416, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000205/  0.605018, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000201/  0.610228, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000194/  0.609841, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000193/  0.609444, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000194/  0.615997, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000188/  0.616800, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000182/  0.613660, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000176/  0.613086, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000172/  0.616553, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000170/  0.618082, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000165/  0.613978, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000164/  0.608172, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000160/  0.607417, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000159/  0.609330, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000155/  0.613224, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000151/  0.613764, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000148/  0.609596, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000149/  0.608923, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617440, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000144/  0.616822, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000140/  0.619747, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000139/  0.616547, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000136/  0.617616, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000135/  0.621177, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000134/  0.619608, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000131/  0.619912, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000129/  0.624936, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000128/  0.622352, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000126/  0.624219, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000123/  0.626572, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000122/  0.625995, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000120/  0.629396, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000119/  0.630367, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000117/  0.633020, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000116/  0.630198, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000114/  0.627201, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000112/  0.631633, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000111/  0.630953, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000110/  0.628141, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000111/  0.626027, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000108/  0.630388, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000107/  0.629969, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d18baccf60047608332e840f06b44fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.62997</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-94</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jjmhwkrg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jjmhwkrg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_152909-jjmhwkrg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v3gcb799 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791453aacdbf422ba1daaa19bc7d686d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113381856638524, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_154146-v3gcb799</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v3gcb799' target=\"_blank\">sweepy-sweep-95</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v3gcb799' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v3gcb799</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.928935/  0.881833, val:  59.58%, val_best:  59.58%, tr:  60.01%, tr_best:  60.01%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.622879/  0.854058, val:  62.92%, val_best:  62.92%, tr:  71.73%, tr_best:  71.73%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.415023/  0.815604, val:  70.00%, val_best:  70.00%, tr:  82.57%, tr_best:  82.57%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.301157/  0.537661, val:  85.42%, val_best:  85.42%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.169007/  0.494498, val:  85.42%, val_best:  85.42%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.108344/  0.441543, val:  89.17%, val_best:  89.17%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.078059/  0.622470, val:  88.33%, val_best:  89.17%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.058447/  0.424832, val:  88.75%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051343/  0.443390, val:  90.42%, val_best:  90.42%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019991/  0.473071, val:  88.75%, val_best:  90.42%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024913/  0.594811, val:  87.92%, val_best:  90.42%, tr:  99.23%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.011130/  0.510964, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003125/  0.552201, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001943/  0.502089, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001624/  0.521189, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001236/  0.545577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001065/  0.524273, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000955/  0.556683, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000850/  0.553951, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000777/  0.562979, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000711/  0.561448, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000666/  0.572040, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000628/  0.581079, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000576/  0.569785, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000538/  0.568354, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000500/  0.574372, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000475/  0.581407, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000450/  0.584895, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000429/  0.586062, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000411/  0.596621, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.597417, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000379/  0.608332, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000358/  0.602874, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000340/  0.604670, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000329/  0.601791, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.601425, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000308/  0.602356, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000292/  0.598435, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.597901, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000277/  0.604077, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.613960, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000256/  0.610192, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000249/  0.611147, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.614404, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000235/  0.615403, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.614356, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.623049, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000215/  0.624923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000211/  0.613693, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.619318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.622931, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000195/  0.624674, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.627025, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.628896, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.624894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.621996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000175/  0.622287, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.615900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617114, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.613777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000159/  0.619844, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.615970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000157/  0.617458, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.619992, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.616505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617499, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.623439, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.627106, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000138/  0.627911, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.630018, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000130/  0.633134, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.632592, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633326, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635861, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000123/  0.634962, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631111, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.628672, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629021, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.628650, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629489, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.628790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.628014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000109/  0.629268, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.627406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000109/  0.628916, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000104/  0.631419, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.634245, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.634045, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000103/  0.632370, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.629538, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.630059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.629965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.636612, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635149, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.633406, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000092/  0.634207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.632478, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed844ac1d52047b990360b2b6114ce45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.63248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweepy-sweep-95</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v3gcb799' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/v3gcb799</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_154146-v3gcb799/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cigu23ij with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d1518c14da4d6da45216312e3d868c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113651619396276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_155253-cigu23ij</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cigu23ij' target=\"_blank\">light-sweep-96</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cigu23ij' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cigu23ij</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.125, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.125, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.125, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.125, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.952000/  1.231085, val:  50.00%, val_best:  50.00%, tr:  59.76%, tr_best:  59.76%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.639533/  0.897232, val:  62.92%, val_best:  62.92%, tr:  69.88%, tr_best:  69.88%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.486535/  0.709101, val:  72.08%, val_best:  72.08%, tr:  78.07%, tr_best:  78.07%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.347768/  0.518460, val:  86.25%, val_best:  86.25%, tr:  85.98%, tr_best:  85.98%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.223573/  0.536972, val:  83.33%, val_best:  86.25%, tr:  92.09%, tr_best:  92.09%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.168641/  0.403525, val:  88.33%, val_best:  88.33%, tr:  94.07%, tr_best:  94.07%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.141347/  0.604021, val:  85.42%, val_best:  88.33%, tr:  95.04%, tr_best:  95.04%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.092805/  0.500604, val:  85.83%, val_best:  88.33%, tr:  96.91%, tr_best:  96.91%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051965/  0.499128, val:  87.92%, val_best:  88.33%, tr:  98.40%, tr_best:  98.40%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.040949/  0.453119, val:  89.17%, val_best:  89.17%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.041244/  0.505729, val:  87.50%, val_best:  89.17%, tr:  98.76%, tr_best:  98.76%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.018430/  0.434031, val:  90.42%, val_best:  90.42%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.007792/  0.504335, val:  88.75%, val_best:  90.42%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006341/  0.583799, val:  88.75%, val_best:  90.42%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.036784/  0.550127, val:  86.25%, val_best:  90.42%, tr:  98.76%, tr_best:  99.89%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.013292/  0.565080, val:  87.50%, val_best:  90.42%, tr:  99.75%, tr_best:  99.89%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.004776/  0.500452, val:  88.33%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.002036/  0.494497, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001467/  0.512010, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001220/  0.535980, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001144/  0.526493, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000904/  0.513010, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000830/  0.536531, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000789/  0.527892, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000674/  0.550877, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000633/  0.542718, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000597/  0.539697, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000560/  0.545034, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000531/  0.557570, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000504/  0.548600, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000476/  0.554127, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000453/  0.571670, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000427/  0.570341, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000417/  0.560136, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000389/  0.570263, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000380/  0.577522, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000362/  0.592854, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000344/  0.591334, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000339/  0.599032, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000325/  0.592993, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000311/  0.605017, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000302/  0.610800, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000296/  0.606672, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000285/  0.606705, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000274/  0.607071, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000273/  0.601730, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000260/  0.605861, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000255/  0.610262, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000245/  0.612981, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000242/  0.613714, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000236/  0.610248, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000229/  0.611894, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000229/  0.610417, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000219/  0.607538, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.611668, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000210/  0.605743, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000205/  0.612771, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000196/  0.614337, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000194/  0.618518, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000187/  0.607529, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000186/  0.612210, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000182/  0.612613, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000176/  0.613569, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000174/  0.614897, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000170/  0.617899, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000167/  0.614508, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000166/  0.615614, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000164/  0.618730, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000161/  0.620691, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000158/  0.621241, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000155/  0.610036, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000152/  0.615297, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000150/  0.619101, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000150/  0.620152, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000148/  0.620037, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000146/  0.622376, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000142/  0.626638, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000141/  0.627804, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000141/  0.627893, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000140/  0.629202, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000135/  0.628153, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000134/  0.629958, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000132/  0.627862, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000128/  0.621927, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000130/  0.624140, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000126/  0.626055, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000124/  0.629551, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000123/  0.627544, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000122/  0.624273, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000121/  0.624468, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000118/  0.626473, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000118/  0.621595, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000115/  0.624036, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000115/  0.624101, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000114/  0.622380, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000111/  0.622888, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000110/  0.623640, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000109/  0.627101, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000109/  0.620720, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000106/  0.621448, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177beb0aca0b4215a1bf3610489168ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▅▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>0.62145</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-sweep-96</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cigu23ij' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cigu23ij</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_155253-cigu23ij/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w106dcjo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_160441-w106dcjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w106dcjo' target=\"_blank\">sleek-sweep-97</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w106dcjo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w106dcjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.25, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.942593/  0.903914, val:  60.83%, val_best:  60.83%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.642465/  0.798651, val:  62.92%, val_best:  62.92%, tr:  70.40%, tr_best:  70.40%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.477290/  0.861618, val:  67.92%, val_best:  67.92%, tr:  78.13%, tr_best:  78.13%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.413692/  0.644274, val:  80.42%, val_best:  80.42%, tr:  83.03%, tr_best:  83.03%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.256515/  0.636068, val:  77.50%, val_best:  80.42%, tr:  90.92%, tr_best:  90.92%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.209704/  0.579852, val:  83.33%, val_best:  83.33%, tr:  92.63%, tr_best:  92.63%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.133868/  0.593811, val:  84.58%, val_best:  84.58%, tr:  95.54%, tr_best:  95.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.123163/  0.505620, val:  85.00%, val_best:  85.00%, tr:  95.60%, tr_best:  95.60%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.089735/  0.646998, val:  82.50%, val_best:  85.00%, tr:  97.16%, tr_best:  97.16%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.079484/  0.484891, val:  87.08%, val_best:  87.08%, tr:  97.25%, tr_best:  97.25%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.049500/  0.508999, val:  88.75%, val_best:  88.75%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.052914/  0.584016, val:  90.00%, val_best:  90.00%, tr:  98.06%, tr_best:  98.53%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.036708/  0.558366, val:  88.33%, val_best:  90.00%, tr:  99.03%, tr_best:  99.03%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.024783/  0.449408, val:  89.17%, val_best:  90.00%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.017506/  0.514594, val:  90.42%, val_best:  90.42%, tr:  99.55%, tr_best:  99.55%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.018535/  0.578328, val:  87.08%, val_best:  90.42%, tr:  99.48%, tr_best:  99.55%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.028552/  0.466015, val:  88.33%, val_best:  90.42%, tr:  99.08%, tr_best:  99.55%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.010837/  0.526453, val:  86.25%, val_best:  90.42%, tr:  99.77%, tr_best:  99.77%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.002893/  0.502844, val:  89.17%, val_best:  90.42%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.001605/  0.543820, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.001220/  0.526082, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.001033/  0.503936, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000911/  0.510902, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000827/  0.513808, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000721/  0.509712, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000669/  0.519355, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000617/  0.527798, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000574/  0.531724, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000538/  0.527529, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000513/  0.528394, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000485/  0.536235, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000463/  0.539451, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000434/  0.539548, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000421/  0.539682, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000403/  0.541043, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000382/  0.544384, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000367/  0.544030, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000357/  0.542826, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000345/  0.543629, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000329/  0.540483, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000317/  0.544864, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000308/  0.546379, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000297/  0.544604, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000287/  0.549397, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000278/  0.547389, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000267/  0.539323, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000263/  0.549534, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000256/  0.546456, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000250/  0.541858, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000240/  0.539060, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000235/  0.537314, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000231/  0.536353, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000229/  0.537821, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000221/  0.532671, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.531339, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000214/  0.534797, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000208/  0.533404, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000205/  0.531805, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000202/  0.535660, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000197/  0.538491, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000194/  0.536451, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000187/  0.539274, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000189/  0.543742, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000181/  0.543535, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000177/  0.542593, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000173/  0.539531, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000171/  0.539073, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000166/  0.542758, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000162/  0.545818, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000161/  0.538167, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000157/  0.535568, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000156/  0.539287, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000153/  0.527536, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000150/  0.539376, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000148/  0.538311, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000146/  0.538875, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000143/  0.536463, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000142/  0.539625, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000141/  0.542708, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000141/  0.546230, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000136/  0.545079, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000134/  0.542630, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000132/  0.541944, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000129/  0.541629, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000128/  0.541077, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000127/  0.535782, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000124/  0.535124, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000122/  0.533963, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000122/  0.536804, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000120/  0.537249, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000119/  0.539175, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000117/  0.544608, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000117/  0.545868, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000114/  0.548575, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000115/  0.544528, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000112/  0.543917, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000112/  0.549869, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000110/  0.550867, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000109/  0.551440, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000108/  0.548797, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281d74116eb14e6da13e3ae69494f0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▄█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▅▅▆▆▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.5488</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-sweep-97</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w106dcjo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w106dcjo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_160441-w106dcjo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4zy8adr3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_161651-4zy8adr3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4zy8adr3' target=\"_blank\">atomic-sweep-98</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4zy8adr3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4zy8adr3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.940631/  0.971694, val:  60.00%, val_best:  60.00%, tr:  60.14%, tr_best:  60.14%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.630883/  0.897482, val:  62.92%, val_best:  62.92%, tr:  71.03%, tr_best:  71.03%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.446882/  0.741105, val:  74.17%, val_best:  74.17%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.291870/  0.508247, val:  86.67%, val_best:  86.67%, tr:  89.27%, tr_best:  89.27%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.166100/  0.532877, val:  87.08%, val_best:  87.08%, tr:  94.32%, tr_best:  94.32%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.097045/  0.471449, val:  87.92%, val_best:  87.92%, tr:  97.05%, tr_best:  97.05%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.079279/  0.542684, val:  85.42%, val_best:  87.92%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.053816/  0.385868, val:  89.17%, val_best:  89.17%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.029766/  0.461558, val:  89.17%, val_best:  89.17%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.043129/  0.407184, val:  90.00%, val_best:  90.00%, tr:  98.56%, tr_best:  99.26%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024331/  0.485701, val:  89.17%, val_best:  90.00%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.009858/  0.397074, val:  90.83%, val_best:  90.83%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.005635/  0.472394, val:  91.67%, val_best:  91.67%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.003003/  0.427595, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001792/  0.446107, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001347/  0.457066, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001128/  0.463881, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001065/  0.469466, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000992/  0.469521, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000860/  0.476817, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000816/  0.467585, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000712/  0.471026, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000669/  0.485179, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000636/  0.480550, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000572/  0.488351, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000537/  0.498910, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000507/  0.499225, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000470/  0.504784, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000457/  0.497928, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000424/  0.497091, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000418/  0.509973, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000394/  0.521349, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000378/  0.521303, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000358/  0.526090, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000344/  0.516998, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000339/  0.518939, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000323/  0.523821, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000310/  0.523994, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000301/  0.531876, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000288/  0.535207, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000284/  0.532543, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000273/  0.537515, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000264/  0.528881, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000258/  0.528008, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000254/  0.533494, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000244/  0.530134, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000239/  0.544103, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000229/  0.536179, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000226/  0.536386, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000219/  0.534645, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000213/  0.541545, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000210/  0.541155, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000208/  0.543737, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000200/  0.538721, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000198/  0.543003, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000196/  0.545501, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000192/  0.545955, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000186/  0.549556, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000182/  0.548140, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000179/  0.547507, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000177/  0.547195, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000172/  0.545982, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000173/  0.546602, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000166/  0.553394, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000162/  0.549833, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000161/  0.554355, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000160/  0.558481, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000153/  0.563451, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000149/  0.561897, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000148/  0.561056, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000149/  0.566028, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000142/  0.568494, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000139/  0.565895, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000137/  0.572022, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000135/  0.575145, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000133/  0.569638, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000132/  0.563469, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000132/  0.555015, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000127/  0.554992, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000125/  0.552628, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000123/  0.552222, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000121/  0.554661, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000121/  0.551329, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000117/  0.560474, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000117/  0.558354, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000114/  0.558434, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000115/  0.555474, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000112/  0.556597, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000111/  0.557073, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000109/  0.559865, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000107/  0.561674, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000105/  0.562137, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000105/  0.563710, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000105/  0.562709, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000103/  0.568955, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000102/  0.569531, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000100/  0.564454, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000101/  0.562444, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000097/  0.568291, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000097/  0.567012, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4640bd29324500bdea730eb5b23288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▁██████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0001</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.56701</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-98</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4zy8adr3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/4zy8adr3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_161651-4zy8adr3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ke9kdze0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_162923-ke9kdze0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ke9kdze0' target=\"_blank\">vital-sweep-99</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ke9kdze0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ke9kdze0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.928935/  0.881833, val:  59.58%, val_best:  59.58%, tr:  60.01%, tr_best:  60.01%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.622879/  0.854058, val:  62.92%, val_best:  62.92%, tr:  71.73%, tr_best:  71.73%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.415023/  0.815604, val:  70.00%, val_best:  70.00%, tr:  82.57%, tr_best:  82.57%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.301157/  0.537661, val:  85.42%, val_best:  85.42%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.169007/  0.494498, val:  85.42%, val_best:  85.42%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.108344/  0.441543, val:  89.17%, val_best:  89.17%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.078059/  0.622470, val:  88.33%, val_best:  89.17%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.058447/  0.424832, val:  88.75%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051343/  0.443390, val:  90.42%, val_best:  90.42%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019991/  0.473071, val:  88.75%, val_best:  90.42%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024913/  0.594811, val:  87.92%, val_best:  90.42%, tr:  99.23%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.011130/  0.510964, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003125/  0.552201, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001943/  0.502089, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001624/  0.521189, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001236/  0.545577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001065/  0.524273, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000955/  0.556683, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000850/  0.553951, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000777/  0.562979, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000711/  0.561448, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000666/  0.572040, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000628/  0.581079, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000576/  0.569785, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000538/  0.568354, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000500/  0.574372, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000475/  0.581407, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000450/  0.584895, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000429/  0.586062, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000411/  0.596621, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.597417, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000379/  0.608332, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000358/  0.602874, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000340/  0.604670, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000329/  0.601791, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.601425, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000308/  0.602356, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000292/  0.598435, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.597901, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000277/  0.604077, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.613960, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000256/  0.610192, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000249/  0.611147, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.614404, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000235/  0.615403, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.614356, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.623049, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000215/  0.624923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000211/  0.613693, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.619318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.622931, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000195/  0.624674, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.627025, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.628896, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.624894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.621996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000175/  0.622287, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.615900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617114, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.613777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000159/  0.619844, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.615970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000157/  0.617458, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.619992, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.616505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617499, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.623439, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.627106, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000138/  0.627911, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.630018, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000130/  0.633134, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.632592, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633326, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635861, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000123/  0.634962, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631111, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.628672, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629021, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.628650, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629489, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.628790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.628014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000109/  0.629268, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.627406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000109/  0.628916, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000104/  0.631419, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.634245, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.634045, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000103/  0.632370, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.629538, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.630059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.629965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.636612, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635149, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.633406, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000092/  0.634207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.632478, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab5a5412b234886ad7f01336e1cef52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.63248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-sweep-99</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ke9kdze0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ke9kdze0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_162923-ke9kdze0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ded09xrv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_164100-ded09xrv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ded09xrv' target=\"_blank\">winter-sweep-100</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ded09xrv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ded09xrv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.102526/  0.891840, val:  61.67%, val_best:  61.67%, tr:  57.15%, tr_best:  57.15%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  0.676047/  0.846906, val:  64.58%, val_best:  64.58%, tr:  68.67%, tr_best:  68.67%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  0.568037/  0.712990, val:  69.58%, val_best:  69.58%, tr:  72.84%, tr_best:  72.84%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.515018/  0.728002, val:  69.58%, val_best:  69.58%, tr:  76.22%, tr_best:  76.22%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.462545/  0.749197, val:  68.75%, val_best:  69.58%, tr:  79.04%, tr_best:  79.04%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.424999/  0.712217, val:  74.58%, val_best:  74.58%, tr:  81.67%, tr_best:  81.67%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.397102/  0.685110, val:  77.50%, val_best:  77.50%, tr:  83.05%, tr_best:  83.05%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.334159/  0.636645, val:  80.00%, val_best:  80.00%, tr:  87.17%, tr_best:  87.17%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.314558/  0.629055, val:  78.75%, val_best:  80.00%, tr:  88.12%, tr_best:  88.12%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.270455/  0.637701, val:  80.83%, val_best:  80.83%, tr:  90.17%, tr_best:  90.17%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.235295/  0.590132, val:  84.17%, val_best:  84.17%, tr:  91.79%, tr_best:  91.79%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.202291/  0.630345, val:  83.75%, val_best:  84.17%, tr:  93.30%, tr_best:  93.30%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.192550/  0.608326, val:  81.67%, val_best:  84.17%, tr:  93.24%, tr_best:  93.30%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.174239/  0.523714, val:  86.67%, val_best:  86.67%, tr:  94.09%, tr_best:  94.09%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.145557/  0.553010, val:  84.17%, val_best:  86.67%, tr:  95.29%, tr_best:  95.29%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.115685/  0.567068, val:  83.75%, val_best:  86.67%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.100401/  0.507739, val:  87.08%, val_best:  87.08%, tr:  97.02%, tr_best:  97.02%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.092628/  0.566173, val:  82.92%, val_best:  87.08%, tr:  97.20%, tr_best:  97.20%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.082998/  0.550493, val:  86.67%, val_best:  87.08%, tr:  97.86%, tr_best:  97.86%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.066391/  0.631953, val:  84.58%, val_best:  87.08%, tr:  98.08%, tr_best:  98.08%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.070879/  0.600485, val:  86.67%, val_best:  87.08%, tr:  97.84%, tr_best:  98.08%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.054729/  0.600564, val:  85.42%, val_best:  87.08%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.051369/  0.625691, val:  88.33%, val_best:  88.33%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.049572/  0.588567, val:  84.58%, val_best:  88.33%, tr:  98.72%, tr_best:  98.72%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.037605/  0.608356, val:  86.67%, val_best:  88.33%, tr:  99.26%, tr_best:  99.26%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.033779/  0.644545, val:  87.08%, val_best:  88.33%, tr:  99.23%, tr_best:  99.26%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.032234/  0.609753, val:  87.50%, val_best:  88.33%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.028216/  0.578241, val:  88.75%, val_best:  88.75%, tr:  99.46%, tr_best:  99.46%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.023328/  0.585950, val:  88.75%, val_best:  88.75%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.020146/  0.589511, val:  89.17%, val_best:  89.17%, tr:  99.84%, tr_best:  99.84%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.018471/  0.627691, val:  87.50%, val_best:  89.17%, tr:  99.71%, tr_best:  99.84%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.016313/  0.620900, val:  87.50%, val_best:  89.17%, tr:  99.80%, tr_best:  99.84%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.018254/  0.622042, val:  88.33%, val_best:  89.17%, tr:  99.71%, tr_best:  99.84%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.014271/  0.629748, val:  87.92%, val_best:  89.17%, tr:  99.91%, tr_best:  99.91%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.014887/  0.644056, val:  87.92%, val_best:  89.17%, tr:  99.77%, tr_best:  99.91%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.012031/  0.631438, val:  88.33%, val_best:  89.17%, tr:  99.93%, tr_best:  99.93%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.009633/  0.699198, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.009175/  0.656648, val:  87.92%, val_best:  89.17%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.008175/  0.675564, val:  87.92%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.008095/  0.687501, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.008041/  0.696628, val:  87.50%, val_best:  89.17%, tr:  99.93%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.007259/  0.693252, val:  87.50%, val_best:  89.17%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.006547/  0.683268, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.005791/  0.712254, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.005570/  0.697888, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.005169/  0.686566, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.005343/  0.687398, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.005195/  0.683452, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.004770/  0.703171, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.004540/  0.687084, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.004946/  0.717088, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.005200/  0.711760, val:  87.50%, val_best:  89.17%, tr:  99.95%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.004182/  0.692423, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.004031/  0.694268, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.003933/  0.724399, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.003935/  0.725704, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.003582/  0.738333, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.003472/  0.740016, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.003424/  0.744965, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.003358/  0.721947, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.003226/  0.749263, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.002988/  0.724674, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.002930/  0.721917, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.002892/  0.727867, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.002814/  0.705299, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.002856/  0.740974, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.002799/  0.732663, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.002641/  0.722606, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.002578/  0.742549, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.002478/  0.732871, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.002322/  0.732474, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.002358/  0.736308, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.002285/  0.763253, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.002211/  0.732512, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.002237/  0.736706, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.002126/  0.735424, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.002051/  0.746579, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.002077/  0.751898, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.002077/  0.728295, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.001928/  0.738721, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.001872/  0.734549, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.001894/  0.737077, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.001866/  0.731036, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001913/  0.725525, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.001789/  0.722066, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.001711/  0.732732, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.001702/  0.733649, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001745/  0.728755, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001637/  0.732074, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001632/  0.730334, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001597/  0.733083, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001565/  0.739654, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.001562/  0.753970, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.001568/  0.735210, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001496/  0.735306, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001469/  0.747610, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.001455/  0.733239, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.001429/  0.753356, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.001408/  0.755688, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.001379/  0.747622, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ed855a66a5402daf5d439f089155e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▆▁▃█▆█▆▆██████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▆▇▇███████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▆▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆▆▇▇███████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▆▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▇▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00138</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>0.74762</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-100</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ded09xrv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ded09xrv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_164100-ded09xrv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ujw0qghm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_165225-ujw0qghm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ujw0qghm' target=\"_blank\">vocal-sweep-101</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ujw0qghm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ujw0qghm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.935111/  0.930279, val:  60.00%, val_best:  60.00%, tr:  60.10%, tr_best:  60.10%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.645711/  0.861115, val:  64.17%, val_best:  64.17%, tr:  70.06%, tr_best:  70.06%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.464411/  0.776048, val:  69.17%, val_best:  69.17%, tr:  79.46%, tr_best:  79.46%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.317624/  0.596377, val:  87.08%, val_best:  87.08%, tr:  87.51%, tr_best:  87.51%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.193335/  0.598724, val:  81.67%, val_best:  87.08%, tr:  92.72%, tr_best:  92.72%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.149819/  0.455364, val:  88.33%, val_best:  88.33%, tr:  95.00%, tr_best:  95.00%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.088756/  0.554473, val:  86.67%, val_best:  88.33%, tr:  97.00%, tr_best:  97.00%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.075862/  0.510597, val:  89.17%, val_best:  89.17%, tr:  97.57%, tr_best:  97.57%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.049052/  0.497094, val:  88.33%, val_best:  89.17%, tr:  98.53%, tr_best:  98.53%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.031018/  0.415403, val:  89.58%, val_best:  89.58%, tr:  99.10%, tr_best:  99.10%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.029693/  0.542248, val:  86.67%, val_best:  89.58%, tr:  98.96%, tr_best:  99.10%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.010855/  0.526078, val:  88.33%, val_best:  89.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.008798/  0.547291, val:  87.50%, val_best:  89.58%, tr:  99.77%, tr_best:  99.80%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.015032/  0.560051, val:  88.33%, val_best:  89.58%, tr:  99.64%, tr_best:  99.80%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.005708/  0.569632, val:  89.17%, val_best:  89.58%, tr:  99.98%, tr_best:  99.98%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002184/  0.554556, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001564/  0.569374, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001318/  0.565142, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001147/  0.569481, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000997/  0.579033, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000912/  0.574463, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000790/  0.574381, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000757/  0.581305, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000706/  0.581143, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000645/  0.587124, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000605/  0.589668, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000579/  0.599829, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000535/  0.593670, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000503/  0.602743, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000494/  0.604577, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000449/  0.597533, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000450/  0.594905, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000420/  0.601580, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000406/  0.607032, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000387/  0.603703, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000381/  0.612077, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000358/  0.613359, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000350/  0.614901, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000338/  0.609380, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000331/  0.605119, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000315/  0.605948, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000306/  0.607426, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000292/  0.604346, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000282/  0.608507, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000278/  0.615299, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000268/  0.611451, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000262/  0.617933, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000255/  0.621305, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000249/  0.622792, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000244/  0.623876, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000243/  0.625618, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000231/  0.624153, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000230/  0.629497, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000221/  0.634032, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000217/  0.631043, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000215/  0.625982, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000212/  0.626839, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000207/  0.630988, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000205/  0.633899, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000203/  0.632317, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000195/  0.636105, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000192/  0.634124, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000189/  0.636216, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000186/  0.631300, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000181/  0.634030, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000178/  0.631975, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000177/  0.631937, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000174/  0.632074, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000171/  0.627129, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000169/  0.628457, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000164/  0.625540, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000162/  0.629360, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000161/  0.627331, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000157/  0.625633, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000157/  0.624692, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000151/  0.627005, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000150/  0.625528, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000147/  0.623157, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000143/  0.622877, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000142/  0.625825, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000140/  0.628313, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000136/  0.623228, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000135/  0.621479, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000134/  0.626184, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000130/  0.624385, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000131/  0.625518, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000129/  0.626592, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000126/  0.630556, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000126/  0.633178, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000125/  0.636248, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000123/  0.639624, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000121/  0.644317, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000119/  0.646648, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000116/  0.643217, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000114/  0.644149, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000114/  0.641993, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644623, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000111/  0.644130, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000110/  0.644194, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000109/  0.649310, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5040ba8950f4482a9de47bf5117a263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▆▅▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00011</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.64931</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-101</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ujw0qghm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ujw0qghm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_165225-ujw0qghm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vypcetsn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_170354-vypcetsn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vypcetsn' target=\"_blank\">scarlet-sweep-102</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vypcetsn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vypcetsn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.928935/  0.881833, val:  59.58%, val_best:  59.58%, tr:  60.01%, tr_best:  60.01%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.622879/  0.854058, val:  62.92%, val_best:  62.92%, tr:  71.73%, tr_best:  71.73%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.415023/  0.815604, val:  70.00%, val_best:  70.00%, tr:  82.57%, tr_best:  82.57%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.301157/  0.537661, val:  85.42%, val_best:  85.42%, tr:  88.71%, tr_best:  88.71%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.169007/  0.494498, val:  85.42%, val_best:  85.42%, tr:  94.05%, tr_best:  94.05%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.108344/  0.441543, val:  89.17%, val_best:  89.17%, tr:  96.39%, tr_best:  96.39%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.078059/  0.622470, val:  88.33%, val_best:  89.17%, tr:  97.54%, tr_best:  97.54%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.058447/  0.424832, val:  88.75%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.051343/  0.443390, val:  90.42%, val_best:  90.42%, tr:  98.35%, tr_best:  98.35%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019991/  0.473071, val:  88.75%, val_best:  90.42%, tr:  99.35%, tr_best:  99.35%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.024913/  0.594811, val:  87.92%, val_best:  90.42%, tr:  99.23%, tr_best:  99.35%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.011130/  0.510964, val:  89.17%, val_best:  90.42%, tr:  99.86%, tr_best:  99.86%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003125/  0.552201, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.001943/  0.502089, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001624/  0.521189, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001236/  0.545577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001065/  0.524273, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000955/  0.556683, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000850/  0.553951, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000777/  0.562979, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000711/  0.561448, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000666/  0.572040, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000628/  0.581079, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000576/  0.569785, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000538/  0.568354, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000500/  0.574372, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000475/  0.581407, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000450/  0.584895, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000429/  0.586062, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000411/  0.596621, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000389/  0.597417, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000379/  0.608332, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000358/  0.602874, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000340/  0.604670, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000329/  0.601791, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000319/  0.601425, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000308/  0.602356, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000292/  0.598435, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000283/  0.597901, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000277/  0.604077, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000268/  0.613960, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000256/  0.610192, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000249/  0.611147, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000242/  0.614404, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000235/  0.615403, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000227/  0.614356, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000222/  0.623049, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000215/  0.624923, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000211/  0.613693, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000205/  0.619318, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000199/  0.622931, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000195/  0.624674, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000190/  0.627025, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000185/  0.628896, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000181/  0.624894, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000177/  0.621996, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000175/  0.622287, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000171/  0.615900, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000164/  0.617114, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000163/  0.613777, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000159/  0.619844, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000158/  0.615970, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000157/  0.617458, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000153/  0.619992, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000147/  0.616505, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000146/  0.617499, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000143/  0.623439, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000140/  0.627106, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000139/  0.631154, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000138/  0.627911, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000135/  0.630018, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000132/  0.634858, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000130/  0.633134, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000128/  0.632592, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000125/  0.633326, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000124/  0.635861, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000123/  0.634962, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000120/  0.631111, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000122/  0.628672, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000118/  0.629021, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000116/  0.628650, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000114/  0.629489, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000115/  0.628790, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000111/  0.628014, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000109/  0.629268, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000107/  0.627406, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000109/  0.628916, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000104/  0.631419, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000104/  0.634245, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000105/  0.634045, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000103/  0.632370, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000101/  0.629538, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000100/  0.630059, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000098/  0.629965, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000099/  0.636612, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000096/  0.635149, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000095/  0.633406, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000094/  0.632647, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000092/  0.634207, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000091/  0.632478, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6d374677444fca8ac403058ed868c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▆▆████████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>0.63248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-sweep-102</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vypcetsn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vypcetsn</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_170354-vypcetsn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2imc0yk3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_171505-2imc0yk3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2imc0yk3' target=\"_blank\">volcanic-sweep-103</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2imc0yk3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2imc0yk3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.125, v_reset=10000, sg_width=5, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.924084/  0.900167, val:  62.50%, val_best:  62.50%, tr:  60.17%, tr_best:  60.17%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.608945/  0.868340, val:  60.00%, val_best:  62.50%, tr:  72.86%, tr_best:  72.86%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.411019/  0.862224, val:  72.08%, val_best:  72.08%, tr:  83.52%, tr_best:  83.52%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.290071/  0.516031, val:  84.17%, val_best:  84.17%, tr:  89.34%, tr_best:  89.34%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.148748/  0.462285, val:  87.92%, val_best:  87.92%, tr:  95.38%, tr_best:  95.38%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.110478/  0.453902, val:  89.17%, val_best:  89.17%, tr:  96.35%, tr_best:  96.35%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.087798/  0.488559, val:  88.75%, val_best:  89.17%, tr:  97.23%, tr_best:  97.23%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.056540/  0.460554, val:  87.92%, val_best:  89.17%, tr:  98.20%, tr_best:  98.20%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.032420/  0.453790, val:  90.00%, val_best:  90.00%, tr:  99.12%, tr_best:  99.12%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.044882/  0.377394, val:  90.00%, val_best:  90.00%, tr:  98.67%, tr_best:  99.12%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.031081/  0.458891, val:  90.00%, val_best:  90.00%, tr:  99.01%, tr_best:  99.12%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.012163/  0.410768, val:  90.83%, val_best:  90.83%, tr:  99.71%, tr_best:  99.71%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.003469/  0.424667, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.002129/  0.401361, val:  91.25%, val_best:  91.67%, tr:  99.98%, tr_best: 100.00%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.001459/  0.412334, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.001118/  0.409213, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.000979/  0.411735, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.000870/  0.414670, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.000784/  0.411883, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000699/  0.427088, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000645/  0.428068, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000597/  0.433039, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000562/  0.453180, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000520/  0.458498, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000493/  0.451457, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000468/  0.455886, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000441/  0.445366, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000417/  0.445858, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000400/  0.447511, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000372/  0.454448, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000364/  0.447946, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000352/  0.454611, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000334/  0.458291, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000324/  0.462057, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000310/  0.460060, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000302/  0.459708, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000290/  0.464959, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000283/  0.463789, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000270/  0.462140, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000260/  0.459259, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000252/  0.460512, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000247/  0.460966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000241/  0.461501, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000232/  0.467219, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000227/  0.465373, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000218/  0.464732, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000214/  0.462301, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000206/  0.463863, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000203/  0.462686, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000201/  0.461293, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000193/  0.463994, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000188/  0.463246, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000183/  0.461719, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000177/  0.464404, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000174/  0.462007, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000172/  0.465646, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000167/  0.467702, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000163/  0.466615, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000162/  0.465064, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000155/  0.466431, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000156/  0.470764, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000151/  0.474162, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.000149/  0.475791, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.000144/  0.477446, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.000143/  0.473104, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.000139/  0.477203, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.000137/  0.482258, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.000136/  0.479527, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.000132/  0.478409, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.000131/  0.478923, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.000130/  0.479966, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.000127/  0.481726, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.000125/  0.477696, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.000124/  0.479414, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.000122/  0.479214, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.000121/  0.478879, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.000119/  0.476968, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.000117/  0.477993, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.000115/  0.480774, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.000115/  0.478548, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000113/  0.480919, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.000112/  0.478824, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000111/  0.480684, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.000109/  0.478337, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000106/  0.473828, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000105/  0.479483, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000105/  0.480563, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000103/  0.480323, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000103/  0.481059, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000102/  0.479574, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000099/  0.478145, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000098/  0.482602, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000097/  0.483561, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000095/  0.486765, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000095/  0.482070, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000093/  0.486749, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000091/  0.483553, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000091/  0.492586, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000089/  0.491570, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000089/  0.493886, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e2f9cae42040f285acc9d1bde4582c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▆▁▆█▆███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>0.49389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-sweep-103</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2imc0yk3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2imc0yk3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250501_171505-2imc0yk3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8x3r2wog with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250501_172720-8x3r2wog</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8x3r2wog' target=\"_blank\">major-sweep-104</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xas47qh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8x3r2wog' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8x3r2wog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': False, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "이 데이터셋의 데이터 개수는 979 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 8dd42b2ecffbb93105d2691b2bcb8c3b\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 977 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = a78e3b87adbdbd41b7bc6f822802b1ef\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 963 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 6085c0f4cab7e9ef4ef036df1c1f49c1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 816 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1ef4b0b9a99f9977cc21d6fdc1679efb\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 448 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = da512699ef27d0f9477673f290efe484\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 149 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 1016cb1fdbfa083feef50c5fde1352bc\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 61 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 0a7560ce3507905f104a1b68798353fd\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 26 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = b3305a42dac5ea0e87bb5467531333f1\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 13 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 3ed7f09bb5667a087d6c9320ff4723b5\n",
      "cache path exists\n",
      "이 데이터셋의 데이터 개수는 4 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "이 데이터셋의 데이터 개수는 240 입니다. (test set은 안바뀌게 해놨다 알제)\n",
      "dataset_hash = 00e20c7fe07597879bf495734895e7e8\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 278 BATCH: 16 train_data_count: 4436\n",
      "len(test_loader): 15 BATCH: 16\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0, v_threshold=0.25, v_reset=0, sg_width=3, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0, TIME=10, sstep=False, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=False, time_different_weight=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  0.974480/  0.867463, val:  64.17%, val_best:  64.17%, tr:  58.93%, tr_best:  58.93%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  0.619049/  0.779950, val:  68.75%, val_best:  68.75%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  0.410511/  0.901403, val:  70.83%, val_best:  70.83%, tr:  83.41%, tr_best:  83.41%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  0.292086/  0.534297, val:  87.08%, val_best:  87.08%, tr:  89.13%, tr_best:  89.13%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  0.162379/  0.612390, val:  82.50%, val_best:  87.08%, tr:  94.36%, tr_best:  94.36%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  0.140578/  0.442822, val:  89.58%, val_best:  89.58%, tr:  95.36%, tr_best:  95.36%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  0.082165/  0.784311, val:  85.00%, val_best:  89.58%, tr:  97.43%, tr_best:  97.43%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  0.059931/  0.539399, val:  87.50%, val_best:  89.58%, tr:  97.97%, tr_best:  97.97%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  0.036210/  0.485832, val:  90.83%, val_best:  90.83%, tr:  98.92%, tr_best:  98.92%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  0.019819/  0.519804, val:  88.75%, val_best:  90.83%, tr:  99.48%, tr_best:  99.48%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  0.032321/  0.569617, val:  87.50%, val_best:  90.83%, tr:  98.90%, tr_best:  99.48%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  0.030033/  0.488638, val:  88.75%, val_best:  90.83%, tr:  98.96%, tr_best:  99.48%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  0.012969/  0.511075, val:  91.25%, val_best:  91.25%, tr:  99.62%, tr_best:  99.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  0.006165/  0.697958, val:  86.25%, val_best:  91.25%, tr:  99.89%, tr_best:  99.89%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  0.003339/  0.576901, val:  89.17%, val_best:  91.25%, tr:  99.95%, tr_best:  99.95%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  0.002147/  0.547625, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  0.001528/  0.554330, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  0.001201/  0.556022, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  0.001124/  0.551401, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  0.000866/  0.574368, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  0.000799/  0.591013, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  0.000714/  0.581928, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  0.000671/  0.589273, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  0.000652/  0.598257, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  0.000555/  0.602714, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.000540/  0.597360, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.000500/  0.592729, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.000474/  0.601346, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.000439/  0.602200, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.000428/  0.595900, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.000411/  0.599700, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.000382/  0.607436, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.000371/  0.604986, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.000348/  0.610807, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.000338/  0.618313, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.000328/  0.620386, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.000313/  0.618911, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.000303/  0.616548, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.000305/  0.621521, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.000285/  0.625960, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.000283/  0.630643, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.000271/  0.629686, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.000262/  0.628815, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.000250/  0.627120, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.000250/  0.628091, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.000242/  0.628134, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.000232/  0.633806, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.000230/  0.632606, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.000220/  0.635183, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.000215/  0.640931, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.000209/  0.649058, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.000209/  0.644091, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.000201/  0.644871, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.000195/  0.649851, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.000196/  0.657061, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.000189/  0.662943, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.000186/  0.667632, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.000181/  0.671509, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.000180/  0.673545, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.000177/  0.670262, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.000173/  0.666736, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.000167/  0.664875, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [False]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.0,0.125,0.25,0.5,0.75]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.125, 0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        # \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.0001,0.001, 0.01,0.1]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['sigmoid','hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [True]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [6]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [False]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [True]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.lif_layer_v_decay, #wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = 'yryz40rf'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
