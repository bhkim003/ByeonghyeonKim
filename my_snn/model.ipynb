{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "        return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        #############밑에부터 수정해라#######\n",
    "        spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = grad_output_current_clone @ weight\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "    \n",
    "class SYNAPSE_FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_FC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Features]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "        spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                    stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_CONV, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "        # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Channel, Height, Width]   \n",
    "        # print('spike.shape', spike.shape)\n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        Channel = self.out_channels\n",
    "        Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0])\n",
    "        spike_now = torch.zeros_like(spike_detach[0])\n",
    "        for t in range(Time):\n",
    "            # print(f'time:{t}', torch.sum(spike_detach[t]/ torch.numel(spike_detach[t])))\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "            spike_past = spike_now\n",
    "            # print(f'time:{t}', torch.sum(output_current[t]/ torch.numel(output_current[t])))\n",
    "\n",
    "        return output_current\n",
    "\n",
    "\n",
    "\n",
    "class LIF_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate):\n",
    "        v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "        spike = (v_one_time >= v_threshold).float() #fire\n",
    "        if surrogate == 'sigmoid':\n",
    "            surrogate = 1\n",
    "        elif surrogate == 'rectangle':\n",
    "            surrogate = 2\n",
    "        elif surrogate == 'rough_rectangle':\n",
    "            surrogate = 3\n",
    "        else:\n",
    "            pass\n",
    "        ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                            torch.tensor([v_threshold], requires_grad=False), \n",
    "                            torch.tensor([v_reset], requires_grad=False), \n",
    "                            torch.tensor([sg_width], requires_grad=False),\n",
    "                            torch.tensor([surrogate], requires_grad=False)) # save before reset\n",
    "        \n",
    "        v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "        return spike, v_one_time\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_spike, grad_output_v):\n",
    "        v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate = ctx.saved_tensors\n",
    "        v_decay=v_decay.item()\n",
    "        v_threshold=v_threshold.item()\n",
    "        v_reset=v_reset.item()\n",
    "        sg_width=sg_width.item()\n",
    "        surrogate=surrogate.item()\n",
    "\n",
    "        grad_input_current = grad_output_spike.clone()\n",
    "        # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "        ################ select one of the following surrogate gradient functions ################\n",
    "        if (surrogate == 1):\n",
    "            #===========surrogate gradient function (sigmoid)\n",
    "            sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "            grad_input_current *= 4*sig*(1-sig)\n",
    "\n",
    "        elif (surrogate == 2):\n",
    "            # ===========surrogate gradient function (rectangle)\n",
    "            grad_input_current *= ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "        elif (surrogate == 3):\n",
    "            #===========surrogate gradient function (rough rectangle)\n",
    "            grad_input_current[(v_one_time - v_threshold).abs() > sg_width/2] = 0\n",
    "        else: \n",
    "            pass\n",
    "        ###########################################################################################\n",
    "        return grad_input_current, None, None, None, None, None, None\n",
    "\n",
    "class LIF_layer(nn.Module):\n",
    "    def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate):\n",
    "        super(LIF_layer, self).__init__()\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.sg_width = sg_width\n",
    "        self.surrogate = surrogate\n",
    "\n",
    "    def forward(self, input_current):\n",
    "        v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "        post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "        # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "        Time = v.shape[0]\n",
    "        for t in range(Time):\n",
    "            # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "            post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                            self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate) \n",
    "        return post_spike \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DimChanger(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(DimChanger, self).__init__()\n",
    "        self.ann_module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        timestep, batch_size, *dim = x.shape\n",
    "        output = self.ann_module(x.reshape(timestep * batch_size, *dim))\n",
    "        _, *dim = output.shape\n",
    "        output = output.view(timestep, batch_size, *dim).contiguous()\n",
    "        return output\n",
    "\n",
    "class tdBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, channel):\n",
    "        super(tdBatchNorm, self).__init__(channel)\n",
    "        # according to tdBN paper, the initialized weight is changed to alpha*Vth\n",
    "        # self.weight.data.mul_(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        out = super().forward(x.reshape(T * B, *spatial_dims))\n",
    "        TB, *spatial_dims = out.shape\n",
    "        out = out.view(T, B, *spatial_dims).contiguous()\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class tdBatchNorm_FC(nn.BatchNorm1d):\n",
    "    def __init__(self, channel):\n",
    "        super(tdBatchNorm_FC, self).__init__(channel)\n",
    "        # according to tdBN paper, the initialized weight is changed to alpha*Vth\n",
    "        # self.weight.data.mul_(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        out = super().forward(x.reshape(T * B, *spatial_dims))\n",
    "        TB, *spatial_dims = out.shape\n",
    "        out = out.view(T, B, *spatial_dims).contiguous()\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def make_layers_conv(cfg, in_c, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     bn_on,\n",
    "                     surrogate):\n",
    "    \n",
    "    layers = []\n",
    "    in_channels = in_c\n",
    "    img_size_var = IMAGE_SIZE\n",
    "    for which in cfg:\n",
    "        if which == 'P':\n",
    "            layers += [DimChanger(nn.AvgPool2d(kernel_size=2, stride=2))]\n",
    "            # layers += [DimChanger(nn.MaxPool2d(kernel_size=2, stride=2))]\n",
    "            img_size_var = img_size_var // 2\n",
    "        else:\n",
    "            out_channels = which\n",
    "            layers += [SYNAPSE_CONV(in_channels=in_channels,\n",
    "                                    out_channels=out_channels, \n",
    "                                    kernel_size=synapse_conv_kernel_size, \n",
    "                                    stride=synapse_conv_stride, \n",
    "                                    padding=synapse_conv_padding, \n",
    "                                    trace_const1=synapse_conv_trace_const1, \n",
    "                                    trace_const2=synapse_conv_trace_const2)]\n",
    "            img_size_var = (img_size_var - synapse_conv_kernel_size + 2*synapse_conv_padding)//synapse_conv_stride + 1\n",
    "           \n",
    "            in_channels = which\n",
    "            \n",
    "            if (bn_on == True):\n",
    "                layers += [tdBatchNorm(in_channels)] # 여기서 in_channel이 out_channel임\n",
    "\n",
    "            layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                    v_decay=lif_layer_v_decay, \n",
    "                                    v_threshold=lif_layer_v_threshold, \n",
    "                                    v_reset=lif_layer_v_reset, \n",
    "                                    sg_width=lif_layer_sg_width,\n",
    "                                    surrogate=surrogate)]\n",
    "            \n",
    "    return nn.Sequential(*layers), in_channels, img_size_var\n",
    "\n",
    "\n",
    "\n",
    "class MY_SNN_CONV(nn.Module):\n",
    "    def __init__(self, cfg, in_c, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     bn_on,\n",
    "                     surrogate):\n",
    "        super(MY_SNN_CONV, self).__init__()\n",
    "        self.layers, self.conv_last_channels, self.img_size_var = make_layers_conv(cfg, in_c, IMAGE_SIZE,\n",
    "                                                                            synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                                                                            synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                                                                            synapse_conv_trace_const2, \n",
    "                                                                            lif_layer_v_init, lif_layer_v_decay, \n",
    "                                                                            lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                                                                            lif_layer_sg_width,\n",
    "                                                                            bn_on,\n",
    "                                                                            surrogate)\n",
    "\n",
    "        self.synapse_FC = SYNAPSE_FC(in_features=self.conv_last_channels*self.img_size_var*self.img_size_var,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                    out_features=synapse_fc_out_features, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.synapse_FC(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "\n",
    "\n",
    "\n",
    "def make_layers_fc(cfg, in_c, IMAGE_SIZE, out_c,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     bn_on,\n",
    "                     surrogate):\n",
    "\n",
    "    layers = []\n",
    "    img_size = IMAGE_SIZE\n",
    "    in_channels = in_c * img_size * img_size\n",
    "    class_num = out_c\n",
    "    for which in cfg:\n",
    "        out_channels = which\n",
    "        layers += [SYNAPSE_FC(in_features=in_channels,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                    out_features=out_channels, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)]\n",
    "\n",
    "\n",
    "\n",
    "        in_channels = which\n",
    "        \n",
    "        if (bn_on == True):\n",
    "            layers += [tdBatchNorm_FC(in_channels)] # 여기서 in_channel이 out_channel임\n",
    "\n",
    "        layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                v_decay=lif_layer_v_decay, \n",
    "                                v_threshold=lif_layer_v_threshold, \n",
    "                                v_reset=lif_layer_v_reset, \n",
    "                                sg_width=lif_layer_sg_width,\n",
    "                                surrogate=surrogate)]\n",
    "\n",
    "    \n",
    "    out_channels = class_num\n",
    "    layers += [SYNAPSE_FC(in_features=in_channels,  \n",
    "                                    out_features=out_channels, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)]\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MY_SNN_FC(nn.Module):\n",
    "    def __init__(self, cfg, in_c, IMAGE_SIZE, out_c,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     bn_on,\n",
    "                     surrogate):\n",
    "        super(MY_SNN_FC, self).__init__()\n",
    "\n",
    "        self.layers = make_layers_fc(cfg, in_c, IMAGE_SIZE, out_c,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     bn_on,\n",
    "                     surrogate)\n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 8,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 100,\n",
    "                    verbose_interval = 100,\n",
    "                    bn_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid'\n",
    "                  ):\n",
    "\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (which_data == 'MNIST'):\n",
    "        data_path = '/data2'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "\n",
    "    if (which_data == 'CIFAR10'):\n",
    "        data_path = '/data2/cifar10'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "    if (which_data == 'FASHION_MNIST'):\n",
    "        data_path = '/data2'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "        \n",
    "\n",
    "    data_iter = IMAGE_PIXEL_CHANNEL = iter(train_loader)\n",
    "    images, labels = data_iter.next()\n",
    "\n",
    "    # 채널 수와 클래스 개수를 확인합니다.\n",
    "    synapse_conv_in_channels = images.shape[1]\n",
    "    synapse_fc_out_features = CLASS_NUM = len(torch.unique(labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     bn_on,\n",
    "                     surrogate).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2,\n",
    "                     bn_on,\n",
    "                     surrogate).to(device)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    val_acc = 0\n",
    "\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print('EPOCH', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # print('iter', i)\n",
    "            net.train()\n",
    "\n",
    "            # print('\\niter', i)\n",
    "            iter_one_train_time_start = time.time()\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            if rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            \n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "\n",
    "\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "\n",
    "\n",
    "            ####### training accruacy ######\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted[0:batch] == labels).sum().item()\n",
    "            if i % verbose_interval == 9:\n",
    "                print(f'iter: {i} / {len(train_loader)}')\n",
    "                print(f'training acc: {100 * correct / total:.2f}%')\n",
    "            ################################\n",
    "\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # # optimizer.zero_grad()와 loss.backward() 호출 후에 실행해야 합니다.\n",
    "            # if (i % 100 == 9):\n",
    "            #     print('\\n\\nepoch', epoch, 'iter', i)\n",
    "            #     for name, param in net.named_parameters():\n",
    "            #         if param.requires_grad:\n",
    "            #             print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "            # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "            if i % verbose_interval == 9:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    how_many_val_image=0\n",
    "                    for data in test_loader:\n",
    "                        how_many_val_image += 1\n",
    "                        inputs, labels = data\n",
    "            \n",
    "                        if rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "\n",
    "                        \n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                    print(f'{epoch}-{i} validation acc: {100 * correct / total:.2f}%\\n')\n",
    "\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "                if val_acc < correct / total:\n",
    "                    val_acc = correct / total\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "        \n",
    "        print(f\"epoch_time: {epoch_time} seconds\")\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "iter: 9 / 469\n",
      "training acc: 15.62%\n",
      "0-9 validation acc: 19.15%\n",
      "\n",
      "iter_one_val_time: 3.9033024311065674 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 73.44%\n",
      "0-109 validation acc: 72.96%\n",
      "\n",
      "iter_one_val_time: 3.925772190093994 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 67.97%\n",
      "0-209 validation acc: 77.71%\n",
      "\n",
      "iter_one_val_time: 3.978728771209717 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 77.34%\n",
      "0-309 validation acc: 84.29%\n",
      "\n",
      "iter_one_val_time: 4.126983880996704 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 79.69%\n",
      "0-409 validation acc: 84.83%\n",
      "\n",
      "iter_one_val_time: 4.208570718765259 seconds\n",
      "epoch_time: 77.89867544174194 seconds\n",
      "\n",
      "\n",
      "EPOCH 1\n",
      "iter: 9 / 469\n",
      "training acc: 86.72%\n",
      "1-9 validation acc: 84.32%\n",
      "\n",
      "iter_one_val_time: 3.959965467453003 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 85.94%\n",
      "1-109 validation acc: 84.78%\n",
      "\n",
      "iter_one_val_time: 4.0003979206085205 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 86.72%\n",
      "1-209 validation acc: 83.45%\n",
      "\n",
      "iter_one_val_time: 4.150163173675537 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 82.81%\n",
      "1-309 validation acc: 86.72%\n",
      "\n",
      "iter_one_val_time: 4.012067079544067 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 85.16%\n",
      "1-409 validation acc: 87.15%\n",
      "\n",
      "iter_one_val_time: 4.113641738891602 seconds\n",
      "epoch_time: 68.99793076515198 seconds\n",
      "\n",
      "\n",
      "EPOCH 2\n",
      "iter: 9 / 469\n",
      "training acc: 83.59%\n",
      "2-9 validation acc: 84.39%\n",
      "\n",
      "iter_one_val_time: 3.9578752517700195 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 93.75%\n",
      "2-109 validation acc: 86.80%\n",
      "\n",
      "iter_one_val_time: 4.149105787277222 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 77.34%\n",
      "2-209 validation acc: 86.18%\n",
      "\n",
      "iter_one_val_time: 4.156176567077637 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 89.06%\n",
      "2-309 validation acc: 88.28%\n",
      "\n",
      "iter_one_val_time: 4.106733560562134 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 89.84%\n",
      "2-409 validation acc: 82.81%\n",
      "\n",
      "iter_one_val_time: 4.101483583450317 seconds\n",
      "epoch_time: 69.87556314468384 seconds\n",
      "\n",
      "\n",
      "EPOCH 3\n",
      "iter: 9 / 469\n",
      "training acc: 83.59%\n",
      "3-9 validation acc: 89.22%\n",
      "\n",
      "iter_one_val_time: 4.173150300979614 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 93.75%\n",
      "3-109 validation acc: 83.72%\n",
      "\n",
      "iter_one_val_time: 4.197291374206543 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 89.84%\n",
      "3-209 validation acc: 87.25%\n",
      "\n",
      "iter_one_val_time: 4.156918525695801 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 89.84%\n",
      "3-309 validation acc: 91.26%\n",
      "\n",
      "iter_one_val_time: 4.062952995300293 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 87.50%\n",
      "3-409 validation acc: 85.19%\n",
      "\n",
      "iter_one_val_time: 4.169048547744751 seconds\n",
      "epoch_time: 69.85169291496277 seconds\n",
      "\n",
      "\n",
      "EPOCH 4\n",
      "iter: 9 / 469\n",
      "training acc: 81.25%\n",
      "4-9 validation acc: 79.59%\n",
      "\n",
      "iter_one_val_time: 3.9984402656555176 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 89.84%\n",
      "4-109 validation acc: 92.08%\n",
      "\n",
      "iter_one_val_time: 3.8589842319488525 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 93.75%\n",
      "4-209 validation acc: 90.59%\n",
      "\n",
      "iter_one_val_time: 4.105362176895142 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 89.06%\n",
      "4-309 validation acc: 88.90%\n",
      "\n",
      "iter_one_val_time: 4.146621942520142 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 92.19%\n",
      "4-409 validation acc: 89.50%\n",
      "\n",
      "iter_one_val_time: 3.9367408752441406 seconds\n",
      "epoch_time: 68.91068363189697 seconds\n",
      "\n",
      "\n",
      "EPOCH 5\n",
      "iter: 9 / 469\n",
      "training acc: 92.19%\n",
      "5-9 validation acc: 92.20%\n",
      "\n",
      "iter_one_val_time: 3.976975440979004 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 92.19%\n",
      "5-109 validation acc: 91.87%\n",
      "\n",
      "iter_one_val_time: 3.8908019065856934 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 95.31%\n",
      "5-209 validation acc: 91.66%\n",
      "\n",
      "iter_one_val_time: 3.976079225540161 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 94.53%\n",
      "5-309 validation acc: 91.27%\n",
      "\n",
      "iter_one_val_time: 4.127652883529663 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 96.88%\n",
      "5-409 validation acc: 93.39%\n",
      "\n",
      "iter_one_val_time: 3.9648494720458984 seconds\n",
      "epoch_time: 68.84873557090759 seconds\n",
      "\n",
      "\n",
      "EPOCH 6\n",
      "iter: 9 / 469\n",
      "training acc: 97.66%\n",
      "6-9 validation acc: 90.47%\n",
      "\n",
      "iter_one_val_time: 4.003001689910889 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 91.41%\n",
      "6-109 validation acc: 88.02%\n",
      "\n",
      "iter_one_val_time: 3.8980557918548584 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 86.72%\n",
      "6-209 validation acc: 89.97%\n",
      "\n",
      "iter_one_val_time: 4.018829345703125 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 91.41%\n",
      "6-309 validation acc: 89.73%\n",
      "\n",
      "iter_one_val_time: 3.999533176422119 seconds\n",
      "iter: 409 / 469\n",
      "training acc: 91.41%\n",
      "6-409 validation acc: 90.89%\n",
      "\n",
      "iter_one_val_time: 4.026512384414673 seconds\n",
      "epoch_time: 68.57347798347473 seconds\n",
      "\n",
      "\n",
      "EPOCH 7\n",
      "iter: 9 / 469\n",
      "training acc: 89.84%\n",
      "7-9 validation acc: 90.27%\n",
      "\n",
      "iter_one_val_time: 3.868109703063965 seconds\n",
      "iter: 109 / 469\n",
      "training acc: 94.53%\n",
      "7-109 validation acc: 92.04%\n",
      "\n",
      "iter_one_val_time: 4.0159687995910645 seconds\n",
      "iter: 209 / 469\n",
      "training acc: 91.41%\n",
      "7-209 validation acc: 93.80%\n",
      "\n",
      "iter_one_val_time: 3.941765069961548 seconds\n",
      "iter: 309 / 469\n",
      "training acc: 88.28%\n",
      "7-309 validation acc: 90.87%\n",
      "\n",
      "iter_one_val_time: 3.9246420860290527 seconds\n"
     ]
    }
   ],
   "source": [
    "decay = 0.3\n",
    "\n",
    "my_snn_system(  devices = \"2,3,4,5\",\n",
    "                my_seed = 42,\n",
    "                TIME = 8,\n",
    "                BATCH = 128,\n",
    "                IMAGE_SIZE = 28,\n",
    "                which_data = 'MNIST',# 'CIFAR10' 'MNIST' 'FASHION_MNIST'\n",
    "                rate_coding = True, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.2,\n",
    "                lif_layer_v_reset = 0.0,\n",
    "                lif_layer_sg_width = 1.0, # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "                cfg = [64, 64,],\n",
    "                pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                learning_rate = 0.001,\n",
    "                epoch_num = 200,\n",
    "                verbose_interval = 100,\n",
    "                bn_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'rough_rectangle' # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                )\n",
    "\n",
    "# 걍 surrogate에 처음부터 1 2 3 이런식으로 넣어주자\n",
    "# 그리고 실험해보자 이거 이런식이면 못고침\n",
    "\n",
    "\n",
    "    # cfg 종류 = {\n",
    "    # 'A': [64, 64], \n",
    "    # 'B': [64, 64, 64, 64], \n",
    "    # 'C': [64, 128, 256],\n",
    "    # 'D': [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "    # 'K': [64, 64],\n",
    "    # }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
