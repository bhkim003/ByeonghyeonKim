{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6cb1d39e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메인 셀\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3,4,5\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "        return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        #############밑에부터 수정해라#######\n",
    "        spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = grad_output_current_clone @ weight\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "     \n",
    "class SYNAPSE_FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_FC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Features]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "        spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                      stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_CONV, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Channel, Height, Width]   \n",
    "        # print('spike.shape', spike.shape)\n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        Channel = self.out_channels\n",
    "        Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0])\n",
    "        spike_now = torch.zeros_like(spike_detach[0])\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current\n",
    "\n",
    "\n",
    "\n",
    "class LIF_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width):\n",
    "        v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "        spike = (v_one_time >= v_threshold).float() #fire\n",
    "        ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                              torch.tensor([v_threshold], requires_grad=False), \n",
    "                              torch.tensor([v_reset], requires_grad=False), \n",
    "                              torch.tensor([sg_width], requires_grad=False)) # save before reset\n",
    "        v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "        return spike, v_one_time\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_spike, grad_output_v):\n",
    "        v_one_time, v_decay, v_threshold, v_reset, sg_width = ctx.saved_tensors\n",
    "        v_decay=v_decay.item()\n",
    "        v_threshold=v_threshold.item()\n",
    "        v_reset=v_reset.item()\n",
    "        sg_width=sg_width.item()\n",
    "\n",
    "        grad_input_current = grad_output_spike.clone()\n",
    "        # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "        ################ select one of the following surrogate gradient functions ################\n",
    "        #===========surrogate gradient function (rectangle)\n",
    "        grad_input_current = grad_input_current * ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "        #===========surrogate gradient function (sigmoid)\n",
    "        # sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "        # grad_input_current =  sig*(1-sig)*grad_input_current\n",
    "\n",
    "        #===========surrogate gradient function (rough rectangle)\n",
    "        # v_minus_th = (v_one_time - v_threshold)\n",
    "        # grad_input_current[v_minus_th <= -.5] = 0\n",
    "        # grad_input_current[v_minus_th > .5] = 0\n",
    "        ###########################################################################################\n",
    "        return grad_input_current, None, None, None, None, None\n",
    "\n",
    "class LIF_layer(nn.Module):\n",
    "    def __init__ (self, v_init = 0.0, v_decay = 0.8, v_threshold = 0.5, v_reset = 0.0, sg_width = 1):\n",
    "        super(LIF_layer, self).__init__()\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.sg_width = sg_width\n",
    "\n",
    "    def forward(self, input_current):\n",
    "        v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "        post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "        # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "        Time = v.shape[0]\n",
    "        for t in range(Time):\n",
    "            # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "            post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                            self.v_decay, self.v_threshold, self.v_reset, self.sg_width) \n",
    "\n",
    "        return post_spike   \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "IMAGE_PIXEL_CHANNEL: 3\n",
      "CLASS_NUM: 10\n",
      "torch.Size([256, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# HEPER PARAMETER\n",
    "TIME = 8\n",
    "BATCH = 256\n",
    "# IMAGE_PIXEL_CHANNEL = 3\n",
    "IMAGE_SIZE = 32\n",
    "# CLASS_NUM = 10\n",
    "\n",
    "## 데이터셋 선택하세요 #########\n",
    "# which_data = 'MNIST'\n",
    "which_data = 'CIFAR10'\n",
    "################################\n",
    "\n",
    "rate_coding = False\n",
    "\n",
    "if (which_data == 'MNIST'):\n",
    "    data_path = '/data2'\n",
    "\n",
    "    if rate_coding :\n",
    "        transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "    else : \n",
    "        transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(trainset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = True,\n",
    "                            num_workers =2)\n",
    "    test_loader = DataLoader(testset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = False,\n",
    "                            num_workers =2)\n",
    "\n",
    "\n",
    "if (which_data == 'CIFAR10'):\n",
    "    data_path = '/data2/cifar10'\n",
    "\n",
    "    if rate_coding :\n",
    "        transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor()])\n",
    "\n",
    "        transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.ToTensor()])\n",
    "    \n",
    "    else :\n",
    "        transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "        transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(trainset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = True,\n",
    "                            num_workers =2)\n",
    "    test_loader = DataLoader(testset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = False,\n",
    "                            num_workers =2)\n",
    "\n",
    "    '''\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "            'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "    '''\n",
    "\n",
    "\n",
    "# 데이터 로더에서 첫 번째 배치를 가져옵니다.\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# 채널 수와 클래스 개수를 확인합니다.\n",
    "IMAGE_PIXEL_CHANNEL = images.shape[1]\n",
    "CLASS_NUM = len(torch.unique(labels))\n",
    "print('IMAGE_PIXEL_CHANNEL:', IMAGE_PIXEL_CHANNEL)\n",
    "print('CLASS_NUM:', CLASS_NUM)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIF_layer 레이어의 하이퍼파라미터\n",
    "lif_layer_v_init = 0.0\n",
    "lif_layer_v_decay = 0.6 \n",
    "lif_layer_v_threshold = 1.2\n",
    "lif_layer_v_reset = 0.0\n",
    "lif_layer_sg_width = 1\n",
    "\n",
    "## SYNAPSE_CONV 레이어의 하이퍼파라미터\n",
    "synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL\n",
    "# synapse_conv_out_channels = layer별 지정\n",
    "synapse_conv_kernel_size = 3\n",
    "synapse_conv_stride = 1\n",
    "synapse_conv_padding = 1\n",
    "synapse_conv_trace_const1 = 1\n",
    "synapse_conv_trace_const2 = lif_layer_v_decay\n",
    "\n",
    "## SYNAPSE_FC 레이어의 하이퍼파라미터\n",
    "# synapse_fc_in_features = 마지막CONV_OUT_CHANNEL * H * W\n",
    "synapse_fc_out_features = CLASS_NUM\n",
    "synapse_fc_trace_const1 = 1\n",
    "synapse_fc_trace_const2 = lif_layer_v_decay\n",
    "\n",
    "\n",
    "def make_layers_conv(cfg, in_c=3):\n",
    "    layers = []\n",
    "    img_size_var = IMAGE_SIZE\n",
    "    in_channels = in_c\n",
    "    for which in cfg:\n",
    "        out_channels = which\n",
    "        layers += [SYNAPSE_CONV(in_channels=in_channels,\n",
    "                                out_channels=out_channels, \n",
    "                                kernel_size=synapse_conv_kernel_size, \n",
    "                                stride=synapse_conv_stride, \n",
    "                                padding=synapse_conv_padding, \n",
    "                                trace_const1=synapse_conv_trace_const1, \n",
    "                                trace_const2=synapse_conv_trace_const2)]\n",
    "        img_size_var = (img_size_var - synapse_conv_kernel_size + 2*synapse_conv_padding)//synapse_conv_stride + 1\n",
    "\n",
    "        layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                v_decay=lif_layer_v_decay, \n",
    "                                v_threshold=lif_layer_v_threshold, \n",
    "                                v_reset=lif_layer_v_reset, \n",
    "                                sg_width=lif_layer_sg_width)]\n",
    "        in_channels = which\n",
    "\n",
    "    return nn.Sequential(*layers), in_channels, img_size_var\n",
    "\n",
    "\n",
    "\n",
    "class MY_SNN_CONV(nn.Module):\n",
    "    def __init__(self, cfg, inc = 3):\n",
    "        super(MY_SNN_CONV, self).__init__()\n",
    "\n",
    "        self.layers, self.conv_last_channels, self.img_size_var = make_layers_conv(cfg, in_c=inc)\n",
    "\n",
    "        self.synapse_FC = SYNAPSE_FC(in_features=self.conv_last_channels*self.img_size_var*self.img_size_var,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                      out_features=synapse_fc_out_features, \n",
    "                                      trace_const1=synapse_fc_trace_const1, \n",
    "                                      trace_const2=synapse_fc_trace_const2)\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.synapse_FC(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "\n",
    "\n",
    "\n",
    "def make_layers_fc(cfg, in_c=3, img_size = 32, out_c=10):\n",
    "    layers = []\n",
    "    in_channels = in_c * img_size * img_size\n",
    "    class_num = out_c\n",
    "    for which in cfg:\n",
    "        out_channels = which\n",
    "        layers += [SYNAPSE_FC(in_features=in_channels,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                      out_features=out_channels, \n",
    "                                      trace_const1=synapse_fc_trace_const1, \n",
    "                                      trace_const2=synapse_fc_trace_const2)]\n",
    "\n",
    "        layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                v_decay=lif_layer_v_decay, \n",
    "                                v_threshold=lif_layer_v_threshold, \n",
    "                                v_reset=lif_layer_v_reset, \n",
    "                                sg_width=lif_layer_sg_width)]\n",
    "        in_channels = which\n",
    "\n",
    "    \n",
    "    out_channels = class_num\n",
    "    layers += [SYNAPSE_FC(in_features=in_channels,  \n",
    "                                    out_features=out_channels, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)]\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MY_SNN_FC(nn.Module):\n",
    "    def __init__(self, cfg, inc = 3):\n",
    "        super(MY_SNN_FC, self).__init__()\n",
    "\n",
    "        self.layers = make_layers_fc(cfg, in_c=inc, img_size = IMAGE_SIZE, out_c=synapse_fc_out_features)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 15.23%\n",
      "validation acc: 14.13%\n",
      "iter_one_val_time: 2.6930177211761475 seconds\n",
      "training acc: 20.31%\n",
      "validation acc: 19.10%\n",
      "iter_one_val_time: 2.596482276916504 seconds\n",
      "epoch_time: 42.077085971832275 seconds\n",
      "\n",
      "\n",
      "epoch 1\n",
      "training acc: 17.58%\n",
      "validation acc: 21.54%\n",
      "iter_one_val_time: 2.611022710800171 seconds\n",
      "training acc: 18.36%\n",
      "validation acc: 21.48%\n",
      "iter_one_val_time: 2.618651866912842 seconds\n",
      "epoch_time: 41.66142702102661 seconds\n",
      "\n",
      "\n",
      "epoch 2\n",
      "training acc: 15.62%\n",
      "validation acc: 19.79%\n",
      "iter_one_val_time: 2.675661563873291 seconds\n",
      "training acc: 22.66%\n",
      "validation acc: 22.34%\n",
      "iter_one_val_time: 2.673121452331543 seconds\n",
      "epoch_time: 42.157081604003906 seconds\n",
      "\n",
      "\n",
      "epoch 3\n",
      "training acc: 21.09%\n",
      "validation acc: 21.27%\n",
      "iter_one_val_time: 2.7151577472686768 seconds\n",
      "training acc: 17.58%\n",
      "validation acc: 20.31%\n",
      "iter_one_val_time: 2.670081377029419 seconds\n",
      "epoch_time: 42.31938409805298 seconds\n",
      "\n",
      "\n",
      "epoch 4\n",
      "training acc: 23.83%\n",
      "validation acc: 22.17%\n",
      "iter_one_val_time: 2.653510093688965 seconds\n",
      "training acc: 19.92%\n",
      "validation acc: 23.76%\n",
      "iter_one_val_time: 2.746478319168091 seconds\n",
      "epoch_time: 42.17416262626648 seconds\n",
      "\n",
      "\n",
      "epoch 5\n",
      "training acc: 20.70%\n",
      "validation acc: 22.00%\n",
      "iter_one_val_time: 2.650418281555176 seconds\n",
      "training acc: 24.22%\n",
      "validation acc: 23.37%\n",
      "iter_one_val_time: 2.6903700828552246 seconds\n",
      "epoch_time: 41.7816801071167 seconds\n",
      "\n",
      "\n",
      "epoch 6\n",
      "training acc: 19.92%\n",
      "validation acc: 24.10%\n",
      "iter_one_val_time: 2.7793712615966797 seconds\n",
      "training acc: 20.70%\n",
      "validation acc: 24.34%\n",
      "iter_one_val_time: 2.6602301597595215 seconds\n",
      "epoch_time: 42.446916580200195 seconds\n",
      "\n",
      "\n",
      "epoch 7\n",
      "training acc: 13.28%\n",
      "validation acc: 22.34%\n",
      "iter_one_val_time: 2.674436092376709 seconds\n",
      "training acc: 23.44%\n",
      "validation acc: 25.55%\n",
      "iter_one_val_time: 2.579936981201172 seconds\n",
      "epoch_time: 42.24275755882263 seconds\n",
      "\n",
      "\n",
      "epoch 8\n",
      "training acc: 20.31%\n",
      "validation acc: 22.75%\n",
      "iter_one_val_time: 2.636456251144409 seconds\n",
      "training acc: 27.34%\n",
      "validation acc: 27.94%\n",
      "iter_one_val_time: 2.7126963138580322 seconds\n",
      "epoch_time: 42.130409240722656 seconds\n",
      "\n",
      "\n",
      "epoch 9\n",
      "training acc: 26.17%\n",
      "validation acc: 25.76%\n",
      "iter_one_val_time: 2.706416130065918 seconds\n",
      "training acc: 26.95%\n",
      "validation acc: 25.14%\n",
      "iter_one_val_time: 2.805079460144043 seconds\n",
      "epoch_time: 42.252681255340576 seconds\n",
      "\n",
      "\n",
      "epoch 10\n",
      "training acc: 21.48%\n",
      "validation acc: 23.84%\n",
      "iter_one_val_time: 2.6817498207092285 seconds\n",
      "training acc: 18.75%\n",
      "validation acc: 20.58%\n",
      "iter_one_val_time: 2.651634931564331 seconds\n",
      "epoch_time: 42.19354271888733 seconds\n",
      "\n",
      "\n",
      "epoch 11\n",
      "training acc: 28.52%\n",
      "validation acc: 22.06%\n",
      "iter_one_val_time: 2.651625394821167 seconds\n",
      "training acc: 25.78%\n",
      "validation acc: 23.77%\n",
      "iter_one_val_time: 2.6085119247436523 seconds\n",
      "epoch_time: 41.88490891456604 seconds\n",
      "\n",
      "\n",
      "epoch 12\n",
      "training acc: 25.39%\n",
      "validation acc: 26.05%\n",
      "iter_one_val_time: 2.6711320877075195 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# inputs: [Batch, Time, Channel, Height, Width]   \u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m batch \u001b[38;5;241m=\u001b[39m BATCH \n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m BATCH: \n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "#########################\n",
    "##### net 선택 #############\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 64], \n",
    "    'B': [64, 64, 64, 64], \n",
    "    'C': [64, 128, 256],\n",
    "    'K': [64, 64],\n",
    "}\n",
    "\n",
    "#### 새로 net 만들기 ####\n",
    "which_cfg = 'A'\n",
    "\n",
    "if (which_cfg >= 'K'):\n",
    "    net = MY_SNN_FC(cfg[which_cfg], inc = synapse_conv_in_channels).to(device)\n",
    "else:\n",
    "    net = MY_SNN_CONV(cfg[which_cfg], inc = synapse_conv_in_channels).to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "val_acc = 0\n",
    "########################\n",
    "\n",
    "#### model 저장해놨던거 쓰기##############\n",
    "net = torch.load(\"net_save/save_now_net.pth\")\n",
    "# net = torch.load(\"net_save/mnist97.pth\")\n",
    "##########################################\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('epoch', epoch)\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        net.train()\n",
    "\n",
    "        # print('\\niter', i)\n",
    "        iter_one_train_time_start = time.time()\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        if rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "        \n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        batch = BATCH \n",
    "        if labels.size(0) != BATCH: \n",
    "            batch = labels.size(0)\n",
    "\n",
    "\n",
    "        ####### training accruacy ######\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted[0:batch] == labels).sum().item()\n",
    "        if i % 100 == 9:\n",
    "            print(f'training acc: {100 * correct / total:.2f}%')\n",
    "        ################################\n",
    "\n",
    "        loss = criterion(outputs[0:batch,:], labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer.zero_grad()와 loss.backward() 호출 후에 실행해야 합니다.\n",
    "        # if (i % 100 == 9):\n",
    "        #     print('\\n\\nepoch', epoch, 'iter', i)\n",
    "        #     for name, param in net.named_parameters():\n",
    "        #         if param.requires_grad:\n",
    "        #             print('\\n\\n\\n\\n' , name, param.grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "        iter_one_train_time_end = time.time()\n",
    "        elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "        # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "        if i % 100 == 9:\n",
    "            iter_one_val_time_start = time.time()\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                how_many_val_image=0\n",
    "                for data in test_loader:\n",
    "                    how_many_val_image += 1\n",
    "                    inputs, labels = data\n",
    "        \n",
    "                    if rate_coding == True :\n",
    "                        inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                    else :\n",
    "                        inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    batch = BATCH \n",
    "                    if labels.size(0) != BATCH: \n",
    "                        batch = labels.size(0)\n",
    "                    correct += (predicted[0:batch] == labels).sum().item()\n",
    "                print(f'validation acc: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "            iter_one_val_time_end = time.time()\n",
    "            elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "            print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "            if val_acc < correct / total:\n",
    "                val_acc = correct / total\n",
    "                torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "    epoch_time_end = time.time()\n",
    "    epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "    \n",
    "    print(f\"epoch_time: {epoch_time} seconds\")\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snntorch import spikegen\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "\n",
    "# Iterate through minibatches\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "# Spiking Data\n",
    "spike_data = spikegen.rate(data_it, num_steps=8)\n",
    "\n",
    "a = torch.Tensor([1, .2, .7, 1])\n",
    "a=spikegen.rate(a, num_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MY_SNN_CONV(\n",
       "    (layers): Sequential(\n",
       "      (0): SYNAPSE_CONV()\n",
       "      (1): LIF_layer()\n",
       "      (2): SYNAPSE_CONV()\n",
       "      (3): LIF_layer()\n",
       "    )\n",
       "    (synapse_FC): SYNAPSE_FC()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
