{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f73a00406f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메인 셀\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "        return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        #############밑에부터 수정해라#######\n",
    "        spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = grad_output_current_clone @ weight\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "     \n",
    "class SYNAPSE_FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_FC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Features]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "        spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                      stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_CONV, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Channel, Height, Width]   \n",
    "        # print('spike.shape', spike.shape)\n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        Channel = self.out_channels\n",
    "        Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0])\n",
    "        spike_now = torch.zeros_like(spike_detach[0])\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current\n",
    "\n",
    "\n",
    "\n",
    "class LIF_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width):\n",
    "        v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "        spike = (v_one_time >= v_threshold).float() #fire\n",
    "        ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                              torch.tensor([v_threshold], requires_grad=False), \n",
    "                              torch.tensor([v_reset], requires_grad=False), \n",
    "                              torch.tensor([sg_width], requires_grad=False)) # save before reset\n",
    "        v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "        return spike, v_one_time\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_spike, grad_output_v):\n",
    "        v_one_time, v_decay, v_threshold, v_reset, sg_width = ctx.saved_tensors\n",
    "        v_decay=v_decay.item()\n",
    "        v_threshold=v_threshold.item()\n",
    "        v_reset=v_reset.item()\n",
    "        sg_width=sg_width.item()\n",
    "\n",
    "        grad_input_current = grad_output_spike.clone()\n",
    "        # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "        ################ select one of the following surrogate gradient functions ################\n",
    "        #===========surrogate gradient function (rectangle)\n",
    "        grad_input_current = grad_input_current * ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "        #===========surrogate gradient function (sigmoid)\n",
    "        # sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "        # grad_input_current =  sig*(1-sig)*grad_input_current\n",
    "\n",
    "        #===========surrogate gradient function (rough rectangle)\n",
    "        # v_minus_th = (v_one_time - v_threshold)\n",
    "        # grad_input_current[v_minus_th <= -.5] = 0\n",
    "        # grad_input_current[v_minus_th > .5] = 0\n",
    "        ###########################################################################################\n",
    "        return grad_input_current, None, None, None, None, None\n",
    "\n",
    "class LIF_layer(nn.Module):\n",
    "    def __init__ (self, v_init = 0.0, v_decay = 0.8, v_threshold = 0.5, v_reset = 0.0, sg_width = 1):\n",
    "        super(LIF_layer, self).__init__()\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.sg_width = sg_width\n",
    "\n",
    "    def forward(self, input_current):\n",
    "        v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "        post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "        # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "        Time = v.shape[0]\n",
    "        for t in range(Time):\n",
    "            # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "            post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                            self.v_decay, self.v_threshold, self.v_reset, self.sg_width) \n",
    "\n",
    "        return post_spike   \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# HEPER PARAMETER\n",
    "TIME = 8\n",
    "BATCH = 256\n",
    "# IMAGE_PIXEL_CHANNEL = 3\n",
    "IMAGE_SIZE = 32\n",
    "# CLASS_NUM = 10\n",
    "\n",
    "###################### Cifar10 ############################\n",
    "####################### DATASET ############################\n",
    "data_path = '/data2/cifar10'\n",
    "transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))]) #https://github.com/kuangliu/pytorch-cifar/issues/19\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform_train)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                          batch_size =BATCH,\n",
    "                          shuffle = True,\n",
    "                          num_workers =2)\n",
    "test_loader = DataLoader(testset,\n",
    "                          batch_size =BATCH,\n",
    "                          shuffle = False,\n",
    "                          num_workers =2)\n",
    "\n",
    "'''\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "'''\n",
    "####################### DATASET END ############################\n",
    "################################################################\n",
    "\n",
    "\n",
    "# ##################### MNIST #############################\n",
    "# ####################### DATASET ############################\n",
    "# data_path = '/data2'\n",
    "# transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "\n",
    "# trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "#                                       train=True,\n",
    "#                                       download=True,\n",
    "#                                       transform=transform)\n",
    "\n",
    "# # 조금만 쓰기\n",
    "# # subset_indices = torch.randperm(len(trainset))[:1000]\n",
    "# # trainset = torch.utils.data.Subset(trainset, subset_indices)\n",
    "\n",
    "# testset = torchvision.datasets.MNIST(root=data_path,\n",
    "#                                      train=False,\n",
    "#                                      download=True,\n",
    "#                                      transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(trainset,\n",
    "#                           batch_size =BATCH,\n",
    "#                           shuffle = True,\n",
    "#                           num_workers =2)\n",
    "# test_loader = DataLoader(testset,\n",
    "#                           batch_size =BATCH,\n",
    "#                           shuffle = False,\n",
    "#                           num_workers =2)\n",
    "# ####################### DATASET END ############################\n",
    "# ################################################################\n",
    "\n",
    "\n",
    "# 데이터 로더에서 첫 번째 배치를 가져옵니다.\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# 채널 수와 클래스 개수를 확인합니다.\n",
    "IMAGE_PIXEL_CHANNEL = images.shape[1]\n",
    "CLASS_NUM = len(torch.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SYNAPSE_CONV 레이어의 하이퍼파라미터\n",
    "synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL\n",
    "# synapse_conv_out_channels = layer별 지정\n",
    "# synapse_conv_kernel_size = layer별 지정\n",
    "synapse_conv_stride = 1\n",
    "synapse_conv_padding = 1\n",
    "synapse_conv_trace_const1 = 1\n",
    "synapse_conv_trace_const2 = 0.7\n",
    "\n",
    "## LIF_layer 레이어의 하이퍼파라미터\n",
    "lif_layer_v_init = 0.0\n",
    "lif_layer_v_decay = 0.5\n",
    "lif_layer_v_threshold = 1.2\n",
    "lif_layer_v_reset = 0.0\n",
    "lif_layer_sg_width = 1\n",
    "\n",
    "## SYNAPSE_FC 레이어의 하이퍼파라미터\n",
    "# synapse_fc_in_features = 마지막CONV_OUT_CHANNEL * H * W\n",
    "synapse_fc_out_features = CLASS_NUM\n",
    "synapse_fc_trace_const1 = 1\n",
    "synapse_fc_trace_const2 = 0.7\n",
    "\n",
    "\n",
    "class MY_SNN_MK1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MY_SNN_MK1, self).__init__()\n",
    "\n",
    "        in_channels = synapse_conv_in_channels\n",
    "        out_channels = 64\n",
    "        self.synapse_conv1 = SYNAPSE_CONV(in_channels=in_channels, \n",
    "                                          out_channels=out_channels, \n",
    "                                          kernel_size=3, \n",
    "                                          stride=synapse_conv_stride, \n",
    "                                          padding=synapse_conv_padding, \n",
    "                                          trace_const1=synapse_conv_trace_const1, \n",
    "                                          trace_const2=synapse_conv_trace_const2)\n",
    "        \n",
    "\n",
    "\n",
    "        in_channels = 64\n",
    "        out_channels = 64\n",
    "        self.synapse_conv2 = SYNAPSE_CONV(in_channels=in_channels, \n",
    "                                          out_channels=out_channels, \n",
    "                                          kernel_size=3, \n",
    "                                          stride=synapse_conv_stride, \n",
    "                                          padding=synapse_conv_padding, \n",
    "                                          trace_const1=synapse_conv_trace_const1, \n",
    "                                          trace_const2=synapse_conv_trace_const2)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.lif_layer = LIF_layer(v_init=lif_layer_v_init, \n",
    "                                   v_decay=lif_layer_v_decay, \n",
    "                                   v_threshold=lif_layer_v_threshold, \n",
    "                                   v_reset=lif_layer_v_reset, \n",
    "                                   sg_width=lif_layer_sg_width)\n",
    "        \n",
    "\n",
    "\n",
    "        self.synapse_FC = SYNAPSE_FC(in_features=out_channels*IMAGE_SIZE*IMAGE_SIZE,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                      out_features=CLASS_NUM, \n",
    "                                      trace_const1=synapse_fc_trace_const1, \n",
    "                                      trace_const2=synapse_fc_trace_const2)\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        \n",
    "        spike_input = self.synapse_conv1(spike_input)\n",
    "        spike_input = self.lif_layer(spike_input)\n",
    "                                     \n",
    "        spike_input = self.synapse_conv2(spike_input)\n",
    "        spike_input = self.lif_layer(spike_input)\n",
    "\n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.synapse_FC(spike_input)\n",
    "        # print('spike_input.shape pre sum', spike_input.shape)\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "        # print('spike_input.shape post sum', spike_input.shape)\n",
    "        return spike_input\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training acc: 11.33%\n",
      "validation acc: 10.29%\n",
      "iter_one_val_time: 1.997922658920288 seconds\n",
      "training acc: 8.20%\n",
      "validation acc: 12.30%\n",
      "iter_one_val_time: 2.0220754146575928 seconds\n",
      "epoch_time: 54.68399477005005 seconds\n",
      "\n",
      "\n",
      "epoch 1\n",
      "training acc: 14.45%\n",
      "validation acc: 14.66%\n",
      "iter_one_val_time: 1.9955949783325195 seconds\n",
      "training acc: 18.75%\n",
      "validation acc: 15.84%\n",
      "iter_one_val_time: 1.9601776599884033 seconds\n",
      "epoch_time: 45.47991609573364 seconds\n",
      "\n",
      "\n",
      "epoch 2\n",
      "training acc: 15.62%\n",
      "validation acc: 16.77%\n",
      "iter_one_val_time: 2.061171054840088 seconds\n",
      "training acc: 16.80%\n",
      "validation acc: 16.84%\n",
      "iter_one_val_time: 2.0819520950317383 seconds\n",
      "epoch_time: 46.0279426574707 seconds\n",
      "\n",
      "\n",
      "epoch 3\n",
      "training acc: 18.36%\n",
      "validation acc: 16.94%\n",
      "iter_one_val_time: 2.0840299129486084 seconds\n",
      "training acc: 18.75%\n",
      "validation acc: 17.75%\n",
      "iter_one_val_time: 1.996851921081543 seconds\n",
      "epoch_time: 45.916043758392334 seconds\n",
      "\n",
      "\n",
      "epoch 4\n",
      "training acc: 16.02%\n",
      "validation acc: 17.53%\n",
      "iter_one_val_time: 1.9867067337036133 seconds\n",
      "training acc: 17.58%\n",
      "validation acc: 18.30%\n",
      "iter_one_val_time: 2.1003286838531494 seconds\n",
      "epoch_time: 45.88574409484863 seconds\n",
      "\n",
      "\n",
      "epoch 5\n",
      "training acc: 17.19%\n",
      "validation acc: 18.59%\n",
      "iter_one_val_time: 2.032343864440918 seconds\n",
      "training acc: 13.67%\n",
      "validation acc: 18.77%\n",
      "iter_one_val_time: 1.956495761871338 seconds\n",
      "epoch_time: 46.143601179122925 seconds\n",
      "\n",
      "\n",
      "epoch 6\n",
      "training acc: 16.80%\n",
      "validation acc: 19.56%\n",
      "iter_one_val_time: 1.9926886558532715 seconds\n",
      "training acc: 22.27%\n",
      "validation acc: 19.27%\n",
      "iter_one_val_time: 1.949730634689331 seconds\n",
      "epoch_time: 46.30546045303345 seconds\n",
      "\n",
      "\n",
      "epoch 7\n",
      "training acc: 12.89%\n",
      "validation acc: 19.77%\n",
      "iter_one_val_time: 2.0669851303100586 seconds\n",
      "training acc: 21.48%\n",
      "validation acc: 19.33%\n",
      "iter_one_val_time: 2.034536600112915 seconds\n",
      "epoch_time: 46.42697858810425 seconds\n",
      "\n",
      "\n",
      "epoch 8\n",
      "training acc: 19.53%\n",
      "validation acc: 20.28%\n",
      "iter_one_val_time: 1.9806771278381348 seconds\n",
      "training acc: 14.45%\n",
      "validation acc: 18.90%\n",
      "iter_one_val_time: 1.910893201828003 seconds\n",
      "epoch_time: 46.27552771568298 seconds\n",
      "\n",
      "\n",
      "epoch 9\n",
      "training acc: 19.92%\n",
      "validation acc: 20.44%\n",
      "iter_one_val_time: 2.0132336616516113 seconds\n",
      "training acc: 17.97%\n",
      "validation acc: 20.11%\n",
      "iter_one_val_time: 1.924259901046753 seconds\n",
      "epoch_time: 46.134119272232056 seconds\n",
      "\n",
      "\n",
      "epoch 10\n",
      "training acc: 21.09%\n",
      "validation acc: 20.02%\n",
      "iter_one_val_time: 1.9016542434692383 seconds\n",
      "training acc: 21.88%\n",
      "validation acc: 20.34%\n",
      "iter_one_val_time: 1.9753153324127197 seconds\n",
      "epoch_time: 46.06420350074768 seconds\n",
      "\n",
      "\n",
      "epoch 11\n",
      "training acc: 17.19%\n",
      "validation acc: 20.13%\n",
      "iter_one_val_time: 2.004340410232544 seconds\n",
      "training acc: 19.92%\n",
      "validation acc: 20.48%\n",
      "iter_one_val_time: 1.9382708072662354 seconds\n",
      "epoch_time: 46.21389198303223 seconds\n",
      "\n",
      "\n",
      "epoch 12\n",
      "training acc: 18.36%\n",
      "validation acc: 20.40%\n",
      "iter_one_val_time: 1.9769093990325928 seconds\n",
      "training acc: 21.48%\n",
      "validation acc: 21.22%\n",
      "iter_one_val_time: 2.0415327548980713 seconds\n",
      "epoch_time: 46.31794786453247 seconds\n",
      "\n",
      "\n",
      "epoch 13\n",
      "training acc: 18.36%\n",
      "validation acc: 21.00%\n",
      "iter_one_val_time: 1.9312481880187988 seconds\n",
      "training acc: 16.41%\n",
      "validation acc: 21.00%\n",
      "iter_one_val_time: 1.8993782997131348 seconds\n",
      "epoch_time: 46.175618410110474 seconds\n",
      "\n",
      "\n",
      "epoch 14\n",
      "training acc: 25.39%\n",
      "validation acc: 20.49%\n",
      "iter_one_val_time: 1.9147496223449707 seconds\n",
      "training acc: 21.88%\n",
      "validation acc: 21.03%\n",
      "iter_one_val_time: 1.974740743637085 seconds\n",
      "epoch_time: 46.23714518547058 seconds\n",
      "\n",
      "\n",
      "epoch 15\n",
      "training acc: 21.09%\n",
      "validation acc: 21.08%\n",
      "iter_one_val_time: 2.0098888874053955 seconds\n",
      "training acc: 19.92%\n",
      "validation acc: 21.08%\n",
      "iter_one_val_time: 1.9215264320373535 seconds\n",
      "epoch_time: 45.99375820159912 seconds\n",
      "\n",
      "\n",
      "epoch 16\n",
      "training acc: 23.05%\n",
      "validation acc: 21.33%\n",
      "iter_one_val_time: 2.0611989498138428 seconds\n",
      "training acc: 24.61%\n",
      "validation acc: 21.25%\n",
      "iter_one_val_time: 2.014207363128662 seconds\n",
      "epoch_time: 46.42648887634277 seconds\n",
      "\n",
      "\n",
      "epoch 17\n",
      "training acc: 17.19%\n",
      "validation acc: 21.22%\n",
      "iter_one_val_time: 1.9697251319885254 seconds\n",
      "training acc: 26.17%\n",
      "validation acc: 21.15%\n",
      "iter_one_val_time: 1.9003934860229492 seconds\n",
      "epoch_time: 46.11992335319519 seconds\n",
      "\n",
      "\n",
      "epoch 18\n",
      "training acc: 23.05%\n",
      "validation acc: 21.34%\n",
      "iter_one_val_time: 1.9953052997589111 seconds\n",
      "training acc: 24.61%\n",
      "validation acc: 21.09%\n",
      "iter_one_val_time: 2.005326986312866 seconds\n",
      "epoch_time: 46.28911328315735 seconds\n",
      "\n",
      "\n",
      "epoch 19\n",
      "training acc: 20.31%\n",
      "validation acc: 22.43%\n",
      "iter_one_val_time: 2.076084852218628 seconds\n",
      "training acc: 24.22%\n",
      "validation acc: 21.26%\n",
      "iter_one_val_time: 2.069650650024414 seconds\n",
      "epoch_time: 46.555399894714355 seconds\n",
      "\n",
      "\n",
      "epoch 20\n",
      "training acc: 20.31%\n",
      "validation acc: 21.88%\n",
      "iter_one_val_time: 1.872126579284668 seconds\n",
      "training acc: 17.58%\n",
      "validation acc: 21.23%\n",
      "iter_one_val_time: 1.94028902053833 seconds\n",
      "epoch_time: 46.353386878967285 seconds\n",
      "\n",
      "\n",
      "epoch 21\n",
      "training acc: 21.88%\n",
      "validation acc: 21.58%\n",
      "iter_one_val_time: 1.9321849346160889 seconds\n",
      "training acc: 21.09%\n",
      "validation acc: 21.42%\n",
      "iter_one_val_time: 1.883619785308838 seconds\n",
      "epoch_time: 45.97266435623169 seconds\n",
      "\n",
      "\n",
      "epoch 22\n",
      "training acc: 20.70%\n",
      "validation acc: 21.51%\n",
      "iter_one_val_time: 2.0226309299468994 seconds\n",
      "training acc: 25.78%\n",
      "validation acc: 21.38%\n",
      "iter_one_val_time: 1.9738876819610596 seconds\n",
      "epoch_time: 46.31909704208374 seconds\n",
      "\n",
      "\n",
      "epoch 23\n",
      "training acc: 17.58%\n",
      "validation acc: 21.49%\n",
      "iter_one_val_time: 2.0171058177948 seconds\n",
      "training acc: 22.27%\n",
      "validation acc: 21.48%\n",
      "iter_one_val_time: 2.0691287517547607 seconds\n",
      "epoch_time: 46.53842043876648 seconds\n",
      "\n",
      "\n",
      "epoch 24\n",
      "training acc: 20.70%\n",
      "validation acc: 21.80%\n",
      "iter_one_val_time: 1.9423320293426514 seconds\n",
      "training acc: 19.53%\n",
      "validation acc: 21.86%\n",
      "iter_one_val_time: 1.9253442287445068 seconds\n",
      "epoch_time: 45.999610900878906 seconds\n",
      "\n",
      "\n",
      "epoch 25\n",
      "training acc: 18.75%\n",
      "validation acc: 22.00%\n",
      "iter_one_val_time: 1.9238455295562744 seconds\n",
      "training acc: 17.97%\n",
      "validation acc: 21.90%\n",
      "iter_one_val_time: 1.9977951049804688 seconds\n",
      "epoch_time: 45.942368507385254 seconds\n",
      "\n",
      "\n",
      "epoch 26\n",
      "training acc: 23.83%\n",
      "validation acc: 21.66%\n",
      "iter_one_val_time: 1.9495742321014404 seconds\n",
      "training acc: 23.83%\n",
      "validation acc: 22.05%\n",
      "iter_one_val_time: 2.000325918197632 seconds\n",
      "epoch_time: 46.35852003097534 seconds\n",
      "\n",
      "\n",
      "epoch 27\n",
      "training acc: 18.75%\n",
      "validation acc: 21.72%\n",
      "iter_one_val_time: 1.985733985900879 seconds\n",
      "training acc: 23.05%\n",
      "validation acc: 22.05%\n",
      "iter_one_val_time: 2.020526647567749 seconds\n",
      "epoch_time: 46.30136060714722 seconds\n",
      "\n",
      "\n",
      "epoch 28\n",
      "training acc: 22.27%\n",
      "validation acc: 21.93%\n",
      "iter_one_val_time: 1.9202685356140137 seconds\n",
      "training acc: 20.31%\n",
      "validation acc: 21.82%\n",
      "iter_one_val_time: 1.9483866691589355 seconds\n",
      "epoch_time: 45.995851039886475 seconds\n",
      "\n",
      "\n",
      "epoch 29\n",
      "training acc: 23.05%\n",
      "validation acc: 22.10%\n",
      "iter_one_val_time: 1.9441461563110352 seconds\n",
      "training acc: 21.88%\n",
      "validation acc: 21.99%\n",
      "iter_one_val_time: 1.9494693279266357 seconds\n",
      "epoch_time: 46.182990074157715 seconds\n",
      "\n",
      "\n",
      "epoch 30\n",
      "training acc: 22.27%\n",
      "validation acc: 21.96%\n",
      "iter_one_val_time: 1.9908967018127441 seconds\n",
      "training acc: 23.44%\n",
      "validation acc: 21.86%\n",
      "iter_one_val_time: 1.8964035511016846 seconds\n",
      "epoch_time: 46.15878987312317 seconds\n",
      "\n",
      "\n",
      "epoch 31\n",
      "training acc: 19.14%\n",
      "validation acc: 22.39%\n",
      "iter_one_val_time: 2.019780158996582 seconds\n",
      "training acc: 18.36%\n",
      "validation acc: 21.99%\n",
      "iter_one_val_time: 2.0524134635925293 seconds\n",
      "epoch_time: 46.32385754585266 seconds\n",
      "\n",
      "\n",
      "epoch 32\n",
      "training acc: 20.31%\n",
      "validation acc: 22.09%\n",
      "iter_one_val_time: 1.9829161167144775 seconds\n",
      "training acc: 25.78%\n",
      "validation acc: 22.11%\n",
      "iter_one_val_time: 1.9477097988128662 seconds\n",
      "epoch_time: 46.26175498962402 seconds\n",
      "\n",
      "\n",
      "epoch 33\n",
      "training acc: 30.08%\n",
      "validation acc: 22.09%\n",
      "iter_one_val_time: 2.0053884983062744 seconds\n",
      "training acc: 23.44%\n",
      "validation acc: 22.42%\n",
      "iter_one_val_time: 1.95369291305542 seconds\n",
      "epoch_time: 46.15106797218323 seconds\n",
      "\n",
      "\n",
      "epoch 34\n",
      "training acc: 17.58%\n",
      "validation acc: 22.27%\n",
      "iter_one_val_time: 2.0477654933929443 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 64\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\u001b[39;00m\n\u001b[1;32m     67\u001b[0m iter_one_train_time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "#########################\n",
    "##### net 선택 #############\n",
    "net = MY_SNN_MK1().to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "val_acc = 0\n",
    "\n",
    "# model 저장해놨던거 가져올거면 아래 코드 써라\n",
    "# net = torch.load(\"net_save/mnist97.pth\")\n",
    "##########################################\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('epoch', epoch)\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        net.train()\n",
    "\n",
    "        # print('\\niter', i)\n",
    "        iter_one_train_time_start = time.time()\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        batch = BATCH \n",
    "        if labels.size(0) != BATCH: \n",
    "            batch = labels.size(0)\n",
    "\n",
    "\n",
    "        ####### training accruacy ######\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted[0:batch] == labels).sum().item()\n",
    "        if i % 100 == 9:\n",
    "            print(f'training acc: {100 * correct / total:.2f}%')\n",
    "        ################################\n",
    "\n",
    "        # print('outputs.shape', outputs.shape)\n",
    "        # print(outputs)\n",
    "        # print('labels.shape', labels.shape)\n",
    "        loss = criterion(outputs[0:batch,:], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "        iter_one_train_time_end = time.time()\n",
    "        elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "        # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "        if i % 100 == 9:\n",
    "            iter_one_val_time_start = time.time()\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                how_many_val_image=0\n",
    "                for data in test_loader:\n",
    "                    how_many_val_image += 1\n",
    "                    images, labels = data\n",
    "                    images = images.repeat(TIME, 1, 1, 1, 1)\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = net(images.permute(1, 0, 2, 3, 4))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    batch = BATCH \n",
    "                    if labels.size(0) != BATCH: \n",
    "                        batch = labels.size(0)\n",
    "                    correct += (predicted[0:batch] == labels).sum().item()\n",
    "                print(f'validation acc: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "            iter_one_val_time_end = time.time()\n",
    "            elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "            print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "            if val_acc < correct / total:\n",
    "                val_acc = correct / total\n",
    "                torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "    epoch_time_end = time.time()\n",
    "    epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "    \n",
    "    print(f\"epoch_time: {epoch_time} seconds\")\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
