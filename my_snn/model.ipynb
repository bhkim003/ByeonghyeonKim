{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "validation acc: 12.16%\n",
      "iter_one_val_time: 1.3670542240142822 seconds\n",
      "validation acc: 52.34%\n",
      "iter_one_val_time: 1.3104736804962158 seconds\n",
      "epoch_time: 35.639596462249756 seconds\n",
      "\n",
      "\n",
      "epoch 1\n",
      "validation acc: 55.12%\n",
      "iter_one_val_time: 1.3030662536621094 seconds\n",
      "validation acc: 64.56%\n",
      "iter_one_val_time: 1.3367977142333984 seconds\n",
      "epoch_time: 26.43061327934265 seconds\n",
      "\n",
      "\n",
      "epoch 2\n",
      "validation acc: 66.61%\n",
      "iter_one_val_time: 1.367187261581421 seconds\n",
      "validation acc: 72.13%\n",
      "iter_one_val_time: 1.4009888172149658 seconds\n",
      "epoch_time: 26.728638887405396 seconds\n",
      "\n",
      "\n",
      "epoch 3\n",
      "validation acc: 71.94%\n",
      "iter_one_val_time: 1.3528149127960205 seconds\n",
      "validation acc: 74.83%\n",
      "iter_one_val_time: 1.2750182151794434 seconds\n",
      "epoch_time: 26.69024085998535 seconds\n",
      "\n",
      "\n",
      "epoch 4\n",
      "validation acc: 75.70%\n",
      "iter_one_val_time: 1.2890114784240723 seconds\n",
      "validation acc: 76.49%\n",
      "iter_one_val_time: 1.28206467628479 seconds\n",
      "epoch_time: 26.76166796684265 seconds\n",
      "\n",
      "\n",
      "epoch 5\n",
      "validation acc: 77.57%\n",
      "iter_one_val_time: 1.3473072052001953 seconds\n",
      "validation acc: 78.31%\n",
      "iter_one_val_time: 1.3126299381256104 seconds\n",
      "epoch_time: 26.83555579185486 seconds\n",
      "\n",
      "\n",
      "epoch 6\n",
      "validation acc: 78.46%\n",
      "iter_one_val_time: 1.3167235851287842 seconds\n",
      "validation acc: 80.13%\n",
      "iter_one_val_time: 1.3579356670379639 seconds\n",
      "epoch_time: 26.840024709701538 seconds\n",
      "\n",
      "\n",
      "epoch 7\n",
      "validation acc: 79.62%\n",
      "iter_one_val_time: 1.324286937713623 seconds\n",
      "validation acc: 81.09%\n",
      "iter_one_val_time: 1.3222200870513916 seconds\n",
      "epoch_time: 26.84294843673706 seconds\n",
      "\n",
      "\n",
      "epoch 8\n",
      "validation acc: 81.08%\n",
      "iter_one_val_time: 1.4238827228546143 seconds\n",
      "validation acc: 81.68%\n",
      "iter_one_val_time: 1.3727164268493652 seconds\n",
      "epoch_time: 26.981044054031372 seconds\n",
      "\n",
      "\n",
      "epoch 9\n",
      "validation acc: 81.98%\n",
      "iter_one_val_time: 1.3450639247894287 seconds\n",
      "validation acc: 83.19%\n",
      "iter_one_val_time: 1.2775852680206299 seconds\n",
      "epoch_time: 26.920657634735107 seconds\n",
      "\n",
      "\n",
      "epoch 10\n",
      "validation acc: 82.30%\n",
      "iter_one_val_time: 1.2673797607421875 seconds\n",
      "validation acc: 83.53%\n",
      "iter_one_val_time: 1.3132424354553223 seconds\n",
      "epoch_time: 26.831233739852905 seconds\n",
      "\n",
      "\n",
      "epoch 11\n",
      "validation acc: 83.18%\n",
      "iter_one_val_time: 1.3110058307647705 seconds\n",
      "validation acc: 83.87%\n",
      "iter_one_val_time: 1.29603910446167 seconds\n",
      "epoch_time: 26.873546361923218 seconds\n",
      "\n",
      "\n",
      "epoch 12\n",
      "validation acc: 81.96%\n",
      "iter_one_val_time: 1.385890245437622 seconds\n",
      "validation acc: 84.10%\n",
      "iter_one_val_time: 1.343702793121338 seconds\n",
      "epoch_time: 26.61599063873291 seconds\n",
      "\n",
      "\n",
      "epoch 13\n",
      "validation acc: 83.03%\n",
      "iter_one_val_time: 1.3761100769042969 seconds\n",
      "validation acc: 84.94%\n",
      "iter_one_val_time: 1.3963556289672852 seconds\n",
      "epoch_time: 26.636893033981323 seconds\n",
      "\n",
      "\n",
      "epoch 14\n",
      "validation acc: 83.91%\n",
      "iter_one_val_time: 1.399904489517212 seconds\n",
      "validation acc: 85.06%\n",
      "iter_one_val_time: 1.4065830707550049 seconds\n",
      "epoch_time: 26.659254789352417 seconds\n",
      "\n",
      "\n",
      "epoch 15\n",
      "validation acc: 84.82%\n",
      "iter_one_val_time: 1.4268066883087158 seconds\n",
      "validation acc: 84.98%\n",
      "iter_one_val_time: 1.335472583770752 seconds\n",
      "epoch_time: 26.527540922164917 seconds\n",
      "\n",
      "\n",
      "epoch 16\n",
      "validation acc: 85.23%\n",
      "iter_one_val_time: 1.3848521709442139 seconds\n",
      "validation acc: 85.90%\n",
      "iter_one_val_time: 1.4135775566101074 seconds\n",
      "epoch_time: 26.712950706481934 seconds\n",
      "\n",
      "\n",
      "epoch 17\n",
      "validation acc: 86.18%\n",
      "iter_one_val_time: 1.3964147567749023 seconds\n",
      "validation acc: 85.99%\n",
      "iter_one_val_time: 1.4476490020751953 seconds\n",
      "epoch_time: 26.741745471954346 seconds\n",
      "\n",
      "\n",
      "epoch 18\n",
      "validation acc: 85.71%\n",
      "iter_one_val_time: 1.3261148929595947 seconds\n",
      "validation acc: 86.77%\n",
      "iter_one_val_time: 1.3943989276885986 seconds\n",
      "epoch_time: 26.569005012512207 seconds\n",
      "\n",
      "\n",
      "epoch 19\n",
      "validation acc: 86.28%\n",
      "iter_one_val_time: 1.3558158874511719 seconds\n",
      "validation acc: 86.94%\n",
      "iter_one_val_time: 1.3862037658691406 seconds\n",
      "epoch_time: 26.61862063407898 seconds\n",
      "\n",
      "\n",
      "epoch 20\n",
      "validation acc: 87.09%\n",
      "iter_one_val_time: 1.4211599826812744 seconds\n",
      "validation acc: 87.15%\n",
      "iter_one_val_time: 1.395456314086914 seconds\n",
      "epoch_time: 26.71588706970215 seconds\n",
      "\n",
      "\n",
      "epoch 21\n",
      "validation acc: 86.82%\n",
      "iter_one_val_time: 1.3668832778930664 seconds\n",
      "validation acc: 85.57%\n",
      "iter_one_val_time: 1.3606042861938477 seconds\n",
      "epoch_time: 26.610612869262695 seconds\n",
      "\n",
      "\n",
      "epoch 22\n",
      "validation acc: 85.66%\n",
      "iter_one_val_time: 1.3369560241699219 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 380\u001b[0m\n\u001b[1;32m    377\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    378\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 380\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\u001b[39;00m\n\u001b[1;32m    383\u001b[0m iter_one_train_time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 메인 셀\n",
    "\n",
    "import sys\n",
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3,4,5\"  # Set the GPU 2 to use\n",
    "\n",
    "data_path = '/data2'\n",
    "\n",
    "class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "        return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        #############밑에부터 수정해라#######\n",
    "        spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = grad_output_current_clone @ weight\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "     \n",
    "class SYNAPSE_FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_FC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Features]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "        spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                      stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_CONV, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Channel, Height, Width]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        Channel = self.out_channels\n",
    "        Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0])\n",
    "        spike_now = torch.zeros_like(spike_detach[0])\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "\n",
    "class LIF_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width):\n",
    "        v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "        spike = (v_one_time >= v_threshold).float() #fire\n",
    "        ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                              torch.tensor([v_threshold], requires_grad=False), \n",
    "                              torch.tensor([v_reset], requires_grad=False), \n",
    "                              torch.tensor([sg_width], requires_grad=False)) # save before reset\n",
    "        v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "        return spike, v_one_time\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_spike, grad_output_v):\n",
    "        v_one_time, v_decay, v_threshold, v_reset, sg_width = ctx.saved_tensors\n",
    "        v_decay=v_decay.item()\n",
    "        v_threshold=v_threshold.item()\n",
    "        v_reset=v_reset.item()\n",
    "        sg_width=sg_width.item()\n",
    "\n",
    "        grad_input_current = grad_output_spike.clone()\n",
    "        # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "        ################ select one of the following surrogate gradient functions ################\n",
    "        #===========surrogate gradient function (rectangle)\n",
    "        grad_input_current = grad_input_current * ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "        #===========surrogate gradient function (sigmoid)\n",
    "        # sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "        # grad_input_current =  sig*(1-sig)*grad_input_current\n",
    "\n",
    "        #===========surrogate gradient function (rough rectangle)\n",
    "        # v_minus_th = (v_one_time - v_threshold)\n",
    "        # grad_input_current[v_minus_th <= -.5] = 0\n",
    "        # grad_input_current[v_minus_th > .5] = 0\n",
    "        ###########################################################################################\n",
    "        return grad_input_current, None, None, None, None, None\n",
    "\n",
    "class LIF_layer(nn.Module):\n",
    "    def __init__ (self, v_init = 0.0, v_decay = 0.8, v_threshold = 0.5, v_reset = 0.0, sg_width = 1):\n",
    "        super(LIF_layer, self).__init__()\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.sg_width = sg_width\n",
    "\n",
    "    def forward(self, input_current):\n",
    "        v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "        post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "        # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "        Time = v.shape[0]\n",
    "        for t in range(Time):\n",
    "            # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "            post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                            self.v_decay, self.v_threshold, self.v_reset, self.sg_width) \n",
    "\n",
    "        return post_spike   \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# HEPER PARAMETER\n",
    "TIME = 8\n",
    "BATCH = 512\n",
    "IMAGE_PIXEL_CHANNEL = 1\n",
    "IMAGE_SIZE = 28\n",
    "CLASS_NUM = 10\n",
    "\n",
    "## SYNAPSE_CONV 레이어의 하이퍼파라미터\n",
    "synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL\n",
    "# synapse_conv_out_channels = layer별 지정\n",
    "# synapse_conv_kernel_size = layer별 지정\n",
    "synapse_conv_stride = 1\n",
    "synapse_conv_padding = 1\n",
    "synapse_conv_trace_const1 = 1\n",
    "synapse_conv_trace_const2 = 0.7\n",
    "\n",
    "## LIF_layer 레이어의 하이퍼파라미터\n",
    "lif_layer_v_init = 0.0\n",
    "lif_layer_v_decay = 0.8\n",
    "lif_layer_v_threshold = 1.2\n",
    "lif_layer_v_reset = 0.0\n",
    "lif_layer_sg_width = 1\n",
    "\n",
    "## SYNAPSE_FC 레이어의 하이퍼파라미터\n",
    "# synapse_fc_in_features = 마지막CONV_OUT_CHANNEL * H * W\n",
    "synapse_fc_out_features = CLASS_NUM\n",
    "synapse_fc_trace_const1 = 1\n",
    "synapse_fc_trace_const2 = 0.7\n",
    "\n",
    "\n",
    "class MY_SNN_MK1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MY_SNN_MK1, self).__init__()\n",
    "\n",
    "        in_channels = synapse_conv_in_channels\n",
    "        out_channels = 64\n",
    "        self.synapse_conv1 = SYNAPSE_CONV(in_channels=in_channels, \n",
    "                                          out_channels=out_channels, \n",
    "                                          kernel_size=3, \n",
    "                                          stride=synapse_conv_stride, \n",
    "                                          padding=synapse_conv_padding, \n",
    "                                          trace_const1=synapse_conv_trace_const1, \n",
    "                                          trace_const2=synapse_conv_trace_const2)\n",
    "        \n",
    "\n",
    "\n",
    "        in_channels = 64\n",
    "        out_channels = 64\n",
    "        self.synapse_conv2 = SYNAPSE_CONV(in_channels=in_channels, \n",
    "                                          out_channels=out_channels, \n",
    "                                          kernel_size=3, \n",
    "                                          stride=synapse_conv_stride, \n",
    "                                          padding=synapse_conv_padding, \n",
    "                                          trace_const1=synapse_conv_trace_const1, \n",
    "                                          trace_const2=synapse_conv_trace_const2)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.lif_layer = LIF_layer(v_init=lif_layer_v_init, \n",
    "                                   v_decay=lif_layer_v_decay, \n",
    "                                   v_threshold=lif_layer_v_threshold, \n",
    "                                   v_reset=lif_layer_v_reset, \n",
    "                                   sg_width=lif_layer_sg_width)\n",
    "        \n",
    "\n",
    "\n",
    "        self.synapse_FC = SYNAPSE_FC(in_features=64*28*28,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                      out_features=CLASS_NUM, \n",
    "                                      trace_const1=synapse_fc_trace_const1, \n",
    "                                      trace_const2=synapse_fc_trace_const2)\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        print('spike_input pre conv1', spike_input.size()\n",
    "        spike_input = self.synapse_conv1(spike_input)\n",
    "        spike_input = self.lif_layer(spike_input)\n",
    "                                     \n",
    "        print('spike_input pre conv2', spike_input.size())\n",
    "        spike_input = self.synapse_conv2(spike_input)\n",
    "        spike_input = self.lif_layer(spike_input)\n",
    "\n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.synapse_FC(spike_input)\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "        return spike_input\n",
    "    \n",
    "\n",
    "############################################################\n",
    "####################### DATASET ############################\n",
    "transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "\n",
    "# 조금만 쓰기\n",
    "# subset_indices = torch.randperm(len(trainset))[:1000]\n",
    "# trainset = torch.utils.data.Subset(trainset, subset_indices)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                          batch_size =BATCH,\n",
    "                          shuffle = True,\n",
    "                          num_workers =2)\n",
    "test_loader = DataLoader(testset,\n",
    "                          batch_size =BATCH,\n",
    "                          shuffle = False,\n",
    "                          num_workers =2)\n",
    "####################### DATASET END ############################\n",
    "################################################################\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# 모델을 GPU로 이동합니다.\n",
    "net = MY_SNN_MK1().to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('epoch', epoch)\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # print('\\niter', i)\n",
    "        iter_one_train_time_start = time.time()\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print('inputs', inputs.size())\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        batch = BATCH \n",
    "        if labels.size(0) != BATCH: \n",
    "            batch = labels.size(0)\n",
    "\n",
    "        loss = criterion(outputs[0:batch,:], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "        iter_one_train_time_end = time.time()\n",
    "        elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "        # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        acc = 0\n",
    "        if i % 100 == 9:\n",
    "            net.eval()\n",
    "            iter_one_val_time_start = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                how_many_val_image=0\n",
    "                for data in test_loader:\n",
    "                    how_many_val_image += 1\n",
    "                    images, labels = data\n",
    "                    images = images.repeat(TIME, 1, 1, 1, 1)\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    batch = BATCH \n",
    "                    if labels.size(0) != BATCH: \n",
    "                        batch = labels.size(0)\n",
    "                    correct += (predicted[0:batch] == labels).sum().item()\n",
    "                    # if how_many_val_image > 10:\n",
    "                    #     break\n",
    "                print(f'validation acc: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "            iter_one_val_time_end = time.time()\n",
    "            elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "            print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "            if acc < correct / total:\n",
    "                acc = correct / total\n",
    "                torch.save(net.state_dict(), \"save_now_net_weights.pth\")\n",
    "                torch.save(net, \"save_now_net.pth\")\n",
    "                torch.save(net.module.state_dict(), \"save_now_net_weights2.pth\")\n",
    "                torch.save(net.module, \"save_now_net2.pth\")\n",
    "    epoch_time_end = time.time()\n",
    "    epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "    print(f\"epoch_time: {epoch_time} seconds\")\n",
    "    print('\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
