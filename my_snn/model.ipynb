{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "        return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        #############밑에부터 수정해라#######\n",
    "        spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = grad_output_current_clone @ weight\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "    \n",
    "class SYNAPSE_FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_FC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Features]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "        spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                    stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_CONV, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Channel, Height, Width]   \n",
    "        # print('spike.shape', spike.shape)\n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        Channel = self.out_channels\n",
    "        Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0])\n",
    "        spike_now = torch.zeros_like(spike_detach[0])\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current\n",
    "\n",
    "\n",
    "\n",
    "class LIF_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width):\n",
    "        v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "        spike = (v_one_time >= v_threshold).float() #fire\n",
    "        ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                            torch.tensor([v_threshold], requires_grad=False), \n",
    "                            torch.tensor([v_reset], requires_grad=False), \n",
    "                            torch.tensor([sg_width], requires_grad=False)) # save before reset\n",
    "        v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "        return spike, v_one_time\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_spike, grad_output_v):\n",
    "        v_one_time, v_decay, v_threshold, v_reset, sg_width = ctx.saved_tensors\n",
    "        v_decay=v_decay.item()\n",
    "        v_threshold=v_threshold.item()\n",
    "        v_reset=v_reset.item()\n",
    "        sg_width=sg_width.item()\n",
    "\n",
    "        grad_input_current = grad_output_spike.clone()\n",
    "        # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "        ################ select one of the following surrogate gradient functions ################\n",
    "        #===========surrogate gradient function (rectangle)\n",
    "        grad_input_current = grad_input_current * ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "        #===========surrogate gradient function (sigmoid)\n",
    "        # sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "        # grad_input_current =  sig*(1-sig)*grad_input_current\n",
    "\n",
    "        #===========surrogate gradient function (rough rectangle)\n",
    "        # v_minus_th = (v_one_time - v_threshold)\n",
    "        # grad_input_current[v_minus_th <= -.5] = 0\n",
    "        # grad_input_current[v_minus_th > .5] = 0\n",
    "        ###########################################################################################\n",
    "        return grad_input_current, None, None, None, None, None\n",
    "\n",
    "class LIF_layer(nn.Module):\n",
    "    def __init__ (self, v_init = 0.0, v_decay = 0.8, v_threshold = 0.5, v_reset = 0.0, sg_width = 1):\n",
    "        super(LIF_layer, self).__init__()\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.sg_width = sg_width\n",
    "\n",
    "    def forward(self, input_current):\n",
    "        v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "        post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "        # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "        Time = v.shape[0]\n",
    "        for t in range(Time):\n",
    "            # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "            post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                            self.v_decay, self.v_threshold, self.v_reset, self.sg_width) \n",
    "\n",
    "        return post_spike \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_layers_conv(cfg, in_c, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width):\n",
    "    \n",
    "    \n",
    "    layers = []\n",
    "    in_channels = in_c\n",
    "    img_size_var = IMAGE_SIZE\n",
    "    for which in cfg:\n",
    "        if which == 'P':\n",
    "            # layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
    "            img_size_var = img_size_var // 2\n",
    "        else:\n",
    "            out_channels = which\n",
    "            layers += [SYNAPSE_CONV(in_channels=in_channels,\n",
    "                                    out_channels=out_channels, \n",
    "                                    kernel_size=synapse_conv_kernel_size, \n",
    "                                    stride=synapse_conv_stride, \n",
    "                                    padding=synapse_conv_padding, \n",
    "                                    trace_const1=synapse_conv_trace_const1, \n",
    "                                    trace_const2=synapse_conv_trace_const2)]\n",
    "            img_size_var = (img_size_var - synapse_conv_kernel_size + 2*synapse_conv_padding)//synapse_conv_stride + 1\n",
    "\n",
    "            layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                    v_decay=lif_layer_v_decay, \n",
    "                                    v_threshold=lif_layer_v_threshold, \n",
    "                                    v_reset=lif_layer_v_reset, \n",
    "                                    sg_width=lif_layer_sg_width)]\n",
    "            in_channels = which\n",
    "\n",
    "    return nn.Sequential(*layers), in_channels, img_size_var\n",
    "\n",
    "\n",
    "\n",
    "class MY_SNN_CONV(nn.Module):\n",
    "    def __init__(self, cfg, in_c, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2):\n",
    "        super(MY_SNN_CONV, self).__init__()\n",
    "\n",
    "        self.layers, self.conv_last_channels, self.img_size_var = make_layers_conv(cfg, in_c, IMAGE_SIZE,\n",
    "                                                                            synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                                                                            synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                                                                            synapse_conv_trace_const2, \n",
    "                                                                            lif_layer_v_init, lif_layer_v_decay, \n",
    "                                                                            lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                                                                            lif_layer_sg_width)\n",
    "\n",
    "        self.synapse_FC = SYNAPSE_FC(in_features=self.conv_last_channels*self.img_size_var*self.img_size_var,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                    out_features=synapse_fc_out_features, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.synapse_FC(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "\n",
    "\n",
    "\n",
    "def make_layers_fc(cfg, in_c, IMAGE_SIZE, out_c,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width):\n",
    "\n",
    "    layers = []\n",
    "    img_size = IMAGE_SIZE\n",
    "    in_channels = in_c * img_size * img_size\n",
    "    class_num = out_c\n",
    "    for which in cfg:\n",
    "        out_channels = which\n",
    "        layers += [SYNAPSE_FC(in_features=in_channels,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                    out_features=out_channels, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)]\n",
    "\n",
    "        layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                v_decay=lif_layer_v_decay, \n",
    "                                v_threshold=lif_layer_v_threshold, \n",
    "                                v_reset=lif_layer_v_reset, \n",
    "                                sg_width=lif_layer_sg_width)]\n",
    "        in_channels = which\n",
    "\n",
    "    \n",
    "    out_channels = class_num\n",
    "    layers += [SYNAPSE_FC(in_features=in_channels,  \n",
    "                                    out_features=out_channels, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)]\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MY_SNN_FC(nn.Module):\n",
    "    def __init__(self, cfg, in_c, IMAGE_SIZE, out_c,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width):\n",
    "        super(MY_SNN_FC, self).__init__()\n",
    "\n",
    "        self.layers = make_layers_fc(cfg, in_c, IMAGE_SIZE, out_c,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width)\n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 8,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = 0.6,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 100,\n",
    "                    verbose_interval = 100\n",
    "                  ):\n",
    "\n",
    "\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (which_data == 'MNIST'):\n",
    "        data_path = '/data2'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "\n",
    "    if (which_data == 'CIFAR10'):\n",
    "        data_path = '/data2/cifar10'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "    if (which_data == 'FASHION_MNIST'):\n",
    "        data_path = '/data2'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "        \n",
    "\n",
    "    data_iter = IMAGE_PIXEL_CHANNEL = iter(train_loader)\n",
    "    images, labels = data_iter.next()\n",
    "\n",
    "    # 채널 수와 클래스 개수를 확인합니다.\n",
    "    synapse_conv_in_channels = images.shape[1]\n",
    "    synapse_fc_out_features = CLASS_NUM = len(torch.unique(labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                     synapse_fc_trace_const1, synapse_fc_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                     synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                     synapse_conv_padding, synapse_conv_trace_const1, \n",
    "                     synapse_conv_trace_const2, \n",
    "                     lif_layer_v_init, lif_layer_v_decay, \n",
    "                     lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                     lif_layer_sg_width,\n",
    "                     synapse_fc_out_features, synapse_fc_trace_const1, synapse_fc_trace_const2).to(device)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    val_acc = 0\n",
    "\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print('epoch', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            net.train()\n",
    "\n",
    "            # print('\\niter', i)\n",
    "            iter_one_train_time_start = time.time()\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            if rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            \n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "\n",
    "\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "\n",
    "\n",
    "            ####### training accruacy ######\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted[0:batch] == labels).sum().item()\n",
    "            if i % verbose_interval == 9:\n",
    "                print(f'training acc: {100 * correct / total:.2f}%')\n",
    "            ################################\n",
    "\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # optimizer.zero_grad()와 loss.backward() 호출 후에 실행해야 합니다.\n",
    "            # if (i % 100 == 9):\n",
    "            #     print('\\n\\nepoch', epoch, 'iter', i)\n",
    "            #     for name, param in net.named_parameters():\n",
    "            #         if param.requires_grad:\n",
    "            #             print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "            # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "            if i % verbose_interval == 9:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    how_many_val_image=0\n",
    "                    for data in test_loader:\n",
    "                        how_many_val_image += 1\n",
    "                        inputs, labels = data\n",
    "            \n",
    "                        if rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "\n",
    "                        \n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                    print(f'validation acc: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "                if val_acc < correct / total:\n",
    "                    val_acc = correct / total\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "        \n",
    "        print(f\"epoch_time: {epoch_time} seconds\")\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training acc: 9.77%\n",
      "validation acc: 9.95%\n",
      "iter_one_val_time: 3.0114831924438477 seconds\n",
      "training acc: 14.84%\n",
      "validation acc: 14.03%\n",
      "iter_one_val_time: 2.9652321338653564 seconds\n",
      "training acc: 21.48%\n",
      "validation acc: 22.36%\n",
      "iter_one_val_time: 2.8394734859466553 seconds\n",
      "epoch_time: 70.53968596458435 seconds\n",
      "\n",
      "\n",
      "epoch 1\n",
      "training acc: 28.12%\n",
      "validation acc: 24.83%\n",
      "iter_one_val_time: 2.5710251331329346 seconds\n",
      "training acc: 24.22%\n",
      "validation acc: 29.57%\n",
      "iter_one_val_time: 2.643888235092163 seconds\n",
      "training acc: 39.84%\n",
      "validation acc: 28.14%\n",
      "iter_one_val_time: 2.7777695655822754 seconds\n",
      "epoch_time: 60.61998009681702 seconds\n",
      "\n",
      "\n",
      "epoch 2\n",
      "training acc: 37.11%\n",
      "validation acc: 34.67%\n",
      "iter_one_val_time: 2.835340738296509 seconds\n",
      "training acc: 44.92%\n",
      "validation acc: 44.66%\n",
      "iter_one_val_time: 2.8635940551757812 seconds\n",
      "training acc: 45.70%\n",
      "validation acc: 47.56%\n",
      "iter_one_val_time: 2.6358370780944824 seconds\n",
      "epoch_time: 61.57899594306946 seconds\n",
      "\n",
      "\n",
      "epoch 3\n",
      "training acc: 44.53%\n",
      "validation acc: 43.13%\n",
      "iter_one_val_time: 2.569619655609131 seconds\n",
      "training acc: 49.22%\n",
      "validation acc: 47.58%\n",
      "iter_one_val_time: 2.6802146434783936 seconds\n",
      "training acc: 55.08%\n",
      "validation acc: 50.72%\n",
      "iter_one_val_time: 2.7857303619384766 seconds\n",
      "epoch_time: 60.63789129257202 seconds\n",
      "\n",
      "\n",
      "epoch 4\n",
      "training acc: 53.52%\n",
      "validation acc: 51.04%\n",
      "iter_one_val_time: 2.768019199371338 seconds\n",
      "training acc: 48.05%\n",
      "validation acc: 54.39%\n",
      "iter_one_val_time: 2.6635212898254395 seconds\n",
      "training acc: 57.03%\n",
      "validation acc: 54.39%\n",
      "iter_one_val_time: 2.7200653553009033 seconds\n",
      "epoch_time: 61.09763169288635 seconds\n",
      "\n",
      "\n",
      "epoch 5\n",
      "training acc: 53.12%\n",
      "validation acc: 55.21%\n",
      "iter_one_val_time: 2.718780755996704 seconds\n",
      "training acc: 57.81%\n",
      "validation acc: 57.90%\n",
      "iter_one_val_time: 2.6027979850769043 seconds\n",
      "training acc: 58.59%\n",
      "validation acc: 59.34%\n",
      "iter_one_val_time: 2.5816731452941895 seconds\n",
      "epoch_time: 61.375147104263306 seconds\n",
      "\n",
      "\n",
      "epoch 6\n",
      "training acc: 63.28%\n",
      "validation acc: 58.90%\n",
      "iter_one_val_time: 2.5903244018554688 seconds\n",
      "training acc: 56.64%\n",
      "validation acc: 59.99%\n",
      "iter_one_val_time: 2.7620456218719482 seconds\n",
      "training acc: 54.69%\n",
      "validation acc: 58.70%\n",
      "iter_one_val_time: 2.572584629058838 seconds\n",
      "epoch_time: 60.787296295166016 seconds\n",
      "\n",
      "\n",
      "epoch 7\n",
      "training acc: 58.59%\n",
      "validation acc: 61.23%\n",
      "iter_one_val_time: 2.7952940464019775 seconds\n",
      "training acc: 57.03%\n",
      "validation acc: 62.77%\n",
      "iter_one_val_time: 2.855985641479492 seconds\n",
      "training acc: 66.41%\n",
      "validation acc: 63.10%\n",
      "iter_one_val_time: 2.745208501815796 seconds\n",
      "epoch_time: 61.380168199539185 seconds\n",
      "\n",
      "\n",
      "epoch 8\n",
      "training acc: 68.36%\n",
      "validation acc: 61.13%\n",
      "iter_one_val_time: 2.7451913356781006 seconds\n",
      "training acc: 61.33%\n",
      "validation acc: 64.25%\n",
      "iter_one_val_time: 2.6973369121551514 seconds\n",
      "training acc: 66.02%\n",
      "validation acc: 65.22%\n",
      "iter_one_val_time: 2.81701397895813 seconds\n",
      "epoch_time: 61.80490851402283 seconds\n",
      "\n",
      "\n",
      "epoch 9\n",
      "training acc: 64.84%\n",
      "validation acc: 63.76%\n",
      "iter_one_val_time: 2.6350607872009277 seconds\n",
      "training acc: 64.45%\n",
      "validation acc: 65.61%\n",
      "iter_one_val_time: 2.600292205810547 seconds\n",
      "training acc: 74.22%\n",
      "validation acc: 66.98%\n",
      "iter_one_val_time: 2.5604629516601562 seconds\n",
      "epoch_time: 60.60130000114441 seconds\n",
      "\n",
      "\n",
      "epoch 10\n",
      "training acc: 61.33%\n",
      "validation acc: 67.29%\n",
      "iter_one_val_time: 2.8069543838500977 seconds\n",
      "training acc: 72.27%\n",
      "validation acc: 67.74%\n",
      "iter_one_val_time: 2.7975268363952637 seconds\n",
      "training acc: 71.48%\n",
      "validation acc: 68.47%\n",
      "iter_one_val_time: 2.617107629776001 seconds\n",
      "epoch_time: 61.548893451690674 seconds\n",
      "\n",
      "\n",
      "epoch 11\n",
      "training acc: 64.45%\n",
      "validation acc: 68.01%\n",
      "iter_one_val_time: 2.7686991691589355 seconds\n",
      "training acc: 52.73%\n",
      "validation acc: 52.08%\n",
      "iter_one_val_time: 2.7493224143981934 seconds\n",
      "training acc: 66.41%\n",
      "validation acc: 62.37%\n",
      "iter_one_val_time: 2.6989052295684814 seconds\n",
      "epoch_time: 61.12746834754944 seconds\n",
      "\n",
      "\n",
      "epoch 12\n",
      "training acc: 59.77%\n",
      "validation acc: 61.65%\n",
      "iter_one_val_time: 2.788769006729126 seconds\n",
      "training acc: 67.97%\n",
      "validation acc: 65.20%\n",
      "iter_one_val_time: 2.6122710704803467 seconds\n",
      "training acc: 62.11%\n",
      "validation acc: 64.99%\n",
      "iter_one_val_time: 3.0450501441955566 seconds\n",
      "epoch_time: 61.14624285697937 seconds\n",
      "\n",
      "\n",
      "epoch 13\n",
      "training acc: 66.80%\n",
      "validation acc: 66.12%\n",
      "iter_one_val_time: 2.872828960418701 seconds\n",
      "training acc: 67.97%\n",
      "validation acc: 67.24%\n",
      "iter_one_val_time: 2.787071943283081 seconds\n",
      "training acc: 62.11%\n",
      "validation acc: 67.25%\n",
      "iter_one_val_time: 2.842494249343872 seconds\n",
      "epoch_time: 61.36844301223755 seconds\n",
      "\n",
      "\n",
      "epoch 14\n",
      "training acc: 72.66%\n",
      "validation acc: 67.67%\n",
      "iter_one_val_time: 2.722405195236206 seconds\n",
      "training acc: 62.89%\n",
      "validation acc: 68.59%\n",
      "iter_one_val_time: 2.9069442749023438 seconds\n",
      "training acc: 72.66%\n",
      "validation acc: 68.03%\n",
      "iter_one_val_time: 2.810882568359375 seconds\n",
      "epoch_time: 61.41173696517944 seconds\n",
      "\n",
      "\n",
      "epoch 15\n",
      "training acc: 66.80%\n",
      "validation acc: 68.53%\n",
      "iter_one_val_time: 2.7321910858154297 seconds\n",
      "training acc: 67.58%\n",
      "validation acc: 66.44%\n",
      "iter_one_val_time: 2.5533273220062256 seconds\n",
      "training acc: 63.28%\n",
      "validation acc: 67.72%\n",
      "iter_one_val_time: 2.677753210067749 seconds\n",
      "epoch_time: 60.793240785598755 seconds\n",
      "\n",
      "\n",
      "epoch 16\n",
      "training acc: 66.41%\n",
      "validation acc: 70.17%\n",
      "iter_one_val_time: 2.9478507041931152 seconds\n",
      "training acc: 71.48%\n",
      "validation acc: 72.40%\n",
      "iter_one_val_time: 2.8035218715667725 seconds\n",
      "training acc: 70.70%\n",
      "validation acc: 73.20%\n",
      "iter_one_val_time: 2.7479519844055176 seconds\n",
      "epoch_time: 61.60798788070679 seconds\n",
      "\n",
      "\n",
      "epoch 17\n",
      "training acc: 73.44%\n",
      "validation acc: 72.92%\n",
      "iter_one_val_time: 2.8253519535064697 seconds\n",
      "training acc: 70.31%\n",
      "validation acc: 73.90%\n",
      "iter_one_val_time: 2.8166933059692383 seconds\n",
      "training acc: 76.17%\n",
      "validation acc: 72.90%\n",
      "iter_one_val_time: 2.574270725250244 seconds\n",
      "epoch_time: 61.316126585006714 seconds\n",
      "\n",
      "\n",
      "epoch 18\n",
      "training acc: 75.00%\n",
      "validation acc: 71.82%\n",
      "iter_one_val_time: 2.5568981170654297 seconds\n",
      "training acc: 69.53%\n",
      "validation acc: 73.55%\n",
      "iter_one_val_time: 2.818146228790283 seconds\n",
      "training acc: 71.09%\n",
      "validation acc: 73.66%\n",
      "iter_one_val_time: 2.9834144115448 seconds\n",
      "epoch_time: 61.11782765388489 seconds\n",
      "\n",
      "\n",
      "epoch 19\n",
      "training acc: 68.75%\n",
      "validation acc: 74.78%\n",
      "iter_one_val_time: 2.926206588745117 seconds\n",
      "training acc: 76.17%\n",
      "validation acc: 74.90%\n",
      "iter_one_val_time: 3.0903964042663574 seconds\n",
      "training acc: 74.22%\n",
      "validation acc: 73.59%\n",
      "iter_one_val_time: 2.9182944297790527 seconds\n",
      "epoch_time: 62.488266944885254 seconds\n",
      "\n",
      "\n",
      "epoch 20\n",
      "training acc: 76.56%\n",
      "validation acc: 74.30%\n",
      "iter_one_val_time: 2.824357032775879 seconds\n",
      "training acc: 72.66%\n",
      "validation acc: 74.64%\n",
      "iter_one_val_time: 2.6260361671447754 seconds\n",
      "training acc: 66.80%\n",
      "validation acc: 72.63%\n",
      "iter_one_val_time: 2.8223865032196045 seconds\n",
      "epoch_time: 61.76267957687378 seconds\n",
      "\n",
      "\n",
      "epoch 21\n",
      "training acc: 75.78%\n",
      "validation acc: 74.42%\n",
      "iter_one_val_time: 2.563593626022339 seconds\n",
      "training acc: 76.17%\n",
      "validation acc: 75.43%\n",
      "iter_one_val_time: 2.81656551361084 seconds\n",
      "training acc: 74.61%\n",
      "validation acc: 76.01%\n",
      "iter_one_val_time: 2.5982236862182617 seconds\n",
      "epoch_time: 61.16833019256592 seconds\n",
      "\n",
      "\n",
      "epoch 22\n",
      "training acc: 77.34%\n",
      "validation acc: 76.10%\n",
      "iter_one_val_time: 2.651299238204956 seconds\n",
      "training acc: 75.00%\n",
      "validation acc: 76.51%\n",
      "iter_one_val_time: 2.784471273422241 seconds\n",
      "training acc: 77.73%\n",
      "validation acc: 76.91%\n",
      "iter_one_val_time: 2.839501142501831 seconds\n",
      "epoch_time: 61.17290019989014 seconds\n",
      "\n",
      "\n",
      "epoch 23\n",
      "training acc: 80.47%\n",
      "validation acc: 77.14%\n",
      "iter_one_val_time: 2.8242857456207275 seconds\n",
      "training acc: 74.61%\n",
      "validation acc: 77.03%\n",
      "iter_one_val_time: 2.8742313385009766 seconds\n",
      "training acc: 70.70%\n",
      "validation acc: 77.00%\n",
      "iter_one_val_time: 2.639221429824829 seconds\n",
      "epoch_time: 61.75209712982178 seconds\n",
      "\n",
      "\n",
      "epoch 24\n",
      "training acc: 76.56%\n",
      "validation acc: 74.95%\n",
      "iter_one_val_time: 2.615122079849243 seconds\n",
      "training acc: 77.34%\n",
      "validation acc: 77.01%\n",
      "iter_one_val_time: 2.810070037841797 seconds\n",
      "training acc: 75.00%\n",
      "validation acc: 77.06%\n",
      "iter_one_val_time: 2.8520009517669678 seconds\n",
      "epoch_time: 61.02523589134216 seconds\n",
      "\n",
      "\n",
      "epoch 25\n",
      "training acc: 74.61%\n",
      "validation acc: 78.18%\n",
      "iter_one_val_time: 2.7448911666870117 seconds\n",
      "training acc: 77.34%\n",
      "validation acc: 76.72%\n",
      "iter_one_val_time: 2.5691840648651123 seconds\n",
      "training acc: 77.73%\n",
      "validation acc: 77.55%\n",
      "iter_one_val_time: 2.7356417179107666 seconds\n",
      "epoch_time: 60.73397254943848 seconds\n",
      "\n",
      "\n",
      "epoch 26\n"
     ]
    }
   ],
   "source": [
    "decay = 0.6\n",
    "\n",
    "my_snn_system(  devices = \"0,1,2,3\",\n",
    "                my_seed = 42,\n",
    "                TIME = 8,\n",
    "                BATCH = 256,\n",
    "                IMAGE_SIZE = 32,\n",
    "                which_data = 'MNIST',# 'CIFAR10' 'MNIST' 'FASION_MNIST'\n",
    "                rate_coding = True, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.2,\n",
    "                lif_layer_v_reset = 0.0,\n",
    "                lif_layer_sg_width = 1,\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "                synapse_conv_trace_const1 = 1,\n",
    "                synapse_conv_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "                synapse_fc_trace_const1 = 1,\n",
    "                synapse_fc_trace_const2 = decay, # lif_layer_v_decay\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = True, # True # False\n",
    "                cfg = [64, 64],\n",
    "                pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                learning_rate = 0.00001,\n",
    "                epoch_num = 100,\n",
    "                verbose_interval = 100,\n",
    "                )\n",
    "\n",
    "    # cfg 종류 = {\n",
    "    # 'A': [64, 64], \n",
    "    # 'B': [64, 64, 64, 64], \n",
    "    # 'C': [64, 128, 256],\n",
    "    # 'D': [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "    # 'K': [64, 64],\n",
    "    # }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
