{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18083/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    \n",
    "                    random_select_ratio = 4,\n",
    "                    leaky_temporal_filter= 1.0,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "\n",
    "                now_T = inputs.shape[1]\n",
    "                window_T = temporal_filter * TIME  # Ìïú Î∞ïÏä§(time window) Í∏∏Ïù¥\n",
    "\n",
    "                # 1) Î∞ïÏä§ Í∞úÏàò Í≥ÑÏÇ∞\n",
    "                num_windows = now_T // window_T   # Îî± Îñ®Ïñ¥ÏßÄÎäî Î∞ïÏä§ Í∞úÏàò\n",
    "\n",
    "                assert num_windows >= 1\n",
    "\n",
    "                # 2) ÎÇ®Îäî ÎÇòÎ®∏ÏßÄ timestepÏùÄ ÏûòÎùºÎ≤ÑÎ¶¨Í≥†, Îî± Î∞ïÏä§ Îã®ÏúÑÍπåÏßÄÎßå ÏÇ¨Ïö©\n",
    "                valid_T = num_windows * window_T\n",
    "                inputs = inputs[:, :valid_T]   # shape: [1, valid_T, C, H, W]\n",
    "\n",
    "                # 3) (B=1, num_windows, window_T, ...) ÌòïÌÉúÎ°ú reshape\n",
    "                B, T, *rest = inputs.shape  # B=1\n",
    "                inputs_reshaped = inputs.view(B, num_windows, window_T, *rest)\n",
    "                # inputs_reshaped: [1, num_windows, window_T, C, H, W] Í∞ÄÏ†ï\n",
    "\n",
    "                # 4) Í∞Å Î∞ïÏä§Î≥Ñ Ìï© (Ïä§ÌååÏù¥ÌÅ¨ Í∞úÏàò) Í≥ÑÏÇ∞\n",
    "                #    dim=(2,3,4,5): window_T, C, H, W Ï†ÑÏ≤¥ Ìï©\n",
    "                window_sums = inputs_reshaped.sum(dim=tuple(range(2, inputs_reshaped.dim())))  # shape: [1, num_windows]\n",
    "                window_sums = window_sums[0]  # [num_windows]\n",
    "\n",
    "                # 5) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ ÏÑ†ÌÉù\n",
    "                #    random_select_ratio ÎπÑÏú®ÎßåÌÅº ÏÉÅÏúÑ Î∞ïÏä§Îßå ÌõÑÎ≥¥Î°ú ÏîÄ\n",
    "                # print(f'num_windows: {num_windows}, random_select_ratio: {random_select_ratio}')\n",
    "                N = min(num_windows, round(random_select_ratio))\n",
    "                # N = max(1, min(num_windows, round(num_windows * random_select_ratio)))\n",
    "                topk_vals, topk_idx = torch.topk(window_sums, k=N)  # top-N Î∞ïÏä§ Ïù∏Îç±Ïä§\n",
    "                # print(f'N: {N}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 6) ÏÉÅÏúÑ NÍ∞ú Î∞ïÏä§ Ï§ëÏóêÏÑú ÎûúÎç§ÌïòÍ≤å ÌïòÎÇò ÏÑ†ÌÉù\n",
    "                #    (python random ÎòêÎäî torch.randperm Îëò Îã§ Í∞ÄÎä•)\n",
    "                rand_pos = random.randint(0, N - 1)\n",
    "                chosen_win_idx = topk_idx[rand_pos].item()  # 0 ~ num_windows-1 Ï§ë ÌïòÎÇò\n",
    "                # print(f'chosen_win_idx: {chosen_win_idx}, topk_vals: {topk_vals}, topk_idx: {topk_idx}')\n",
    "\n",
    "                # 7) ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Í∑∏ Î∞ïÏä§ Íµ¨Í∞ÑÎßå ÏÇ¨Ïö©\n",
    "                start_idx = chosen_win_idx * window_T\n",
    "                end_idx = start_idx + window_T\n",
    "                inputs = inputs[:, start_idx:end_idx]\n",
    "                # shape: [1, window_T, C, H, W]\n",
    "\n",
    "                # print(f'inputs.shape after random select: {inputs.shape}')\n",
    "\n",
    "\n",
    "                if temporal_filter_accumulation == False:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # print(f'inputs.shape: {inputs.shape}')\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if labels.item() == 6:\n",
    "            #     # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            #     ##############################################################################################\n",
    "            #     dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            #     #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if temporal_filter_accumulation == False:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"1\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 random_select_ratio = 4,\n",
    "#                 leaky_temporal_filter= 0.0,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hvjcn4p3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_144428-hvjcn4p3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvjcn4p3' target=\"_blank\">dutiful-sweep-44</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvjcn4p3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvjcn4p3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251114_144435_358', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 374.0\n",
      "lif layer 1 self.abs_max_v: 374.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 486.0\n",
      "lif layer 1 self.abs_max_v: 532.5\n",
      "fc layer 2 self.abs_max_out: 140.0\n",
      "lif layer 2 self.abs_max_v: 140.0\n",
      "fc layer 1 self.abs_max_out: 584.0\n",
      "lif layer 1 self.abs_max_v: 761.5\n",
      "fc layer 2 self.abs_max_out: 278.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "lif layer 1 self.abs_max_v: 785.0\n",
      "fc layer 1 self.abs_max_out: 591.0\n",
      "lif layer 1 self.abs_max_v: 859.5\n",
      "fc layer 2 self.abs_max_out: 323.0\n",
      "lif layer 2 self.abs_max_v: 336.5\n",
      "fc layer 1 self.abs_max_out: 774.0\n",
      "lif layer 1 self.abs_max_v: 1007.5\n",
      "fc layer 2 self.abs_max_out: 330.0\n",
      "lif layer 2 self.abs_max_v: 373.5\n",
      "lif layer 2 self.abs_max_v: 390.0\n",
      "fc layer 2 self.abs_max_out: 372.0\n",
      "lif layer 2 self.abs_max_v: 439.0\n",
      "lif layer 2 self.abs_max_v: 508.5\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 1094.5\n",
      "fc layer 2 self.abs_max_out: 464.0\n",
      "lif layer 2 self.abs_max_v: 672.0\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "lif layer 1 self.abs_max_v: 1103.0\n",
      "fc layer 1 self.abs_max_out: 909.0\n",
      "lif layer 1 self.abs_max_v: 1124.0\n",
      "fc layer 2 self.abs_max_out: 552.0\n",
      "lif layer 2 self.abs_max_v: 722.5\n",
      "fc layer 1 self.abs_max_out: 1097.0\n",
      "lif layer 1 self.abs_max_v: 1133.0\n",
      "fc layer 1 self.abs_max_out: 1489.0\n",
      "lif layer 1 self.abs_max_v: 1489.0\n",
      "lif layer 2 self.abs_max_v: 746.0\n",
      "lif layer 2 self.abs_max_v: 784.5\n",
      "fc layer 2 self.abs_max_out: 556.0\n",
      "fc layer 1 self.abs_max_out: 1649.0\n",
      "lif layer 1 self.abs_max_v: 1649.0\n",
      "lif layer 2 self.abs_max_v: 824.5\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 2 self.abs_max_out: 571.0\n",
      "lif layer 2 self.abs_max_v: 870.0\n",
      "fc layer 2 self.abs_max_out: 717.0\n",
      "lif layer 2 self.abs_max_v: 943.0\n",
      "lif layer 2 self.abs_max_v: 963.5\n",
      "fc layer 2 self.abs_max_out: 772.0\n",
      "lif layer 2 self.abs_max_v: 983.5\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "lif layer 2 self.abs_max_v: 1105.0\n",
      "fc layer 3 self.abs_max_out: 162.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "lif layer 2 self.abs_max_v: 1198.0\n",
      "lif layer 2 self.abs_max_v: 1344.0\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "lif layer 2 self.abs_max_v: 1417.5\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "lif layer 1 self.abs_max_v: 1712.0\n",
      "fc layer 1 self.abs_max_out: 1990.0\n",
      "lif layer 1 self.abs_max_v: 1990.0\n",
      "fc layer 1 self.abs_max_out: 2051.0\n",
      "lif layer 1 self.abs_max_v: 2051.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 1482.0\n",
      "fc layer 1 self.abs_max_out: 2090.0\n",
      "lif layer 1 self.abs_max_v: 2090.0\n",
      "fc layer 1 self.abs_max_out: 2128.0\n",
      "lif layer 1 self.abs_max_v: 2128.0\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "lif layer 1 self.abs_max_v: 2353.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "fc layer 3 self.abs_max_out: 206.0\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "fc layer 2 self.abs_max_out: 1091.0\n",
      "fc layer 2 self.abs_max_out: 1205.0\n",
      "fc layer 2 self.abs_max_out: 1255.0\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "fc layer 1 self.abs_max_out: 2450.0\n",
      "lif layer 1 self.abs_max_v: 2450.0\n",
      "fc layer 1 self.abs_max_out: 2816.0\n",
      "lif layer 1 self.abs_max_v: 2816.0\n",
      "lif layer 2 self.abs_max_v: 1536.5\n",
      "lif layer 2 self.abs_max_v: 1611.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 1662.0\n",
      "lif layer 2 self.abs_max_v: 1702.5\n",
      "lif layer 2 self.abs_max_v: 1719.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 1 self.abs_max_out: 2962.0\n",
      "lif layer 1 self.abs_max_v: 2962.0\n",
      "lif layer 2 self.abs_max_v: 1796.5\n",
      "fc layer 1 self.abs_max_out: 2985.0\n",
      "lif layer 1 self.abs_max_v: 2985.0\n",
      "fc layer 1 self.abs_max_out: 2987.0\n",
      "lif layer 1 self.abs_max_v: 2987.0\n",
      "fc layer 1 self.abs_max_out: 3006.0\n",
      "lif layer 1 self.abs_max_v: 3006.0\n",
      "fc layer 1 self.abs_max_out: 3154.0\n",
      "lif layer 1 self.abs_max_v: 3154.0\n",
      "lif layer 1 self.abs_max_v: 3273.0\n",
      "lif layer 1 self.abs_max_v: 3333.5\n",
      "lif layer 2 self.abs_max_v: 1845.0\n",
      "fc layer 1 self.abs_max_out: 3201.0\n",
      "fc layer 1 self.abs_max_out: 3230.0\n",
      "fc layer 1 self.abs_max_out: 3241.0\n",
      "fc layer 1 self.abs_max_out: 3365.0\n",
      "lif layer 1 self.abs_max_v: 3365.0\n",
      "fc layer 2 self.abs_max_out: 1463.0\n",
      "fc layer 1 self.abs_max_out: 3398.0\n",
      "lif layer 1 self.abs_max_v: 3398.0\n",
      "lif layer 2 self.abs_max_v: 1977.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.214507/  2.209265, val:  34.58%, val_best:  34.58%, tr:  66.91%, tr_best:  66.91%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.6244%\n",
      "layer   2  Sparsity: 88.4539%\n",
      "layer   3  Sparsity: 95.1944%\n",
      "total_backward_count 9790 real_backward_count 5212  53.238%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "fc layer 1 self.abs_max_out: 3446.0\n",
      "lif layer 1 self.abs_max_v: 3446.0\n",
      "fc layer 1 self.abs_max_out: 3463.0\n",
      "lif layer 1 self.abs_max_v: 3463.0\n",
      "fc layer 1 self.abs_max_out: 3493.0\n",
      "lif layer 1 self.abs_max_v: 3493.0\n",
      "lif layer 1 self.abs_max_v: 3733.5\n",
      "lif layer 1 self.abs_max_v: 3862.0\n",
      "lif layer 1 self.abs_max_v: 3914.0\n",
      "lif layer 1 self.abs_max_v: 4052.0\n",
      "fc layer 1 self.abs_max_out: 3561.0\n",
      "fc layer 1 self.abs_max_out: 3562.0\n",
      "fc layer 1 self.abs_max_out: 3598.0\n",
      "fc layer 1 self.abs_max_out: 3700.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.192025/  2.216366, val:  46.25%, val_best:  46.25%, tr:  87.95%, tr_best:  87.95%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.6101%\n",
      "layer   2  Sparsity: 87.7713%\n",
      "layer   3  Sparsity: 93.7892%\n",
      "total_backward_count 19580 real_backward_count 8573  43.784%\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 2 self.abs_max_out: 1579.0\n",
      "lif layer 1 self.abs_max_v: 4076.5\n",
      "fc layer 1 self.abs_max_out: 3712.0\n",
      "fc layer 2 self.abs_max_out: 1639.0\n",
      "lif layer 2 self.abs_max_v: 1997.0\n",
      "lif layer 2 self.abs_max_v: 2025.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "fc layer 2 self.abs_max_out: 1686.0\n",
      "fc layer 2 self.abs_max_out: 1712.0\n",
      "fc layer 2 self.abs_max_out: 1722.0\n",
      "fc layer 1 self.abs_max_out: 3791.0\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "lif layer 2 self.abs_max_v: 2058.5\n",
      "fc layer 1 self.abs_max_out: 3897.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.185552/  2.221227, val:  49.17%, val_best:  49.17%, tr:  93.16%, tr_best:  93.16%, epoch time: 86.06 seconds, 1.43 minutes\n",
      "layer   1  Sparsity: 88.6030%\n",
      "layer   2  Sparsity: 87.4814%\n",
      "layer   3  Sparsity: 92.5710%\n",
      "total_backward_count 29370 real_backward_count 11577  39.418%\n",
      "fc layer 1 self.abs_max_out: 3930.0\n",
      "fc layer 1 self.abs_max_out: 4295.0\n",
      "lif layer 1 self.abs_max_v: 4295.0\n",
      "fc layer 1 self.abs_max_out: 4382.0\n",
      "lif layer 1 self.abs_max_v: 4382.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.188071/  2.225340, val:  39.58%, val_best:  49.17%, tr:  95.10%, tr_best:  95.10%, epoch time: 85.05 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 88.6162%\n",
      "layer   2  Sparsity: 87.4340%\n",
      "layer   3  Sparsity: 91.6226%\n",
      "total_backward_count 39160 real_backward_count 14228  36.333%\n",
      "fc layer 2 self.abs_max_out: 1872.0\n",
      "lif layer 2 self.abs_max_v: 2124.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.184635/  2.212653, val:  41.25%, val_best:  49.17%, tr:  97.34%, tr_best:  97.34%, epoch time: 89.28 seconds, 1.49 minutes\n",
      "layer   1  Sparsity: 88.6104%\n",
      "layer   2  Sparsity: 86.9786%\n",
      "layer   3  Sparsity: 90.9649%\n",
      "total_backward_count 48950 real_backward_count 16585  33.882%\n",
      "lif layer 2 self.abs_max_v: 2246.5\n",
      "fc layer 2 self.abs_max_out: 1875.0\n",
      "fc layer 1 self.abs_max_out: 4456.0\n",
      "lif layer 1 self.abs_max_v: 4456.0\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.180538/  2.228420, val:  47.92%, val_best:  49.17%, tr:  96.73%, tr_best:  97.34%, epoch time: 89.98 seconds, 1.50 minutes\n",
      "layer   1  Sparsity: 88.5970%\n",
      "layer   2  Sparsity: 87.0281%\n",
      "layer   3  Sparsity: 90.6661%\n",
      "total_backward_count 58740 real_backward_count 18828  32.053%\n",
      "fc layer 1 self.abs_max_out: 4476.0\n",
      "lif layer 1 self.abs_max_v: 4476.0\n",
      "fc layer 2 self.abs_max_out: 1949.0\n",
      "fc layer 2 self.abs_max_out: 1980.0\n",
      "lif layer 1 self.abs_max_v: 4674.5\n",
      "lif layer 1 self.abs_max_v: 4694.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.186863/  2.222567, val:  54.17%, val_best:  54.17%, tr:  97.75%, tr_best:  97.75%, epoch time: 93.56 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.5876%\n",
      "layer   2  Sparsity: 87.3325%\n",
      "layer   3  Sparsity: 90.5254%\n",
      "total_backward_count 68530 real_backward_count 21052  30.719%\n",
      "lif layer 1 self.abs_max_v: 4846.0\n",
      "fc layer 2 self.abs_max_out: 2013.0\n",
      "lif layer 1 self.abs_max_v: 5024.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.174676/  2.211912, val:  42.92%, val_best:  54.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 91.66 seconds, 1.53 minutes\n",
      "layer   1  Sparsity: 88.5989%\n",
      "layer   2  Sparsity: 87.0169%\n",
      "layer   3  Sparsity: 89.6634%\n",
      "total_backward_count 78320 real_backward_count 23117  29.516%\n",
      "fc layer 1 self.abs_max_out: 4509.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.175235/  2.202890, val:  53.75%, val_best:  54.17%, tr:  97.85%, tr_best:  99.18%, epoch time: 91.29 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 88.6154%\n",
      "layer   2  Sparsity: 87.2966%\n",
      "layer   3  Sparsity: 89.7038%\n",
      "total_backward_count 88110 real_backward_count 25270  28.680%\n",
      "fc layer 1 self.abs_max_out: 4542.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.165983/  2.212350, val:  51.25%, val_best:  54.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 90.57 seconds, 1.51 minutes\n",
      "layer   1  Sparsity: 88.5997%\n",
      "layer   2  Sparsity: 87.2955%\n",
      "layer   3  Sparsity: 89.4051%\n",
      "total_backward_count 97900 real_backward_count 27261  27.846%\n",
      "lif layer 1 self.abs_max_v: 5216.0\n",
      "lif layer 1 self.abs_max_v: 5259.5\n",
      "fc layer 1 self.abs_max_out: 4549.0\n",
      "fc layer 1 self.abs_max_out: 4569.0\n",
      "fc layer 1 self.abs_max_out: 4599.0\n",
      "fc layer 1 self.abs_max_out: 4678.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.160120/  2.201127, val:  48.75%, val_best:  54.17%, tr:  98.77%, tr_best:  99.18%, epoch time: 92.72 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6256%\n",
      "layer   2  Sparsity: 86.9647%\n",
      "layer   3  Sparsity: 88.8256%\n",
      "total_backward_count 107690 real_backward_count 29172  27.089%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.152179/  2.191458, val:  51.25%, val_best:  54.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 95.15 seconds, 1.59 minutes\n",
      "layer   1  Sparsity: 88.6302%\n",
      "layer   2  Sparsity: 86.6389%\n",
      "layer   3  Sparsity: 88.7364%\n",
      "total_backward_count 117480 real_backward_count 31130  26.498%\n",
      "lif layer 1 self.abs_max_v: 5308.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.155226/  2.203386, val:  35.83%, val_best:  54.17%, tr:  98.67%, tr_best:  99.18%, epoch time: 97.95 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 88.6097%\n",
      "layer   2  Sparsity: 86.6240%\n",
      "layer   3  Sparsity: 89.0245%\n",
      "total_backward_count 127270 real_backward_count 33028  25.951%\n",
      "fc layer 2 self.abs_max_out: 2048.0\n",
      "fc layer 2 self.abs_max_out: 2071.0\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.154773/  2.197130, val:  46.25%, val_best:  54.17%, tr:  98.47%, tr_best:  99.18%, epoch time: 93.71 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6175%\n",
      "layer   2  Sparsity: 86.8050%\n",
      "layer   3  Sparsity: 89.1034%\n",
      "total_backward_count 137060 real_backward_count 34922  25.479%\n",
      "fc layer 1 self.abs_max_out: 4687.0\n",
      "fc layer 2 self.abs_max_out: 2200.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "fc layer 2 self.abs_max_out: 2329.0\n",
      "lif layer 2 self.abs_max_v: 2329.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.150460/  2.201620, val:  53.33%, val_best:  54.17%, tr:  99.28%, tr_best:  99.28%, epoch time: 93.74 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6124%\n",
      "layer   2  Sparsity: 86.5372%\n",
      "layer   3  Sparsity: 88.7824%\n",
      "total_backward_count 146850 real_backward_count 36680  24.978%\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "fc layer 1 self.abs_max_out: 4734.0\n",
      "lif layer 1 self.abs_max_v: 5776.5\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.150595/  2.192587, val:  57.50%, val_best:  57.50%, tr:  99.18%, tr_best:  99.28%, epoch time: 91.09 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 88.6059%\n",
      "layer   2  Sparsity: 86.4609%\n",
      "layer   3  Sparsity: 88.5840%\n",
      "total_backward_count 156640 real_backward_count 38518  24.590%\n",
      "fc layer 1 self.abs_max_out: 4759.0\n",
      "fc layer 1 self.abs_max_out: 4835.0\n",
      "fc layer 1 self.abs_max_out: 4855.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.138488/  2.192827, val:  54.58%, val_best:  57.50%, tr:  98.77%, tr_best:  99.28%, epoch time: 91.21 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 88.5619%\n",
      "layer   2  Sparsity: 86.1043%\n",
      "layer   3  Sparsity: 88.3866%\n",
      "total_backward_count 166430 real_backward_count 40273  24.198%\n",
      "fc layer 1 self.abs_max_out: 4875.0\n",
      "fc layer 1 self.abs_max_out: 4881.0\n",
      "fc layer 1 self.abs_max_out: 4897.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.145848/  2.186115, val:  58.75%, val_best:  58.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 91.04 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 88.6007%\n",
      "layer   2  Sparsity: 86.1594%\n",
      "layer   3  Sparsity: 88.4944%\n",
      "total_backward_count 176220 real_backward_count 42048  23.861%\n",
      "fc layer 1 self.abs_max_out: 4942.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.143627/  2.187439, val:  62.50%, val_best:  62.50%, tr:  98.77%, tr_best:  99.49%, epoch time: 95.26 seconds, 1.59 minutes\n",
      "layer   1  Sparsity: 88.6095%\n",
      "layer   2  Sparsity: 85.6200%\n",
      "layer   3  Sparsity: 88.3430%\n",
      "total_backward_count 186010 real_backward_count 43819  23.557%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.138833/  2.190806, val:  47.08%, val_best:  62.50%, tr:  99.28%, tr_best:  99.49%, epoch time: 93.62 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6064%\n",
      "layer   2  Sparsity: 85.6922%\n",
      "layer   3  Sparsity: 88.0118%\n",
      "total_backward_count 195800 real_backward_count 45523  23.250%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.138905/  2.188722, val:  54.58%, val_best:  62.50%, tr:  99.49%, tr_best:  99.49%, epoch time: 91.20 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 88.5890%\n",
      "layer   2  Sparsity: 85.6000%\n",
      "layer   3  Sparsity: 88.0872%\n",
      "total_backward_count 205590 real_backward_count 47154  22.936%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.142231/  2.192366, val:  55.83%, val_best:  62.50%, tr:  99.49%, tr_best:  99.49%, epoch time: 92.48 seconds, 1.54 minutes\n",
      "layer   1  Sparsity: 88.6408%\n",
      "layer   2  Sparsity: 85.9053%\n",
      "layer   3  Sparsity: 88.3806%\n",
      "total_backward_count 215380 real_backward_count 48914  22.711%\n",
      "lif layer 1 self.abs_max_v: 5794.5\n",
      "fc layer 1 self.abs_max_out: 4951.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.138848/  2.172008, val:  60.00%, val_best:  62.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 91.25 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 88.6182%\n",
      "layer   2  Sparsity: 86.0351%\n",
      "layer   3  Sparsity: 88.1789%\n",
      "total_backward_count 225170 real_backward_count 50593  22.469%\n",
      "fc layer 1 self.abs_max_out: 4965.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.129169/  2.177329, val:  60.83%, val_best:  62.50%, tr:  98.77%, tr_best:  99.59%, epoch time: 93.75 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.5987%\n",
      "layer   2  Sparsity: 85.8958%\n",
      "layer   3  Sparsity: 87.8605%\n",
      "total_backward_count 234960 real_backward_count 52282  22.251%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.133115/  2.174961, val:  60.83%, val_best:  62.50%, tr:  99.18%, tr_best:  99.59%, epoch time: 93.35 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6368%\n",
      "layer   2  Sparsity: 86.2674%\n",
      "layer   3  Sparsity: 88.3273%\n",
      "total_backward_count 244750 real_backward_count 53931  22.035%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.135245/  2.184785, val:  58.75%, val_best:  62.50%, tr:  98.77%, tr_best:  99.59%, epoch time: 93.24 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6221%\n",
      "layer   2  Sparsity: 86.3783%\n",
      "layer   3  Sparsity: 88.4998%\n",
      "total_backward_count 254540 real_backward_count 55656  21.865%\n",
      "lif layer 1 self.abs_max_v: 5915.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.137884/  2.184665, val:  54.58%, val_best:  62.50%, tr:  99.08%, tr_best:  99.59%, epoch time: 94.96 seconds, 1.58 minutes\n",
      "layer   1  Sparsity: 88.6099%\n",
      "layer   2  Sparsity: 86.4151%\n",
      "layer   3  Sparsity: 88.5084%\n",
      "total_backward_count 264330 real_backward_count 57315  21.683%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.137626/  2.177822, val:  64.17%, val_best:  64.17%, tr:  98.98%, tr_best:  99.59%, epoch time: 94.72 seconds, 1.58 minutes\n",
      "layer   1  Sparsity: 88.6090%\n",
      "layer   2  Sparsity: 85.9639%\n",
      "layer   3  Sparsity: 88.0407%\n",
      "total_backward_count 274120 real_backward_count 58933  21.499%\n",
      "fc layer 1 self.abs_max_out: 5053.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.129576/  2.181115, val:  59.17%, val_best:  64.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 93.80 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.5984%\n",
      "layer   2  Sparsity: 86.0815%\n",
      "layer   3  Sparsity: 87.8641%\n",
      "total_backward_count 283910 real_backward_count 60532  21.321%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.126576/  2.165567, val:  52.92%, val_best:  64.17%, tr:  99.08%, tr_best:  99.59%, epoch time: 95.73 seconds, 1.60 minutes\n",
      "layer   1  Sparsity: 88.6150%\n",
      "layer   2  Sparsity: 86.1067%\n",
      "layer   3  Sparsity: 87.6457%\n",
      "total_backward_count 293700 real_backward_count 62155  21.163%\n",
      "lif layer 1 self.abs_max_v: 5960.0\n",
      "lif layer 1 self.abs_max_v: 6058.0\n",
      "lif layer 1 self.abs_max_v: 6068.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.126818/  2.173407, val:  58.75%, val_best:  64.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 93.65 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6102%\n",
      "layer   2  Sparsity: 85.8632%\n",
      "layer   3  Sparsity: 87.4396%\n",
      "total_backward_count 303490 real_backward_count 63738  21.002%\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "fc layer 1 self.abs_max_out: 5062.0\n",
      "lif layer 1 self.abs_max_v: 6076.5\n",
      "lif layer 1 self.abs_max_v: 6100.5\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.130452/  2.184099, val:  55.83%, val_best:  64.17%, tr:  99.39%, tr_best:  99.59%, epoch time: 93.66 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6481%\n",
      "layer   2  Sparsity: 85.9409%\n",
      "layer   3  Sparsity: 87.8927%\n",
      "total_backward_count 313280 real_backward_count 65339  20.856%\n",
      "lif layer 1 self.abs_max_v: 6181.5\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.131739/  2.185982, val:  55.83%, val_best:  64.17%, tr:  99.08%, tr_best:  99.59%, epoch time: 94.47 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.5873%\n",
      "layer   2  Sparsity: 86.1271%\n",
      "layer   3  Sparsity: 88.2465%\n",
      "total_backward_count 323070 real_backward_count 66901  20.708%\n",
      "fc layer 1 self.abs_max_out: 5068.0\n",
      "fc layer 1 self.abs_max_out: 5091.0\n",
      "fc layer 1 self.abs_max_out: 5147.0\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.133225/  2.185700, val:  52.92%, val_best:  64.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 93.88 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6096%\n",
      "layer   2  Sparsity: 86.2228%\n",
      "layer   3  Sparsity: 88.1360%\n",
      "total_backward_count 332860 real_backward_count 68477  20.572%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.124306/  2.173108, val:  55.83%, val_best:  64.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 93.16 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.5919%\n",
      "layer   2  Sparsity: 85.7579%\n",
      "layer   3  Sparsity: 87.7692%\n",
      "total_backward_count 342650 real_backward_count 70099  20.458%\n",
      "fc layer 1 self.abs_max_out: 5192.0\n",
      "lif layer 1 self.abs_max_v: 6472.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.129157/  2.180235, val:  55.42%, val_best:  64.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 95.01 seconds, 1.58 minutes\n",
      "layer   1  Sparsity: 88.6332%\n",
      "layer   2  Sparsity: 85.7538%\n",
      "layer   3  Sparsity: 87.8794%\n",
      "total_backward_count 352440 real_backward_count 71654  20.331%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.123116/  2.166035, val:  65.42%, val_best:  65.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 94.10 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.6222%\n",
      "layer   2  Sparsity: 85.9158%\n",
      "layer   3  Sparsity: 88.0698%\n",
      "total_backward_count 362230 real_backward_count 73166  20.199%\n",
      "fc layer 1 self.abs_max_out: 5199.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.114430/  2.167534, val:  58.75%, val_best:  65.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 93.79 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6114%\n",
      "layer   2  Sparsity: 86.0792%\n",
      "layer   3  Sparsity: 87.9795%\n",
      "total_backward_count 372020 real_backward_count 74660  20.069%\n",
      "fc layer 1 self.abs_max_out: 5217.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.120301/  2.169285, val:  58.33%, val_best:  65.42%, tr:  99.08%, tr_best:  99.80%, epoch time: 93.62 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.5961%\n",
      "layer   2  Sparsity: 86.0987%\n",
      "layer   3  Sparsity: 88.2559%\n",
      "total_backward_count 381810 real_backward_count 76220  19.963%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.119155/  2.167081, val:  60.42%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 93.34 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6114%\n",
      "layer   2  Sparsity: 86.1991%\n",
      "layer   3  Sparsity: 88.1101%\n",
      "total_backward_count 391600 real_backward_count 77712  19.845%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.118472/  2.159438, val:  57.08%, val_best:  65.42%, tr:  99.18%, tr_best:  99.80%, epoch time: 93.44 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6298%\n",
      "layer   2  Sparsity: 86.3235%\n",
      "layer   3  Sparsity: 88.0482%\n",
      "total_backward_count 401390 real_backward_count 79223  19.737%\n",
      "fc layer 1 self.abs_max_out: 5282.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.112028/  2.164937, val:  60.83%, val_best:  65.42%, tr:  99.49%, tr_best:  99.80%, epoch time: 93.48 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6076%\n",
      "layer   2  Sparsity: 86.3692%\n",
      "layer   3  Sparsity: 87.9420%\n",
      "total_backward_count 411180 real_backward_count 80738  19.636%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.109299/  2.160827, val:  55.00%, val_best:  65.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 91.98 seconds, 1.53 minutes\n",
      "layer   1  Sparsity: 88.6143%\n",
      "layer   2  Sparsity: 86.2976%\n",
      "layer   3  Sparsity: 88.0190%\n",
      "total_backward_count 420970 real_backward_count 82218  19.531%\n",
      "lif layer 1 self.abs_max_v: 6473.0\n",
      "lif layer 1 self.abs_max_v: 6673.5\n",
      "lif layer 1 self.abs_max_v: 6677.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.115295/  2.163414, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 93.72 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6113%\n",
      "layer   2  Sparsity: 86.5381%\n",
      "layer   3  Sparsity: 88.1194%\n",
      "total_backward_count 430760 real_backward_count 83683  19.427%\n",
      "lif layer 2 self.abs_max_v: 2414.5\n",
      "fc layer 1 self.abs_max_out: 5293.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.111986/  2.159403, val:  59.58%, val_best:  65.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 92.38 seconds, 1.54 minutes\n",
      "layer   1  Sparsity: 88.6128%\n",
      "layer   2  Sparsity: 85.8882%\n",
      "layer   3  Sparsity: 87.7772%\n",
      "total_backward_count 440550 real_backward_count 85147  19.327%\n",
      "lif layer 1 self.abs_max_v: 6919.5\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.101119/  2.155109, val:  60.42%, val_best:  65.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 90.52 seconds, 1.51 minutes\n",
      "layer   1  Sparsity: 88.6044%\n",
      "layer   2  Sparsity: 85.7210%\n",
      "layer   3  Sparsity: 87.4040%\n",
      "total_backward_count 450340 real_backward_count 86588  19.227%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.106560/  2.159204, val:  56.67%, val_best:  65.83%, tr:  98.88%, tr_best:  99.80%, epoch time: 93.49 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6100%\n",
      "layer   2  Sparsity: 85.9795%\n",
      "layer   3  Sparsity: 87.7285%\n",
      "total_backward_count 460130 real_backward_count 88059  19.138%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.106702/  2.158394, val:  55.00%, val_best:  65.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 94.30 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.6120%\n",
      "layer   2  Sparsity: 85.8993%\n",
      "layer   3  Sparsity: 87.7350%\n",
      "total_backward_count 469920 real_backward_count 89490  19.044%\n",
      "fc layer 1 self.abs_max_out: 5304.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.102904/  2.158873, val:  58.33%, val_best:  65.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 92.94 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6317%\n",
      "layer   2  Sparsity: 85.8795%\n",
      "layer   3  Sparsity: 87.6705%\n",
      "total_backward_count 479710 real_backward_count 90882  18.945%\n",
      "fc layer 1 self.abs_max_out: 5318.0\n",
      "lif layer 1 self.abs_max_v: 7100.0\n",
      "lif layer 1 self.abs_max_v: 7230.0\n",
      "lif layer 1 self.abs_max_v: 7272.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.104633/  2.148333, val:  61.25%, val_best:  65.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 92.77 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6089%\n",
      "layer   2  Sparsity: 85.9776%\n",
      "layer   3  Sparsity: 88.0577%\n",
      "total_backward_count 489500 real_backward_count 92320  18.860%\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 1 self.abs_max_out: 5327.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.105950/  2.160089, val:  56.67%, val_best:  65.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 93.58 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 88.6151%\n",
      "layer   2  Sparsity: 85.8845%\n",
      "layer   3  Sparsity: 88.1007%\n",
      "total_backward_count 499290 real_backward_count 93778  18.782%\n",
      "fc layer 1 self.abs_max_out: 5344.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.104995/  2.152827, val:  60.42%, val_best:  65.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 94.39 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.6137%\n",
      "layer   2  Sparsity: 85.6184%\n",
      "layer   3  Sparsity: 87.8251%\n",
      "total_backward_count 509080 real_backward_count 95171  18.695%\n",
      "fc layer 1 self.abs_max_out: 5398.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.104016/  2.163388, val:  62.08%, val_best:  65.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 94.28 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.6165%\n",
      "layer   2  Sparsity: 85.6943%\n",
      "layer   3  Sparsity: 88.0210%\n",
      "total_backward_count 518870 real_backward_count 96639  18.625%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.108314/  2.150529, val:  64.17%, val_best:  65.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 93.94 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.5960%\n",
      "layer   2  Sparsity: 85.7841%\n",
      "layer   3  Sparsity: 88.0936%\n",
      "total_backward_count 528660 real_backward_count 97985  18.535%\n",
      "fc layer 1 self.abs_max_out: 5413.0\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.108289/  2.160984, val:  61.67%, val_best:  65.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 92.72 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6068%\n",
      "layer   2  Sparsity: 85.9309%\n",
      "layer   3  Sparsity: 88.5108%\n",
      "total_backward_count 538450 real_backward_count 99341  18.449%\n",
      "fc layer 1 self.abs_max_out: 5448.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.109572/  2.157148, val:  67.50%, val_best:  67.50%, tr:  99.28%, tr_best:  99.80%, epoch time: 94.33 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 88.6021%\n",
      "layer   2  Sparsity: 86.0787%\n",
      "layer   3  Sparsity: 88.6746%\n",
      "total_backward_count 548240 real_backward_count 100741  18.375%\n",
      "fc layer 1 self.abs_max_out: 5496.0\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.107263/  2.153048, val:  56.25%, val_best:  67.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 92.97 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6094%\n",
      "layer   2  Sparsity: 85.8715%\n",
      "layer   3  Sparsity: 88.2712%\n",
      "total_backward_count 558030 real_backward_count 102123  18.301%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.098872/  2.151700, val:  57.08%, val_best:  67.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 96.77 seconds, 1.61 minutes\n",
      "layer   1  Sparsity: 88.6239%\n",
      "layer   2  Sparsity: 85.7759%\n",
      "layer   3  Sparsity: 87.8971%\n",
      "total_backward_count 567820 real_backward_count 103440  18.217%\n",
      "fc layer 1 self.abs_max_out: 5509.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.101237/  2.152346, val:  62.92%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 95.74 seconds, 1.60 minutes\n",
      "layer   1  Sparsity: 88.6154%\n",
      "layer   2  Sparsity: 86.1476%\n",
      "layer   3  Sparsity: 88.2457%\n",
      "total_backward_count 577610 real_backward_count 104818  18.147%\n",
      "lif layer 1 self.abs_max_v: 7317.5\n",
      "lif layer 1 self.abs_max_v: 7431.0\n",
      "lif layer 1 self.abs_max_v: 7457.5\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.101121/  2.144422, val:  57.08%, val_best:  67.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 96.29 seconds, 1.60 minutes\n",
      "layer   1  Sparsity: 88.6280%\n",
      "layer   2  Sparsity: 86.1860%\n",
      "layer   3  Sparsity: 88.0448%\n",
      "total_backward_count 587400 real_backward_count 106171  18.075%\n",
      "fc layer 1 self.abs_max_out: 5520.0\n",
      "lif layer 1 self.abs_max_v: 7477.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.094035/  2.151274, val:  60.83%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 92.84 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 88.6247%\n",
      "layer   2  Sparsity: 86.3842%\n",
      "layer   3  Sparsity: 88.2057%\n",
      "total_backward_count 597190 real_backward_count 107517  18.004%\n",
      "lif layer 1 self.abs_max_v: 7723.0\n",
      "lif layer 1 self.abs_max_v: 7854.5\n",
      "lif layer 1 self.abs_max_v: 7946.5\n",
      "lif layer 1 self.abs_max_v: 7959.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.096258/  2.141430, val:  63.75%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 97.94 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 88.6213%\n",
      "layer   2  Sparsity: 86.1780%\n",
      "layer   3  Sparsity: 88.1709%\n",
      "total_backward_count 606980 real_backward_count 108921  17.945%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.094975/  2.152740, val:  61.25%, val_best:  67.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 99.65 seconds, 1.66 minutes\n",
      "layer   1  Sparsity: 88.5726%\n",
      "layer   2  Sparsity: 86.2209%\n",
      "layer   3  Sparsity: 87.9898%\n",
      "total_backward_count 616770 real_backward_count 110275  17.879%\n",
      "fc layer 2 self.abs_max_out: 2346.0\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.087116/  2.133571, val:  64.17%, val_best:  67.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 103.17 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.6023%\n",
      "layer   2  Sparsity: 86.1344%\n",
      "layer   3  Sparsity: 87.7134%\n",
      "total_backward_count 626560 real_backward_count 111553  17.804%\n",
      "fc layer 1 self.abs_max_out: 5530.0\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.085669/  2.141570, val:  63.75%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 99.92 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 88.6373%\n",
      "layer   2  Sparsity: 86.3815%\n",
      "layer   3  Sparsity: 87.7460%\n",
      "total_backward_count 636350 real_backward_count 112875  17.738%\n",
      "fc layer 2 self.abs_max_out: 2455.0\n",
      "lif layer 2 self.abs_max_v: 2455.0\n",
      "fc layer 1 self.abs_max_out: 5536.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.084434/  2.141804, val:  61.25%, val_best:  67.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 104.07 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.5970%\n",
      "layer   2  Sparsity: 86.4651%\n",
      "layer   3  Sparsity: 87.7500%\n",
      "total_backward_count 646140 real_backward_count 114200  17.674%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.089446/  2.143937, val:  62.50%, val_best:  67.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 103.91 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6014%\n",
      "layer   2  Sparsity: 86.2079%\n",
      "layer   3  Sparsity: 87.6988%\n",
      "total_backward_count 655930 real_backward_count 115482  17.606%\n",
      "fc layer 1 self.abs_max_out: 5544.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.087042/  2.139426, val:  59.58%, val_best:  67.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 103.26 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.5895%\n",
      "layer   2  Sparsity: 86.2295%\n",
      "layer   3  Sparsity: 87.7873%\n",
      "total_backward_count 665720 real_backward_count 116788  17.543%\n",
      "fc layer 1 self.abs_max_out: 5549.0\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.087332/  2.142313, val:  64.58%, val_best:  67.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 101.19 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 88.6267%\n",
      "layer   2  Sparsity: 86.2233%\n",
      "layer   3  Sparsity: 87.7758%\n",
      "total_backward_count 675510 real_backward_count 118056  17.477%\n",
      "lif layer 1 self.abs_max_v: 7993.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.094682/  2.148171, val:  57.92%, val_best:  67.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 104.11 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5960%\n",
      "layer   2  Sparsity: 86.3852%\n",
      "layer   3  Sparsity: 87.9767%\n",
      "total_backward_count 685300 real_backward_count 119319  17.411%\n",
      "fc layer 1 self.abs_max_out: 5574.0\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.095897/  2.147294, val:  66.25%, val_best:  67.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 103.62 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6078%\n",
      "layer   2  Sparsity: 86.2384%\n",
      "layer   3  Sparsity: 88.0015%\n",
      "total_backward_count 695090 real_backward_count 120674  17.361%\n",
      "lif layer 2 self.abs_max_v: 2486.0\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.092476/  2.146234, val:  66.25%, val_best:  67.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.99 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6111%\n",
      "layer   2  Sparsity: 86.4525%\n",
      "layer   3  Sparsity: 87.9909%\n",
      "total_backward_count 704880 real_backward_count 121941  17.300%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.087118/  2.135026, val:  68.75%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 102.68 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 88.6127%\n",
      "layer   2  Sparsity: 86.2473%\n",
      "layer   3  Sparsity: 87.7275%\n",
      "total_backward_count 714670 real_backward_count 123233  17.243%\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.081346/  2.130907, val:  63.33%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 103.70 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.5860%\n",
      "layer   2  Sparsity: 86.4728%\n",
      "layer   3  Sparsity: 87.6452%\n",
      "total_backward_count 724460 real_backward_count 124511  17.187%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.080208/  2.143123, val:  60.83%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 105.61 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.5803%\n",
      "layer   2  Sparsity: 86.4337%\n",
      "layer   3  Sparsity: 87.7831%\n",
      "total_backward_count 734250 real_backward_count 125820  17.136%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.082058/  2.128812, val:  67.92%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 104.27 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5748%\n",
      "layer   2  Sparsity: 86.4364%\n",
      "layer   3  Sparsity: 87.4591%\n",
      "total_backward_count 744040 real_backward_count 127044  17.075%\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "lif layer 2 self.abs_max_v: 2647.0\n",
      "lif layer 1 self.abs_max_v: 8097.0\n",
      "lif layer 1 self.abs_max_v: 8237.5\n",
      "lif layer 1 self.abs_max_v: 8250.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.082592/  2.139106, val:  64.17%, val_best:  68.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 102.06 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 88.6241%\n",
      "layer   2  Sparsity: 86.3895%\n",
      "layer   3  Sparsity: 87.8203%\n",
      "total_backward_count 753830 real_backward_count 128358  17.027%\n",
      "lif layer 2 self.abs_max_v: 2662.5\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.081002/  2.143580, val:  60.42%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 102.77 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 88.6165%\n",
      "layer   2  Sparsity: 86.2373%\n",
      "layer   3  Sparsity: 87.8130%\n",
      "total_backward_count 763620 real_backward_count 129570  16.968%\n",
      "fc layer 1 self.abs_max_out: 5609.0\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.084510/  2.138873, val:  60.83%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 104.12 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.6177%\n",
      "layer   2  Sparsity: 86.1316%\n",
      "layer   3  Sparsity: 87.9446%\n",
      "total_backward_count 773410 real_backward_count 130845  16.918%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.080803/  2.139375, val:  66.25%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 104.26 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5949%\n",
      "layer   2  Sparsity: 86.2822%\n",
      "layer   3  Sparsity: 87.9162%\n",
      "total_backward_count 783200 real_backward_count 132063  16.862%\n",
      "fc layer 1 self.abs_max_out: 5610.0\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.074866/  2.125382, val:  65.42%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 105.03 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6160%\n",
      "layer   2  Sparsity: 86.1956%\n",
      "layer   3  Sparsity: 87.5568%\n",
      "total_backward_count 792990 real_backward_count 133273  16.806%\n",
      "fc layer 1 self.abs_max_out: 5630.0\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.071257/  2.137933, val:  48.33%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 102.65 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 88.6133%\n",
      "layer   2  Sparsity: 86.3789%\n",
      "layer   3  Sparsity: 87.8550%\n",
      "total_backward_count 802780 real_backward_count 134460  16.749%\n",
      "fc layer 1 self.abs_max_out: 5663.0\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.066556/  2.112803, val:  64.58%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 103.11 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.6148%\n",
      "layer   2  Sparsity: 86.4778%\n",
      "layer   3  Sparsity: 87.9689%\n",
      "total_backward_count 812570 real_backward_count 135698  16.700%\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.062246/  2.125593, val:  58.75%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 104.02 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6036%\n",
      "layer   2  Sparsity: 86.4161%\n",
      "layer   3  Sparsity: 87.8950%\n",
      "total_backward_count 822360 real_backward_count 136936  16.652%\n",
      "lif layer 1 self.abs_max_v: 8460.5\n",
      "lif layer 1 self.abs_max_v: 8632.5\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.068606/  2.132288, val:  67.08%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.32 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.6153%\n",
      "layer   2  Sparsity: 86.5660%\n",
      "layer   3  Sparsity: 88.0397%\n",
      "total_backward_count 832150 real_backward_count 138166  16.603%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.073285/  2.134838, val:  61.25%, val_best:  68.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 103.90 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6341%\n",
      "layer   2  Sparsity: 86.4786%\n",
      "layer   3  Sparsity: 88.0656%\n",
      "total_backward_count 841940 real_backward_count 139362  16.552%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.071890/  2.124626, val:  67.08%, val_best:  68.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 105.14 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.5841%\n",
      "layer   2  Sparsity: 86.2857%\n",
      "layer   3  Sparsity: 87.9554%\n",
      "total_backward_count 851730 real_backward_count 140550  16.502%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.071279/  2.126254, val:  67.92%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 103.71 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6148%\n",
      "layer   2  Sparsity: 86.4501%\n",
      "layer   3  Sparsity: 87.9587%\n",
      "total_backward_count 861520 real_backward_count 141733  16.452%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.068596/  2.126846, val:  64.17%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 102.92 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.6311%\n",
      "layer   2  Sparsity: 86.5564%\n",
      "layer   3  Sparsity: 87.7907%\n",
      "total_backward_count 871310 real_backward_count 142903  16.401%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.067910/  2.124141, val:  68.33%, val_best:  68.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 103.81 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6076%\n",
      "layer   2  Sparsity: 86.4374%\n",
      "layer   3  Sparsity: 87.8934%\n",
      "total_backward_count 881100 real_backward_count 144092  16.354%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.065871/  2.120729, val:  64.58%, val_best:  68.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 103.95 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6117%\n",
      "layer   2  Sparsity: 86.2685%\n",
      "layer   3  Sparsity: 87.7608%\n",
      "total_backward_count 890890 real_backward_count 145297  16.309%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.068479/  2.124384, val:  71.67%, val_best:  71.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 103.18 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.5733%\n",
      "layer   2  Sparsity: 86.2624%\n",
      "layer   3  Sparsity: 87.9579%\n",
      "total_backward_count 900680 real_backward_count 146393  16.254%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.072037/  2.123823, val:  67.50%, val_best:  71.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 103.33 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.6038%\n",
      "layer   2  Sparsity: 86.1479%\n",
      "layer   3  Sparsity: 88.0111%\n",
      "total_backward_count 910470 real_backward_count 147533  16.204%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.065426/  2.125025, val:  72.92%, val_best:  72.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 103.53 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6104%\n",
      "layer   2  Sparsity: 86.3987%\n",
      "layer   3  Sparsity: 88.1721%\n",
      "total_backward_count 920260 real_backward_count 148658  16.154%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.067590/  2.126725, val:  67.08%, val_best:  72.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 102.82 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 88.6006%\n",
      "layer   2  Sparsity: 86.4105%\n",
      "layer   3  Sparsity: 88.1027%\n",
      "total_backward_count 930050 real_backward_count 149824  16.109%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.058823/  2.114156, val:  67.92%, val_best:  72.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 103.52 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.5702%\n",
      "layer   2  Sparsity: 86.3446%\n",
      "layer   3  Sparsity: 87.7293%\n",
      "total_backward_count 939840 real_backward_count 151029  16.070%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.052418/  2.112687, val:  61.25%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.42 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5829%\n",
      "layer   2  Sparsity: 86.1500%\n",
      "layer   3  Sparsity: 87.5947%\n",
      "total_backward_count 949630 real_backward_count 152101  16.017%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.056153/  2.121371, val:  66.67%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 105.94 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.5953%\n",
      "layer   2  Sparsity: 86.2971%\n",
      "layer   3  Sparsity: 87.7895%\n",
      "total_backward_count 959420 real_backward_count 153218  15.970%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.052857/  2.120801, val:  62.08%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.13 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5743%\n",
      "layer   2  Sparsity: 86.3842%\n",
      "layer   3  Sparsity: 87.9118%\n",
      "total_backward_count 969210 real_backward_count 154356  15.926%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.056800/  2.116878, val:  70.42%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.12 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.5963%\n",
      "layer   2  Sparsity: 86.1650%\n",
      "layer   3  Sparsity: 87.8589%\n",
      "total_backward_count 979000 real_backward_count 155435  15.877%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.054206/  2.112884, val:  72.08%, val_best:  72.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.42 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.6277%\n",
      "layer   2  Sparsity: 85.9876%\n",
      "layer   3  Sparsity: 87.9099%\n",
      "total_backward_count 988790 real_backward_count 156556  15.833%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.053554/  2.112968, val:  70.83%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.56 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6120%\n",
      "layer   2  Sparsity: 86.1565%\n",
      "layer   3  Sparsity: 88.0801%\n",
      "total_backward_count 998580 real_backward_count 157695  15.792%\n",
      "lif layer 2 self.abs_max_v: 2736.5\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.052322/  2.113116, val:  65.00%, val_best:  72.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 103.65 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6064%\n",
      "layer   2  Sparsity: 86.0554%\n",
      "layer   3  Sparsity: 87.9046%\n",
      "total_backward_count 1008370 real_backward_count 158773  15.746%\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.051767/  2.123094, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 102.88 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 88.5938%\n",
      "layer   2  Sparsity: 86.0534%\n",
      "layer   3  Sparsity: 87.7301%\n",
      "total_backward_count 1018160 real_backward_count 159776  15.693%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.057141/  2.112526, val:  71.67%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.71 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6129%\n",
      "layer   2  Sparsity: 85.9099%\n",
      "layer   3  Sparsity: 87.8047%\n",
      "total_backward_count 1027950 real_backward_count 160867  15.649%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.050684/  2.116741, val:  65.42%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 103.31 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.6201%\n",
      "layer   2  Sparsity: 86.0087%\n",
      "layer   3  Sparsity: 87.6755%\n",
      "total_backward_count 1037740 real_backward_count 162023  15.613%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.048821/  2.111292, val:  64.17%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 103.32 seconds, 1.72 minutes\n",
      "layer   1  Sparsity: 88.5969%\n",
      "layer   2  Sparsity: 86.0610%\n",
      "layer   3  Sparsity: 87.7534%\n",
      "total_backward_count 1047530 real_backward_count 163102  15.570%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.048648/  2.106858, val:  58.75%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.98 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6118%\n",
      "layer   2  Sparsity: 85.8479%\n",
      "layer   3  Sparsity: 87.4314%\n",
      "total_backward_count 1057320 real_backward_count 164158  15.526%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.048203/  2.108732, val:  72.08%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 104.98 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.5784%\n",
      "layer   2  Sparsity: 85.9236%\n",
      "layer   3  Sparsity: 87.5195%\n",
      "total_backward_count 1067110 real_backward_count 165195  15.481%\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.043690/  2.108664, val:  71.67%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.86 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 88.5923%\n",
      "layer   2  Sparsity: 86.0797%\n",
      "layer   3  Sparsity: 87.4543%\n",
      "total_backward_count 1076900 real_backward_count 166244  15.437%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.041802/  2.109005, val:  68.33%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.50 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6309%\n",
      "layer   2  Sparsity: 86.0318%\n",
      "layer   3  Sparsity: 87.4266%\n",
      "total_backward_count 1086690 real_backward_count 167302  15.396%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.049314/  2.107569, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.37 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.5845%\n",
      "layer   2  Sparsity: 86.0366%\n",
      "layer   3  Sparsity: 87.4077%\n",
      "total_backward_count 1096480 real_backward_count 168369  15.355%\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "fc layer 1 self.abs_max_out: 5673.0\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.047195/  2.114172, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.49 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6261%\n",
      "layer   2  Sparsity: 86.0365%\n",
      "layer   3  Sparsity: 87.5660%\n",
      "total_backward_count 1106270 real_backward_count 169424  15.315%\n",
      "fc layer 1 self.abs_max_out: 5678.0\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.047418/  2.111787, val:  69.58%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.21 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6071%\n",
      "layer   2  Sparsity: 85.8892%\n",
      "layer   3  Sparsity: 87.4925%\n",
      "total_backward_count 1116060 real_backward_count 170453  15.273%\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.047819/  2.113596, val:  66.25%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 103.62 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6043%\n",
      "layer   2  Sparsity: 86.1788%\n",
      "layer   3  Sparsity: 87.7382%\n",
      "total_backward_count 1125850 real_backward_count 171530  15.236%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.050833/  2.111273, val:  70.00%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.80 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.5878%\n",
      "layer   2  Sparsity: 86.2456%\n",
      "layer   3  Sparsity: 87.7307%\n",
      "total_backward_count 1135640 real_backward_count 172573  15.196%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.048847/  2.112376, val:  62.50%, val_best:  72.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 105.43 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.5967%\n",
      "layer   2  Sparsity: 86.2360%\n",
      "layer   3  Sparsity: 87.7538%\n",
      "total_backward_count 1145430 real_backward_count 173634  15.159%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.052459/  2.107835, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 107.45 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.6139%\n",
      "layer   2  Sparsity: 86.2158%\n",
      "layer   3  Sparsity: 87.7343%\n",
      "total_backward_count 1155220 real_backward_count 174669  15.120%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.050162/  2.115349, val:  63.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.89 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6075%\n",
      "layer   2  Sparsity: 86.1459%\n",
      "layer   3  Sparsity: 87.7520%\n",
      "total_backward_count 1165010 real_backward_count 175713  15.083%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.048729/  2.109674, val:  67.92%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.27 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6465%\n",
      "layer   2  Sparsity: 86.3360%\n",
      "layer   3  Sparsity: 88.1376%\n",
      "total_backward_count 1174800 real_backward_count 176752  15.045%\n",
      "fc layer 1 self.abs_max_out: 5681.0\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.048774/  2.109199, val:  75.83%, val_best:  77.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 106.05 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6098%\n",
      "layer   2  Sparsity: 86.2521%\n",
      "layer   3  Sparsity: 87.9287%\n",
      "total_backward_count 1184590 real_backward_count 177756  15.006%\n",
      "fc layer 1 self.abs_max_out: 5682.0\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.048348/  2.115788, val:  67.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.22 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6158%\n",
      "layer   2  Sparsity: 86.0676%\n",
      "layer   3  Sparsity: 87.8041%\n",
      "total_backward_count 1194380 real_backward_count 178763  14.967%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.047157/  2.109945, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.67 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.6303%\n",
      "layer   2  Sparsity: 85.9193%\n",
      "layer   3  Sparsity: 87.6520%\n",
      "total_backward_count 1204170 real_backward_count 179813  14.933%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.051835/  2.109807, val:  72.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.56 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.5960%\n",
      "layer   2  Sparsity: 86.0054%\n",
      "layer   3  Sparsity: 87.7219%\n",
      "total_backward_count 1213960 real_backward_count 180834  14.896%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.052731/  2.108452, val:  69.17%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 105.79 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6075%\n",
      "layer   2  Sparsity: 86.1259%\n",
      "layer   3  Sparsity: 87.7995%\n",
      "total_backward_count 1223750 real_backward_count 181840  14.859%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.047139/  2.104796, val:  72.92%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.32 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6346%\n",
      "layer   2  Sparsity: 86.2128%\n",
      "layer   3  Sparsity: 87.9074%\n",
      "total_backward_count 1233540 real_backward_count 182833  14.822%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.047112/  2.113691, val:  67.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.25 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6480%\n",
      "layer   2  Sparsity: 86.1288%\n",
      "layer   3  Sparsity: 87.7251%\n",
      "total_backward_count 1243330 real_backward_count 183849  14.787%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.044914/  2.112255, val:  69.17%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.30 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6045%\n",
      "layer   2  Sparsity: 86.0373%\n",
      "layer   3  Sparsity: 87.7507%\n",
      "total_backward_count 1253120 real_backward_count 184816  14.748%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.038212/  2.097087, val:  75.00%, val_best:  77.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 107.37 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.6231%\n",
      "layer   2  Sparsity: 85.9656%\n",
      "layer   3  Sparsity: 87.4580%\n",
      "total_backward_count 1262910 real_backward_count 185779  14.710%\n",
      "fc layer 1 self.abs_max_out: 5685.0\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.033713/  2.092560, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.79 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.5862%\n",
      "layer   2  Sparsity: 85.9135%\n",
      "layer   3  Sparsity: 87.3410%\n",
      "total_backward_count 1272700 real_backward_count 186741  14.673%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.032319/  2.091521, val:  60.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 107.73 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 88.6237%\n",
      "layer   2  Sparsity: 86.0617%\n",
      "layer   3  Sparsity: 87.4049%\n",
      "total_backward_count 1282490 real_backward_count 187702  14.636%\n",
      "fc layer 1 self.abs_max_out: 5694.0\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.031841/  2.099623, val:  72.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 108.27 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 88.6125%\n",
      "layer   2  Sparsity: 85.9389%\n",
      "layer   3  Sparsity: 87.5350%\n",
      "total_backward_count 1292280 real_backward_count 188705  14.602%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.027953/  2.097605, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.80 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.5995%\n",
      "layer   2  Sparsity: 85.9622%\n",
      "layer   3  Sparsity: 87.6374%\n",
      "total_backward_count 1302070 real_backward_count 189600  14.561%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.035530/  2.100154, val:  72.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.19 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6095%\n",
      "layer   2  Sparsity: 85.9913%\n",
      "layer   3  Sparsity: 87.6102%\n",
      "total_backward_count 1311860 real_backward_count 190523  14.523%\n",
      "lif layer 2 self.abs_max_v: 2789.5\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.034714/  2.090131, val:  64.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.44 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6352%\n",
      "layer   2  Sparsity: 85.9525%\n",
      "layer   3  Sparsity: 87.5626%\n",
      "total_backward_count 1321650 real_backward_count 191436  14.485%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.030571/  2.091677, val:  71.25%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.51 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.6111%\n",
      "layer   2  Sparsity: 85.9849%\n",
      "layer   3  Sparsity: 87.7903%\n",
      "total_backward_count 1331440 real_backward_count 192403  14.451%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.031190/  2.085915, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 108.45 seconds, 1.81 minutes\n",
      "layer   1  Sparsity: 88.6177%\n",
      "layer   2  Sparsity: 86.0311%\n",
      "layer   3  Sparsity: 87.8387%\n",
      "total_backward_count 1341230 real_backward_count 193400  14.420%\n",
      "lif layer 2 self.abs_max_v: 2828.0\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.020792/  2.091854, val:  65.83%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.25 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6195%\n",
      "layer   2  Sparsity: 85.8354%\n",
      "layer   3  Sparsity: 87.7484%\n",
      "total_backward_count 1351020 real_backward_count 194359  14.386%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.022586/  2.091102, val:  68.33%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.21 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6020%\n",
      "layer   2  Sparsity: 85.9552%\n",
      "layer   3  Sparsity: 87.9554%\n",
      "total_backward_count 1360810 real_backward_count 195305  14.352%\n",
      "lif layer 2 self.abs_max_v: 2858.0\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.024426/  2.089587, val:  72.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.68 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6169%\n",
      "layer   2  Sparsity: 85.9544%\n",
      "layer   3  Sparsity: 87.8887%\n",
      "total_backward_count 1370600 real_backward_count 196232  14.317%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.021472/  2.089266, val:  64.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 108.20 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 88.5948%\n",
      "layer   2  Sparsity: 85.8927%\n",
      "layer   3  Sparsity: 87.6463%\n",
      "total_backward_count 1380390 real_backward_count 197163  14.283%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.024861/  2.095545, val:  69.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 109.71 seconds, 1.83 minutes\n",
      "layer   1  Sparsity: 88.6265%\n",
      "layer   2  Sparsity: 85.9537%\n",
      "layer   3  Sparsity: 87.9228%\n",
      "total_backward_count 1390180 real_backward_count 198058  14.247%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.025569/  2.088692, val:  67.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.53 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.6234%\n",
      "layer   2  Sparsity: 86.0305%\n",
      "layer   3  Sparsity: 87.8604%\n",
      "total_backward_count 1399970 real_backward_count 198991  14.214%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.024474/  2.086019, val:  72.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.26 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.5725%\n",
      "layer   2  Sparsity: 85.9219%\n",
      "layer   3  Sparsity: 87.7677%\n",
      "total_backward_count 1409760 real_backward_count 199888  14.179%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.021719/  2.089383, val:  67.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 109.29 seconds, 1.82 minutes\n",
      "layer   1  Sparsity: 88.6155%\n",
      "layer   2  Sparsity: 85.9424%\n",
      "layer   3  Sparsity: 87.7253%\n",
      "total_backward_count 1419550 real_backward_count 200818  14.147%\n",
      "fc layer 1 self.abs_max_out: 5697.0\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.020024/  2.087151, val:  70.83%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.82 seconds, 1.80 minutes\n",
      "layer   1  Sparsity: 88.6091%\n",
      "layer   2  Sparsity: 85.9416%\n",
      "layer   3  Sparsity: 87.7024%\n",
      "total_backward_count 1429340 real_backward_count 201759  14.116%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.021963/  2.091497, val:  68.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.93 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6071%\n",
      "layer   2  Sparsity: 85.9224%\n",
      "layer   3  Sparsity: 87.5890%\n",
      "total_backward_count 1439130 real_backward_count 202689  14.084%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.013702/  2.090946, val:  70.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.44 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.6095%\n",
      "layer   2  Sparsity: 86.0324%\n",
      "layer   3  Sparsity: 87.7848%\n",
      "total_backward_count 1448920 real_backward_count 203513  14.046%\n",
      "fc layer 1 self.abs_max_out: 5709.0\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.018088/  2.089459, val:  75.83%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.00 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6031%\n",
      "layer   2  Sparsity: 86.0039%\n",
      "layer   3  Sparsity: 87.7445%\n",
      "total_backward_count 1458710 real_backward_count 204393  14.012%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.019137/  2.080519, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.28 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6127%\n",
      "layer   2  Sparsity: 86.0494%\n",
      "layer   3  Sparsity: 87.8132%\n",
      "total_backward_count 1468500 real_backward_count 205253  13.977%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.013767/  2.088625, val:  73.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.78 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6055%\n",
      "layer   2  Sparsity: 85.9331%\n",
      "layer   3  Sparsity: 87.8493%\n",
      "total_backward_count 1478290 real_backward_count 206092  13.941%\n",
      "fc layer 1 self.abs_max_out: 5712.0\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.021235/  2.095130, val:  70.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.53 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6169%\n",
      "layer   2  Sparsity: 85.8956%\n",
      "layer   3  Sparsity: 87.8331%\n",
      "total_backward_count 1488080 real_backward_count 207015  13.912%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.016569/  2.083817, val:  67.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.62 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6002%\n",
      "layer   2  Sparsity: 86.0486%\n",
      "layer   3  Sparsity: 87.6198%\n",
      "total_backward_count 1497870 real_backward_count 207858  13.877%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.015897/  2.087610, val:  75.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.88 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6096%\n",
      "layer   2  Sparsity: 86.0155%\n",
      "layer   3  Sparsity: 87.6185%\n",
      "total_backward_count 1507660 real_backward_count 208743  13.845%\n",
      "lif layer 2 self.abs_max_v: 2869.5\n",
      "lif layer 2 self.abs_max_v: 2883.5\n",
      "lif layer 2 self.abs_max_v: 2929.0\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.016792/  2.087103, val:  69.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.87 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6266%\n",
      "layer   2  Sparsity: 86.0669%\n",
      "layer   3  Sparsity: 87.7807%\n",
      "total_backward_count 1517450 real_backward_count 209599  13.813%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.011998/  2.089053, val:  68.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.30 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5819%\n",
      "layer   2  Sparsity: 85.9456%\n",
      "layer   3  Sparsity: 87.7187%\n",
      "total_backward_count 1527240 real_backward_count 210416  13.778%\n",
      "lif layer 2 self.abs_max_v: 2993.5\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.011667/  2.089727, val:  74.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.90 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6204%\n",
      "layer   2  Sparsity: 86.0141%\n",
      "layer   3  Sparsity: 87.7218%\n",
      "total_backward_count 1537030 real_backward_count 211273  13.746%\n",
      "fc layer 1 self.abs_max_out: 5730.0\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.015279/  2.085716, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.83 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.5795%\n",
      "layer   2  Sparsity: 85.9447%\n",
      "layer   3  Sparsity: 87.6648%\n",
      "total_backward_count 1546820 real_backward_count 212084  13.711%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.019474/  2.092405, val:  65.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.42 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6081%\n",
      "layer   2  Sparsity: 86.0107%\n",
      "layer   3  Sparsity: 87.7372%\n",
      "total_backward_count 1556610 real_backward_count 212969  13.682%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.019892/  2.092643, val:  72.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.91 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6053%\n",
      "layer   2  Sparsity: 86.1570%\n",
      "layer   3  Sparsity: 87.7937%\n",
      "total_backward_count 1566400 real_backward_count 213805  13.649%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.021379/  2.090513, val:  73.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.73 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6179%\n",
      "layer   2  Sparsity: 86.0445%\n",
      "layer   3  Sparsity: 87.7772%\n",
      "total_backward_count 1576190 real_backward_count 214625  13.617%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.017776/  2.080481, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.31 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5843%\n",
      "layer   2  Sparsity: 86.0221%\n",
      "layer   3  Sparsity: 87.4334%\n",
      "total_backward_count 1585980 real_backward_count 215498  13.588%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.006070/  2.081665, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.73 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6147%\n",
      "layer   2  Sparsity: 85.9465%\n",
      "layer   3  Sparsity: 87.2487%\n",
      "total_backward_count 1595770 real_backward_count 216349  13.558%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.008006/  2.078653, val:  75.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 106.27 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6159%\n",
      "layer   2  Sparsity: 86.1295%\n",
      "layer   3  Sparsity: 87.3970%\n",
      "total_backward_count 1605560 real_backward_count 217169  13.526%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.006299/  2.082663, val:  72.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.01 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.5787%\n",
      "layer   2  Sparsity: 85.9312%\n",
      "layer   3  Sparsity: 87.2183%\n",
      "total_backward_count 1615350 real_backward_count 217960  13.493%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.000924/  2.076222, val:  72.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 103.76 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.5920%\n",
      "layer   2  Sparsity: 86.0526%\n",
      "layer   3  Sparsity: 87.5331%\n",
      "total_backward_count 1625140 real_backward_count 218761  13.461%\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.003022/  2.089303, val:  64.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.45 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.5923%\n",
      "layer   2  Sparsity: 86.0361%\n",
      "layer   3  Sparsity: 87.4833%\n",
      "total_backward_count 1634930 real_backward_count 219486  13.425%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.014135/  2.082787, val:  65.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.95 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6068%\n",
      "layer   2  Sparsity: 86.1106%\n",
      "layer   3  Sparsity: 87.3943%\n",
      "total_backward_count 1644720 real_backward_count 220273  13.393%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.006959/  2.070985, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.47 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6398%\n",
      "layer   2  Sparsity: 86.1126%\n",
      "layer   3  Sparsity: 87.2699%\n",
      "total_backward_count 1654510 real_backward_count 221084  13.363%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.997614/  2.065910, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.69 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6279%\n",
      "layer   2  Sparsity: 86.0313%\n",
      "layer   3  Sparsity: 87.1657%\n",
      "total_backward_count 1664300 real_backward_count 221848  13.330%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.996173/  2.082099, val:  70.00%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 106.03 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6176%\n",
      "layer   2  Sparsity: 86.0411%\n",
      "layer   3  Sparsity: 87.4352%\n",
      "total_backward_count 1674090 real_backward_count 222576  13.295%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.994197/  2.065379, val:  69.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.94 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6442%\n",
      "layer   2  Sparsity: 86.0067%\n",
      "layer   3  Sparsity: 87.2532%\n",
      "total_backward_count 1683880 real_backward_count 223359  13.265%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.994753/  2.071317, val:  72.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.08 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6295%\n",
      "layer   2  Sparsity: 86.0211%\n",
      "layer   3  Sparsity: 87.3373%\n",
      "total_backward_count 1693670 real_backward_count 224154  13.235%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.996394/  2.072984, val:  75.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 105.35 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6148%\n",
      "layer   2  Sparsity: 86.0414%\n",
      "layer   3  Sparsity: 87.2976%\n",
      "total_backward_count 1703460 real_backward_count 224888  13.202%\n",
      "fc layer 1 self.abs_max_out: 5741.0\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.994164/  2.075501, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.53 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6211%\n",
      "layer   2  Sparsity: 85.9986%\n",
      "layer   3  Sparsity: 87.1976%\n",
      "total_backward_count 1713250 real_backward_count 225642  13.170%\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "fc layer 1 self.abs_max_out: 5743.0\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.993506/  2.064849, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.36 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.5796%\n",
      "layer   2  Sparsity: 85.8677%\n",
      "layer   3  Sparsity: 87.1097%\n",
      "total_backward_count 1723040 real_backward_count 226415  13.140%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.986696/  2.062251, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 107.17 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.6118%\n",
      "layer   2  Sparsity: 85.9459%\n",
      "layer   3  Sparsity: 86.9934%\n",
      "total_backward_count 1732830 real_backward_count 227118  13.107%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.984806/  2.066361, val:  71.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.53 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.5967%\n",
      "layer   2  Sparsity: 86.0806%\n",
      "layer   3  Sparsity: 87.2568%\n",
      "total_backward_count 1742620 real_backward_count 227880  13.077%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.982596/  2.070581, val:  77.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.68 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.6200%\n",
      "layer   2  Sparsity: 86.0438%\n",
      "layer   3  Sparsity: 87.4613%\n",
      "total_backward_count 1752410 real_backward_count 228624  13.046%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.986515/  2.070424, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.59 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6177%\n",
      "layer   2  Sparsity: 85.9507%\n",
      "layer   3  Sparsity: 87.1905%\n",
      "total_backward_count 1762200 real_backward_count 229385  13.017%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.990074/  2.065533, val:  67.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.52 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6269%\n",
      "layer   2  Sparsity: 86.0908%\n",
      "layer   3  Sparsity: 87.2829%\n",
      "total_backward_count 1771990 real_backward_count 230110  12.986%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.989801/  2.071889, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.29 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.5947%\n",
      "layer   2  Sparsity: 86.0361%\n",
      "layer   3  Sparsity: 87.4062%\n",
      "total_backward_count 1781780 real_backward_count 230812  12.954%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.992373/  2.069587, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 103.64 seconds, 1.73 minutes\n",
      "layer   1  Sparsity: 88.6079%\n",
      "layer   2  Sparsity: 85.9259%\n",
      "layer   3  Sparsity: 87.4337%\n",
      "total_backward_count 1791570 real_backward_count 231570  12.926%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.989978/  2.074245, val:  68.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.93 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6028%\n",
      "layer   2  Sparsity: 85.9952%\n",
      "layer   3  Sparsity: 87.3299%\n",
      "total_backward_count 1801360 real_backward_count 232255  12.893%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.992607/  2.076835, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.10 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6200%\n",
      "layer   2  Sparsity: 86.0548%\n",
      "layer   3  Sparsity: 87.2449%\n",
      "total_backward_count 1811150 real_backward_count 232952  12.862%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.992956/  2.069530, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.20 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6029%\n",
      "layer   2  Sparsity: 86.1178%\n",
      "layer   3  Sparsity: 87.1345%\n",
      "total_backward_count 1820940 real_backward_count 233757  12.837%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.989587/  2.071633, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.65 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6155%\n",
      "layer   2  Sparsity: 86.0617%\n",
      "layer   3  Sparsity: 87.1635%\n",
      "total_backward_count 1830730 real_backward_count 234489  12.808%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.989997/  2.066229, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 104.62 seconds, 1.74 minutes\n",
      "layer   1  Sparsity: 88.6136%\n",
      "layer   2  Sparsity: 85.8956%\n",
      "layer   3  Sparsity: 87.0807%\n",
      "total_backward_count 1840520 real_backward_count 235239  12.781%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.992708/  2.072125, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.91 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6060%\n",
      "layer   2  Sparsity: 85.9298%\n",
      "layer   3  Sparsity: 87.1724%\n",
      "total_backward_count 1850310 real_backward_count 235992  12.754%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.987867/  2.065712, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.33 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6121%\n",
      "layer   2  Sparsity: 86.0048%\n",
      "layer   3  Sparsity: 87.1635%\n",
      "total_backward_count 1860100 real_backward_count 236677  12.724%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.988213/  2.067422, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.00 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6145%\n",
      "layer   2  Sparsity: 85.9874%\n",
      "layer   3  Sparsity: 87.2489%\n",
      "total_backward_count 1869890 real_backward_count 237386  12.695%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.988004/  2.068282, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.55 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.5968%\n",
      "layer   2  Sparsity: 86.0344%\n",
      "layer   3  Sparsity: 87.4588%\n",
      "total_backward_count 1879680 real_backward_count 238077  12.666%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.982721/  2.067499, val:  79.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 104.76 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6222%\n",
      "layer   2  Sparsity: 86.0281%\n",
      "layer   3  Sparsity: 87.3763%\n",
      "total_backward_count 1889470 real_backward_count 238666  12.631%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.983384/  2.065599, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.59 seconds, 1.78 minutes\n",
      "layer   1  Sparsity: 88.6121%\n",
      "layer   2  Sparsity: 86.0097%\n",
      "layer   3  Sparsity: 87.1902%\n",
      "total_backward_count 1899260 real_backward_count 239339  12.602%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.979139/  2.054948, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.19 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.6021%\n",
      "layer   2  Sparsity: 85.9620%\n",
      "layer   3  Sparsity: 87.2558%\n",
      "total_backward_count 1909050 real_backward_count 240025  12.573%\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.973367/  2.053054, val:  66.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 106.18 seconds, 1.77 minutes\n",
      "layer   1  Sparsity: 88.5961%\n",
      "layer   2  Sparsity: 86.0542%\n",
      "layer   3  Sparsity: 87.2563%\n",
      "total_backward_count 1918840 real_backward_count 240694  12.544%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.971965/  2.054495, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.08 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 88.6017%\n",
      "layer   2  Sparsity: 86.1598%\n",
      "layer   3  Sparsity: 87.2262%\n",
      "total_backward_count 1928630 real_backward_count 241366  12.515%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.976425/  2.060217, val:  77.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.80 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6333%\n",
      "layer   2  Sparsity: 85.9748%\n",
      "layer   3  Sparsity: 87.0434%\n",
      "total_backward_count 1938420 real_backward_count 242056  12.487%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.971243/  2.057750, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 105.41 seconds, 1.76 minutes\n",
      "layer   1  Sparsity: 88.6006%\n",
      "layer   2  Sparsity: 86.0400%\n",
      "layer   3  Sparsity: 87.2397%\n",
      "total_backward_count 1948210 real_backward_count 242700  12.458%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.973585/  2.059026, val:  70.42%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 107.38 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 88.6078%\n",
      "layer   2  Sparsity: 86.0082%\n",
      "layer   3  Sparsity: 87.2614%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd97d9416e4e41dea821eb6968c3a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.97358</td></tr><tr><td>val_acc_best</td><td>0.80833</td></tr><tr><td>val_acc_now</td><td>0.70417</td></tr><tr><td>val_loss</td><td>2.05903</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-sweep-44</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvjcn4p3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hvjcn4p3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_144428-hvjcn4p3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ipkd4dwz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_202512-ipkd4dwz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipkd4dwz' target=\"_blank\">lively-sweep-47</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipkd4dwz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipkd4dwz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251114_202524_365', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 192.0\n",
      "lif layer 1 self.abs_max_v: 192.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 160.0\n",
      "lif layer 2 self.abs_max_v: 160.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 1 self.abs_max_out: 238.0\n",
      "lif layer 1 self.abs_max_v: 247.0\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "lif layer 2 self.abs_max_v: 242.0\n",
      "fc layer 3 self.abs_max_out: 61.0\n",
      "fc layer 1 self.abs_max_out: 285.0\n",
      "lif layer 1 self.abs_max_v: 382.0\n",
      "fc layer 2 self.abs_max_out: 272.0\n",
      "lif layer 2 self.abs_max_v: 369.5\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 2 self.abs_max_out: 296.0\n",
      "fc layer 1 self.abs_max_out: 413.0\n",
      "lif layer 1 self.abs_max_v: 413.0\n",
      "fc layer 2 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 416.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 1 self.abs_max_out: 599.0\n",
      "lif layer 1 self.abs_max_v: 599.0\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 552.5\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "lif layer 1 self.abs_max_v: 779.0\n",
      "fc layer 3 self.abs_max_out: 144.0\n",
      "fc layer 1 self.abs_max_out: 805.0\n",
      "lif layer 1 self.abs_max_v: 805.0\n",
      "fc layer 1 self.abs_max_out: 828.0\n",
      "lif layer 1 self.abs_max_v: 828.0\n",
      "lif layer 2 self.abs_max_v: 578.5\n",
      "fc layer 2 self.abs_max_out: 442.0\n",
      "lif layer 2 self.abs_max_v: 611.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 2 self.abs_max_out: 521.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "lif layer 2 self.abs_max_v: 644.0\n",
      "lif layer 2 self.abs_max_v: 646.0\n",
      "lif layer 2 self.abs_max_v: 684.0\n",
      "lif layer 2 self.abs_max_v: 689.0\n",
      "lif layer 2 self.abs_max_v: 728.5\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "lif layer 2 self.abs_max_v: 857.5\n",
      "fc layer 1 self.abs_max_out: 1287.0\n",
      "lif layer 1 self.abs_max_v: 1287.0\n",
      "fc layer 2 self.abs_max_out: 601.0\n",
      "lif layer 2 self.abs_max_v: 898.5\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "lif layer 2 self.abs_max_v: 1005.5\n",
      "fc layer 2 self.abs_max_out: 651.0\n",
      "fc layer 1 self.abs_max_out: 1366.0\n",
      "lif layer 1 self.abs_max_v: 1729.5\n",
      "lif layer 1 self.abs_max_v: 2075.0\n",
      "fc layer 1 self.abs_max_out: 1613.0\n",
      "lif layer 1 self.abs_max_v: 2130.5\n",
      "fc layer 1 self.abs_max_out: 1764.0\n",
      "lif layer 1 self.abs_max_v: 2308.5\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "fc layer 2 self.abs_max_out: 664.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "lif layer 2 self.abs_max_v: 1040.0\n",
      "lif layer 2 self.abs_max_v: 1049.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 1195.0\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 880.0\n",
      "fc layer 2 self.abs_max_out: 901.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "lif layer 2 self.abs_max_v: 1201.0\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "lif layer 2 self.abs_max_v: 1206.0\n",
      "lif layer 2 self.abs_max_v: 1239.0\n",
      "lif layer 2 self.abs_max_v: 1301.5\n",
      "lif layer 2 self.abs_max_v: 1374.0\n",
      "lif layer 2 self.abs_max_v: 1387.0\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "fc layer 2 self.abs_max_out: 947.0\n",
      "fc layer 2 self.abs_max_out: 1018.0\n",
      "fc layer 2 self.abs_max_out: 1052.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 1 self.abs_max_out: 2049.0\n",
      "lif layer 1 self.abs_max_v: 2340.0\n",
      "lif layer 1 self.abs_max_v: 2405.5\n",
      "lif layer 1 self.abs_max_v: 2471.0\n",
      "fc layer 2 self.abs_max_out: 1068.0\n",
      "fc layer 2 self.abs_max_out: 1072.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 2 self.abs_max_out: 1109.0\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "fc layer 2 self.abs_max_out: 1184.0\n",
      "lif layer 2 self.abs_max_v: 1388.5\n",
      "lif layer 2 self.abs_max_v: 1413.0\n",
      "lif layer 2 self.abs_max_v: 1418.0\n",
      "lif layer 2 self.abs_max_v: 1485.0\n",
      "lif layer 1 self.abs_max_v: 2550.0\n",
      "lif layer 1 self.abs_max_v: 2659.0\n",
      "lif layer 1 self.abs_max_v: 2875.5\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "lif layer 1 self.abs_max_v: 2951.0\n",
      "fc layer 2 self.abs_max_out: 1203.0\n",
      "lif layer 2 self.abs_max_v: 1577.0\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "lif layer 2 self.abs_max_v: 1602.0\n",
      "lif layer 1 self.abs_max_v: 3180.5\n",
      "fc layer 1 self.abs_max_out: 2135.0\n",
      "lif layer 1 self.abs_max_v: 3317.5\n",
      "lif layer 1 self.abs_max_v: 3318.5\n",
      "fc layer 2 self.abs_max_out: 1224.0\n",
      "fc layer 2 self.abs_max_out: 1229.0\n",
      "fc layer 1 self.abs_max_out: 2154.0\n",
      "lif layer 2 self.abs_max_v: 1640.0\n",
      "fc layer 1 self.abs_max_out: 2232.0\n",
      "lif layer 2 self.abs_max_v: 1653.0\n",
      "lif layer 2 self.abs_max_v: 1716.0\n",
      "lif layer 2 self.abs_max_v: 1793.5\n",
      "fc layer 1 self.abs_max_out: 2375.0\n",
      "lif layer 1 self.abs_max_v: 3431.0\n",
      "lif layer 1 self.abs_max_v: 3525.5\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 1 self.abs_max_out: 2486.0\n",
      "fc layer 2 self.abs_max_out: 1255.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "lif layer 2 self.abs_max_v: 1854.5\n",
      "lif layer 2 self.abs_max_v: 1858.0\n",
      "lif layer 2 self.abs_max_v: 1934.0\n",
      "lif layer 2 self.abs_max_v: 1958.0\n",
      "lif layer 1 self.abs_max_v: 3665.5\n",
      "lif layer 1 self.abs_max_v: 3692.5\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "fc layer 1 self.abs_max_out: 2574.0\n",
      "lif layer 1 self.abs_max_v: 3795.5\n",
      "lif layer 1 self.abs_max_v: 4088.0\n",
      "lif layer 1 self.abs_max_v: 4165.0\n",
      "lif layer 1 self.abs_max_v: 4286.5\n",
      "lif layer 1 self.abs_max_v: 4312.5\n",
      "fc layer 1 self.abs_max_out: 2622.0\n",
      "lif layer 2 self.abs_max_v: 1960.5\n",
      "lif layer 2 self.abs_max_v: 1970.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.542283/  1.895365, val:  28.75%, val_best:  28.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 92.73 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 89.9124%\n",
      "layer   2  Sparsity: 75.4202%\n",
      "layer   3  Sparsity: 71.1372%\n",
      "total_backward_count 9790 real_backward_count 1543  15.761%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "lif layer 2 self.abs_max_v: 2032.5\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "lif layer 2 self.abs_max_v: 2098.5\n",
      "lif layer 2 self.abs_max_v: 2100.0\n",
      "fc layer 2 self.abs_max_out: 1298.0\n",
      "fc layer 1 self.abs_max_out: 2715.0\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1310.0\n",
      "fc layer 2 self.abs_max_out: 1352.0\n",
      "fc layer 2 self.abs_max_out: 1492.0\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 1 self.abs_max_out: 2741.0\n",
      "fc layer 1 self.abs_max_out: 2843.0\n",
      "lif layer 1 self.abs_max_v: 4354.0\n",
      "fc layer 1 self.abs_max_out: 2905.0\n",
      "fc layer 1 self.abs_max_out: 2961.0\n",
      "fc layer 1 self.abs_max_out: 2988.0\n",
      "fc layer 1 self.abs_max_out: 3216.0\n",
      "lif layer 1 self.abs_max_v: 4427.0\n",
      "lif layer 1 self.abs_max_v: 4654.5\n",
      "lif layer 1 self.abs_max_v: 4702.5\n",
      "lif layer 1 self.abs_max_v: 4961.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.341670/  1.792337, val:  45.42%, val_best:  45.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 94.21 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 89.9304%\n",
      "layer   2  Sparsity: 74.9143%\n",
      "layer   3  Sparsity: 70.5399%\n",
      "total_backward_count 19580 real_backward_count 2872  14.668%\n",
      "lif layer 2 self.abs_max_v: 2134.0\n",
      "lif layer 2 self.abs_max_v: 2246.5\n",
      "lif layer 2 self.abs_max_v: 2278.5\n",
      "lif layer 2 self.abs_max_v: 2308.5\n",
      "fc layer 2 self.abs_max_out: 1524.0\n",
      "lif layer 2 self.abs_max_v: 2324.0\n",
      "lif layer 2 self.abs_max_v: 2381.0\n",
      "lif layer 2 self.abs_max_v: 2477.5\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "lif layer 2 self.abs_max_v: 2499.0\n",
      "fc layer 1 self.abs_max_out: 3253.0\n",
      "lif layer 2 self.abs_max_v: 2538.5\n",
      "lif layer 1 self.abs_max_v: 5029.5\n",
      "lif layer 1 self.abs_max_v: 5071.5\n",
      "lif layer 1 self.abs_max_v: 5119.0\n",
      "lif layer 2 self.abs_max_v: 2562.0\n",
      "lif layer 1 self.abs_max_v: 5162.5\n",
      "lif layer 2 self.abs_max_v: 2566.5\n",
      "lif layer 2 self.abs_max_v: 2609.5\n",
      "lif layer 2 self.abs_max_v: 2626.0\n",
      "lif layer 2 self.abs_max_v: 2653.0\n",
      "fc layer 1 self.abs_max_out: 3319.0\n",
      "lif layer 2 self.abs_max_v: 2693.5\n",
      "lif layer 2 self.abs_max_v: 2766.0\n",
      "lif layer 1 self.abs_max_v: 5580.0\n",
      "lif layer 1 self.abs_max_v: 5723.0\n",
      "fc layer 1 self.abs_max_out: 3376.0\n",
      "lif layer 1 self.abs_max_v: 5814.5\n",
      "fc layer 1 self.abs_max_out: 3517.0\n",
      "fc layer 1 self.abs_max_out: 3620.0\n",
      "lif layer 1 self.abs_max_v: 6509.5\n",
      "lif layer 1 self.abs_max_v: 6756.0\n",
      "lif layer 1 self.abs_max_v: 6986.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.317967/  1.763711, val:  38.33%, val_best:  45.42%, tr:  99.49%, tr_best:  99.69%, epoch time: 90.63 seconds, 1.51 minutes\n",
      "layer   1  Sparsity: 89.9077%\n",
      "layer   2  Sparsity: 74.7223%\n",
      "layer   3  Sparsity: 71.6592%\n",
      "total_backward_count 29370 real_backward_count 4209  14.331%\n",
      "fc layer 2 self.abs_max_out: 1545.0\n",
      "fc layer 2 self.abs_max_out: 1547.0\n",
      "fc layer 2 self.abs_max_out: 1599.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "lif layer 2 self.abs_max_v: 2771.0\n",
      "fc layer 1 self.abs_max_out: 3851.0\n",
      "fc layer 1 self.abs_max_out: 3956.0\n",
      "lif layer 1 self.abs_max_v: 7161.5\n",
      "lif layer 1 self.abs_max_v: 7319.0\n",
      "lif layer 1 self.abs_max_v: 7565.5\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "lif layer 2 self.abs_max_v: 2909.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.296535/  1.712101, val:  48.33%, val_best:  48.33%, tr:  99.69%, tr_best:  99.69%, epoch time: 90.38 seconds, 1.51 minutes\n",
      "layer   1  Sparsity: 89.9152%\n",
      "layer   2  Sparsity: 76.0636%\n",
      "layer   3  Sparsity: 72.7715%\n",
      "total_backward_count 39160 real_backward_count 5564  14.208%\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 808.0\n",
      "fc layer 3 self.abs_max_out: 844.0\n",
      "fc layer 3 self.abs_max_out: 864.0\n",
      "fc layer 3 self.abs_max_out: 874.0\n",
      "fc layer 2 self.abs_max_out: 1693.0\n",
      "fc layer 1 self.abs_max_out: 4114.0\n",
      "fc layer 1 self.abs_max_out: 4231.0\n",
      "lif layer 1 self.abs_max_v: 7684.5\n",
      "lif layer 1 self.abs_max_v: 7799.5\n",
      "lif layer 1 self.abs_max_v: 8046.0\n",
      "lif layer 2 self.abs_max_v: 2953.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.276128/  1.677369, val:  44.17%, val_best:  48.33%, tr:  99.39%, tr_best:  99.69%, epoch time: 91.16 seconds, 1.52 minutes\n",
      "layer   1  Sparsity: 89.9287%\n",
      "layer   2  Sparsity: 75.5101%\n",
      "layer   3  Sparsity: 72.8110%\n",
      "total_backward_count 48950 real_backward_count 6838  13.969%\n",
      "fc layer 2 self.abs_max_out: 1771.0\n",
      "fc layer 1 self.abs_max_out: 4301.0\n",
      "lif layer 1 self.abs_max_v: 8177.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.254267/  1.695402, val:  40.42%, val_best:  48.33%, tr:  99.49%, tr_best:  99.69%, epoch time: 88.67 seconds, 1.48 minutes\n",
      "layer   1  Sparsity: 89.9016%\n",
      "layer   2  Sparsity: 74.9033%\n",
      "layer   3  Sparsity: 72.1041%\n",
      "total_backward_count 58740 real_backward_count 8078  13.752%\n",
      "lif layer 2 self.abs_max_v: 3006.0\n",
      "fc layer 2 self.abs_max_out: 1850.0\n",
      "lif layer 2 self.abs_max_v: 3097.5\n",
      "lif layer 2 self.abs_max_v: 3151.5\n",
      "lif layer 2 self.abs_max_v: 3242.0\n",
      "lif layer 2 self.abs_max_v: 3388.5\n",
      "lif layer 2 self.abs_max_v: 3424.0\n",
      "fc layer 2 self.abs_max_out: 1856.0\n",
      "lif layer 2 self.abs_max_v: 3473.5\n",
      "lif layer 2 self.abs_max_v: 3499.5\n",
      "fc layer 2 self.abs_max_out: 1957.0\n",
      "lif layer 2 self.abs_max_v: 3513.5\n",
      "lif layer 2 self.abs_max_v: 3631.0\n",
      "lif layer 2 self.abs_max_v: 3646.5\n",
      "fc layer 2 self.abs_max_out: 2053.0\n",
      "lif layer 2 self.abs_max_v: 3788.5\n",
      "fc layer 1 self.abs_max_out: 4381.0\n",
      "lif layer 1 self.abs_max_v: 8318.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.241364/  1.612558, val:  50.00%, val_best:  50.00%, tr:  99.59%, tr_best:  99.69%, epoch time: 87.70 seconds, 1.46 minutes\n",
      "layer   1  Sparsity: 89.9516%\n",
      "layer   2  Sparsity: 74.6928%\n",
      "layer   3  Sparsity: 72.5694%\n",
      "total_backward_count 68530 real_backward_count 9288  13.553%\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "fc layer 1 self.abs_max_out: 4546.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.214421/  1.576797, val:  51.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.69%, epoch time: 86.62 seconds, 1.44 minutes\n",
      "layer   1  Sparsity: 89.9063%\n",
      "layer   2  Sparsity: 75.1378%\n",
      "layer   3  Sparsity: 73.1789%\n",
      "total_backward_count 78320 real_backward_count 10488  13.391%\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "lif layer 2 self.abs_max_v: 3814.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.198972/  1.518512, val:  50.00%, val_best:  51.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 85.46 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 89.9285%\n",
      "layer   2  Sparsity: 76.1123%\n",
      "layer   3  Sparsity: 72.4893%\n",
      "total_backward_count 88110 real_backward_count 11710  13.290%\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "lif layer 2 self.abs_max_v: 3826.5\n",
      "fc layer 3 self.abs_max_out: 888.0\n",
      "fc layer 3 self.abs_max_out: 892.0\n",
      "fc layer 3 self.abs_max_out: 914.0\n",
      "lif layer 1 self.abs_max_v: 8320.5\n",
      "lif layer 1 self.abs_max_v: 8458.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.143995/  1.608868, val:  49.17%, val_best:  51.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 84.30 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 89.8996%\n",
      "layer   2  Sparsity: 76.9959%\n",
      "layer   3  Sparsity: 72.8432%\n",
      "total_backward_count 97900 real_backward_count 12873  13.149%\n",
      "lif layer 1 self.abs_max_v: 8464.5\n",
      "lif layer 2 self.abs_max_v: 3848.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.138408/  1.693930, val:  43.33%, val_best:  51.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 85.22 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 89.9083%\n",
      "layer   2  Sparsity: 76.5754%\n",
      "layer   3  Sparsity: 72.7153%\n",
      "total_backward_count 107690 real_backward_count 14061  13.057%\n",
      "fc layer 3 self.abs_max_out: 926.0\n",
      "fc layer 3 self.abs_max_out: 941.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.206961/  1.573929, val:  57.50%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 85.20 seconds, 1.42 minutes\n",
      "layer   1  Sparsity: 89.9230%\n",
      "layer   2  Sparsity: 75.6735%\n",
      "layer   3  Sparsity: 73.3322%\n",
      "total_backward_count 117480 real_backward_count 15275  13.002%\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "fc layer 1 self.abs_max_out: 4793.0\n",
      "fc layer 1 self.abs_max_out: 4983.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.141551/  1.591049, val:  48.33%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 84.49 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 89.9323%\n",
      "layer   2  Sparsity: 76.0850%\n",
      "layer   3  Sparsity: 73.1005%\n",
      "total_backward_count 127270 real_backward_count 16403  12.888%\n",
      "fc layer 2 self.abs_max_out: 2173.0\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.153894/  1.611202, val:  43.75%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 83.42 seconds, 1.39 minutes\n",
      "layer   1  Sparsity: 89.9394%\n",
      "layer   2  Sparsity: 76.2470%\n",
      "layer   3  Sparsity: 73.5422%\n",
      "total_backward_count 137060 real_backward_count 17554  12.808%\n",
      "fc layer 3 self.abs_max_out: 957.0\n",
      "fc layer 2 self.abs_max_out: 2176.0\n",
      "lif layer 1 self.abs_max_v: 8651.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.139742/  1.666342, val:  36.25%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 83.74 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 89.9422%\n",
      "layer   2  Sparsity: 76.8435%\n",
      "layer   3  Sparsity: 73.4391%\n",
      "total_backward_count 146850 real_backward_count 18674  12.716%\n",
      "fc layer 3 self.abs_max_out: 976.0\n",
      "fc layer 3 self.abs_max_out: 994.0\n",
      "fc layer 3 self.abs_max_out: 1008.0\n",
      "fc layer 3 self.abs_max_out: 1032.0\n",
      "fc layer 3 self.abs_max_out: 1038.0\n",
      "lif layer 1 self.abs_max_v: 8826.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.141516/  1.544710, val:  45.00%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.8928%\n",
      "layer   2  Sparsity: 76.2021%\n",
      "layer   3  Sparsity: 73.6206%\n",
      "total_backward_count 156640 real_backward_count 19831  12.660%\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.163586/  1.569575, val:  50.83%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 80.64 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.9247%\n",
      "layer   2  Sparsity: 76.0368%\n",
      "layer   3  Sparsity: 74.5329%\n",
      "total_backward_count 166430 real_backward_count 20981  12.607%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.195514/  1.569559, val:  54.17%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 80.92 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 89.9338%\n",
      "layer   2  Sparsity: 76.0476%\n",
      "layer   3  Sparsity: 74.6066%\n",
      "total_backward_count 176220 real_backward_count 22147  12.568%\n",
      "lif layer 2 self.abs_max_v: 3891.0\n",
      "lif layer 2 self.abs_max_v: 4038.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.204571/  1.594264, val:  47.08%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.8993%\n",
      "layer   2  Sparsity: 75.8624%\n",
      "layer   3  Sparsity: 75.4741%\n",
      "total_backward_count 186010 real_backward_count 23300  12.526%\n",
      "fc layer 2 self.abs_max_out: 2282.0\n",
      "lif layer 2 self.abs_max_v: 4042.5\n",
      "lif layer 2 self.abs_max_v: 4086.5\n",
      "lif layer 2 self.abs_max_v: 4096.5\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.184429/  1.623552, val:  39.58%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.9266%\n",
      "layer   2  Sparsity: 76.5614%\n",
      "layer   3  Sparsity: 76.5781%\n",
      "total_backward_count 195800 real_backward_count 24443  12.484%\n",
      "lif layer 2 self.abs_max_v: 4158.5\n",
      "fc layer 2 self.abs_max_out: 2321.0\n",
      "fc layer 1 self.abs_max_out: 5028.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.181860/  1.635751, val:  42.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 80.70 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.9093%\n",
      "layer   2  Sparsity: 76.1823%\n",
      "layer   3  Sparsity: 76.4699%\n",
      "total_backward_count 205590 real_backward_count 25570  12.437%\n",
      "lif layer 1 self.abs_max_v: 8933.5\n",
      "fc layer 1 self.abs_max_out: 5215.0\n",
      "fc layer 1 self.abs_max_out: 5303.0\n",
      "lif layer 1 self.abs_max_v: 9753.0\n",
      "lif layer 1 self.abs_max_v: 9874.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.209499/  1.713539, val:  30.00%, val_best:  57.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.8752%\n",
      "layer   2  Sparsity: 74.9313%\n",
      "layer   3  Sparsity: 75.5475%\n",
      "total_backward_count 215380 real_backward_count 26728  12.410%\n",
      "fc layer 1 self.abs_max_out: 5306.0\n",
      "fc layer 1 self.abs_max_out: 5457.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.214608/  1.547502, val:  56.25%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 80.54 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.8943%\n",
      "layer   2  Sparsity: 75.0374%\n",
      "layer   3  Sparsity: 76.0999%\n",
      "total_backward_count 225170 real_backward_count 27874  12.379%\n",
      "fc layer 1 self.abs_max_out: 5480.0\n",
      "lif layer 1 self.abs_max_v: 9963.0\n",
      "fc layer 1 self.abs_max_out: 5821.0\n",
      "lif layer 2 self.abs_max_v: 4292.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.197411/  1.549703, val:  56.67%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 80.80 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 89.9324%\n",
      "layer   2  Sparsity: 75.0120%\n",
      "layer   3  Sparsity: 76.2474%\n",
      "total_backward_count 234960 real_backward_count 29011  12.347%\n",
      "lif layer 1 self.abs_max_v: 10364.0\n",
      "lif layer 1 self.abs_max_v: 10798.0\n",
      "fc layer 1 self.abs_max_out: 5907.0\n",
      "fc layer 1 self.abs_max_out: 5926.0\n",
      "fc layer 1 self.abs_max_out: 5947.0\n",
      "fc layer 1 self.abs_max_out: 5953.0\n",
      "fc layer 1 self.abs_max_out: 6021.0\n",
      "fc layer 1 self.abs_max_out: 6093.0\n",
      "lif layer 1 self.abs_max_v: 10940.5\n",
      "lif layer 1 self.abs_max_v: 11180.5\n",
      "fc layer 1 self.abs_max_out: 6329.0\n",
      "lif layer 1 self.abs_max_v: 11188.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.209406/  1.521535, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 89.9088%\n",
      "layer   2  Sparsity: 75.5121%\n",
      "layer   3  Sparsity: 76.6687%\n",
      "total_backward_count 244750 real_backward_count 30103  12.299%\n",
      "lif layer 1 self.abs_max_v: 11466.5\n",
      "lif layer 1 self.abs_max_v: 11874.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.204031/  1.527232, val:  63.75%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.9129%\n",
      "layer   2  Sparsity: 76.1659%\n",
      "layer   3  Sparsity: 77.6810%\n",
      "total_backward_count 254540 real_backward_count 31214  12.263%\n",
      "fc layer 2 self.abs_max_out: 2396.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.231500/  1.551189, val:  61.25%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 89.9057%\n",
      "layer   2  Sparsity: 76.2208%\n",
      "layer   3  Sparsity: 78.4098%\n",
      "total_backward_count 264330 real_backward_count 32292  12.217%\n",
      "fc layer 2 self.abs_max_out: 2470.0\n",
      "fc layer 2 self.abs_max_out: 2486.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.235832/  1.503201, val:  66.25%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 81.65 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 89.9359%\n",
      "layer   2  Sparsity: 76.3233%\n",
      "layer   3  Sparsity: 77.7877%\n",
      "total_backward_count 274120 real_backward_count 33375  12.175%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.175267/  1.559018, val:  58.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9501%\n",
      "layer   2  Sparsity: 76.5315%\n",
      "layer   3  Sparsity: 77.3550%\n",
      "total_backward_count 283910 real_backward_count 34463  12.139%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.195371/  1.513162, val:  53.33%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.9432%\n",
      "layer   2  Sparsity: 76.4871%\n",
      "layer   3  Sparsity: 77.3465%\n",
      "total_backward_count 293700 real_backward_count 35627  12.130%\n",
      "fc layer 2 self.abs_max_out: 2523.0\n",
      "fc layer 2 self.abs_max_out: 2535.0\n",
      "lif layer 2 self.abs_max_v: 4324.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.194350/  1.568251, val:  50.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.9375%\n",
      "layer   2  Sparsity: 76.2016%\n",
      "layer   3  Sparsity: 76.7356%\n",
      "total_backward_count 303490 real_backward_count 36704  12.094%\n",
      "lif layer 2 self.abs_max_v: 4329.0\n",
      "lif layer 2 self.abs_max_v: 4373.0\n",
      "lif layer 2 self.abs_max_v: 4502.5\n",
      "lif layer 2 self.abs_max_v: 4521.0\n",
      "lif layer 2 self.abs_max_v: 4546.0\n",
      "lif layer 2 self.abs_max_v: 4703.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.164200/  1.546808, val:  57.92%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9529%\n",
      "layer   2  Sparsity: 76.1823%\n",
      "layer   3  Sparsity: 75.3126%\n",
      "total_backward_count 313280 real_backward_count 37781  12.060%\n",
      "fc layer 2 self.abs_max_out: 2789.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.147447/  1.508726, val:  58.75%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9042%\n",
      "layer   2  Sparsity: 77.0965%\n",
      "layer   3  Sparsity: 75.2417%\n",
      "total_backward_count 323070 real_backward_count 38844  12.023%\n",
      "fc layer 1 self.abs_max_out: 6349.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.132838/  1.548161, val:  52.50%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9425%\n",
      "layer   2  Sparsity: 76.9845%\n",
      "layer   3  Sparsity: 74.7034%\n",
      "total_backward_count 332860 real_backward_count 39905  11.989%\n",
      "fc layer 2 self.abs_max_out: 2821.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.109895/  1.559294, val:  46.67%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9542%\n",
      "layer   2  Sparsity: 76.9032%\n",
      "layer   3  Sparsity: 75.1275%\n",
      "total_backward_count 342650 real_backward_count 40957  11.953%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.101156/  1.502669, val:  48.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9414%\n",
      "layer   2  Sparsity: 76.3533%\n",
      "layer   3  Sparsity: 74.4895%\n",
      "total_backward_count 352440 real_backward_count 42035  11.927%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.077892/  1.441287, val:  59.58%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9009%\n",
      "layer   2  Sparsity: 75.6881%\n",
      "layer   3  Sparsity: 74.9148%\n",
      "total_backward_count 362230 real_backward_count 43114  11.902%\n",
      "fc layer 3 self.abs_max_out: 1069.0\n",
      "fc layer 3 self.abs_max_out: 1090.0\n",
      "fc layer 3 self.abs_max_out: 1098.0\n",
      "fc layer 3 self.abs_max_out: 1115.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.059192/  1.484699, val:  54.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9130%\n",
      "layer   2  Sparsity: 74.9318%\n",
      "layer   3  Sparsity: 75.0481%\n",
      "total_backward_count 372020 real_backward_count 44110  11.857%\n",
      "lif layer 1 self.abs_max_v: 11951.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.095627/  1.482792, val:  54.58%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9538%\n",
      "layer   2  Sparsity: 74.9128%\n",
      "layer   3  Sparsity: 75.6012%\n",
      "total_backward_count 381810 real_backward_count 45202  11.839%\n",
      "lif layer 2 self.abs_max_v: 4791.5\n",
      "lif layer 2 self.abs_max_v: 4848.5\n",
      "fc layer 1 self.abs_max_out: 6356.0\n",
      "fc layer 2 self.abs_max_out: 2861.0\n",
      "fc layer 2 self.abs_max_out: 2929.0\n",
      "lif layer 2 self.abs_max_v: 5036.0\n",
      "lif layer 2 self.abs_max_v: 5133.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.077848/  1.497367, val:  52.92%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9514%\n",
      "layer   2  Sparsity: 75.7215%\n",
      "layer   3  Sparsity: 75.2906%\n",
      "total_backward_count 391600 real_backward_count 46225  11.804%\n",
      "fc layer 1 self.abs_max_out: 6666.0\n",
      "fc layer 1 self.abs_max_out: 6671.0\n",
      "lif layer 1 self.abs_max_v: 12368.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.074967/  1.437759, val:  62.08%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9058%\n",
      "layer   2  Sparsity: 76.1940%\n",
      "layer   3  Sparsity: 75.0461%\n",
      "total_backward_count 401390 real_backward_count 47273  11.777%\n",
      "fc layer 1 self.abs_max_out: 6721.0\n",
      "lif layer 1 self.abs_max_v: 12445.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.104813/  1.443092, val:  60.83%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9306%\n",
      "layer   2  Sparsity: 76.2084%\n",
      "layer   3  Sparsity: 75.6247%\n",
      "total_backward_count 411180 real_backward_count 48331  11.754%\n",
      "fc layer 1 self.abs_max_out: 6767.0\n",
      "lif layer 1 self.abs_max_v: 12540.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.101470/  1.539938, val:  48.33%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9377%\n",
      "layer   2  Sparsity: 76.2573%\n",
      "layer   3  Sparsity: 76.0620%\n",
      "total_backward_count 420970 real_backward_count 49418  11.739%\n",
      "lif layer 1 self.abs_max_v: 12544.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.117980/  1.415602, val:  62.08%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9313%\n",
      "layer   2  Sparsity: 76.2582%\n",
      "layer   3  Sparsity: 75.9631%\n",
      "total_backward_count 430760 real_backward_count 50496  11.723%\n",
      "fc layer 1 self.abs_max_out: 6786.0\n",
      "fc layer 1 self.abs_max_out: 6898.0\n",
      "fc layer 1 self.abs_max_out: 7102.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.101910/  1.493945, val:  52.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9412%\n",
      "layer   2  Sparsity: 76.2406%\n",
      "layer   3  Sparsity: 76.2246%\n",
      "total_backward_count 440550 real_backward_count 51584  11.709%\n",
      "lif layer 1 self.abs_max_v: 12814.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.114042/  1.533079, val:  45.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9425%\n",
      "layer   2  Sparsity: 75.7420%\n",
      "layer   3  Sparsity: 76.6854%\n",
      "total_backward_count 450340 real_backward_count 52642  11.689%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.132144/  1.522043, val:  53.33%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8813%\n",
      "layer   2  Sparsity: 75.7636%\n",
      "layer   3  Sparsity: 76.8349%\n",
      "total_backward_count 460130 real_backward_count 53676  11.665%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.101831/  1.485060, val:  59.58%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9285%\n",
      "layer   2  Sparsity: 75.9351%\n",
      "layer   3  Sparsity: 76.6375%\n",
      "total_backward_count 469920 real_backward_count 54750  11.651%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.114102/  1.532799, val:  54.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8834%\n",
      "layer   2  Sparsity: 75.7682%\n",
      "layer   3  Sparsity: 76.6543%\n",
      "total_backward_count 479710 real_backward_count 55760  11.624%\n",
      "lif layer 2 self.abs_max_v: 5174.0\n",
      "lif layer 1 self.abs_max_v: 12817.5\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.093280/  1.467942, val:  52.92%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9222%\n",
      "layer   2  Sparsity: 76.4281%\n",
      "layer   3  Sparsity: 77.1135%\n",
      "total_backward_count 489500 real_backward_count 56853  11.615%\n",
      "fc layer 1 self.abs_max_out: 7221.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.043167/  1.427920, val:  53.33%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9135%\n",
      "layer   2  Sparsity: 76.2684%\n",
      "layer   3  Sparsity: 76.5926%\n",
      "total_backward_count 499290 real_backward_count 57902  11.597%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.011096/  1.529387, val:  47.08%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9420%\n",
      "layer   2  Sparsity: 75.6682%\n",
      "layer   3  Sparsity: 76.1219%\n",
      "total_backward_count 509080 real_backward_count 58934  11.577%\n",
      "lif layer 1 self.abs_max_v: 12979.5\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.049642/  1.449740, val:  59.58%, val_best:  66.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.8975%\n",
      "layer   2  Sparsity: 75.2520%\n",
      "layer   3  Sparsity: 76.0959%\n",
      "total_backward_count 518870 real_backward_count 60018  11.567%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.054745/  1.475240, val:  54.58%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9061%\n",
      "layer   2  Sparsity: 75.5521%\n",
      "layer   3  Sparsity: 76.8043%\n",
      "total_backward_count 528660 real_backward_count 61051  11.548%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.082770/  1.494718, val:  55.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9110%\n",
      "layer   2  Sparsity: 75.4648%\n",
      "layer   3  Sparsity: 76.1183%\n",
      "total_backward_count 538450 real_backward_count 62051  11.524%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.097465/  1.471177, val:  61.67%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9094%\n",
      "layer   2  Sparsity: 75.5664%\n",
      "layer   3  Sparsity: 76.0696%\n",
      "total_backward_count 548240 real_backward_count 63124  11.514%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.064879/  1.470972, val:  59.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.9140%\n",
      "layer   2  Sparsity: 75.8844%\n",
      "layer   3  Sparsity: 76.2622%\n",
      "total_backward_count 558030 real_backward_count 64147  11.495%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.060007/  1.451168, val:  58.75%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9482%\n",
      "layer   2  Sparsity: 75.9111%\n",
      "layer   3  Sparsity: 76.0831%\n",
      "total_backward_count 567820 real_backward_count 65174  11.478%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.039396/  1.414409, val:  58.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9105%\n",
      "layer   2  Sparsity: 75.8489%\n",
      "layer   3  Sparsity: 76.5649%\n",
      "total_backward_count 577610 real_backward_count 66131  11.449%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.074552/  1.455725, val:  58.33%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9455%\n",
      "layer   2  Sparsity: 76.0016%\n",
      "layer   3  Sparsity: 77.1524%\n",
      "total_backward_count 587400 real_backward_count 67133  11.429%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.054288/  1.447536, val:  52.50%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.8839%\n",
      "layer   2  Sparsity: 76.3770%\n",
      "layer   3  Sparsity: 76.5999%\n",
      "total_backward_count 597190 real_backward_count 68171  11.415%\n",
      "fc layer 1 self.abs_max_out: 7471.0\n",
      "lif layer 1 self.abs_max_v: 13047.5\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.066805/  1.381963, val:  58.75%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.9057%\n",
      "layer   2  Sparsity: 76.4813%\n",
      "layer   3  Sparsity: 76.3366%\n",
      "total_backward_count 606980 real_backward_count 69225  11.405%\n",
      "fc layer 1 self.abs_max_out: 7794.0\n",
      "lif layer 1 self.abs_max_v: 13678.5\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.034132/  1.434298, val:  56.67%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9119%\n",
      "layer   2  Sparsity: 76.9204%\n",
      "layer   3  Sparsity: 76.0967%\n",
      "total_backward_count 616770 real_backward_count 70258  11.391%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.025999/  1.494853, val:  50.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9161%\n",
      "layer   2  Sparsity: 76.8765%\n",
      "layer   3  Sparsity: 76.3883%\n",
      "total_backward_count 626560 real_backward_count 71260  11.373%\n",
      "lif layer 1 self.abs_max_v: 13781.0\n",
      "lif layer 1 self.abs_max_v: 14593.5\n",
      "fc layer 1 self.abs_max_out: 7847.0\n",
      "fc layer 1 self.abs_max_out: 7864.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.048240/  1.418198, val:  57.50%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.9422%\n",
      "layer   2  Sparsity: 76.6927%\n",
      "layer   3  Sparsity: 76.2635%\n",
      "total_backward_count 636350 real_backward_count 72255  11.355%\n",
      "fc layer 1 self.abs_max_out: 8204.0\n",
      "lif layer 1 self.abs_max_v: 14656.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.014413/  1.422251, val:  58.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9321%\n",
      "layer   2  Sparsity: 77.3492%\n",
      "layer   3  Sparsity: 75.5843%\n",
      "total_backward_count 646140 real_backward_count 73232  11.334%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.018803/  1.413028, val:  58.33%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9295%\n",
      "layer   2  Sparsity: 77.5643%\n",
      "layer   3  Sparsity: 75.7372%\n",
      "total_backward_count 655930 real_backward_count 74189  11.311%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.030101/  1.396308, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9136%\n",
      "layer   2  Sparsity: 77.5991%\n",
      "layer   3  Sparsity: 75.9286%\n",
      "total_backward_count 665720 real_backward_count 75168  11.291%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.068730/  1.452438, val:  60.83%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9023%\n",
      "layer   2  Sparsity: 76.8829%\n",
      "layer   3  Sparsity: 75.8198%\n",
      "total_backward_count 675510 real_backward_count 76179  11.277%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.056699/  1.504845, val:  53.33%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9325%\n",
      "layer   2  Sparsity: 76.5734%\n",
      "layer   3  Sparsity: 75.8505%\n",
      "total_backward_count 685300 real_backward_count 77170  11.261%\n",
      "fc layer 1 self.abs_max_out: 8338.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.072058/  1.422276, val:  61.67%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9456%\n",
      "layer   2  Sparsity: 76.9797%\n",
      "layer   3  Sparsity: 76.9354%\n",
      "total_backward_count 695090 real_backward_count 78207  11.251%\n",
      "fc layer 1 self.abs_max_out: 8380.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.072555/  1.492976, val:  55.00%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9137%\n",
      "layer   2  Sparsity: 76.6007%\n",
      "layer   3  Sparsity: 77.2414%\n",
      "total_backward_count 704880 real_backward_count 79236  11.241%\n",
      "fc layer 1 self.abs_max_out: 8550.0\n",
      "lif layer 1 self.abs_max_v: 15140.5\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.069162/  1.439531, val:  61.25%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9243%\n",
      "layer   2  Sparsity: 76.3975%\n",
      "layer   3  Sparsity: 76.7646%\n",
      "total_backward_count 714670 real_backward_count 80253  11.229%\n",
      "fc layer 1 self.abs_max_out: 8634.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.049974/  1.460283, val:  58.33%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9258%\n",
      "layer   2  Sparsity: 76.7822%\n",
      "layer   3  Sparsity: 76.2664%\n",
      "total_backward_count 724460 real_backward_count 81190  11.207%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.047769/  1.451951, val:  51.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9322%\n",
      "layer   2  Sparsity: 76.9268%\n",
      "layer   3  Sparsity: 76.3183%\n",
      "total_backward_count 734250 real_backward_count 82209  11.196%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.033226/  1.333833, val:  65.42%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9102%\n",
      "layer   2  Sparsity: 76.5849%\n",
      "layer   3  Sparsity: 75.3648%\n",
      "total_backward_count 744040 real_backward_count 83207  11.183%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.029894/  1.437607, val:  61.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9609%\n",
      "layer   2  Sparsity: 76.9055%\n",
      "layer   3  Sparsity: 75.9109%\n",
      "total_backward_count 753830 real_backward_count 84207  11.171%\n",
      "fc layer 2 self.abs_max_out: 2934.0\n",
      "lif layer 2 self.abs_max_v: 5212.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.040133/  1.451900, val:  54.58%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.8987%\n",
      "layer   2  Sparsity: 77.2115%\n",
      "layer   3  Sparsity: 75.7584%\n",
      "total_backward_count 763620 real_backward_count 85175  11.154%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.016675/  1.437389, val:  54.58%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8882%\n",
      "layer   2  Sparsity: 76.7114%\n",
      "layer   3  Sparsity: 75.9846%\n",
      "total_backward_count 773410 real_backward_count 86131  11.137%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.016465/  1.390920, val:  63.75%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9045%\n",
      "layer   2  Sparsity: 76.2816%\n",
      "layer   3  Sparsity: 74.9421%\n",
      "total_backward_count 783200 real_backward_count 87122  11.124%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.014228/  1.365318, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9233%\n",
      "layer   2  Sparsity: 76.4500%\n",
      "layer   3  Sparsity: 75.0824%\n",
      "total_backward_count 792990 real_backward_count 88091  11.109%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.994395/  1.532555, val:  44.17%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9046%\n",
      "layer   2  Sparsity: 76.7551%\n",
      "layer   3  Sparsity: 75.4046%\n",
      "total_backward_count 802780 real_backward_count 89046  11.092%\n",
      "fc layer 1 self.abs_max_out: 8854.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.039081/  1.388809, val:  58.33%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9381%\n",
      "layer   2  Sparsity: 76.6434%\n",
      "layer   3  Sparsity: 76.2076%\n",
      "total_backward_count 812570 real_backward_count 90048  11.082%\n",
      "fc layer 1 self.abs_max_out: 9374.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.007575/  1.451122, val:  49.58%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9188%\n",
      "layer   2  Sparsity: 76.3095%\n",
      "layer   3  Sparsity: 76.5230%\n",
      "total_backward_count 822360 real_backward_count 90995  11.065%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.992571/  1.371957, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9340%\n",
      "layer   2  Sparsity: 75.9862%\n",
      "layer   3  Sparsity: 76.0865%\n",
      "total_backward_count 832150 real_backward_count 91978  11.053%\n",
      "fc layer 1 self.abs_max_out: 9448.0\n",
      "fc layer 1 self.abs_max_out: 9782.0\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.969806/  1.388651, val:  60.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9527%\n",
      "layer   2  Sparsity: 76.5361%\n",
      "layer   3  Sparsity: 75.1747%\n",
      "total_backward_count 841940 real_backward_count 92923  11.037%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.970726/  1.383819, val:  57.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9294%\n",
      "layer   2  Sparsity: 76.5319%\n",
      "layer   3  Sparsity: 75.2534%\n",
      "total_backward_count 851730 real_backward_count 93840  11.018%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.968148/  1.352218, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9184%\n",
      "layer   2  Sparsity: 76.2072%\n",
      "layer   3  Sparsity: 74.5536%\n",
      "total_backward_count 861520 real_backward_count 94773  11.001%\n",
      "fc layer 1 self.abs_max_out: 9841.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.954736/  1.347275, val:  62.08%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9418%\n",
      "layer   2  Sparsity: 76.2594%\n",
      "layer   3  Sparsity: 74.2624%\n",
      "total_backward_count 871310 real_backward_count 95723  10.986%\n",
      "fc layer 1 self.abs_max_out: 10029.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.969936/  1.355556, val:  60.42%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9080%\n",
      "layer   2  Sparsity: 76.7042%\n",
      "layer   3  Sparsity: 74.4973%\n",
      "total_backward_count 881100 real_backward_count 96706  10.976%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.949114/  1.362713, val:  55.42%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8959%\n",
      "layer   2  Sparsity: 76.8430%\n",
      "layer   3  Sparsity: 74.7773%\n",
      "total_backward_count 890890 real_backward_count 97700  10.967%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.953000/  1.347314, val:  65.00%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9249%\n",
      "layer   2  Sparsity: 76.8847%\n",
      "layer   3  Sparsity: 75.2101%\n",
      "total_backward_count 900680 real_backward_count 98665  10.955%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.928551/  1.315907, val:  62.08%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8910%\n",
      "layer   2  Sparsity: 76.6587%\n",
      "layer   3  Sparsity: 75.2314%\n",
      "total_backward_count 910470 real_backward_count 99640  10.944%\n",
      "lif layer 2 self.abs_max_v: 5323.0\n",
      "fc layer 2 self.abs_max_out: 2960.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.928711/  1.359696, val:  60.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9371%\n",
      "layer   2  Sparsity: 77.5631%\n",
      "layer   3  Sparsity: 74.9990%\n",
      "total_backward_count 920260 real_backward_count 100617  10.934%\n",
      "fc layer 2 self.abs_max_out: 2992.0\n",
      "lif layer 2 self.abs_max_v: 5439.5\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.920161/  1.333990, val:  56.67%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9472%\n",
      "layer   2  Sparsity: 77.9265%\n",
      "layer   3  Sparsity: 74.5235%\n",
      "total_backward_count 930050 real_backward_count 101618  10.926%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.930963/  1.299072, val:  62.08%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8990%\n",
      "layer   2  Sparsity: 77.5454%\n",
      "layer   3  Sparsity: 74.8441%\n",
      "total_backward_count 939840 real_backward_count 102657  10.923%\n",
      "fc layer 3 self.abs_max_out: 1126.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.910922/  1.324436, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9155%\n",
      "layer   2  Sparsity: 77.5320%\n",
      "layer   3  Sparsity: 74.6708%\n",
      "total_backward_count 949630 real_backward_count 103633  10.913%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.909571/  1.316121, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9225%\n",
      "layer   2  Sparsity: 77.4213%\n",
      "layer   3  Sparsity: 74.5098%\n",
      "total_backward_count 959420 real_backward_count 104621  10.905%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.921354/  1.404217, val:  51.25%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9036%\n",
      "layer   2  Sparsity: 76.9705%\n",
      "layer   3  Sparsity: 74.1335%\n",
      "total_backward_count 969210 real_backward_count 105597  10.895%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.905163/  1.313431, val:  67.50%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9317%\n",
      "layer   2  Sparsity: 77.1815%\n",
      "layer   3  Sparsity: 74.0478%\n",
      "total_backward_count 979000 real_backward_count 106494  10.878%\n",
      "fc layer 3 self.abs_max_out: 1151.0\n",
      "fc layer 3 self.abs_max_out: 1155.0\n",
      "fc layer 3 self.abs_max_out: 1181.0\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.906762/  1.396062, val:  58.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9364%\n",
      "layer   2  Sparsity: 77.1552%\n",
      "layer   3  Sparsity: 74.8572%\n",
      "total_backward_count 988790 real_backward_count 107428  10.865%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.898545/  1.318833, val:  62.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9272%\n",
      "layer   2  Sparsity: 76.9046%\n",
      "layer   3  Sparsity: 74.2064%\n",
      "total_backward_count 998580 real_backward_count 108405  10.856%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.875278/  1.290161, val:  62.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9165%\n",
      "layer   2  Sparsity: 76.6422%\n",
      "layer   3  Sparsity: 73.8365%\n",
      "total_backward_count 1008370 real_backward_count 109365  10.846%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.885397/  1.319462, val:  57.50%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9085%\n",
      "layer   2  Sparsity: 76.8931%\n",
      "layer   3  Sparsity: 73.7448%\n",
      "total_backward_count 1018160 real_backward_count 110274  10.831%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.874678/  1.289925, val:  62.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9275%\n",
      "layer   2  Sparsity: 77.1119%\n",
      "layer   3  Sparsity: 73.1513%\n",
      "total_backward_count 1027950 real_backward_count 111200  10.818%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.880409/  1.291652, val:  64.17%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9119%\n",
      "layer   2  Sparsity: 76.9966%\n",
      "layer   3  Sparsity: 73.7578%\n",
      "total_backward_count 1037740 real_backward_count 112207  10.813%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.885429/  1.337233, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9088%\n",
      "layer   2  Sparsity: 77.2218%\n",
      "layer   3  Sparsity: 74.4040%\n",
      "total_backward_count 1047530 real_backward_count 113119  10.799%\n",
      "fc layer 3 self.abs_max_out: 1190.0\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.876384/  1.301376, val:  59.17%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9049%\n",
      "layer   2  Sparsity: 76.8813%\n",
      "layer   3  Sparsity: 74.2760%\n",
      "total_backward_count 1057320 real_backward_count 114062  10.788%\n",
      "fc layer 3 self.abs_max_out: 1211.0\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.849144/  1.296658, val:  65.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9035%\n",
      "layer   2  Sparsity: 76.7074%\n",
      "layer   3  Sparsity: 73.8006%\n",
      "total_backward_count 1067110 real_backward_count 115024  10.779%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.852562/  1.269556, val:  62.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9469%\n",
      "layer   2  Sparsity: 76.7617%\n",
      "layer   3  Sparsity: 73.7833%\n",
      "total_backward_count 1076900 real_backward_count 115938  10.766%\n",
      "fc layer 3 self.abs_max_out: 1218.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.848645/  1.299536, val:  62.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9315%\n",
      "layer   2  Sparsity: 76.3825%\n",
      "layer   3  Sparsity: 73.6212%\n",
      "total_backward_count 1086690 real_backward_count 116841  10.752%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.856192/  1.267378, val:  62.50%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.8874%\n",
      "layer   2  Sparsity: 76.2162%\n",
      "layer   3  Sparsity: 73.2520%\n",
      "total_backward_count 1096480 real_backward_count 117758  10.740%\n",
      "fc layer 3 self.abs_max_out: 1223.0\n",
      "fc layer 3 self.abs_max_out: 1234.0\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.858881/  1.242783, val:  66.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9191%\n",
      "layer   2  Sparsity: 75.9159%\n",
      "layer   3  Sparsity: 73.1257%\n",
      "total_backward_count 1106270 real_backward_count 118741  10.733%\n",
      "fc layer 3 self.abs_max_out: 1275.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.816024/  1.301117, val:  56.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9135%\n",
      "layer   2  Sparsity: 75.7080%\n",
      "layer   3  Sparsity: 73.4832%\n",
      "total_backward_count 1116060 real_backward_count 119701  10.725%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.825903/  1.325527, val:  54.58%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9562%\n",
      "layer   2  Sparsity: 75.6844%\n",
      "layer   3  Sparsity: 73.4028%\n",
      "total_backward_count 1125850 real_backward_count 120560  10.708%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.817567/  1.225728, val:  67.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9249%\n",
      "layer   2  Sparsity: 75.5884%\n",
      "layer   3  Sparsity: 72.4967%\n",
      "total_backward_count 1135640 real_backward_count 121470  10.696%\n",
      "fc layer 3 self.abs_max_out: 1279.0\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.806487/  1.278133, val:  60.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9603%\n",
      "layer   2  Sparsity: 75.3110%\n",
      "layer   3  Sparsity: 72.2719%\n",
      "total_backward_count 1145430 real_backward_count 122426  10.688%\n",
      "fc layer 3 self.abs_max_out: 1283.0\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.806314/  1.184983, val:  63.33%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9191%\n",
      "layer   2  Sparsity: 75.5560%\n",
      "layer   3  Sparsity: 72.2052%\n",
      "total_backward_count 1155220 real_backward_count 123338  10.677%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.833194/  1.265667, val:  61.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9148%\n",
      "layer   2  Sparsity: 75.2888%\n",
      "layer   3  Sparsity: 72.2585%\n",
      "total_backward_count 1165010 real_backward_count 124234  10.664%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.836627/  1.260387, val:  63.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9487%\n",
      "layer   2  Sparsity: 74.9222%\n",
      "layer   3  Sparsity: 72.2702%\n",
      "total_backward_count 1174800 real_backward_count 125193  10.657%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.810435/  1.273332, val:  65.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9155%\n",
      "layer   2  Sparsity: 75.0435%\n",
      "layer   3  Sparsity: 71.6500%\n",
      "total_backward_count 1184590 real_backward_count 126064  10.642%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.820510/  1.261547, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9352%\n",
      "layer   2  Sparsity: 75.3493%\n",
      "layer   3  Sparsity: 72.1134%\n",
      "total_backward_count 1194380 real_backward_count 126963  10.630%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.831712/  1.313476, val:  57.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9447%\n",
      "layer   2  Sparsity: 75.2324%\n",
      "layer   3  Sparsity: 72.7461%\n",
      "total_backward_count 1204170 real_backward_count 127923  10.623%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.822019/  1.225013, val:  65.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9558%\n",
      "layer   2  Sparsity: 75.0286%\n",
      "layer   3  Sparsity: 72.8459%\n",
      "total_backward_count 1213960 real_backward_count 128888  10.617%\n",
      "fc layer 3 self.abs_max_out: 1323.0\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.789008/  1.208488, val:  70.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9112%\n",
      "layer   2  Sparsity: 75.0776%\n",
      "layer   3  Sparsity: 72.0659%\n",
      "total_backward_count 1223750 real_backward_count 129823  10.609%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.787275/  1.239504, val:  62.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9173%\n",
      "layer   2  Sparsity: 75.0644%\n",
      "layer   3  Sparsity: 72.0333%\n",
      "total_backward_count 1233540 real_backward_count 130731  10.598%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.772492/  1.373452, val:  53.75%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9333%\n",
      "layer   2  Sparsity: 75.6094%\n",
      "layer   3  Sparsity: 71.8693%\n",
      "total_backward_count 1243330 real_backward_count 131660  10.589%\n",
      "fc layer 3 self.abs_max_out: 1337.0\n",
      "fc layer 3 self.abs_max_out: 1473.0\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.767147/  1.258300, val:  67.50%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9033%\n",
      "layer   2  Sparsity: 75.1667%\n",
      "layer   3  Sparsity: 71.8475%\n",
      "total_backward_count 1253120 real_backward_count 132571  10.579%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.781146/  1.251892, val:  57.08%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9127%\n",
      "layer   2  Sparsity: 75.2457%\n",
      "layer   3  Sparsity: 72.1328%\n",
      "total_backward_count 1262910 real_backward_count 133494  10.570%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.778178/  1.213550, val:  62.92%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9500%\n",
      "layer   2  Sparsity: 75.5619%\n",
      "layer   3  Sparsity: 71.3532%\n",
      "total_backward_count 1272700 real_backward_count 134399  10.560%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.755237/  1.291318, val:  57.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9443%\n",
      "layer   2  Sparsity: 75.0827%\n",
      "layer   3  Sparsity: 71.2508%\n",
      "total_backward_count 1282490 real_backward_count 135250  10.546%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.757964/  1.179533, val:  65.83%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9451%\n",
      "layer   2  Sparsity: 75.0117%\n",
      "layer   3  Sparsity: 71.4146%\n",
      "total_backward_count 1292280 real_backward_count 136213  10.541%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.747628/  1.219224, val:  61.25%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9595%\n",
      "layer   2  Sparsity: 75.0409%\n",
      "layer   3  Sparsity: 71.9966%\n",
      "total_backward_count 1302070 real_backward_count 137077  10.528%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.795111/  1.261828, val:  60.42%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8986%\n",
      "layer   2  Sparsity: 74.4356%\n",
      "layer   3  Sparsity: 73.3181%\n",
      "total_backward_count 1311860 real_backward_count 137993  10.519%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.774708/  1.284453, val:  55.42%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9446%\n",
      "layer   2  Sparsity: 74.4246%\n",
      "layer   3  Sparsity: 72.3345%\n",
      "total_backward_count 1321650 real_backward_count 138891  10.509%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.771821/  1.198906, val:  64.17%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9015%\n",
      "layer   2  Sparsity: 74.7366%\n",
      "layer   3  Sparsity: 71.4114%\n",
      "total_backward_count 1331440 real_backward_count 139784  10.499%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.747985/  1.225104, val:  57.50%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9343%\n",
      "layer   2  Sparsity: 74.5017%\n",
      "layer   3  Sparsity: 71.6497%\n",
      "total_backward_count 1341230 real_backward_count 140655  10.487%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.771806/  1.241187, val:  58.33%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8933%\n",
      "layer   2  Sparsity: 74.5742%\n",
      "layer   3  Sparsity: 71.0071%\n",
      "total_backward_count 1351020 real_backward_count 141482  10.472%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.768923/  1.215281, val:  64.58%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9114%\n",
      "layer   2  Sparsity: 74.7950%\n",
      "layer   3  Sparsity: 70.4317%\n",
      "total_backward_count 1360810 real_backward_count 142401  10.464%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.760610/  1.199880, val:  64.17%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9453%\n",
      "layer   2  Sparsity: 74.8347%\n",
      "layer   3  Sparsity: 70.9204%\n",
      "total_backward_count 1370600 real_backward_count 143358  10.460%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.763593/  1.265355, val:  55.83%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9503%\n",
      "layer   2  Sparsity: 74.8308%\n",
      "layer   3  Sparsity: 70.8677%\n",
      "total_backward_count 1380390 real_backward_count 144277  10.452%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.779757/  1.200388, val:  65.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8835%\n",
      "layer   2  Sparsity: 74.8838%\n",
      "layer   3  Sparsity: 71.0408%\n",
      "total_backward_count 1390180 real_backward_count 145195  10.444%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.778699/  1.298753, val:  57.08%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9495%\n",
      "layer   2  Sparsity: 74.3596%\n",
      "layer   3  Sparsity: 71.7115%\n",
      "total_backward_count 1399970 real_backward_count 146118  10.437%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.763878/  1.227105, val:  61.25%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9748%\n",
      "layer   2  Sparsity: 74.3724%\n",
      "layer   3  Sparsity: 71.7812%\n",
      "total_backward_count 1409760 real_backward_count 146983  10.426%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.774455/  1.301144, val:  60.83%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9459%\n",
      "layer   2  Sparsity: 74.4721%\n",
      "layer   3  Sparsity: 72.3333%\n",
      "total_backward_count 1419550 real_backward_count 147862  10.416%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.752157/  1.175180, val:  63.33%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8935%\n",
      "layer   2  Sparsity: 74.8811%\n",
      "layer   3  Sparsity: 72.5900%\n",
      "total_backward_count 1429340 real_backward_count 148691  10.403%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.752993/  1.240273, val:  61.67%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9358%\n",
      "layer   2  Sparsity: 74.8923%\n",
      "layer   3  Sparsity: 72.5271%\n",
      "total_backward_count 1439130 real_backward_count 149564  10.393%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.720756/  1.162159, val:  66.25%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9337%\n",
      "layer   2  Sparsity: 75.0498%\n",
      "layer   3  Sparsity: 72.2639%\n",
      "total_backward_count 1448920 real_backward_count 150369  10.378%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.768213/  1.282480, val:  57.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9688%\n",
      "layer   2  Sparsity: 75.3290%\n",
      "layer   3  Sparsity: 73.4003%\n",
      "total_backward_count 1458710 real_backward_count 151231  10.367%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.756728/  1.237154, val:  64.58%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9210%\n",
      "layer   2  Sparsity: 75.2781%\n",
      "layer   3  Sparsity: 72.6684%\n",
      "total_backward_count 1468500 real_backward_count 152078  10.356%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.736398/  1.282953, val:  54.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9048%\n",
      "layer   2  Sparsity: 75.2409%\n",
      "layer   3  Sparsity: 72.4832%\n",
      "total_backward_count 1478290 real_backward_count 152924  10.345%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.732778/  1.221688, val:  65.83%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8981%\n",
      "layer   2  Sparsity: 74.8540%\n",
      "layer   3  Sparsity: 72.1112%\n",
      "total_backward_count 1488080 real_backward_count 153815  10.336%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.719876/  1.209646, val:  57.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9234%\n",
      "layer   2  Sparsity: 74.7652%\n",
      "layer   3  Sparsity: 72.4823%\n",
      "total_backward_count 1497870 real_backward_count 154654  10.325%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.751229/  1.161432, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8621%\n",
      "layer   2  Sparsity: 74.7402%\n",
      "layer   3  Sparsity: 72.5145%\n",
      "total_backward_count 1507660 real_backward_count 155526  10.316%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.741413/  1.187016, val:  66.67%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9064%\n",
      "layer   2  Sparsity: 74.6355%\n",
      "layer   3  Sparsity: 72.4347%\n",
      "total_backward_count 1517450 real_backward_count 156427  10.309%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.731440/  1.217995, val:  66.25%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8987%\n",
      "layer   2  Sparsity: 75.1881%\n",
      "layer   3  Sparsity: 73.2219%\n",
      "total_backward_count 1527240 real_backward_count 157260  10.297%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.727737/  1.176732, val:  62.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8938%\n",
      "layer   2  Sparsity: 75.0829%\n",
      "layer   3  Sparsity: 72.3730%\n",
      "total_backward_count 1537030 real_backward_count 158128  10.288%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.722587/  1.204890, val:  57.50%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9272%\n",
      "layer   2  Sparsity: 75.0813%\n",
      "layer   3  Sparsity: 72.5922%\n",
      "total_backward_count 1546820 real_backward_count 158959  10.277%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.727383/  1.194450, val:  61.25%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9168%\n",
      "layer   2  Sparsity: 75.0898%\n",
      "layer   3  Sparsity: 72.1747%\n",
      "total_backward_count 1556610 real_backward_count 159830  10.268%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.727131/  1.200322, val:  60.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9347%\n",
      "layer   2  Sparsity: 75.0002%\n",
      "layer   3  Sparsity: 71.6024%\n",
      "total_backward_count 1566400 real_backward_count 160714  10.260%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.711920/  1.175223, val:  62.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9140%\n",
      "layer   2  Sparsity: 74.5911%\n",
      "layer   3  Sparsity: 71.6997%\n",
      "total_backward_count 1576190 real_backward_count 161550  10.249%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.726336/  1.134155, val:  69.17%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9030%\n",
      "layer   2  Sparsity: 74.7765%\n",
      "layer   3  Sparsity: 71.2009%\n",
      "total_backward_count 1585980 real_backward_count 162415  10.241%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.731505/  1.210904, val:  58.33%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8906%\n",
      "layer   2  Sparsity: 74.6823%\n",
      "layer   3  Sparsity: 71.6885%\n",
      "total_backward_count 1595770 real_backward_count 163297  10.233%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.711759/  1.238969, val:  59.17%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8890%\n",
      "layer   2  Sparsity: 74.5424%\n",
      "layer   3  Sparsity: 70.9873%\n",
      "total_backward_count 1605560 real_backward_count 164142  10.223%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.724247/  1.218558, val:  65.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.9074%\n",
      "layer   2  Sparsity: 74.3445%\n",
      "layer   3  Sparsity: 72.3277%\n",
      "total_backward_count 1615350 real_backward_count 164976  10.213%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.743254/  1.200704, val:  63.33%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9111%\n",
      "layer   2  Sparsity: 74.2621%\n",
      "layer   3  Sparsity: 72.6224%\n",
      "total_backward_count 1625140 real_backward_count 165879  10.207%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.730212/  1.261055, val:  62.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9173%\n",
      "layer   2  Sparsity: 74.3669%\n",
      "layer   3  Sparsity: 73.8770%\n",
      "total_backward_count 1634930 real_backward_count 166748  10.199%\n",
      "lif layer 2 self.abs_max_v: 5525.5\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.736831/  1.282090, val:  55.42%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9352%\n",
      "layer   2  Sparsity: 74.5588%\n",
      "layer   3  Sparsity: 74.6420%\n",
      "total_backward_count 1644720 real_backward_count 167595  10.190%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.761440/  1.240640, val:  64.58%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9169%\n",
      "layer   2  Sparsity: 74.7331%\n",
      "layer   3  Sparsity: 74.5400%\n",
      "total_backward_count 1654510 real_backward_count 168435  10.180%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.757057/  1.211854, val:  65.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9236%\n",
      "layer   2  Sparsity: 74.9244%\n",
      "layer   3  Sparsity: 75.1846%\n",
      "total_backward_count 1664300 real_backward_count 169308  10.173%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.735172/  1.198272, val:  66.25%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.8862%\n",
      "layer   2  Sparsity: 75.0879%\n",
      "layer   3  Sparsity: 75.3680%\n",
      "total_backward_count 1674090 real_backward_count 170120  10.162%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.756173/  1.258092, val:  62.08%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8881%\n",
      "layer   2  Sparsity: 74.9064%\n",
      "layer   3  Sparsity: 75.2627%\n",
      "total_backward_count 1683880 real_backward_count 170988  10.154%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.776565/  1.183348, val:  63.33%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9435%\n",
      "layer   2  Sparsity: 74.2918%\n",
      "layer   3  Sparsity: 75.6011%\n",
      "total_backward_count 1693670 real_backward_count 171849  10.147%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.751608/  1.254882, val:  67.92%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9478%\n",
      "layer   2  Sparsity: 74.3310%\n",
      "layer   3  Sparsity: 75.4492%\n",
      "total_backward_count 1703460 real_backward_count 172648  10.135%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.794888/  1.244114, val:  62.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9422%\n",
      "layer   2  Sparsity: 74.2625%\n",
      "layer   3  Sparsity: 75.0684%\n",
      "total_backward_count 1713250 real_backward_count 173458  10.125%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.779551/  1.205520, val:  72.08%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.8723%\n",
      "layer   2  Sparsity: 74.4027%\n",
      "layer   3  Sparsity: 74.6580%\n",
      "total_backward_count 1723040 real_backward_count 174331  10.118%\n",
      "lif layer 2 self.abs_max_v: 5581.0\n",
      "lif layer 2 self.abs_max_v: 5590.5\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.756104/  1.244675, val:  58.75%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9198%\n",
      "layer   2  Sparsity: 74.4904%\n",
      "layer   3  Sparsity: 74.7885%\n",
      "total_backward_count 1732830 real_backward_count 175158  10.108%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.762284/  1.309423, val:  53.75%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9545%\n",
      "layer   2  Sparsity: 74.7712%\n",
      "layer   3  Sparsity: 74.4632%\n",
      "total_backward_count 1742620 real_backward_count 175958  10.097%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.766691/  1.212220, val:  68.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9497%\n",
      "layer   2  Sparsity: 74.5539%\n",
      "layer   3  Sparsity: 74.2287%\n",
      "total_backward_count 1752410 real_backward_count 176774  10.087%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.780494/  1.232225, val:  60.83%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9219%\n",
      "layer   2  Sparsity: 74.5258%\n",
      "layer   3  Sparsity: 74.8539%\n",
      "total_backward_count 1762200 real_backward_count 177604  10.079%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.786719/  1.206506, val:  63.75%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9449%\n",
      "layer   2  Sparsity: 74.9287%\n",
      "layer   3  Sparsity: 73.8091%\n",
      "total_backward_count 1771990 real_backward_count 178395  10.067%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.756762/  1.259424, val:  59.17%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8882%\n",
      "layer   2  Sparsity: 74.9992%\n",
      "layer   3  Sparsity: 73.8928%\n",
      "total_backward_count 1781780 real_backward_count 179225  10.059%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.782617/  1.222310, val:  71.25%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9217%\n",
      "layer   2  Sparsity: 75.1517%\n",
      "layer   3  Sparsity: 74.0594%\n",
      "total_backward_count 1791570 real_backward_count 180042  10.049%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.777758/  1.265903, val:  60.42%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8917%\n",
      "layer   2  Sparsity: 75.1589%\n",
      "layer   3  Sparsity: 74.3822%\n",
      "total_backward_count 1801360 real_backward_count 180920  10.044%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.767830/  1.301129, val:  55.42%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9148%\n",
      "layer   2  Sparsity: 75.0836%\n",
      "layer   3  Sparsity: 74.5370%\n",
      "total_backward_count 1811150 real_backward_count 181748  10.035%\n",
      "lif layer 2 self.abs_max_v: 5622.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.753894/  1.199048, val:  68.75%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.8959%\n",
      "layer   2  Sparsity: 75.2790%\n",
      "layer   3  Sparsity: 73.7199%\n",
      "total_backward_count 1820940 real_backward_count 182620  10.029%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.745973/  1.250181, val:  60.00%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9351%\n",
      "layer   2  Sparsity: 74.5467%\n",
      "layer   3  Sparsity: 73.4442%\n",
      "total_backward_count 1830730 real_backward_count 183452  10.021%\n",
      "fc layer 2 self.abs_max_out: 3077.0\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.768479/  1.278555, val:  65.83%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9176%\n",
      "layer   2  Sparsity: 74.2954%\n",
      "layer   3  Sparsity: 73.2477%\n",
      "total_backward_count 1840520 real_backward_count 184295  10.013%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.769221/  1.235498, val:  69.58%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9216%\n",
      "layer   2  Sparsity: 74.9885%\n",
      "layer   3  Sparsity: 74.1294%\n",
      "total_backward_count 1850310 real_backward_count 185115  10.005%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.768049/  1.181352, val:  66.25%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.9254%\n",
      "layer   2  Sparsity: 74.6570%\n",
      "layer   3  Sparsity: 74.3132%\n",
      "total_backward_count 1860100 real_backward_count 185938   9.996%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.743275/  1.164286, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9274%\n",
      "layer   2  Sparsity: 74.9703%\n",
      "layer   3  Sparsity: 74.2592%\n",
      "total_backward_count 1869890 real_backward_count 186709   9.985%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.751947/  1.223522, val:  55.83%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8983%\n",
      "layer   2  Sparsity: 74.8223%\n",
      "layer   3  Sparsity: 73.6637%\n",
      "total_backward_count 1879680 real_backward_count 187566   9.979%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.721070/  1.161433, val:  69.58%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9138%\n",
      "layer   2  Sparsity: 74.8009%\n",
      "layer   3  Sparsity: 73.6521%\n",
      "total_backward_count 1889470 real_backward_count 188381   9.970%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.718365/  1.246808, val:  55.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9204%\n",
      "layer   2  Sparsity: 75.0577%\n",
      "layer   3  Sparsity: 73.6630%\n",
      "total_backward_count 1899260 real_backward_count 189201   9.962%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.695271/  1.182504, val:  62.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9086%\n",
      "layer   2  Sparsity: 74.9363%\n",
      "layer   3  Sparsity: 73.8565%\n",
      "total_backward_count 1909050 real_backward_count 190024   9.954%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.696743/  1.205678, val:  62.92%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9345%\n",
      "layer   2  Sparsity: 74.9540%\n",
      "layer   3  Sparsity: 73.4521%\n",
      "total_backward_count 1918840 real_backward_count 190851   9.946%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.705233/  1.199432, val:  64.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.9335%\n",
      "layer   2  Sparsity: 74.9135%\n",
      "layer   3  Sparsity: 73.0723%\n",
      "total_backward_count 1928630 real_backward_count 191702   9.940%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.706810/  1.299751, val:  55.83%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.8995%\n",
      "layer   2  Sparsity: 74.7842%\n",
      "layer   3  Sparsity: 73.9215%\n",
      "total_backward_count 1938420 real_backward_count 192499   9.931%\n",
      "lif layer 2 self.abs_max_v: 5705.5\n",
      "lif layer 2 self.abs_max_v: 5733.0\n",
      "lif layer 2 self.abs_max_v: 5866.5\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.703357/  1.223104, val:  59.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.9364%\n",
      "layer   2  Sparsity: 74.9414%\n",
      "layer   3  Sparsity: 72.9039%\n",
      "total_backward_count 1948210 real_backward_count 193327   9.923%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.689650/  1.206260, val:  64.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.9208%\n",
      "layer   2  Sparsity: 75.1883%\n",
      "layer   3  Sparsity: 72.8374%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439deb664a694f8b8b756f8f900464f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñá‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÖ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñá‚ñá‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñÖ‚ñà‚ñÖ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.68965</td></tr><tr><td>val_acc_best</td><td>0.77083</td></tr><tr><td>val_acc_now</td><td>0.64583</td></tr><tr><td>val_loss</td><td>1.20626</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-47</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipkd4dwz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ipkd4dwz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_202512-ipkd4dwz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2544ytfg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_004633-2544ytfg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2544ytfg' target=\"_blank\">divine-sweep-54</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2544ytfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2544ytfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251115_004641_559', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 4, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 629.0\n",
      "lif layer 1 self.abs_max_v: 629.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 372.0\n",
      "lif layer 2 self.abs_max_v: 372.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 812.0\n",
      "lif layer 1 self.abs_max_v: 936.5\n",
      "fc layer 2 self.abs_max_out: 916.0\n",
      "lif layer 2 self.abs_max_v: 886.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 1 self.abs_max_out: 881.0\n",
      "lif layer 1 self.abs_max_v: 1291.0\n",
      "fc layer 2 self.abs_max_out: 1122.0\n",
      "lif layer 2 self.abs_max_v: 1378.5\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "lif layer 2 self.abs_max_v: 1477.5\n",
      "lif layer 1 self.abs_max_v: 1316.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1576.0\n",
      "lif layer 1 self.abs_max_v: 1392.0\n",
      "fc layer 1 self.abs_max_out: 1293.0\n",
      "lif layer 1 self.abs_max_v: 1750.5\n",
      "fc layer 1 self.abs_max_out: 1453.0\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "lif layer 2 self.abs_max_v: 1921.5\n",
      "fc layer 1 self.abs_max_out: 1478.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "fc layer 2 self.abs_max_out: 1322.0\n",
      "fc layer 3 self.abs_max_out: 359.0\n",
      "fc layer 1 self.abs_max_out: 1567.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "lif layer 2 self.abs_max_v: 2281.5\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 1 self.abs_max_out: 1821.0\n",
      "lif layer 1 self.abs_max_v: 1858.5\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "lif layer 2 self.abs_max_v: 2324.0\n",
      "lif layer 1 self.abs_max_v: 1998.5\n",
      "lif layer 1 self.abs_max_v: 2019.5\n",
      "fc layer 3 self.abs_max_out: 696.0\n",
      "fc layer 1 self.abs_max_out: 1936.0\n",
      "lif layer 1 self.abs_max_v: 2038.5\n",
      "lif layer 2 self.abs_max_v: 2401.5\n",
      "fc layer 1 self.abs_max_out: 2234.0\n",
      "lif layer 1 self.abs_max_v: 2234.0\n",
      "fc layer 1 self.abs_max_out: 2898.0\n",
      "lif layer 1 self.abs_max_v: 2898.0\n",
      "lif layer 2 self.abs_max_v: 2524.0\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "lif layer 2 self.abs_max_v: 2825.0\n",
      "lif layer 2 self.abs_max_v: 2965.0\n",
      "lif layer 2 self.abs_max_v: 3167.5\n",
      "lif layer 2 self.abs_max_v: 3398.0\n",
      "fc layer 2 self.abs_max_out: 1906.0\n",
      "fc layer 2 self.abs_max_out: 2314.0\n",
      "fc layer 2 self.abs_max_out: 2362.0\n",
      "fc layer 3 self.abs_max_out: 725.0\n",
      "lif layer 2 self.abs_max_v: 3827.5\n",
      "lif layer 2 self.abs_max_v: 3908.5\n",
      "fc layer 1 self.abs_max_out: 2910.0\n",
      "lif layer 1 self.abs_max_v: 2910.0\n",
      "lif layer 1 self.abs_max_v: 2913.5\n",
      "fc layer 1 self.abs_max_out: 3504.0\n",
      "lif layer 1 self.abs_max_v: 3504.0\n",
      "lif layer 1 self.abs_max_v: 4146.0\n",
      "fc layer 2 self.abs_max_out: 2558.0\n",
      "lif layer 2 self.abs_max_v: 3947.5\n",
      "lif layer 2 self.abs_max_v: 4004.0\n",
      "fc layer 3 self.abs_max_out: 784.0\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 4248.5\n",
      "lif layer 2 self.abs_max_v: 4149.0\n",
      "lif layer 1 self.abs_max_v: 4280.5\n",
      "lif layer 2 self.abs_max_v: 4169.5\n",
      "fc layer 2 self.abs_max_out: 2580.0\n",
      "lif layer 2 self.abs_max_v: 4266.0\n",
      "fc layer 3 self.abs_max_out: 905.0\n",
      "fc layer 2 self.abs_max_out: 2827.0\n",
      "lif layer 1 self.abs_max_v: 4680.5\n",
      "lif layer 1 self.abs_max_v: 4987.0\n",
      "lif layer 2 self.abs_max_v: 4439.0\n",
      "lif layer 1 self.abs_max_v: 5077.0\n",
      "lif layer 1 self.abs_max_v: 5254.5\n",
      "fc layer 3 self.abs_max_out: 1018.0\n",
      "fc layer 1 self.abs_max_out: 3898.0\n",
      "fc layer 3 self.abs_max_out: 1037.0\n",
      "lif layer 1 self.abs_max_v: 5310.5\n",
      "fc layer 2 self.abs_max_out: 2880.0\n",
      "fc layer 2 self.abs_max_out: 2887.0\n",
      "fc layer 1 self.abs_max_out: 3976.0\n",
      "fc layer 1 self.abs_max_out: 4408.0\n",
      "lif layer 1 self.abs_max_v: 5528.0\n",
      "lif layer 1 self.abs_max_v: 5585.5\n",
      "lif layer 1 self.abs_max_v: 5716.0\n",
      "fc layer 2 self.abs_max_out: 2900.0\n",
      "fc layer 2 self.abs_max_out: 2956.0\n",
      "lif layer 2 self.abs_max_v: 4481.0\n",
      "lif layer 1 self.abs_max_v: 5834.5\n",
      "fc layer 2 self.abs_max_out: 2976.0\n",
      "lif layer 2 self.abs_max_v: 4499.0\n",
      "lif layer 2 self.abs_max_v: 4789.5\n",
      "fc layer 2 self.abs_max_out: 2984.0\n",
      "fc layer 2 self.abs_max_out: 3140.0\n",
      "fc layer 3 self.abs_max_out: 1049.0\n",
      "fc layer 3 self.abs_max_out: 1054.0\n",
      "fc layer 2 self.abs_max_out: 3246.0\n",
      "lif layer 1 self.abs_max_v: 5897.5\n",
      "lif layer 1 self.abs_max_v: 6139.0\n",
      "fc layer 2 self.abs_max_out: 3272.0\n",
      "lif layer 1 self.abs_max_v: 6982.5\n",
      "lif layer 1 self.abs_max_v: 7292.5\n",
      "fc layer 1 self.abs_max_out: 5252.0\n",
      "fc layer 3 self.abs_max_out: 1148.0\n",
      "fc layer 2 self.abs_max_out: 3454.0\n",
      "fc layer 3 self.abs_max_out: 1171.0\n",
      "fc layer 3 self.abs_max_out: 1213.0\n",
      "fc layer 3 self.abs_max_out: 1273.0\n",
      "fc layer 3 self.abs_max_out: 1289.0\n",
      "lif layer 1 self.abs_max_v: 7301.0\n",
      "lif layer 1 self.abs_max_v: 7414.5\n",
      "fc layer 1 self.abs_max_out: 5416.0\n",
      "lif layer 1 self.abs_max_v: 7876.5\n",
      "lif layer 1 self.abs_max_v: 8825.5\n",
      "lif layer 2 self.abs_max_v: 5011.0\n",
      "lif layer 2 self.abs_max_v: 5204.5\n",
      "lif layer 2 self.abs_max_v: 5262.0\n",
      "lif layer 2 self.abs_max_v: 5273.0\n",
      "lif layer 2 self.abs_max_v: 5365.5\n",
      "fc layer 1 self.abs_max_out: 5620.0\n",
      "fc layer 1 self.abs_max_out: 5627.0\n",
      "fc layer 2 self.abs_max_out: 3457.0\n",
      "lif layer 2 self.abs_max_v: 5453.5\n",
      "fc layer 2 self.abs_max_out: 3609.0\n",
      "fc layer 1 self.abs_max_out: 5816.0\n",
      "lif layer 1 self.abs_max_v: 9170.5\n",
      "fc layer 2 self.abs_max_out: 3667.0\n",
      "fc layer 2 self.abs_max_out: 3713.0\n",
      "fc layer 1 self.abs_max_out: 6111.0\n",
      "lif layer 2 self.abs_max_v: 5739.5\n",
      "lif layer 2 self.abs_max_v: 5815.0\n",
      "fc layer 1 self.abs_max_out: 6239.0\n",
      "fc layer 1 self.abs_max_out: 6248.0\n",
      "fc layer 1 self.abs_max_out: 6410.0\n",
      "fc layer 2 self.abs_max_out: 3820.0\n",
      "fc layer 2 self.abs_max_out: 4045.0\n",
      "lif layer 2 self.abs_max_v: 5837.0\n",
      "lif layer 1 self.abs_max_v: 9373.5\n",
      "lif layer 1 self.abs_max_v: 10091.0\n",
      "lif layer 2 self.abs_max_v: 6006.0\n",
      "lif layer 2 self.abs_max_v: 6091.0\n",
      "lif layer 2 self.abs_max_v: 6187.5\n",
      "lif layer 2 self.abs_max_v: 6243.0\n",
      "fc layer 1 self.abs_max_out: 7103.0\n",
      "lif layer 2 self.abs_max_v: 6252.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.925422/  2.055446, val:  37.92%, val_best:  37.92%, tr:  91.01%, tr_best:  91.01%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   2  Sparsity: 72.8297%\n",
      "layer   3  Sparsity: 73.3743%\n",
      "total_backward_count 9790 real_backward_count 2825  28.856%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 4052.0\n",
      "fc layer 2 self.abs_max_out: 4128.0\n",
      "fc layer 2 self.abs_max_out: 4280.0\n",
      "fc layer 2 self.abs_max_out: 4283.0\n",
      "lif layer 1 self.abs_max_v: 10296.5\n",
      "fc layer 1 self.abs_max_out: 7195.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.890231/  2.040440, val:  44.17%, val_best:  44.17%, tr:  98.37%, tr_best:  98.37%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1152%\n",
      "layer   2  Sparsity: 74.0972%\n",
      "layer   3  Sparsity: 72.3173%\n",
      "total_backward_count 19580 real_backward_count 4714  24.076%\n",
      "fc layer 1 self.abs_max_out: 7271.0\n",
      "fc layer 3 self.abs_max_out: 1294.0\n",
      "fc layer 1 self.abs_max_out: 7305.0\n",
      "lif layer 1 self.abs_max_v: 11095.5\n",
      "lif layer 1 self.abs_max_v: 11380.0\n",
      "lif layer 2 self.abs_max_v: 6363.5\n",
      "lif layer 1 self.abs_max_v: 11435.5\n",
      "lif layer 1 self.abs_max_v: 11743.0\n",
      "fc layer 1 self.abs_max_out: 7893.0\n",
      "lif layer 2 self.abs_max_v: 6420.5\n",
      "lif layer 2 self.abs_max_v: 6461.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.903936/  2.047358, val:  42.50%, val_best:  44.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   2  Sparsity: 74.2123%\n",
      "layer   3  Sparsity: 71.9294%\n",
      "total_backward_count 29370 real_backward_count 6430  21.893%\n",
      "fc layer 2 self.abs_max_out: 4497.0\n",
      "fc layer 1 self.abs_max_out: 7962.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.909024/  2.054613, val:  51.67%, val_best:  51.67%, tr:  98.88%, tr_best:  99.59%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   2  Sparsity: 73.9890%\n",
      "layer   3  Sparsity: 71.2730%\n",
      "total_backward_count 39160 real_backward_count 8004  20.439%\n",
      "fc layer 1 self.abs_max_out: 8076.0\n",
      "fc layer 1 self.abs_max_out: 8288.0\n",
      "lif layer 1 self.abs_max_v: 12384.0\n",
      "lif layer 1 self.abs_max_v: 12674.0\n",
      "lif layer 1 self.abs_max_v: 12748.0\n",
      "lif layer 1 self.abs_max_v: 12867.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.901292/  2.029845, val:  43.75%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   2  Sparsity: 73.9773%\n",
      "layer   3  Sparsity: 71.5427%\n",
      "total_backward_count 48950 real_backward_count 9462  19.330%\n",
      "fc layer 1 self.abs_max_out: 8342.0\n",
      "fc layer 1 self.abs_max_out: 8618.0\n",
      "lif layer 1 self.abs_max_v: 13030.0\n",
      "lif layer 1 self.abs_max_v: 13071.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.909949/  2.058983, val:  49.17%, val_best:  51.67%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0903%\n",
      "layer   2  Sparsity: 73.7521%\n",
      "layer   3  Sparsity: 70.9640%\n",
      "total_backward_count 58740 real_backward_count 10898  18.553%\n",
      "lif layer 2 self.abs_max_v: 6527.0\n",
      "lif layer 2 self.abs_max_v: 6594.0\n",
      "lif layer 2 self.abs_max_v: 6658.0\n",
      "lif layer 1 self.abs_max_v: 13325.0\n",
      "lif layer 1 self.abs_max_v: 13779.5\n",
      "lif layer 1 self.abs_max_v: 14082.5\n",
      "lif layer 1 self.abs_max_v: 14104.5\n",
      "lif layer 1 self.abs_max_v: 14332.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.905963/  2.029126, val:  49.58%, val_best:  51.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1324%\n",
      "layer   2  Sparsity: 74.2640%\n",
      "layer   3  Sparsity: 71.1238%\n",
      "total_backward_count 68530 real_backward_count 12347  18.017%\n",
      "fc layer 1 self.abs_max_out: 8756.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.887411/  2.031230, val:  53.33%, val_best:  53.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   2  Sparsity: 74.0677%\n",
      "layer   3  Sparsity: 70.6765%\n",
      "total_backward_count 78320 real_backward_count 13682  17.469%\n",
      "fc layer 1 self.abs_max_out: 9010.0\n",
      "lif layer 2 self.abs_max_v: 6683.0\n",
      "lif layer 2 self.abs_max_v: 6721.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.900047/  2.002836, val:  52.08%, val_best:  53.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1163%\n",
      "layer   2  Sparsity: 74.1470%\n",
      "layer   3  Sparsity: 71.7340%\n",
      "total_backward_count 88110 real_backward_count 15089  17.125%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.893032/  2.029399, val:  55.00%, val_best:  55.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 74.8720%\n",
      "layer   3  Sparsity: 71.3151%\n",
      "total_backward_count 97900 real_backward_count 16451  16.804%\n",
      "fc layer 2 self.abs_max_out: 4594.0\n",
      "fc layer 2 self.abs_max_out: 4728.0\n",
      "fc layer 1 self.abs_max_out: 9011.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.889824/  2.026373, val:  52.50%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 74.0290%\n",
      "layer   3  Sparsity: 71.7727%\n",
      "total_backward_count 107690 real_backward_count 17762  16.494%\n",
      "fc layer 1 self.abs_max_out: 9533.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.909341/  2.041408, val:  50.83%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   2  Sparsity: 73.7729%\n",
      "layer   3  Sparsity: 72.2562%\n",
      "total_backward_count 117480 real_backward_count 19176  16.323%\n",
      "fc layer 1 self.abs_max_out: 9750.0\n",
      "lif layer 1 self.abs_max_v: 14566.5\n",
      "lif layer 1 self.abs_max_v: 15556.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.904329/  2.031247, val:  50.83%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1194%\n",
      "layer   2  Sparsity: 74.3423%\n",
      "layer   3  Sparsity: 72.8603%\n",
      "total_backward_count 127270 real_backward_count 20451  16.069%\n",
      "fc layer 2 self.abs_max_out: 4867.0\n",
      "lif layer 2 self.abs_max_v: 7042.0\n",
      "lif layer 2 self.abs_max_v: 7356.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.909249/  2.051151, val:  45.00%, val_best:  55.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1252%\n",
      "layer   2  Sparsity: 74.0205%\n",
      "layer   3  Sparsity: 72.9206%\n",
      "total_backward_count 137060 real_backward_count 21766  15.881%\n",
      "fc layer 1 self.abs_max_out: 9799.0\n",
      "fc layer 2 self.abs_max_out: 4890.0\n",
      "fc layer 2 self.abs_max_out: 4914.0\n",
      "lif layer 1 self.abs_max_v: 15798.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.910810/  2.031868, val:  44.58%, val_best:  55.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1278%\n",
      "layer   2  Sparsity: 73.4574%\n",
      "layer   3  Sparsity: 72.3445%\n",
      "total_backward_count 146850 real_backward_count 23003  15.664%\n",
      "fc layer 1 self.abs_max_out: 9900.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.907988/  2.025498, val:  55.00%, val_best:  55.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 74.4333%\n",
      "layer   3  Sparsity: 72.6466%\n",
      "total_backward_count 156640 real_backward_count 24255  15.485%\n",
      "fc layer 1 self.abs_max_out: 10141.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.898325/  2.031240, val:  62.50%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1132%\n",
      "layer   2  Sparsity: 73.9874%\n",
      "layer   3  Sparsity: 72.7190%\n",
      "total_backward_count 166430 real_backward_count 25505  15.325%\n",
      "fc layer 2 self.abs_max_out: 4985.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.903997/  2.016768, val:  61.67%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1151%\n",
      "layer   2  Sparsity: 73.5886%\n",
      "layer   3  Sparsity: 72.5982%\n",
      "total_backward_count 176220 real_backward_count 26784  15.199%\n",
      "fc layer 2 self.abs_max_out: 5032.0\n",
      "fc layer 1 self.abs_max_out: 10394.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.903367/  2.026644, val:  56.67%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 73.8959%\n",
      "layer   3  Sparsity: 72.8874%\n",
      "total_backward_count 186010 real_backward_count 28088  15.100%\n",
      "fc layer 1 self.abs_max_out: 10644.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.896098/  2.038616, val:  42.08%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1150%\n",
      "layer   2  Sparsity: 73.3526%\n",
      "layer   3  Sparsity: 72.5381%\n",
      "total_backward_count 195800 real_backward_count 29252  14.940%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.899613/  2.049316, val:  43.75%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   2  Sparsity: 73.4522%\n",
      "layer   3  Sparsity: 72.3005%\n",
      "total_backward_count 205590 real_backward_count 30443  14.808%\n",
      "fc layer 2 self.abs_max_out: 5075.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.906444/  2.046715, val:  50.83%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 72.7112%\n",
      "layer   3  Sparsity: 72.6204%\n",
      "total_backward_count 215380 real_backward_count 31696  14.716%\n",
      "fc layer 2 self.abs_max_out: 5081.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.908305/  2.013476, val:  60.00%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   2  Sparsity: 73.0101%\n",
      "layer   3  Sparsity: 72.8170%\n",
      "total_backward_count 225170 real_backward_count 32931  14.625%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.894022/  2.006213, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1193%\n",
      "layer   2  Sparsity: 72.9079%\n",
      "layer   3  Sparsity: 72.0256%\n",
      "total_backward_count 234960 real_backward_count 34177  14.546%\n",
      "fc layer 2 self.abs_max_out: 5142.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.890671/  1.998314, val:  62.08%, val_best:  63.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   2  Sparsity: 72.7771%\n",
      "layer   3  Sparsity: 72.1247%\n",
      "total_backward_count 244750 real_backward_count 35377  14.454%\n",
      "fc layer 1 self.abs_max_out: 10840.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.893881/  1.993393, val:  60.00%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   2  Sparsity: 73.1029%\n",
      "layer   3  Sparsity: 72.0144%\n",
      "total_backward_count 254540 real_backward_count 36614  14.384%\n",
      "fc layer 2 self.abs_max_out: 5342.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.880170/  1.997001, val:  57.92%, val_best:  63.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 72.5628%\n",
      "layer   3  Sparsity: 72.8778%\n",
      "total_backward_count 264330 real_backward_count 37821  14.308%\n",
      "lif layer 1 self.abs_max_v: 16193.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.874924/  2.006222, val:  59.17%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1193%\n",
      "layer   2  Sparsity: 72.5578%\n",
      "layer   3  Sparsity: 73.4605%\n",
      "total_backward_count 274120 real_backward_count 39044  14.243%\n",
      "lif layer 1 self.abs_max_v: 16929.5\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.884740/  2.018126, val:  54.58%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1293%\n",
      "layer   2  Sparsity: 72.3795%\n",
      "layer   3  Sparsity: 73.3603%\n",
      "total_backward_count 283910 real_backward_count 40207  14.162%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.883550/  1.992249, val:  54.17%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1242%\n",
      "layer   2  Sparsity: 73.2188%\n",
      "layer   3  Sparsity: 73.1919%\n",
      "total_backward_count 293700 real_backward_count 41450  14.113%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.876733/  2.000796, val:  60.42%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1243%\n",
      "layer   2  Sparsity: 72.4408%\n",
      "layer   3  Sparsity: 72.6803%\n",
      "total_backward_count 303490 real_backward_count 42679  14.063%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.881190/  2.019377, val:  52.92%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1337%\n",
      "layer   2  Sparsity: 72.8358%\n",
      "layer   3  Sparsity: 73.4452%\n",
      "total_backward_count 313280 real_backward_count 43881  14.007%\n",
      "lif layer 1 self.abs_max_v: 17328.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.870595/  1.997196, val:  59.58%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   2  Sparsity: 72.5989%\n",
      "layer   3  Sparsity: 72.3383%\n",
      "total_backward_count 323070 real_backward_count 45067  13.950%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.868421/  1.987272, val:  54.17%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1287%\n",
      "layer   2  Sparsity: 72.8913%\n",
      "layer   3  Sparsity: 72.9300%\n",
      "total_backward_count 332860 real_backward_count 46259  13.897%\n",
      "fc layer 1 self.abs_max_out: 11156.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.867028/  1.986271, val:  57.08%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1349%\n",
      "layer   2  Sparsity: 72.6847%\n",
      "layer   3  Sparsity: 72.9748%\n",
      "total_backward_count 342650 real_backward_count 47446  13.847%\n",
      "fc layer 2 self.abs_max_out: 5434.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.876172/  2.010221, val:  55.00%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1258%\n",
      "layer   2  Sparsity: 72.6450%\n",
      "layer   3  Sparsity: 73.6645%\n",
      "total_backward_count 352440 real_backward_count 48659  13.806%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.884070/  2.010538, val:  60.00%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 71.8917%\n",
      "layer   3  Sparsity: 73.8468%\n",
      "total_backward_count 362230 real_backward_count 49864  13.766%\n",
      "fc layer 2 self.abs_max_out: 5461.0\n",
      "fc layer 2 self.abs_max_out: 5468.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.885591/  2.008484, val:  50.00%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   2  Sparsity: 71.7845%\n",
      "layer   3  Sparsity: 74.2308%\n",
      "total_backward_count 372020 real_backward_count 51008  13.711%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.890019/  2.016266, val:  59.58%, val_best:  63.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1335%\n",
      "layer   2  Sparsity: 72.0153%\n",
      "layer   3  Sparsity: 73.9879%\n",
      "total_backward_count 381810 real_backward_count 52229  13.679%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.883680/  2.010096, val:  53.33%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1326%\n",
      "layer   2  Sparsity: 72.2480%\n",
      "layer   3  Sparsity: 73.9742%\n",
      "total_backward_count 391600 real_backward_count 53344  13.622%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.884766/  2.004797, val:  63.75%, val_best:  63.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   2  Sparsity: 72.2788%\n",
      "layer   3  Sparsity: 74.2795%\n",
      "total_backward_count 401390 real_backward_count 54496  13.577%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.898215/  2.007460, val:  69.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1205%\n",
      "layer   2  Sparsity: 72.0039%\n",
      "layer   3  Sparsity: 74.0240%\n",
      "total_backward_count 411180 real_backward_count 55700  13.546%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.889349/  1.994263, val:  62.50%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1187%\n",
      "layer   2  Sparsity: 72.1103%\n",
      "layer   3  Sparsity: 73.7330%\n",
      "total_backward_count 420970 real_backward_count 56900  13.516%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.877948/  1.986813, val:  66.67%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   2  Sparsity: 71.9849%\n",
      "layer   3  Sparsity: 74.1370%\n",
      "total_backward_count 430760 real_backward_count 58010  13.467%\n",
      "fc layer 1 self.abs_max_out: 11532.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.887066/  2.008465, val:  55.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1321%\n",
      "layer   2  Sparsity: 72.6551%\n",
      "layer   3  Sparsity: 74.8224%\n",
      "total_backward_count 440550 real_backward_count 59162  13.429%\n",
      "lif layer 1 self.abs_max_v: 17484.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.873367/  1.974228, val:  58.33%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1261%\n",
      "layer   2  Sparsity: 72.1225%\n",
      "layer   3  Sparsity: 73.7502%\n",
      "total_backward_count 450340 real_backward_count 60263  13.382%\n",
      "fc layer 2 self.abs_max_out: 5692.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.864253/  1.975230, val:  60.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   2  Sparsity: 71.8313%\n",
      "layer   3  Sparsity: 73.3707%\n",
      "total_backward_count 460130 real_backward_count 61421  13.349%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.860342/  2.006928, val:  57.50%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   2  Sparsity: 72.3211%\n",
      "layer   3  Sparsity: 73.9252%\n",
      "total_backward_count 469920 real_backward_count 62541  13.309%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.884739/  2.007368, val:  60.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 72.2766%\n",
      "layer   3  Sparsity: 75.0925%\n",
      "total_backward_count 479710 real_backward_count 63669  13.272%\n",
      "lif layer 1 self.abs_max_v: 17726.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.891057/  1.997968, val:  56.25%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   2  Sparsity: 71.9623%\n",
      "layer   3  Sparsity: 75.0470%\n",
      "total_backward_count 489500 real_backward_count 64835  13.245%\n",
      "lif layer 1 self.abs_max_v: 18253.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.888109/  1.998820, val:  54.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   2  Sparsity: 71.7091%\n",
      "layer   3  Sparsity: 75.0907%\n",
      "total_backward_count 499290 real_backward_count 65996  13.218%\n",
      "fc layer 1 self.abs_max_out: 11538.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.883429/  2.025186, val:  50.42%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1211%\n",
      "layer   2  Sparsity: 71.5541%\n",
      "layer   3  Sparsity: 75.1995%\n",
      "total_backward_count 509080 real_backward_count 67104  13.181%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.886141/  2.006017, val:  63.33%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   2  Sparsity: 71.3934%\n",
      "layer   3  Sparsity: 74.8783%\n",
      "total_backward_count 518870 real_backward_count 68288  13.161%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.883705/  1.997946, val:  52.50%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   2  Sparsity: 71.4073%\n",
      "layer   3  Sparsity: 74.7514%\n",
      "total_backward_count 528660 real_backward_count 69472  13.141%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.882582/  2.000968, val:  51.67%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   2  Sparsity: 71.3644%\n",
      "layer   3  Sparsity: 73.9536%\n",
      "total_backward_count 538450 real_backward_count 70556  13.104%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.884033/  2.015100, val:  59.58%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   2  Sparsity: 71.4152%\n",
      "layer   3  Sparsity: 74.6413%\n",
      "total_backward_count 548240 real_backward_count 71758  13.089%\n",
      "fc layer 1 self.abs_max_out: 11552.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.877101/  2.025159, val:  54.17%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   2  Sparsity: 71.8426%\n",
      "layer   3  Sparsity: 75.1194%\n",
      "total_backward_count 558030 real_backward_count 72919  13.067%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.878224/  1.984889, val:  62.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1263%\n",
      "layer   2  Sparsity: 71.6807%\n",
      "layer   3  Sparsity: 74.2397%\n",
      "total_backward_count 567820 real_backward_count 74059  13.043%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.878006/  1.984610, val:  66.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   2  Sparsity: 71.5048%\n",
      "layer   3  Sparsity: 75.6278%\n",
      "total_backward_count 577610 real_backward_count 75149  13.010%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.878862/  2.003686, val:  55.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1280%\n",
      "layer   2  Sparsity: 71.6836%\n",
      "layer   3  Sparsity: 74.5591%\n",
      "total_backward_count 587400 real_backward_count 76255  12.982%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.870463/  1.975104, val:  58.33%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 71.7369%\n",
      "layer   3  Sparsity: 74.0368%\n",
      "total_backward_count 597190 real_backward_count 77442  12.968%\n",
      "fc layer 1 self.abs_max_out: 11610.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.868376/  1.976210, val:  70.00%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   2  Sparsity: 71.3027%\n",
      "layer   3  Sparsity: 74.1513%\n",
      "total_backward_count 606980 real_backward_count 78603  12.950%\n",
      "lif layer 1 self.abs_max_v: 19392.5\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.863649/  2.001299, val:  65.42%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 71.2962%\n",
      "layer   3  Sparsity: 74.4259%\n",
      "total_backward_count 616770 real_backward_count 79802  12.939%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.872728/  1.999707, val:  55.42%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1001%\n",
      "layer   2  Sparsity: 71.4244%\n",
      "layer   3  Sparsity: 75.2838%\n",
      "total_backward_count 626560 real_backward_count 80895  12.911%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.879638/  1.991044, val:  65.83%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1214%\n",
      "layer   2  Sparsity: 71.0707%\n",
      "layer   3  Sparsity: 76.1587%\n",
      "total_backward_count 636350 real_backward_count 81995  12.885%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.879791/  2.001223, val:  62.92%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1146%\n",
      "layer   2  Sparsity: 70.6195%\n",
      "layer   3  Sparsity: 74.3941%\n",
      "total_backward_count 646140 real_backward_count 83119  12.864%\n",
      "lif layer 2 self.abs_max_v: 7420.0\n",
      "lif layer 2 self.abs_max_v: 7475.0\n",
      "fc layer 1 self.abs_max_out: 11817.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.871952/  1.982380, val:  66.67%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   2  Sparsity: 71.5215%\n",
      "layer   3  Sparsity: 74.8183%\n",
      "total_backward_count 655930 real_backward_count 84200  12.837%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.867679/  1.982060, val:  60.83%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 71.0793%\n",
      "layer   3  Sparsity: 74.5721%\n",
      "total_backward_count 665720 real_backward_count 85286  12.811%\n",
      "fc layer 2 self.abs_max_out: 5722.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.880275/  1.982533, val:  75.00%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   2  Sparsity: 71.0498%\n",
      "layer   3  Sparsity: 74.7007%\n",
      "total_backward_count 675510 real_backward_count 86349  12.783%\n",
      "fc layer 2 self.abs_max_out: 5939.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.866710/  2.009319, val:  50.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1165%\n",
      "layer   2  Sparsity: 70.8231%\n",
      "layer   3  Sparsity: 73.7093%\n",
      "total_backward_count 685300 real_backward_count 87418  12.756%\n",
      "fc layer 2 self.abs_max_out: 6087.0\n",
      "lif layer 2 self.abs_max_v: 7659.5\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.862917/  1.974633, val:  68.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1340%\n",
      "layer   2  Sparsity: 70.8063%\n",
      "layer   3  Sparsity: 73.6493%\n",
      "total_backward_count 695090 real_backward_count 88513  12.734%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.870164/  2.008315, val:  55.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 70.9642%\n",
      "layer   3  Sparsity: 74.0397%\n",
      "total_backward_count 704880 real_backward_count 89642  12.717%\n",
      "fc layer 2 self.abs_max_out: 6247.0\n",
      "lif layer 2 self.abs_max_v: 7774.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.872224/  1.989846, val:  72.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1095%\n",
      "layer   2  Sparsity: 70.8293%\n",
      "layer   3  Sparsity: 74.1465%\n",
      "total_backward_count 714670 real_backward_count 90684  12.689%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.860293/  1.997221, val:  64.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1140%\n",
      "layer   2  Sparsity: 70.7194%\n",
      "layer   3  Sparsity: 74.3859%\n",
      "total_backward_count 724460 real_backward_count 91725  12.661%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.872837/  1.997113, val:  56.67%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1159%\n",
      "layer   2  Sparsity: 71.1497%\n",
      "layer   3  Sparsity: 75.0283%\n",
      "total_backward_count 734250 real_backward_count 92855  12.646%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.864527/  1.967087, val:  73.75%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   2  Sparsity: 70.7767%\n",
      "layer   3  Sparsity: 73.6632%\n",
      "total_backward_count 744040 real_backward_count 93934  12.625%\n",
      "lif layer 1 self.abs_max_v: 19549.5\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.872370/  2.004809, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1416%\n",
      "layer   2  Sparsity: 70.9529%\n",
      "layer   3  Sparsity: 75.3231%\n",
      "total_backward_count 753830 real_backward_count 95041  12.608%\n",
      "fc layer 1 self.abs_max_out: 12072.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.876474/  2.016800, val:  53.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   2  Sparsity: 71.0137%\n",
      "layer   3  Sparsity: 75.4710%\n",
      "total_backward_count 763620 real_backward_count 96062  12.580%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.869247/  1.987931, val:  52.50%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   2  Sparsity: 70.8712%\n",
      "layer   3  Sparsity: 75.2189%\n",
      "total_backward_count 773410 real_backward_count 97183  12.566%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.868846/  1.989694, val:  65.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 70.9264%\n",
      "layer   3  Sparsity: 75.4674%\n",
      "total_backward_count 783200 real_backward_count 98293  12.550%\n",
      "fc layer 1 self.abs_max_out: 12074.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.867989/  1.996571, val:  68.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1103%\n",
      "layer   2  Sparsity: 70.4292%\n",
      "layer   3  Sparsity: 74.4391%\n",
      "total_backward_count 792990 real_backward_count 99390  12.534%\n",
      "fc layer 1 self.abs_max_out: 12188.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.871105/  1.999535, val:  51.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   2  Sparsity: 70.8363%\n",
      "layer   3  Sparsity: 75.4235%\n",
      "total_backward_count 802780 real_backward_count 100457  12.514%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.861683/  1.993903, val:  55.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1205%\n",
      "layer   2  Sparsity: 70.9397%\n",
      "layer   3  Sparsity: 74.5096%\n",
      "total_backward_count 812570 real_backward_count 101488  12.490%\n",
      "lif layer 1 self.abs_max_v: 19619.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.875009/  2.001828, val:  57.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   2  Sparsity: 70.5868%\n",
      "layer   3  Sparsity: 75.9008%\n",
      "total_backward_count 822360 real_backward_count 102544  12.469%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.882001/  1.989445, val:  73.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1181%\n",
      "layer   2  Sparsity: 70.3608%\n",
      "layer   3  Sparsity: 75.2738%\n",
      "total_backward_count 832150 real_backward_count 103578  12.447%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.874537/  2.015558, val:  69.58%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1314%\n",
      "layer   2  Sparsity: 70.9673%\n",
      "layer   3  Sparsity: 75.6410%\n",
      "total_backward_count 841940 real_backward_count 104656  12.430%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.871405/  1.994763, val:  62.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1155%\n",
      "layer   2  Sparsity: 70.9336%\n",
      "layer   3  Sparsity: 75.1518%\n",
      "total_backward_count 851730 real_backward_count 105711  12.411%\n",
      "fc layer 1 self.abs_max_out: 12464.0\n",
      "lif layer 1 self.abs_max_v: 19769.5\n",
      "lif layer 1 self.abs_max_v: 20548.5\n",
      "lif layer 2 self.abs_max_v: 7996.5\n",
      "lif layer 2 self.abs_max_v: 8303.5\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.867394/  1.991302, val:  64.17%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 70.4940%\n",
      "layer   3  Sparsity: 75.7166%\n",
      "total_backward_count 861520 real_backward_count 106768  12.393%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.874687/  2.004777, val:  60.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1242%\n",
      "layer   2  Sparsity: 70.5746%\n",
      "layer   3  Sparsity: 75.8136%\n",
      "total_backward_count 871310 real_backward_count 107852  12.378%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.876218/  1.995819, val:  67.92%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   2  Sparsity: 70.5893%\n",
      "layer   3  Sparsity: 76.3094%\n",
      "total_backward_count 881100 real_backward_count 108891  12.359%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.869636/  1.999701, val:  63.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   2  Sparsity: 70.7702%\n",
      "layer   3  Sparsity: 75.8311%\n",
      "total_backward_count 890890 real_backward_count 110012  12.349%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.856891/  1.980727, val:  77.50%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1123%\n",
      "layer   2  Sparsity: 70.7213%\n",
      "layer   3  Sparsity: 74.7679%\n",
      "total_backward_count 900680 real_backward_count 111032  12.328%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.858860/  1.974735, val:  67.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 70.7067%\n",
      "layer   3  Sparsity: 75.0614%\n",
      "total_backward_count 910470 real_backward_count 112026  12.304%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.848119/  1.983542, val:  66.67%, val_best:  77.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1281%\n",
      "layer   2  Sparsity: 71.0938%\n",
      "layer   3  Sparsity: 75.0772%\n",
      "total_backward_count 920260 real_backward_count 113099  12.290%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.860720/  1.980301, val:  57.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1201%\n",
      "layer   2  Sparsity: 70.5220%\n",
      "layer   3  Sparsity: 75.6212%\n",
      "total_backward_count 930050 real_backward_count 114152  12.274%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.865811/  1.970479, val:  71.25%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   2  Sparsity: 70.6667%\n",
      "layer   3  Sparsity: 75.4734%\n",
      "total_backward_count 939840 real_backward_count 115217  12.259%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.854605/  1.964829, val:  63.75%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1084%\n",
      "layer   2  Sparsity: 70.7606%\n",
      "layer   3  Sparsity: 74.9932%\n",
      "total_backward_count 949630 real_backward_count 116240  12.241%\n",
      "lif layer 1 self.abs_max_v: 20851.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.847186/  1.960919, val:  62.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   2  Sparsity: 70.5750%\n",
      "layer   3  Sparsity: 74.2625%\n",
      "total_backward_count 959420 real_backward_count 117266  12.223%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.850285/  1.975229, val:  57.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0945%\n",
      "layer   2  Sparsity: 70.4044%\n",
      "layer   3  Sparsity: 74.7324%\n",
      "total_backward_count 969210 real_backward_count 118291  12.205%\n",
      "lif layer 1 self.abs_max_v: 21083.5\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.856225/  1.969379, val:  70.83%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1119%\n",
      "layer   2  Sparsity: 70.8000%\n",
      "layer   3  Sparsity: 75.8993%\n",
      "total_backward_count 979000 real_backward_count 119268  12.183%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.839155/  1.968706, val:  72.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1259%\n",
      "layer   2  Sparsity: 70.7193%\n",
      "layer   3  Sparsity: 75.8018%\n",
      "total_backward_count 988790 real_backward_count 120219  12.158%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.833310/  1.958351, val:  77.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1113%\n",
      "layer   2  Sparsity: 70.8688%\n",
      "layer   3  Sparsity: 75.1075%\n",
      "total_backward_count 998580 real_backward_count 121192  12.136%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.846687/  1.970784, val:  74.17%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 70.7432%\n",
      "layer   3  Sparsity: 75.1644%\n",
      "total_backward_count 1008370 real_backward_count 122140  12.113%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.839339/  1.954231, val:  75.00%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 70.7909%\n",
      "layer   3  Sparsity: 74.8432%\n",
      "total_backward_count 1018160 real_backward_count 123107  12.091%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.836039/  1.949113, val:  70.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1138%\n",
      "layer   2  Sparsity: 70.6289%\n",
      "layer   3  Sparsity: 74.4231%\n",
      "total_backward_count 1027950 real_backward_count 124115  12.074%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.841771/  1.973496, val:  76.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   2  Sparsity: 70.6612%\n",
      "layer   3  Sparsity: 74.7253%\n",
      "total_backward_count 1037740 real_backward_count 125160  12.061%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.855517/  1.994417, val:  53.75%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   2  Sparsity: 70.6520%\n",
      "layer   3  Sparsity: 75.5325%\n",
      "total_backward_count 1047530 real_backward_count 126171  12.045%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.858234/  1.974568, val:  67.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   2  Sparsity: 70.3300%\n",
      "layer   3  Sparsity: 75.1639%\n",
      "total_backward_count 1057320 real_backward_count 127183  12.029%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.855142/  1.970528, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   2  Sparsity: 70.0863%\n",
      "layer   3  Sparsity: 75.0030%\n",
      "total_backward_count 1067110 real_backward_count 128162  12.010%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.850258/  1.982644, val:  69.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1273%\n",
      "layer   2  Sparsity: 70.3389%\n",
      "layer   3  Sparsity: 75.7836%\n",
      "total_backward_count 1076900 real_backward_count 129143  11.992%\n",
      "lif layer 1 self.abs_max_v: 21475.5\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.839625/  1.953201, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1138%\n",
      "layer   2  Sparsity: 70.6448%\n",
      "layer   3  Sparsity: 75.5945%\n",
      "total_backward_count 1086690 real_backward_count 130120  11.974%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.845258/  1.974637, val:  61.67%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   2  Sparsity: 70.5691%\n",
      "layer   3  Sparsity: 75.0033%\n",
      "total_backward_count 1096480 real_backward_count 131091  11.956%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.839607/  1.964430, val:  69.58%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   2  Sparsity: 70.3366%\n",
      "layer   3  Sparsity: 74.6100%\n",
      "total_backward_count 1106270 real_backward_count 132125  11.943%\n",
      "fc layer 1 self.abs_max_out: 12486.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.843772/  1.979327, val:  61.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   2  Sparsity: 70.4120%\n",
      "layer   3  Sparsity: 74.4920%\n",
      "total_backward_count 1116060 real_backward_count 133120  11.928%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.835482/  1.961498, val:  69.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1384%\n",
      "layer   2  Sparsity: 70.4153%\n",
      "layer   3  Sparsity: 73.8649%\n",
      "total_backward_count 1125850 real_backward_count 134063  11.908%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.844629/  1.990845, val:  69.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1117%\n",
      "layer   2  Sparsity: 70.5478%\n",
      "layer   3  Sparsity: 75.1019%\n",
      "total_backward_count 1135640 real_backward_count 135052  11.892%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.846200/  1.977722, val:  56.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1361%\n",
      "layer   2  Sparsity: 70.3018%\n",
      "layer   3  Sparsity: 74.8564%\n",
      "total_backward_count 1145430 real_backward_count 135995  11.873%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.844633/  1.943137, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   2  Sparsity: 70.6677%\n",
      "layer   3  Sparsity: 74.8551%\n",
      "total_backward_count 1155220 real_backward_count 136942  11.854%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.837007/  1.954319, val:  60.42%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   2  Sparsity: 70.3361%\n",
      "layer   3  Sparsity: 75.2369%\n",
      "total_backward_count 1165010 real_backward_count 137949  11.841%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.831105/  1.955744, val:  67.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1306%\n",
      "layer   2  Sparsity: 70.8092%\n",
      "layer   3  Sparsity: 74.5280%\n",
      "total_backward_count 1174800 real_backward_count 138935  11.826%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.833016/  1.959695, val:  73.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1070%\n",
      "layer   2  Sparsity: 70.5897%\n",
      "layer   3  Sparsity: 74.7442%\n",
      "total_backward_count 1184590 real_backward_count 139900  11.810%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.836788/  1.983265, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1142%\n",
      "layer   2  Sparsity: 70.5511%\n",
      "layer   3  Sparsity: 75.2744%\n",
      "total_backward_count 1194380 real_backward_count 140819  11.790%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.839932/  1.970198, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1289%\n",
      "layer   2  Sparsity: 70.6318%\n",
      "layer   3  Sparsity: 75.4985%\n",
      "total_backward_count 1204170 real_backward_count 141763  11.773%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.840189/  1.965388, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1364%\n",
      "layer   2  Sparsity: 70.3239%\n",
      "layer   3  Sparsity: 75.1086%\n",
      "total_backward_count 1213960 real_backward_count 142705  11.755%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.841971/  1.963132, val:  69.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0998%\n",
      "layer   2  Sparsity: 70.0890%\n",
      "layer   3  Sparsity: 75.3151%\n",
      "total_backward_count 1223750 real_backward_count 143705  11.743%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.846375/  1.954345, val:  70.83%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 70.4735%\n",
      "layer   3  Sparsity: 75.4005%\n",
      "total_backward_count 1233540 real_backward_count 144685  11.729%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.833652/  1.976706, val:  57.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1102%\n",
      "layer   2  Sparsity: 70.1775%\n",
      "layer   3  Sparsity: 74.4162%\n",
      "total_backward_count 1243330 real_backward_count 145622  11.712%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.835258/  1.956090, val:  64.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   2  Sparsity: 70.4639%\n",
      "layer   3  Sparsity: 74.4304%\n",
      "total_backward_count 1253120 real_backward_count 146533  11.693%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.840150/  1.990690, val:  70.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   2  Sparsity: 70.5098%\n",
      "layer   3  Sparsity: 74.8830%\n",
      "total_backward_count 1262910 real_backward_count 147483  11.678%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.852985/  1.968138, val:  71.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1293%\n",
      "layer   2  Sparsity: 70.7753%\n",
      "layer   3  Sparsity: 75.1909%\n",
      "total_backward_count 1272700 real_backward_count 148415  11.661%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.828256/  1.973447, val:  53.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1270%\n",
      "layer   2  Sparsity: 70.6205%\n",
      "layer   3  Sparsity: 74.8596%\n",
      "total_backward_count 1282490 real_backward_count 149343  11.645%\n",
      "fc layer 2 self.abs_max_out: 6255.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.837544/  1.991495, val:  61.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1301%\n",
      "layer   2  Sparsity: 70.1394%\n",
      "layer   3  Sparsity: 75.1959%\n",
      "total_backward_count 1292280 real_backward_count 150368  11.636%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.841321/  1.963720, val:  68.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1374%\n",
      "layer   2  Sparsity: 70.2379%\n",
      "layer   3  Sparsity: 74.6455%\n",
      "total_backward_count 1302070 real_backward_count 151287  11.619%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.816104/  1.941500, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 70.2222%\n",
      "layer   3  Sparsity: 73.2933%\n",
      "total_backward_count 1311860 real_backward_count 152241  11.605%\n",
      "fc layer 2 self.abs_max_out: 6278.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.824079/  1.962126, val:  64.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1257%\n",
      "layer   2  Sparsity: 70.5829%\n",
      "layer   3  Sparsity: 74.7300%\n",
      "total_backward_count 1321650 real_backward_count 153139  11.587%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.839795/  1.962266, val:  59.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   2  Sparsity: 70.3678%\n",
      "layer   3  Sparsity: 74.8016%\n",
      "total_backward_count 1331440 real_backward_count 154131  11.576%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.826968/  1.953696, val:  75.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1184%\n",
      "layer   2  Sparsity: 70.7058%\n",
      "layer   3  Sparsity: 74.2097%\n",
      "total_backward_count 1341230 real_backward_count 155029  11.559%\n",
      "fc layer 1 self.abs_max_out: 12492.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.836759/  1.951067, val:  67.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 70.5098%\n",
      "layer   3  Sparsity: 73.7505%\n",
      "total_backward_count 1351020 real_backward_count 155965  11.544%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.830431/  1.949623, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   2  Sparsity: 70.4960%\n",
      "layer   3  Sparsity: 73.7418%\n",
      "total_backward_count 1360810 real_backward_count 156916  11.531%\n",
      "fc layer 1 self.abs_max_out: 12577.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.817601/  1.956283, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1282%\n",
      "layer   2  Sparsity: 70.2853%\n",
      "layer   3  Sparsity: 73.3564%\n",
      "total_backward_count 1370600 real_backward_count 157842  11.516%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.834124/  1.983162, val:  58.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1343%\n",
      "layer   2  Sparsity: 70.0975%\n",
      "layer   3  Sparsity: 74.4018%\n",
      "total_backward_count 1380390 real_backward_count 158807  11.505%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.844765/  1.968332, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   2  Sparsity: 70.1933%\n",
      "layer   3  Sparsity: 74.6799%\n",
      "total_backward_count 1390180 real_backward_count 159725  11.490%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.839427/  1.971083, val:  69.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1317%\n",
      "layer   2  Sparsity: 70.0226%\n",
      "layer   3  Sparsity: 73.7692%\n",
      "total_backward_count 1399970 real_backward_count 160686  11.478%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.830631/  1.969159, val:  72.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1530%\n",
      "layer   2  Sparsity: 70.6261%\n",
      "layer   3  Sparsity: 74.7381%\n",
      "total_backward_count 1409760 real_backward_count 161619  11.464%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.829809/  1.949832, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1258%\n",
      "layer   2  Sparsity: 70.3422%\n",
      "layer   3  Sparsity: 74.3772%\n",
      "total_backward_count 1419550 real_backward_count 162538  11.450%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.825604/  1.952030, val:  67.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 70.5003%\n",
      "layer   3  Sparsity: 75.3474%\n",
      "total_backward_count 1429340 real_backward_count 163464  11.436%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.826572/  1.961013, val:  62.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1184%\n",
      "layer   2  Sparsity: 70.2638%\n",
      "layer   3  Sparsity: 75.1450%\n",
      "total_backward_count 1439130 real_backward_count 164432  11.426%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.833796/  1.951960, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1194%\n",
      "layer   2  Sparsity: 70.4626%\n",
      "layer   3  Sparsity: 75.2411%\n",
      "total_backward_count 1448920 real_backward_count 165357  11.412%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.830899/  1.961671, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1439%\n",
      "layer   2  Sparsity: 70.5070%\n",
      "layer   3  Sparsity: 74.5299%\n",
      "total_backward_count 1458710 real_backward_count 166251  11.397%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.835110/  1.964612, val:  75.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   2  Sparsity: 70.2607%\n",
      "layer   3  Sparsity: 75.1130%\n",
      "total_backward_count 1468500 real_backward_count 167138  11.382%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.821517/  1.943437, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   2  Sparsity: 70.5611%\n",
      "layer   3  Sparsity: 75.2851%\n",
      "total_backward_count 1478290 real_backward_count 168010  11.365%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.806531/  1.944991, val:  63.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 70.5079%\n",
      "layer   3  Sparsity: 73.9808%\n",
      "total_backward_count 1488080 real_backward_count 168913  11.351%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.817466/  1.958405, val:  63.75%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   2  Sparsity: 70.4055%\n",
      "layer   3  Sparsity: 74.2547%\n",
      "total_backward_count 1497870 real_backward_count 169821  11.337%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.831004/  1.940063, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0511%\n",
      "layer   2  Sparsity: 70.4253%\n",
      "layer   3  Sparsity: 74.0758%\n",
      "total_backward_count 1507660 real_backward_count 170833  11.331%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.826249/  1.958901, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   2  Sparsity: 70.5110%\n",
      "layer   3  Sparsity: 74.2121%\n",
      "total_backward_count 1517450 real_backward_count 171706  11.315%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.824779/  1.957358, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   2  Sparsity: 70.3623%\n",
      "layer   3  Sparsity: 73.7902%\n",
      "total_backward_count 1527240 real_backward_count 172557  11.299%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.820435/  1.960047, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 70.1547%\n",
      "layer   3  Sparsity: 73.3612%\n",
      "total_backward_count 1537030 real_backward_count 173471  11.286%\n",
      "fc layer 1 self.abs_max_out: 12715.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.802093/  1.947381, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1150%\n",
      "layer   2  Sparsity: 70.3398%\n",
      "layer   3  Sparsity: 74.2308%\n",
      "total_backward_count 1546820 real_backward_count 174361  11.272%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.808471/  1.948013, val:  78.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   2  Sparsity: 70.3977%\n",
      "layer   3  Sparsity: 73.7022%\n",
      "total_backward_count 1556610 real_backward_count 175249  11.258%\n",
      "fc layer 1 self.abs_max_out: 12729.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.811316/  1.952482, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1156%\n",
      "layer   2  Sparsity: 70.3436%\n",
      "layer   3  Sparsity: 71.8870%\n",
      "total_backward_count 1566400 real_backward_count 176120  11.244%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.811749/  1.933420, val:  69.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   2  Sparsity: 70.2504%\n",
      "layer   3  Sparsity: 73.3788%\n",
      "total_backward_count 1576190 real_backward_count 177051  11.233%\n",
      "fc layer 1 self.abs_max_out: 12750.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.813388/  1.924055, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0948%\n",
      "layer   2  Sparsity: 70.2276%\n",
      "layer   3  Sparsity: 73.3539%\n",
      "total_backward_count 1585980 real_backward_count 177961  11.221%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.801267/  1.932225, val:  64.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 70.1690%\n",
      "layer   3  Sparsity: 73.2768%\n",
      "total_backward_count 1595770 real_backward_count 178902  11.211%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.818132/  1.963563, val:  66.25%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 70.3208%\n",
      "layer   3  Sparsity: 73.6762%\n",
      "total_backward_count 1605560 real_backward_count 179765  11.196%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.825126/  1.954372, val:  65.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   2  Sparsity: 70.2777%\n",
      "layer   3  Sparsity: 74.0185%\n",
      "total_backward_count 1615350 real_backward_count 180694  11.186%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.809833/  1.942129, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0967%\n",
      "layer   2  Sparsity: 70.4996%\n",
      "layer   3  Sparsity: 73.8775%\n",
      "total_backward_count 1625140 real_backward_count 181646  11.177%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.818118/  1.935816, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1042%\n",
      "layer   2  Sparsity: 70.4653%\n",
      "layer   3  Sparsity: 74.6564%\n",
      "total_backward_count 1634930 real_backward_count 182525  11.164%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.814900/  1.951198, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1223%\n",
      "layer   2  Sparsity: 70.1356%\n",
      "layer   3  Sparsity: 74.1261%\n",
      "total_backward_count 1644720 real_backward_count 183425  11.152%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.814811/  1.954143, val:  69.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   2  Sparsity: 69.8242%\n",
      "layer   3  Sparsity: 73.6290%\n",
      "total_backward_count 1654510 real_backward_count 184382  11.144%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.800374/  1.927106, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1118%\n",
      "layer   2  Sparsity: 70.0589%\n",
      "layer   3  Sparsity: 72.8270%\n",
      "total_backward_count 1664300 real_backward_count 185318  11.135%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.802133/  1.928471, val:  72.50%, val_best:  82.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   2  Sparsity: 70.2897%\n",
      "layer   3  Sparsity: 73.0064%\n",
      "total_backward_count 1674090 real_backward_count 186185  11.122%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.806409/  1.955250, val:  66.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 70.2679%\n",
      "layer   3  Sparsity: 73.6187%\n",
      "total_backward_count 1683880 real_backward_count 187086  11.110%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.801648/  1.935938, val:  76.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1229%\n",
      "layer   2  Sparsity: 70.0932%\n",
      "layer   3  Sparsity: 73.6351%\n",
      "total_backward_count 1693670 real_backward_count 187944  11.097%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.797118/  1.937074, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1274%\n",
      "layer   2  Sparsity: 69.7708%\n",
      "layer   3  Sparsity: 74.0944%\n",
      "total_backward_count 1703460 real_backward_count 188793  11.083%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.793416/  1.917398, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1213%\n",
      "layer   2  Sparsity: 70.1198%\n",
      "layer   3  Sparsity: 73.8624%\n",
      "total_backward_count 1713250 real_backward_count 189658  11.070%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.793253/  1.915796, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   2  Sparsity: 69.7689%\n",
      "layer   3  Sparsity: 73.2843%\n",
      "total_backward_count 1723040 real_backward_count 190562  11.060%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.783648/  1.917541, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   2  Sparsity: 70.1388%\n",
      "layer   3  Sparsity: 73.5691%\n",
      "total_backward_count 1732830 real_backward_count 191438  11.048%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.789766/  1.926801, val:  60.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1339%\n",
      "layer   2  Sparsity: 69.9515%\n",
      "layer   3  Sparsity: 73.8294%\n",
      "total_backward_count 1742620 real_backward_count 192284  11.034%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.801740/  1.951985, val:  61.25%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1276%\n",
      "layer   2  Sparsity: 69.9091%\n",
      "layer   3  Sparsity: 74.1783%\n",
      "total_backward_count 1752410 real_backward_count 193169  11.023%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.808992/  1.927229, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   2  Sparsity: 70.0962%\n",
      "layer   3  Sparsity: 73.6406%\n",
      "total_backward_count 1762200 real_backward_count 194080  11.014%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.795205/  1.909240, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1263%\n",
      "layer   2  Sparsity: 70.0723%\n",
      "layer   3  Sparsity: 72.8374%\n",
      "total_backward_count 1771990 real_backward_count 194928  11.001%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.797122/  1.940198, val:  73.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   2  Sparsity: 70.0249%\n",
      "layer   3  Sparsity: 73.7484%\n",
      "total_backward_count 1781780 real_backward_count 195818  10.990%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.803215/  1.936907, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1042%\n",
      "layer   2  Sparsity: 70.0215%\n",
      "layer   3  Sparsity: 73.7719%\n",
      "total_backward_count 1791570 real_backward_count 196669  10.977%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.801315/  1.947113, val:  61.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 70.1552%\n",
      "layer   3  Sparsity: 73.5252%\n",
      "total_backward_count 1801360 real_backward_count 197533  10.966%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.801859/  1.931973, val:  64.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   2  Sparsity: 70.2306%\n",
      "layer   3  Sparsity: 74.3668%\n",
      "total_backward_count 1811150 real_backward_count 198426  10.956%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.782986/  1.950898, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   2  Sparsity: 69.7537%\n",
      "layer   3  Sparsity: 74.0422%\n",
      "total_backward_count 1820940 real_backward_count 199335  10.947%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.799866/  1.925734, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1162%\n",
      "layer   2  Sparsity: 70.0797%\n",
      "layer   3  Sparsity: 74.0567%\n",
      "total_backward_count 1830730 real_backward_count 200186  10.935%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.794038/  1.937282, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 70.1539%\n",
      "layer   3  Sparsity: 74.8824%\n",
      "total_backward_count 1840520 real_backward_count 201093  10.926%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.803110/  1.933879, val:  71.67%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   2  Sparsity: 70.2582%\n",
      "layer   3  Sparsity: 75.0688%\n",
      "total_backward_count 1850310 real_backward_count 201949  10.914%\n",
      "fc layer 1 self.abs_max_out: 12801.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.802705/  1.946042, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   2  Sparsity: 69.9961%\n",
      "layer   3  Sparsity: 74.7843%\n",
      "total_backward_count 1860100 real_backward_count 202826  10.904%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.803916/  1.924481, val:  72.50%, val_best:  82.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   2  Sparsity: 69.9726%\n",
      "layer   3  Sparsity: 74.0450%\n",
      "total_backward_count 1869890 real_backward_count 203692  10.893%\n",
      "fc layer 1 self.abs_max_out: 12830.0\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.803827/  1.927054, val:  59.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   2  Sparsity: 70.1831%\n",
      "layer   3  Sparsity: 73.8356%\n",
      "total_backward_count 1879680 real_backward_count 204573  10.883%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.799983/  1.934221, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 70.2343%\n",
      "layer   3  Sparsity: 74.2474%\n",
      "total_backward_count 1889470 real_backward_count 205435  10.873%\n",
      "fc layer 3 self.abs_max_out: 1311.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.790545/  1.953565, val:  72.92%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   2  Sparsity: 70.2790%\n",
      "layer   3  Sparsity: 74.3896%\n",
      "total_backward_count 1899260 real_backward_count 206290  10.862%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.802772/  1.935894, val:  62.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   2  Sparsity: 70.0749%\n",
      "layer   3  Sparsity: 74.7414%\n",
      "total_backward_count 1909050 real_backward_count 207139  10.850%\n",
      "fc layer 1 self.abs_max_out: 12875.0\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.789695/  1.929987, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1207%\n",
      "layer   2  Sparsity: 69.9666%\n",
      "layer   3  Sparsity: 73.2742%\n",
      "total_backward_count 1918840 real_backward_count 208019  10.841%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.786325/  1.920636, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1159%\n",
      "layer   2  Sparsity: 70.1353%\n",
      "layer   3  Sparsity: 74.1897%\n",
      "total_backward_count 1928630 real_backward_count 208852  10.829%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.775690/  1.904014, val:  68.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 70.0449%\n",
      "layer   3  Sparsity: 73.5976%\n",
      "total_backward_count 1938420 real_backward_count 209775  10.822%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.777894/  1.920568, val:  65.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1231%\n",
      "layer   2  Sparsity: 70.1877%\n",
      "layer   3  Sparsity: 73.6894%\n",
      "total_backward_count 1948210 real_backward_count 210602  10.810%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.788033/  1.920642, val:  71.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   2  Sparsity: 69.8347%\n",
      "layer   3  Sparsity: 73.5358%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5368be8115e42e4befd684a3dd91eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.78803</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.71667</td></tr><tr><td>val_loss</td><td>1.92064</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-54</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2544ytfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2544ytfg</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_004633-2544ytfg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gcau59ik with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_050548-gcau59ik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcau59ik' target=\"_blank\">vague-sweep-61</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcau59ik' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcau59ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251115_050556_178', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 435.0\n",
      "lif layer 1 self.abs_max_v: 435.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1139.0\n",
      "lif layer 2 self.abs_max_v: 1139.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 1 self.abs_max_out: 608.0\n",
      "lif layer 1 self.abs_max_v: 762.0\n",
      "lif layer 2 self.abs_max_v: 1213.5\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 1 self.abs_max_out: 664.0\n",
      "lif layer 1 self.abs_max_v: 783.5\n",
      "lif layer 2 self.abs_max_v: 1564.5\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 942.0\n",
      "lif layer 2 self.abs_max_v: 1760.5\n",
      "fc layer 1 self.abs_max_out: 765.0\n",
      "lif layer 1 self.abs_max_v: 965.0\n",
      "lif layer 2 self.abs_max_v: 1866.5\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 1 self.abs_max_out: 821.0\n",
      "lif layer 1 self.abs_max_v: 1080.5\n",
      "lif layer 2 self.abs_max_v: 1940.5\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 863.0\n",
      "lif layer 1 self.abs_max_v: 1127.5\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 1209.0\n",
      "lif layer 2 self.abs_max_v: 2013.0\n",
      "fc layer 1 self.abs_max_out: 985.0\n",
      "fc layer 2 self.abs_max_out: 1186.0\n",
      "lif layer 1 self.abs_max_v: 1270.0\n",
      "fc layer 2 self.abs_max_out: 1233.0\n",
      "lif layer 1 self.abs_max_v: 1449.0\n",
      "lif layer 1 self.abs_max_v: 1482.5\n",
      "lif layer 1 self.abs_max_v: 1674.5\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "fc layer 1 self.abs_max_out: 1246.0\n",
      "fc layer 3 self.abs_max_out: 512.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 1 self.abs_max_out: 1364.0\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "lif layer 2 self.abs_max_v: 2224.5\n",
      "lif layer 1 self.abs_max_v: 1831.0\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "lif layer 1 self.abs_max_v: 1833.5\n",
      "fc layer 1 self.abs_max_out: 1683.0\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "lif layer 1 self.abs_max_v: 1877.0\n",
      "fc layer 1 self.abs_max_out: 1813.0\n",
      "lif layer 1 self.abs_max_v: 1962.5\n",
      "lif layer 1 self.abs_max_v: 2073.0\n",
      "fc layer 1 self.abs_max_out: 1992.0\n",
      "lif layer 2 self.abs_max_v: 2228.5\n",
      "lif layer 2 self.abs_max_v: 2248.5\n",
      "lif layer 2 self.abs_max_v: 2263.5\n",
      "lif layer 2 self.abs_max_v: 2305.5\n",
      "lif layer 2 self.abs_max_v: 2466.0\n",
      "lif layer 2 self.abs_max_v: 2614.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "lif layer 1 self.abs_max_v: 2308.0\n",
      "lif layer 1 self.abs_max_v: 2507.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "lif layer 1 self.abs_max_v: 2579.0\n",
      "lif layer 1 self.abs_max_v: 2641.5\n",
      "lif layer 1 self.abs_max_v: 2875.0\n",
      "lif layer 2 self.abs_max_v: 2701.0\n",
      "lif layer 2 self.abs_max_v: 2703.5\n",
      "lif layer 1 self.abs_max_v: 2964.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "lif layer 2 self.abs_max_v: 2828.0\n",
      "lif layer 1 self.abs_max_v: 3170.0\n",
      "fc layer 1 self.abs_max_out: 2009.0\n",
      "lif layer 1 self.abs_max_v: 3196.5\n",
      "lif layer 1 self.abs_max_v: 3216.0\n",
      "fc layer 2 self.abs_max_out: 1597.0\n",
      "lif layer 1 self.abs_max_v: 3417.5\n",
      "lif layer 2 self.abs_max_v: 2838.0\n",
      "lif layer 2 self.abs_max_v: 2858.0\n",
      "lif layer 2 self.abs_max_v: 2940.0\n",
      "fc layer 1 self.abs_max_out: 2053.0\n",
      "lif layer 1 self.abs_max_v: 3478.0\n",
      "fc layer 1 self.abs_max_out: 2143.0\n",
      "lif layer 1 self.abs_max_v: 3882.0\n",
      "lif layer 1 self.abs_max_v: 3996.0\n",
      "fc layer 2 self.abs_max_out: 1619.0\n",
      "fc layer 1 self.abs_max_out: 2248.0\n",
      "fc layer 1 self.abs_max_out: 2304.0\n",
      "lif layer 1 self.abs_max_v: 4137.5\n",
      "lif layer 1 self.abs_max_v: 4144.5\n",
      "fc layer 1 self.abs_max_out: 2427.0\n",
      "lif layer 1 self.abs_max_v: 4192.5\n",
      "lif layer 1 self.abs_max_v: 4298.5\n",
      "fc layer 2 self.abs_max_out: 1709.0\n",
      "fc layer 1 self.abs_max_out: 2595.0\n",
      "fc layer 1 self.abs_max_out: 2623.0\n",
      "fc layer 1 self.abs_max_out: 2930.0\n",
      "lif layer 1 self.abs_max_v: 4405.0\n",
      "lif layer 1 self.abs_max_v: 4860.5\n",
      "fc layer 1 self.abs_max_out: 3230.0\n",
      "lif layer 1 self.abs_max_v: 5282.0\n",
      "lif layer 2 self.abs_max_v: 2993.5\n",
      "fc layer 2 self.abs_max_out: 1857.0\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 2 self.abs_max_out: 1876.0\n",
      "lif layer 2 self.abs_max_v: 3009.5\n",
      "lif layer 2 self.abs_max_v: 3023.0\n",
      "lif layer 2 self.abs_max_v: 3173.5\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "lif layer 2 self.abs_max_v: 3187.0\n",
      "lif layer 2 self.abs_max_v: 3250.5\n",
      "lif layer 1 self.abs_max_v: 5355.0\n",
      "lif layer 1 self.abs_max_v: 5533.5\n",
      "lif layer 1 self.abs_max_v: 5805.5\n",
      "fc layer 2 self.abs_max_out: 1906.0\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "fc layer 2 self.abs_max_out: 2110.0\n",
      "fc layer 3 self.abs_max_out: 704.0\n",
      "lif layer 1 self.abs_max_v: 5874.0\n",
      "fc layer 1 self.abs_max_out: 3262.0\n",
      "lif layer 2 self.abs_max_v: 3288.0\n",
      "lif layer 2 self.abs_max_v: 3366.0\n",
      "lif layer 2 self.abs_max_v: 3388.0\n",
      "lif layer 2 self.abs_max_v: 3493.0\n",
      "fc layer 1 self.abs_max_out: 3355.0\n",
      "fc layer 1 self.abs_max_out: 3363.0\n",
      "fc layer 1 self.abs_max_out: 3449.0\n",
      "lif layer 1 self.abs_max_v: 6157.5\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "fc layer 1 self.abs_max_out: 3763.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.872618/  2.024239, val:  33.33%, val_best:  33.33%, tr:  95.30%, tr_best:  95.30%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9204%\n",
      "layer   2  Sparsity: 67.9587%\n",
      "layer   3  Sparsity: 65.1759%\n",
      "total_backward_count 9790 real_backward_count 2255  23.034%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 6360.0\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "lif layer 2 self.abs_max_v: 3554.0\n",
      "lif layer 2 self.abs_max_v: 3750.0\n",
      "lif layer 1 self.abs_max_v: 6574.5\n",
      "fc layer 2 self.abs_max_out: 2165.0\n",
      "fc layer 1 self.abs_max_out: 3804.0\n",
      "lif layer 1 self.abs_max_v: 6795.5\n",
      "lif layer 1 self.abs_max_v: 7182.0\n",
      "lif layer 1 self.abs_max_v: 7333.0\n",
      "fc layer 1 self.abs_max_out: 3961.0\n",
      "lif layer 1 self.abs_max_v: 7627.5\n",
      "fc layer 1 self.abs_max_out: 4016.0\n",
      "lif layer 1 self.abs_max_v: 7830.0\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "fc layer 1 self.abs_max_out: 4103.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.807226/  1.981075, val:  49.17%, val_best:  49.17%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9477%\n",
      "layer   2  Sparsity: 70.5589%\n",
      "layer   3  Sparsity: 64.7144%\n",
      "total_backward_count 19580 real_backward_count 3851  19.668%\n",
      "fc layer 2 self.abs_max_out: 2186.0\n",
      "fc layer 3 self.abs_max_out: 723.0\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "fc layer 2 self.abs_max_out: 2269.0\n",
      "fc layer 2 self.abs_max_out: 2295.0\n",
      "fc layer 2 self.abs_max_out: 2333.0\n",
      "fc layer 2 self.abs_max_out: 2401.0\n",
      "fc layer 1 self.abs_max_out: 4146.0\n",
      "lif layer 1 self.abs_max_v: 7897.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.792119/  1.954049, val:  51.67%, val_best:  51.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9611%\n",
      "layer   2  Sparsity: 70.2034%\n",
      "layer   3  Sparsity: 63.3956%\n",
      "total_backward_count 29370 real_backward_count 5272  17.950%\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "lif layer 2 self.abs_max_v: 3785.0\n",
      "lif layer 2 self.abs_max_v: 3883.0\n",
      "fc layer 1 self.abs_max_out: 4182.0\n",
      "lif layer 2 self.abs_max_v: 3990.5\n",
      "lif layer 2 self.abs_max_v: 4126.0\n",
      "lif layer 2 self.abs_max_v: 4248.0\n",
      "fc layer 1 self.abs_max_out: 4203.0\n",
      "fc layer 1 self.abs_max_out: 4318.0\n",
      "lif layer 1 self.abs_max_v: 8124.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.774023/  1.967647, val:  42.92%, val_best:  51.67%, tr:  98.98%, tr_best:  99.59%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8953%\n",
      "layer   2  Sparsity: 71.0413%\n",
      "layer   3  Sparsity: 62.3907%\n",
      "total_backward_count 39160 real_backward_count 6620  16.905%\n",
      "fc layer 1 self.abs_max_out: 4368.0\n",
      "fc layer 1 self.abs_max_out: 4514.0\n",
      "fc layer 1 self.abs_max_out: 4680.0\n",
      "lif layer 1 self.abs_max_v: 8301.5\n",
      "lif layer 1 self.abs_max_v: 8460.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.779174/  1.968590, val:  48.33%, val_best:  51.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9150%\n",
      "layer   2  Sparsity: 73.1107%\n",
      "layer   3  Sparsity: 63.0733%\n",
      "total_backward_count 48950 real_backward_count 7833  16.002%\n",
      "fc layer 2 self.abs_max_out: 2417.0\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "lif layer 1 self.abs_max_v: 8654.5\n",
      "lif layer 1 self.abs_max_v: 8808.5\n",
      "fc layer 1 self.abs_max_out: 4738.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.782447/  1.988654, val:  45.42%, val_best:  51.67%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9048%\n",
      "layer   2  Sparsity: 73.3413%\n",
      "layer   3  Sparsity: 64.0404%\n",
      "total_backward_count 58740 real_backward_count 9009  15.337%\n",
      "lif layer 1 self.abs_max_v: 8874.0\n",
      "fc layer 1 self.abs_max_out: 4945.0\n",
      "fc layer 1 self.abs_max_out: 4957.0\n",
      "fc layer 1 self.abs_max_out: 4974.0\n",
      "lif layer 1 self.abs_max_v: 9234.0\n",
      "lif layer 2 self.abs_max_v: 4277.5\n",
      "fc layer 1 self.abs_max_out: 5026.0\n",
      "fc layer 1 self.abs_max_out: 5170.0\n",
      "fc layer 1 self.abs_max_out: 5210.0\n",
      "lif layer 1 self.abs_max_v: 9243.0\n",
      "lif layer 1 self.abs_max_v: 9414.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.801202/  1.957094, val:  47.50%, val_best:  51.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9108%\n",
      "layer   2  Sparsity: 72.3038%\n",
      "layer   3  Sparsity: 65.0171%\n",
      "total_backward_count 68530 real_backward_count 10184  14.861%\n",
      "fc layer 1 self.abs_max_out: 5553.0\n",
      "lif layer 1 self.abs_max_v: 9454.0\n",
      "lif layer 1 self.abs_max_v: 9898.0\n",
      "lif layer 1 self.abs_max_v: 10006.0\n",
      "fc layer 3 self.abs_max_out: 779.0\n",
      "fc layer 1 self.abs_max_out: 5874.0\n",
      "lif layer 1 self.abs_max_v: 10243.5\n",
      "fc layer 3 self.abs_max_out: 805.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.780441/  1.932169, val:  50.83%, val_best:  51.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9221%\n",
      "layer   2  Sparsity: 73.2450%\n",
      "layer   3  Sparsity: 64.3439%\n",
      "total_backward_count 78320 real_backward_count 11257  14.373%\n",
      "fc layer 1 self.abs_max_out: 6136.0\n",
      "lif layer 1 self.abs_max_v: 10753.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.753582/  1.903177, val:  47.92%, val_best:  51.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9222%\n",
      "layer   2  Sparsity: 75.2365%\n",
      "layer   3  Sparsity: 64.8078%\n",
      "total_backward_count 88110 real_backward_count 12434  14.112%\n",
      "lif layer 1 self.abs_max_v: 10876.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.749137/  1.929775, val:  55.42%, val_best:  55.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9303%\n",
      "layer   2  Sparsity: 74.8886%\n",
      "layer   3  Sparsity: 64.4453%\n",
      "total_backward_count 97900 real_backward_count 13556  13.847%\n",
      "lif layer 1 self.abs_max_v: 11165.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.743760/  1.922521, val:  49.17%, val_best:  55.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9007%\n",
      "layer   2  Sparsity: 75.5260%\n",
      "layer   3  Sparsity: 63.7068%\n",
      "total_backward_count 107690 real_backward_count 14699  13.649%\n",
      "lif layer 1 self.abs_max_v: 11400.0\n",
      "lif layer 1 self.abs_max_v: 11583.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.732003/  1.893896, val:  58.75%, val_best:  58.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8864%\n",
      "layer   2  Sparsity: 75.3926%\n",
      "layer   3  Sparsity: 64.4870%\n",
      "total_backward_count 117480 real_backward_count 15775  13.428%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.729439/  1.927222, val:  41.25%, val_best:  58.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8884%\n",
      "layer   2  Sparsity: 75.6763%\n",
      "layer   3  Sparsity: 65.0501%\n",
      "total_backward_count 127270 real_backward_count 16835  13.228%\n",
      "lif layer 2 self.abs_max_v: 4297.0\n",
      "lif layer 2 self.abs_max_v: 4467.0\n",
      "lif layer 2 self.abs_max_v: 4501.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.744339/  1.922959, val:  49.58%, val_best:  58.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9371%\n",
      "layer   2  Sparsity: 75.6384%\n",
      "layer   3  Sparsity: 64.4013%\n",
      "total_backward_count 137060 real_backward_count 17877  13.043%\n",
      "lif layer 2 self.abs_max_v: 4725.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.741942/  1.903600, val:  57.50%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8855%\n",
      "layer   2  Sparsity: 75.7878%\n",
      "layer   3  Sparsity: 64.1474%\n",
      "total_backward_count 146850 real_backward_count 18889  12.863%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.732080/  1.891642, val:  56.25%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8978%\n",
      "layer   2  Sparsity: 76.4811%\n",
      "layer   3  Sparsity: 64.4324%\n",
      "total_backward_count 156640 real_backward_count 19935  12.727%\n",
      "fc layer 2 self.abs_max_out: 2680.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.712547/  1.888826, val:  54.58%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9238%\n",
      "layer   2  Sparsity: 76.6340%\n",
      "layer   3  Sparsity: 64.8047%\n",
      "total_backward_count 166430 real_backward_count 20969  12.599%\n",
      "fc layer 2 self.abs_max_out: 2701.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.730461/  1.854913, val:  58.75%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8702%\n",
      "layer   2  Sparsity: 76.0952%\n",
      "layer   3  Sparsity: 63.7380%\n",
      "total_backward_count 176220 real_backward_count 22009  12.490%\n",
      "fc layer 1 self.abs_max_out: 6206.0\n",
      "lif layer 1 self.abs_max_v: 11800.5\n",
      "fc layer 1 self.abs_max_out: 6290.0\n",
      "lif layer 1 self.abs_max_v: 12190.5\n",
      "lif layer 1 self.abs_max_v: 12293.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.696210/  1.846314, val:  59.17%, val_best:  59.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8965%\n",
      "layer   2  Sparsity: 76.3998%\n",
      "layer   3  Sparsity: 64.1124%\n",
      "total_backward_count 186010 real_backward_count 23072  12.404%\n",
      "fc layer 1 self.abs_max_out: 6615.0\n",
      "fc layer 1 self.abs_max_out: 6943.0\n",
      "lif layer 1 self.abs_max_v: 12374.0\n",
      "lif layer 1 self.abs_max_v: 12477.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.714630/  1.920846, val:  40.42%, val_best:  59.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9078%\n",
      "layer   2  Sparsity: 75.3259%\n",
      "layer   3  Sparsity: 64.4023%\n",
      "total_backward_count 195800 real_backward_count 24048  12.282%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.711055/  1.867084, val:  61.67%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.9045%\n",
      "layer   2  Sparsity: 75.7663%\n",
      "layer   3  Sparsity: 65.5731%\n",
      "total_backward_count 205590 real_backward_count 25044  12.182%\n",
      "fc layer 1 self.abs_max_out: 7124.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.707108/  1.870528, val:  57.08%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9184%\n",
      "layer   2  Sparsity: 76.5219%\n",
      "layer   3  Sparsity: 66.4843%\n",
      "total_backward_count 215380 real_backward_count 26054  12.097%\n",
      "fc layer 2 self.abs_max_out: 2724.0\n",
      "lif layer 2 self.abs_max_v: 4853.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.705796/  1.855202, val:  58.33%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9299%\n",
      "layer   2  Sparsity: 76.2713%\n",
      "layer   3  Sparsity: 66.1591%\n",
      "total_backward_count 225170 real_backward_count 27064  12.019%\n",
      "fc layer 3 self.abs_max_out: 807.0\n",
      "fc layer 1 self.abs_max_out: 7239.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.712503/  1.877655, val:  60.42%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8737%\n",
      "layer   2  Sparsity: 76.1566%\n",
      "layer   3  Sparsity: 65.5882%\n",
      "total_backward_count 234960 real_backward_count 28103  11.961%\n",
      "fc layer 2 self.abs_max_out: 2901.0\n",
      "lif layer 2 self.abs_max_v: 4978.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.723521/  1.881739, val:  62.08%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9384%\n",
      "layer   2  Sparsity: 75.3238%\n",
      "layer   3  Sparsity: 65.3835%\n",
      "total_backward_count 244750 real_backward_count 29031  11.861%\n",
      "lif layer 2 self.abs_max_v: 5121.0\n",
      "lif layer 2 self.abs_max_v: 5145.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.708939/  1.855826, val:  66.67%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8999%\n",
      "layer   2  Sparsity: 75.1142%\n",
      "layer   3  Sparsity: 65.5034%\n",
      "total_backward_count 254540 real_backward_count 30055  11.808%\n",
      "fc layer 3 self.abs_max_out: 819.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.701145/  1.873850, val:  58.75%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9126%\n",
      "layer   2  Sparsity: 75.2430%\n",
      "layer   3  Sparsity: 65.4761%\n",
      "total_backward_count 264330 real_backward_count 31027  11.738%\n",
      "lif layer 1 self.abs_max_v: 12525.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.702606/  1.872973, val:  54.17%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8490%\n",
      "layer   2  Sparsity: 75.0811%\n",
      "layer   3  Sparsity: 66.2701%\n",
      "total_backward_count 274120 real_backward_count 31965  11.661%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.711231/  1.907574, val:  50.42%, val_best:  66.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9202%\n",
      "layer   2  Sparsity: 74.5785%\n",
      "layer   3  Sparsity: 65.2163%\n",
      "total_backward_count 283910 real_backward_count 32878  11.580%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.725234/  1.861043, val:  58.33%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8927%\n",
      "layer   2  Sparsity: 74.5213%\n",
      "layer   3  Sparsity: 64.3598%\n",
      "total_backward_count 293700 real_backward_count 33807  11.511%\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "fc layer 3 self.abs_max_out: 885.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.719433/  1.874225, val:  62.08%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9522%\n",
      "layer   2  Sparsity: 74.5622%\n",
      "layer   3  Sparsity: 65.3831%\n",
      "total_backward_count 303490 real_backward_count 34762  11.454%\n",
      "lif layer 1 self.abs_max_v: 12631.0\n",
      "lif layer 1 self.abs_max_v: 13159.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.708135/  1.874610, val:  52.92%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8939%\n",
      "layer   2  Sparsity: 74.4626%\n",
      "layer   3  Sparsity: 66.0288%\n",
      "total_backward_count 313280 real_backward_count 35692  11.393%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.714048/  1.884466, val:  57.92%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8983%\n",
      "layer   2  Sparsity: 74.2390%\n",
      "layer   3  Sparsity: 65.0036%\n",
      "total_backward_count 323070 real_backward_count 36545  11.312%\n",
      "lif layer 1 self.abs_max_v: 13185.0\n",
      "lif layer 1 self.abs_max_v: 13740.5\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.704767/  1.872448, val:  59.58%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9404%\n",
      "layer   2  Sparsity: 74.8583%\n",
      "layer   3  Sparsity: 65.2023%\n",
      "total_backward_count 332860 real_backward_count 37469  11.257%\n",
      "fc layer 1 self.abs_max_out: 7336.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.712714/  1.879713, val:  52.08%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.8977%\n",
      "layer   2  Sparsity: 74.5791%\n",
      "layer   3  Sparsity: 65.8574%\n",
      "total_backward_count 342650 real_backward_count 38402  11.207%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.702642/  1.867513, val:  57.92%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8764%\n",
      "layer   2  Sparsity: 74.3020%\n",
      "layer   3  Sparsity: 65.1518%\n",
      "total_backward_count 352440 real_backward_count 39292  11.149%\n",
      "lif layer 2 self.abs_max_v: 5242.5\n",
      "fc layer 1 self.abs_max_out: 7358.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.690687/  1.852339, val:  64.17%, val_best:  66.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9582%\n",
      "layer   2  Sparsity: 74.2770%\n",
      "layer   3  Sparsity: 65.1148%\n",
      "total_backward_count 362230 real_backward_count 40158  11.086%\n",
      "fc layer 2 self.abs_max_out: 3050.0\n",
      "fc layer 1 self.abs_max_out: 7381.0\n",
      "lif layer 1 self.abs_max_v: 14172.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.681391/  1.845273, val:  55.83%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9374%\n",
      "layer   2  Sparsity: 74.1515%\n",
      "layer   3  Sparsity: 64.9564%\n",
      "total_backward_count 372020 real_backward_count 41037  11.031%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.680236/  1.864640, val:  51.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8535%\n",
      "layer   2  Sparsity: 74.6394%\n",
      "layer   3  Sparsity: 65.6215%\n",
      "total_backward_count 381810 real_backward_count 41920  10.979%\n",
      "fc layer 1 self.abs_max_out: 7411.0\n",
      "fc layer 1 self.abs_max_out: 7468.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.677456/  1.855540, val:  56.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9306%\n",
      "layer   2  Sparsity: 75.1461%\n",
      "layer   3  Sparsity: 65.7113%\n",
      "total_backward_count 391600 real_backward_count 42793  10.928%\n",
      "fc layer 1 self.abs_max_out: 7592.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.676401/  1.840481, val:  59.17%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8726%\n",
      "layer   2  Sparsity: 75.2197%\n",
      "layer   3  Sparsity: 65.2748%\n",
      "total_backward_count 401390 real_backward_count 43681  10.882%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.677331/  1.835861, val:  63.75%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8993%\n",
      "layer   2  Sparsity: 75.3118%\n",
      "layer   3  Sparsity: 64.4981%\n",
      "total_backward_count 411180 real_backward_count 44563  10.838%\n",
      "lif layer 2 self.abs_max_v: 5245.0\n",
      "fc layer 1 self.abs_max_out: 7685.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.675031/  1.835417, val:  57.92%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9649%\n",
      "layer   2  Sparsity: 74.4502%\n",
      "layer   3  Sparsity: 64.3879%\n",
      "total_backward_count 420970 real_backward_count 45430  10.792%\n",
      "lif layer 1 self.abs_max_v: 14653.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.671657/  1.834972, val:  65.42%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9816%\n",
      "layer   2  Sparsity: 74.9431%\n",
      "layer   3  Sparsity: 64.9328%\n",
      "total_backward_count 430760 real_backward_count 46210  10.728%\n",
      "fc layer 1 self.abs_max_out: 7703.0\n",
      "fc layer 1 self.abs_max_out: 7990.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.672270/  1.853972, val:  57.92%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9680%\n",
      "layer   2  Sparsity: 75.0551%\n",
      "layer   3  Sparsity: 65.6125%\n",
      "total_backward_count 440550 real_backward_count 47062  10.683%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.658772/  1.836315, val:  57.92%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8692%\n",
      "layer   2  Sparsity: 74.3588%\n",
      "layer   3  Sparsity: 64.8833%\n",
      "total_backward_count 450340 real_backward_count 47931  10.643%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.667939/  1.845979, val:  62.92%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8677%\n",
      "layer   2  Sparsity: 74.7295%\n",
      "layer   3  Sparsity: 65.1003%\n",
      "total_backward_count 460130 real_backward_count 48820  10.610%\n",
      "lif layer 1 self.abs_max_v: 14686.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.667081/  1.842893, val:  55.00%, val_best:  66.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9764%\n",
      "layer   2  Sparsity: 74.4212%\n",
      "layer   3  Sparsity: 64.8066%\n",
      "total_backward_count 469920 real_backward_count 49656  10.567%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.659011/  1.843850, val:  63.33%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8553%\n",
      "layer   2  Sparsity: 74.3727%\n",
      "layer   3  Sparsity: 64.9042%\n",
      "total_backward_count 479710 real_backward_count 50510  10.529%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.661276/  1.829804, val:  57.92%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9011%\n",
      "layer   2  Sparsity: 73.7176%\n",
      "layer   3  Sparsity: 65.6758%\n",
      "total_backward_count 489500 real_backward_count 51395  10.499%\n",
      "fc layer 1 self.abs_max_out: 8050.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.668595/  1.825655, val:  61.25%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.9316%\n",
      "layer   2  Sparsity: 73.6019%\n",
      "layer   3  Sparsity: 66.2224%\n",
      "total_backward_count 499290 real_backward_count 52214  10.458%\n",
      "lif layer 1 self.abs_max_v: 14849.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.653875/  1.849860, val:  49.17%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8751%\n",
      "layer   2  Sparsity: 73.4040%\n",
      "layer   3  Sparsity: 65.3033%\n",
      "total_backward_count 509080 real_backward_count 53045  10.420%\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.652556/  1.828799, val:  65.00%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9245%\n",
      "layer   2  Sparsity: 72.9681%\n",
      "layer   3  Sparsity: 65.2801%\n",
      "total_backward_count 518870 real_backward_count 53874  10.383%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.649225/  1.834643, val:  58.33%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9226%\n",
      "layer   2  Sparsity: 72.8832%\n",
      "layer   3  Sparsity: 65.8084%\n",
      "total_backward_count 528660 real_backward_count 54743  10.355%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.662452/  1.852978, val:  57.08%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9432%\n",
      "layer   2  Sparsity: 72.9304%\n",
      "layer   3  Sparsity: 66.1382%\n",
      "total_backward_count 538450 real_backward_count 55569  10.320%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.660526/  1.843034, val:  61.25%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9228%\n",
      "layer   2  Sparsity: 73.1218%\n",
      "layer   3  Sparsity: 65.0557%\n",
      "total_backward_count 548240 real_backward_count 56384  10.285%\n",
      "lif layer 1 self.abs_max_v: 14891.5\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.644872/  1.829561, val:  57.92%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.8856%\n",
      "layer   2  Sparsity: 73.3338%\n",
      "layer   3  Sparsity: 64.1448%\n",
      "total_backward_count 558030 real_backward_count 57216  10.253%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.649652/  1.812341, val:  66.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9432%\n",
      "layer   2  Sparsity: 73.6854%\n",
      "layer   3  Sparsity: 64.5626%\n",
      "total_backward_count 567820 real_backward_count 58012  10.217%\n",
      "lif layer 1 self.abs_max_v: 14925.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.646510/  1.819870, val:  62.92%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9278%\n",
      "layer   2  Sparsity: 73.4466%\n",
      "layer   3  Sparsity: 64.4019%\n",
      "total_backward_count 577610 real_backward_count 58781  10.177%\n",
      "fc layer 1 self.abs_max_out: 8619.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.658716/  1.832429, val:  55.42%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9191%\n",
      "layer   2  Sparsity: 72.6356%\n",
      "layer   3  Sparsity: 64.9991%\n",
      "total_backward_count 587400 real_backward_count 59607  10.148%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.650479/  1.842633, val:  61.25%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9390%\n",
      "layer   2  Sparsity: 72.3359%\n",
      "layer   3  Sparsity: 64.9840%\n",
      "total_backward_count 597190 real_backward_count 60414  10.116%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.658907/  1.823090, val:  63.33%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8799%\n",
      "layer   2  Sparsity: 72.9096%\n",
      "layer   3  Sparsity: 65.3499%\n",
      "total_backward_count 606980 real_backward_count 61225  10.087%\n",
      "fc layer 2 self.abs_max_out: 3111.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.636719/  1.835447, val:  58.33%, val_best:  66.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9143%\n",
      "layer   2  Sparsity: 72.7201%\n",
      "layer   3  Sparsity: 65.0061%\n",
      "total_backward_count 616770 real_backward_count 62045  10.060%\n",
      "lif layer 1 self.abs_max_v: 14958.5\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.644701/  1.814447, val:  60.00%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9285%\n",
      "layer   2  Sparsity: 72.3638%\n",
      "layer   3  Sparsity: 64.8135%\n",
      "total_backward_count 626560 real_backward_count 62830  10.028%\n",
      "fc layer 2 self.abs_max_out: 3117.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.621941/  1.825030, val:  60.83%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9022%\n",
      "layer   2  Sparsity: 72.3436%\n",
      "layer   3  Sparsity: 63.9199%\n",
      "total_backward_count 636350 real_backward_count 63643  10.001%\n",
      "fc layer 2 self.abs_max_out: 3136.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.612473/  1.814201, val:  57.08%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9089%\n",
      "layer   2  Sparsity: 72.1735%\n",
      "layer   3  Sparsity: 64.5440%\n",
      "total_backward_count 646140 real_backward_count 64438   9.973%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.612094/  1.807835, val:  57.92%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8960%\n",
      "layer   2  Sparsity: 72.2414%\n",
      "layer   3  Sparsity: 64.9643%\n",
      "total_backward_count 655930 real_backward_count 65229   9.945%\n",
      "fc layer 2 self.abs_max_out: 3262.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.607285/  1.814062, val:  60.00%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8785%\n",
      "layer   2  Sparsity: 72.5028%\n",
      "layer   3  Sparsity: 64.3933%\n",
      "total_backward_count 665720 real_backward_count 65984   9.912%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.598319/  1.766584, val:  64.17%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8953%\n",
      "layer   2  Sparsity: 72.2445%\n",
      "layer   3  Sparsity: 64.6446%\n",
      "total_backward_count 675510 real_backward_count 66723   9.877%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.580858/  1.767490, val:  58.75%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8997%\n",
      "layer   2  Sparsity: 72.2460%\n",
      "layer   3  Sparsity: 65.3489%\n",
      "total_backward_count 685300 real_backward_count 67477   9.846%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.589938/  1.786369, val:  60.42%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9208%\n",
      "layer   2  Sparsity: 71.5735%\n",
      "layer   3  Sparsity: 64.2591%\n",
      "total_backward_count 695090 real_backward_count 68265   9.821%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.592791/  1.783938, val:  64.17%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9000%\n",
      "layer   2  Sparsity: 71.8687%\n",
      "layer   3  Sparsity: 64.3530%\n",
      "total_backward_count 704880 real_backward_count 69023   9.792%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.593485/  1.770970, val:  66.67%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9142%\n",
      "layer   2  Sparsity: 71.2239%\n",
      "layer   3  Sparsity: 65.1367%\n",
      "total_backward_count 714670 real_backward_count 69771   9.763%\n",
      "fc layer 1 self.abs_max_out: 8656.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.593763/  1.788856, val:  62.50%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8995%\n",
      "layer   2  Sparsity: 71.4300%\n",
      "layer   3  Sparsity: 64.6305%\n",
      "total_backward_count 724460 real_backward_count 70530   9.736%\n",
      "fc layer 2 self.abs_max_out: 3287.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.608610/  1.784081, val:  61.67%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 85.9073%\n",
      "layer   2  Sparsity: 71.5935%\n",
      "layer   3  Sparsity: 64.5308%\n",
      "total_backward_count 734250 real_backward_count 71320   9.713%\n",
      "fc layer 1 self.abs_max_out: 8815.0\n",
      "lif layer 1 self.abs_max_v: 15110.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.598463/  1.781248, val:  62.08%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9099%\n",
      "layer   2  Sparsity: 71.8900%\n",
      "layer   3  Sparsity: 64.5607%\n",
      "total_backward_count 744040 real_backward_count 72065   9.686%\n",
      "lif layer 1 self.abs_max_v: 15140.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.596430/  1.787340, val:  63.75%, val_best:  66.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9277%\n",
      "layer   2  Sparsity: 71.9001%\n",
      "layer   3  Sparsity: 65.4453%\n",
      "total_backward_count 753830 real_backward_count 72850   9.664%\n",
      "fc layer 2 self.abs_max_out: 3379.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.591739/  1.792326, val:  58.33%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9217%\n",
      "layer   2  Sparsity: 72.0315%\n",
      "layer   3  Sparsity: 65.1288%\n",
      "total_backward_count 763620 real_backward_count 73596   9.638%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.593327/  1.771199, val:  64.17%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9605%\n",
      "layer   2  Sparsity: 71.5323%\n",
      "layer   3  Sparsity: 64.3680%\n",
      "total_backward_count 773410 real_backward_count 74338   9.612%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.570111/  1.776635, val:  65.00%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9004%\n",
      "layer   2  Sparsity: 71.8842%\n",
      "layer   3  Sparsity: 65.2229%\n",
      "total_backward_count 783200 real_backward_count 75076   9.586%\n",
      "fc layer 2 self.abs_max_out: 3446.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.592927/  1.793442, val:  58.75%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8929%\n",
      "layer   2  Sparsity: 72.2591%\n",
      "layer   3  Sparsity: 64.8275%\n",
      "total_backward_count 792990 real_backward_count 75821   9.561%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.583969/  1.801930, val:  57.50%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9017%\n",
      "layer   2  Sparsity: 71.8719%\n",
      "layer   3  Sparsity: 64.3072%\n",
      "total_backward_count 802780 real_backward_count 76528   9.533%\n",
      "fc layer 1 self.abs_max_out: 8985.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.586863/  1.793150, val:  60.83%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8988%\n",
      "layer   2  Sparsity: 71.1658%\n",
      "layer   3  Sparsity: 65.6018%\n",
      "total_backward_count 812570 real_backward_count 77267   9.509%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.579102/  1.766470, val:  62.08%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8655%\n",
      "layer   2  Sparsity: 71.3932%\n",
      "layer   3  Sparsity: 65.9429%\n",
      "total_backward_count 822360 real_backward_count 78057   9.492%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.578975/  1.771039, val:  59.17%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8933%\n",
      "layer   2  Sparsity: 71.4196%\n",
      "layer   3  Sparsity: 65.0915%\n",
      "total_backward_count 832150 real_backward_count 78847   9.475%\n",
      "lif layer 1 self.abs_max_v: 15220.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.581433/  1.768814, val:  63.33%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9434%\n",
      "layer   2  Sparsity: 71.2413%\n",
      "layer   3  Sparsity: 65.3663%\n",
      "total_backward_count 841940 real_backward_count 79547   9.448%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.582637/  1.789811, val:  59.17%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9044%\n",
      "layer   2  Sparsity: 71.4090%\n",
      "layer   3  Sparsity: 66.0435%\n",
      "total_backward_count 851730 real_backward_count 80276   9.425%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.575706/  1.757584, val:  66.25%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8501%\n",
      "layer   2  Sparsity: 72.0178%\n",
      "layer   3  Sparsity: 65.8564%\n",
      "total_backward_count 861520 real_backward_count 81013   9.403%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.582655/  1.764786, val:  60.83%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8698%\n",
      "layer   2  Sparsity: 72.2089%\n",
      "layer   3  Sparsity: 65.9565%\n",
      "total_backward_count 871310 real_backward_count 81731   9.380%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.591806/  1.770583, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9195%\n",
      "layer   2  Sparsity: 71.6927%\n",
      "layer   3  Sparsity: 65.7845%\n",
      "total_backward_count 881100 real_backward_count 82471   9.360%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.576071/  1.776594, val:  61.67%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9117%\n",
      "layer   2  Sparsity: 71.9320%\n",
      "layer   3  Sparsity: 64.9794%\n",
      "total_backward_count 890890 real_backward_count 83253   9.345%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.567772/  1.768347, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8620%\n",
      "layer   2  Sparsity: 71.7873%\n",
      "layer   3  Sparsity: 65.7237%\n",
      "total_backward_count 900680 real_backward_count 83939   9.320%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.560481/  1.754638, val:  65.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9211%\n",
      "layer   2  Sparsity: 72.0206%\n",
      "layer   3  Sparsity: 65.6225%\n",
      "total_backward_count 910470 real_backward_count 84593   9.291%\n",
      "lif layer 1 self.abs_max_v: 15346.5\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.569728/  1.756877, val:  66.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9257%\n",
      "layer   2  Sparsity: 72.1465%\n",
      "layer   3  Sparsity: 65.4433%\n",
      "total_backward_count 920260 real_backward_count 85357   9.275%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.556436/  1.753766, val:  65.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9354%\n",
      "layer   2  Sparsity: 72.1352%\n",
      "layer   3  Sparsity: 64.7940%\n",
      "total_backward_count 930050 real_backward_count 86079   9.255%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.538572/  1.729237, val:  66.67%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9437%\n",
      "layer   2  Sparsity: 72.0069%\n",
      "layer   3  Sparsity: 65.0644%\n",
      "total_backward_count 939840 real_backward_count 86806   9.236%\n",
      "fc layer 1 self.abs_max_out: 9130.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.548077/  1.747241, val:  62.92%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8784%\n",
      "layer   2  Sparsity: 72.5611%\n",
      "layer   3  Sparsity: 65.8044%\n",
      "total_backward_count 949630 real_backward_count 87525   9.217%\n",
      "lif layer 1 self.abs_max_v: 15441.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.550948/  1.757367, val:  71.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9265%\n",
      "layer   2  Sparsity: 71.9966%\n",
      "layer   3  Sparsity: 65.5981%\n",
      "total_backward_count 959420 real_backward_count 88217   9.195%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.559863/  1.768213, val:  68.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9192%\n",
      "layer   2  Sparsity: 72.1231%\n",
      "layer   3  Sparsity: 65.1158%\n",
      "total_backward_count 969210 real_backward_count 88883   9.171%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.563351/  1.743287, val:  66.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8761%\n",
      "layer   2  Sparsity: 71.8338%\n",
      "layer   3  Sparsity: 64.5454%\n",
      "total_backward_count 979000 real_backward_count 89538   9.146%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.534840/  1.739978, val:  66.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8956%\n",
      "layer   2  Sparsity: 71.9239%\n",
      "layer   3  Sparsity: 64.3428%\n",
      "total_backward_count 988790 real_backward_count 90252   9.128%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.536574/  1.727662, val:  65.00%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8936%\n",
      "layer   2  Sparsity: 71.4841%\n",
      "layer   3  Sparsity: 64.3279%\n",
      "total_backward_count 998580 real_backward_count 90937   9.107%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.513505/  1.714356, val:  60.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9722%\n",
      "layer   2  Sparsity: 71.0178%\n",
      "layer   3  Sparsity: 64.5417%\n",
      "total_backward_count 1008370 real_backward_count 91594   9.083%\n",
      "lif layer 1 self.abs_max_v: 15470.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.518543/  1.723273, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8865%\n",
      "layer   2  Sparsity: 71.2402%\n",
      "layer   3  Sparsity: 64.5349%\n",
      "total_backward_count 1018160 real_backward_count 92277   9.063%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.522794/  1.747327, val:  65.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9420%\n",
      "layer   2  Sparsity: 71.2637%\n",
      "layer   3  Sparsity: 64.0531%\n",
      "total_backward_count 1027950 real_backward_count 92935   9.041%\n",
      "fc layer 2 self.abs_max_out: 3527.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.536466/  1.763571, val:  61.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9175%\n",
      "layer   2  Sparsity: 71.0642%\n",
      "layer   3  Sparsity: 64.8858%\n",
      "total_backward_count 1037740 real_backward_count 93595   9.019%\n",
      "fc layer 2 self.abs_max_out: 3598.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.537050/  1.742835, val:  60.42%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8923%\n",
      "layer   2  Sparsity: 70.5726%\n",
      "layer   3  Sparsity: 64.9418%\n",
      "total_backward_count 1047530 real_backward_count 94242   8.997%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.537766/  1.745617, val:  58.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9082%\n",
      "layer   2  Sparsity: 70.9405%\n",
      "layer   3  Sparsity: 64.3006%\n",
      "total_backward_count 1057320 real_backward_count 94887   8.974%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.531439/  1.735755, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9127%\n",
      "layer   2  Sparsity: 70.7888%\n",
      "layer   3  Sparsity: 64.8098%\n",
      "total_backward_count 1067110 real_backward_count 95566   8.956%\n",
      "lif layer 1 self.abs_max_v: 15602.5\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.515657/  1.734163, val:  68.75%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8476%\n",
      "layer   2  Sparsity: 70.9911%\n",
      "layer   3  Sparsity: 65.2794%\n",
      "total_backward_count 1076900 real_backward_count 96252   8.938%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.518287/  1.724555, val:  64.58%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9587%\n",
      "layer   2  Sparsity: 71.1893%\n",
      "layer   3  Sparsity: 65.0009%\n",
      "total_backward_count 1086690 real_backward_count 96907   8.918%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.507842/  1.715052, val:  63.75%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9298%\n",
      "layer   2  Sparsity: 70.7143%\n",
      "layer   3  Sparsity: 64.7712%\n",
      "total_backward_count 1096480 real_backward_count 97565   8.898%\n",
      "fc layer 2 self.abs_max_out: 3662.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.512061/  1.729183, val:  68.33%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9344%\n",
      "layer   2  Sparsity: 70.6214%\n",
      "layer   3  Sparsity: 64.3269%\n",
      "total_backward_count 1106270 real_backward_count 98208   8.877%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.505475/  1.708161, val:  69.58%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8176%\n",
      "layer   2  Sparsity: 71.2612%\n",
      "layer   3  Sparsity: 63.8564%\n",
      "total_backward_count 1116060 real_backward_count 98849   8.857%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.489970/  1.699163, val:  65.83%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8959%\n",
      "layer   2  Sparsity: 71.1475%\n",
      "layer   3  Sparsity: 64.4058%\n",
      "total_backward_count 1125850 real_backward_count 99520   8.840%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.477380/  1.704205, val:  62.92%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9228%\n",
      "layer   2  Sparsity: 71.3091%\n",
      "layer   3  Sparsity: 64.4997%\n",
      "total_backward_count 1135640 real_backward_count 100147   8.819%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.488181/  1.691926, val:  65.83%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8968%\n",
      "layer   2  Sparsity: 71.2742%\n",
      "layer   3  Sparsity: 64.8117%\n",
      "total_backward_count 1145430 real_backward_count 100766   8.797%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.475905/  1.685828, val:  65.42%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 85.9640%\n",
      "layer   2  Sparsity: 71.2519%\n",
      "layer   3  Sparsity: 64.5909%\n",
      "total_backward_count 1155220 real_backward_count 101418   8.779%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.486437/  1.722684, val:  60.83%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8892%\n",
      "layer   2  Sparsity: 70.8948%\n",
      "layer   3  Sparsity: 64.2941%\n",
      "total_backward_count 1165010 real_backward_count 102020   8.757%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.484998/  1.682796, val:  70.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8959%\n",
      "layer   2  Sparsity: 71.1459%\n",
      "layer   3  Sparsity: 63.6423%\n",
      "total_backward_count 1174800 real_backward_count 102653   8.738%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.476764/  1.711964, val:  70.42%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9007%\n",
      "layer   2  Sparsity: 71.5152%\n",
      "layer   3  Sparsity: 63.4946%\n",
      "total_backward_count 1184590 real_backward_count 103262   8.717%\n",
      "lif layer 1 self.abs_max_v: 15623.5\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.484710/  1.718178, val:  66.67%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9253%\n",
      "layer   2  Sparsity: 71.7171%\n",
      "layer   3  Sparsity: 64.3099%\n",
      "total_backward_count 1194380 real_backward_count 103899   8.699%\n",
      "lif layer 1 self.abs_max_v: 15694.5\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.495852/  1.715263, val:  70.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8961%\n",
      "layer   2  Sparsity: 71.7257%\n",
      "layer   3  Sparsity: 64.3148%\n",
      "total_backward_count 1204170 real_backward_count 104593   8.686%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.502411/  1.690513, val:  66.25%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8881%\n",
      "layer   2  Sparsity: 71.3443%\n",
      "layer   3  Sparsity: 63.6966%\n",
      "total_backward_count 1213960 real_backward_count 105244   8.669%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.490607/  1.712377, val:  65.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9103%\n",
      "layer   2  Sparsity: 71.6098%\n",
      "layer   3  Sparsity: 63.8249%\n",
      "total_backward_count 1223750 real_backward_count 105898   8.654%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.486479/  1.699735, val:  62.92%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8645%\n",
      "layer   2  Sparsity: 71.7467%\n",
      "layer   3  Sparsity: 64.0235%\n",
      "total_backward_count 1233540 real_backward_count 106552   8.638%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.479150/  1.713563, val:  68.75%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9220%\n",
      "layer   2  Sparsity: 71.3173%\n",
      "layer   3  Sparsity: 64.6958%\n",
      "total_backward_count 1243330 real_backward_count 107177   8.620%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.484492/  1.708393, val:  67.08%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9445%\n",
      "layer   2  Sparsity: 71.0564%\n",
      "layer   3  Sparsity: 64.0780%\n",
      "total_backward_count 1253120 real_backward_count 107768   8.600%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.484100/  1.711088, val:  62.92%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8852%\n",
      "layer   2  Sparsity: 70.9136%\n",
      "layer   3  Sparsity: 64.3368%\n",
      "total_backward_count 1262910 real_backward_count 108380   8.582%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.471885/  1.688779, val:  70.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9158%\n",
      "layer   2  Sparsity: 71.1265%\n",
      "layer   3  Sparsity: 64.3371%\n",
      "total_backward_count 1272700 real_backward_count 108985   8.563%\n",
      "lif layer 1 self.abs_max_v: 15968.5\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.475397/  1.709265, val:  62.08%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8846%\n",
      "layer   2  Sparsity: 71.1272%\n",
      "layer   3  Sparsity: 64.5585%\n",
      "total_backward_count 1282490 real_backward_count 109625   8.548%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.473820/  1.683079, val:  69.58%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8820%\n",
      "layer   2  Sparsity: 71.3616%\n",
      "layer   3  Sparsity: 64.9045%\n",
      "total_backward_count 1292280 real_backward_count 110303   8.536%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.470327/  1.695060, val:  68.33%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9277%\n",
      "layer   2  Sparsity: 71.2020%\n",
      "layer   3  Sparsity: 64.4863%\n",
      "total_backward_count 1302070 real_backward_count 110921   8.519%\n",
      "fc layer 1 self.abs_max_out: 9735.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.485716/  1.705844, val:  60.42%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9045%\n",
      "layer   2  Sparsity: 71.0576%\n",
      "layer   3  Sparsity: 64.0050%\n",
      "total_backward_count 1311860 real_backward_count 111576   8.505%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.466517/  1.711695, val:  59.17%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9117%\n",
      "layer   2  Sparsity: 71.2599%\n",
      "layer   3  Sparsity: 64.4135%\n",
      "total_backward_count 1321650 real_backward_count 112224   8.491%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.472668/  1.668474, val:  70.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8605%\n",
      "layer   2  Sparsity: 71.1495%\n",
      "layer   3  Sparsity: 64.2946%\n",
      "total_backward_count 1331440 real_backward_count 112820   8.474%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.452181/  1.672492, val:  70.00%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9181%\n",
      "layer   2  Sparsity: 71.5454%\n",
      "layer   3  Sparsity: 64.4717%\n",
      "total_backward_count 1341230 real_backward_count 113452   8.459%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.446169/  1.664024, val:  69.17%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 85.9449%\n",
      "layer   2  Sparsity: 71.0457%\n",
      "layer   3  Sparsity: 64.5574%\n",
      "total_backward_count 1351020 real_backward_count 114013   8.439%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.454680/  1.685253, val:  65.42%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8958%\n",
      "layer   2  Sparsity: 71.2378%\n",
      "layer   3  Sparsity: 64.6293%\n",
      "total_backward_count 1360810 real_backward_count 114644   8.425%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.462158/  1.682788, val:  71.67%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9248%\n",
      "layer   2  Sparsity: 71.9162%\n",
      "layer   3  Sparsity: 64.0447%\n",
      "total_backward_count 1370600 real_backward_count 115245   8.408%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.449871/  1.676658, val:  65.83%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8414%\n",
      "layer   2  Sparsity: 71.9564%\n",
      "layer   3  Sparsity: 64.5233%\n",
      "total_backward_count 1380390 real_backward_count 115873   8.394%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.442986/  1.651455, val:  70.00%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8819%\n",
      "layer   2  Sparsity: 71.4777%\n",
      "layer   3  Sparsity: 64.8229%\n",
      "total_backward_count 1390180 real_backward_count 116476   8.378%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.436233/  1.653634, val:  62.50%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8992%\n",
      "layer   2  Sparsity: 71.1315%\n",
      "layer   3  Sparsity: 63.7753%\n",
      "total_backward_count 1399970 real_backward_count 117120   8.366%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.433465/  1.682148, val:  70.00%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8994%\n",
      "layer   2  Sparsity: 71.6254%\n",
      "layer   3  Sparsity: 62.9027%\n",
      "total_backward_count 1409760 real_backward_count 117737   8.352%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.441537/  1.664230, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9071%\n",
      "layer   2  Sparsity: 72.2744%\n",
      "layer   3  Sparsity: 63.9680%\n",
      "total_backward_count 1419550 real_backward_count 118347   8.337%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.425642/  1.668959, val:  57.92%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9499%\n",
      "layer   2  Sparsity: 72.0199%\n",
      "layer   3  Sparsity: 64.3494%\n",
      "total_backward_count 1429340 real_backward_count 118882   8.317%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.425043/  1.675964, val:  69.58%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9324%\n",
      "layer   2  Sparsity: 71.7922%\n",
      "layer   3  Sparsity: 64.5454%\n",
      "total_backward_count 1439130 real_backward_count 119495   8.303%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.434741/  1.670309, val:  68.75%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9068%\n",
      "layer   2  Sparsity: 71.2797%\n",
      "layer   3  Sparsity: 64.5810%\n",
      "total_backward_count 1448920 real_backward_count 120039   8.285%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.429303/  1.678118, val:  68.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8979%\n",
      "layer   2  Sparsity: 71.3527%\n",
      "layer   3  Sparsity: 65.0124%\n",
      "total_backward_count 1458710 real_backward_count 120632   8.270%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.423515/  1.654039, val:  69.17%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9027%\n",
      "layer   2  Sparsity: 71.6847%\n",
      "layer   3  Sparsity: 65.3108%\n",
      "total_backward_count 1468500 real_backward_count 121223   8.255%\n",
      "lif layer 2 self.abs_max_v: 5393.5\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.425427/  1.660519, val:  75.42%, val_best:  75.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9673%\n",
      "layer   2  Sparsity: 71.7011%\n",
      "layer   3  Sparsity: 64.4665%\n",
      "total_backward_count 1478290 real_backward_count 121779   8.238%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.427685/  1.666641, val:  63.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9248%\n",
      "layer   2  Sparsity: 71.4591%\n",
      "layer   3  Sparsity: 65.0844%\n",
      "total_backward_count 1488080 real_backward_count 122364   8.223%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.440271/  1.677647, val:  58.33%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9195%\n",
      "layer   2  Sparsity: 71.8877%\n",
      "layer   3  Sparsity: 64.6400%\n",
      "total_backward_count 1497870 real_backward_count 122963   8.209%\n",
      "lif layer 1 self.abs_max_v: 16110.5\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.439539/  1.646589, val:  65.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8636%\n",
      "layer   2  Sparsity: 71.8838%\n",
      "layer   3  Sparsity: 64.5276%\n",
      "total_backward_count 1507660 real_backward_count 123573   8.196%\n",
      "lif layer 1 self.abs_max_v: 16137.5\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.429141/  1.660875, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9120%\n",
      "layer   2  Sparsity: 71.7249%\n",
      "layer   3  Sparsity: 64.6194%\n",
      "total_backward_count 1517450 real_backward_count 124097   8.178%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.437912/  1.675876, val:  63.75%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9524%\n",
      "layer   2  Sparsity: 71.8039%\n",
      "layer   3  Sparsity: 64.0891%\n",
      "total_backward_count 1527240 real_backward_count 124673   8.163%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.434268/  1.655790, val:  72.08%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8758%\n",
      "layer   2  Sparsity: 71.7895%\n",
      "layer   3  Sparsity: 64.0103%\n",
      "total_backward_count 1537030 real_backward_count 125183   8.144%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.429397/  1.673979, val:  65.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9064%\n",
      "layer   2  Sparsity: 72.1898%\n",
      "layer   3  Sparsity: 64.2164%\n",
      "total_backward_count 1546820 real_backward_count 125714   8.127%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.426105/  1.647932, val:  71.67%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9384%\n",
      "layer   2  Sparsity: 71.8605%\n",
      "layer   3  Sparsity: 64.8036%\n",
      "total_backward_count 1556610 real_backward_count 126272   8.112%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.417369/  1.643031, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9453%\n",
      "layer   2  Sparsity: 71.5387%\n",
      "layer   3  Sparsity: 64.5752%\n",
      "total_backward_count 1566400 real_backward_count 126814   8.096%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.426258/  1.666205, val:  64.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9475%\n",
      "layer   2  Sparsity: 71.5587%\n",
      "layer   3  Sparsity: 64.4178%\n",
      "total_backward_count 1576190 real_backward_count 127339   8.079%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.430898/  1.650398, val:  75.42%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9103%\n",
      "layer   2  Sparsity: 71.1436%\n",
      "layer   3  Sparsity: 65.1110%\n",
      "total_backward_count 1585980 real_backward_count 127903   8.065%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.423876/  1.667413, val:  74.17%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8891%\n",
      "layer   2  Sparsity: 71.3406%\n",
      "layer   3  Sparsity: 65.2648%\n",
      "total_backward_count 1595770 real_backward_count 128468   8.051%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.421118/  1.665661, val:  75.00%, val_best:  75.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9012%\n",
      "layer   2  Sparsity: 71.5192%\n",
      "layer   3  Sparsity: 64.3482%\n",
      "total_backward_count 1605560 real_backward_count 128980   8.033%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.424439/  1.658387, val:  72.08%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8992%\n",
      "layer   2  Sparsity: 71.6699%\n",
      "layer   3  Sparsity: 64.4509%\n",
      "total_backward_count 1615350 real_backward_count 129499   8.017%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.406979/  1.643888, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9298%\n",
      "layer   2  Sparsity: 71.9389%\n",
      "layer   3  Sparsity: 64.4124%\n",
      "total_backward_count 1625140 real_backward_count 129998   7.999%\n",
      "fc layer 3 self.abs_max_out: 962.0\n",
      "lif layer 2 self.abs_max_v: 5446.5\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.403212/  1.652530, val:  72.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9537%\n",
      "layer   2  Sparsity: 71.8328%\n",
      "layer   3  Sparsity: 64.5062%\n",
      "total_backward_count 1634930 real_backward_count 130562   7.986%\n",
      "lif layer 1 self.abs_max_v: 16204.5\n",
      "lif layer 1 self.abs_max_v: 17383.5\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.394723/  1.645228, val:  58.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8564%\n",
      "layer   2  Sparsity: 71.6692%\n",
      "layer   3  Sparsity: 64.5438%\n",
      "total_backward_count 1644720 real_backward_count 131107   7.971%\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.385679/  1.640470, val:  67.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9445%\n",
      "layer   2  Sparsity: 71.4076%\n",
      "layer   3  Sparsity: 64.5153%\n",
      "total_backward_count 1654510 real_backward_count 131649   7.957%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.390960/  1.625766, val:  61.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9285%\n",
      "layer   2  Sparsity: 71.5323%\n",
      "layer   3  Sparsity: 64.8759%\n",
      "total_backward_count 1664300 real_backward_count 132186   7.942%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.388845/  1.616822, val:  67.50%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9193%\n",
      "layer   2  Sparsity: 71.4957%\n",
      "layer   3  Sparsity: 65.1131%\n",
      "total_backward_count 1674090 real_backward_count 132660   7.924%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.392286/  1.629959, val:  70.42%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8697%\n",
      "layer   2  Sparsity: 71.6944%\n",
      "layer   3  Sparsity: 64.5659%\n",
      "total_backward_count 1683880 real_backward_count 133159   7.908%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.388003/  1.647370, val:  63.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9127%\n",
      "layer   2  Sparsity: 71.6781%\n",
      "layer   3  Sparsity: 64.8581%\n",
      "total_backward_count 1693670 real_backward_count 133701   7.894%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.390164/  1.628482, val:  64.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8731%\n",
      "layer   2  Sparsity: 71.5916%\n",
      "layer   3  Sparsity: 64.5645%\n",
      "total_backward_count 1703460 real_backward_count 134236   7.880%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.390020/  1.623535, val:  76.25%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8963%\n",
      "layer   2  Sparsity: 71.9727%\n",
      "layer   3  Sparsity: 64.9356%\n",
      "total_backward_count 1713250 real_backward_count 134774   7.867%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.393637/  1.633119, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8628%\n",
      "layer   2  Sparsity: 71.6930%\n",
      "layer   3  Sparsity: 65.0655%\n",
      "total_backward_count 1723040 real_backward_count 135285   7.852%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.385294/  1.610944, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9085%\n",
      "layer   2  Sparsity: 71.1581%\n",
      "layer   3  Sparsity: 64.4072%\n",
      "total_backward_count 1732830 real_backward_count 135794   7.837%\n",
      "lif layer 2 self.abs_max_v: 5538.5\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.375254/  1.613167, val:  64.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 85.9080%\n",
      "layer   2  Sparsity: 71.4087%\n",
      "layer   3  Sparsity: 64.5460%\n",
      "total_backward_count 1742620 real_backward_count 136287   7.821%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.380094/  1.644388, val:  69.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 85.8933%\n",
      "layer   2  Sparsity: 71.7043%\n",
      "layer   3  Sparsity: 64.9297%\n",
      "total_backward_count 1752410 real_backward_count 136803   7.807%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.381332/  1.632977, val:  66.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9264%\n",
      "layer   2  Sparsity: 71.5614%\n",
      "layer   3  Sparsity: 65.5767%\n",
      "total_backward_count 1762200 real_backward_count 137367   7.795%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.378483/  1.629382, val:  71.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8749%\n",
      "layer   2  Sparsity: 71.0890%\n",
      "layer   3  Sparsity: 64.7330%\n",
      "total_backward_count 1771990 real_backward_count 137900   7.782%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.376707/  1.632938, val:  69.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9005%\n",
      "layer   2  Sparsity: 71.5961%\n",
      "layer   3  Sparsity: 65.3428%\n",
      "total_backward_count 1781780 real_backward_count 138402   7.768%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.387915/  1.638846, val:  71.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8540%\n",
      "layer   2  Sparsity: 71.6546%\n",
      "layer   3  Sparsity: 65.7164%\n",
      "total_backward_count 1791570 real_backward_count 138925   7.754%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.390463/  1.636849, val:  63.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.9023%\n",
      "layer   2  Sparsity: 71.3596%\n",
      "layer   3  Sparsity: 64.9722%\n",
      "total_backward_count 1801360 real_backward_count 139434   7.740%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.384713/  1.642662, val:  64.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9192%\n",
      "layer   2  Sparsity: 70.8994%\n",
      "layer   3  Sparsity: 64.9413%\n",
      "total_backward_count 1811150 real_backward_count 139938   7.726%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.386036/  1.643368, val:  70.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8896%\n",
      "layer   2  Sparsity: 71.0647%\n",
      "layer   3  Sparsity: 64.7469%\n",
      "total_backward_count 1820940 real_backward_count 140471   7.714%\n",
      "fc layer 3 self.abs_max_out: 982.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.386734/  1.628489, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8773%\n",
      "layer   2  Sparsity: 71.6229%\n",
      "layer   3  Sparsity: 64.4381%\n",
      "total_backward_count 1830730 real_backward_count 141015   7.703%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.379733/  1.633102, val:  74.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9003%\n",
      "layer   2  Sparsity: 71.7801%\n",
      "layer   3  Sparsity: 64.9485%\n",
      "total_backward_count 1840520 real_backward_count 141575   7.692%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.379106/  1.616828, val:  72.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.8830%\n",
      "layer   2  Sparsity: 71.6210%\n",
      "layer   3  Sparsity: 64.5395%\n",
      "total_backward_count 1850310 real_backward_count 142087   7.679%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.375694/  1.630373, val:  75.42%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8641%\n",
      "layer   2  Sparsity: 71.5649%\n",
      "layer   3  Sparsity: 64.3225%\n",
      "total_backward_count 1860100 real_backward_count 142605   7.667%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.377569/  1.616959, val:  72.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 85.8518%\n",
      "layer   2  Sparsity: 71.6215%\n",
      "layer   3  Sparsity: 64.8272%\n",
      "total_backward_count 1869890 real_backward_count 143078   7.652%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.375881/  1.643478, val:  60.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9092%\n",
      "layer   2  Sparsity: 71.5363%\n",
      "layer   3  Sparsity: 64.5790%\n",
      "total_backward_count 1879680 real_backward_count 143567   7.638%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.374597/  1.627390, val:  75.83%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9043%\n",
      "layer   2  Sparsity: 71.1591%\n",
      "layer   3  Sparsity: 64.5307%\n",
      "total_backward_count 1889470 real_backward_count 144066   7.625%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.383796/  1.641672, val:  68.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9236%\n",
      "layer   2  Sparsity: 71.3289%\n",
      "layer   3  Sparsity: 64.0739%\n",
      "total_backward_count 1899260 real_backward_count 144536   7.610%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.369791/  1.612367, val:  67.08%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9150%\n",
      "layer   2  Sparsity: 71.8265%\n",
      "layer   3  Sparsity: 63.7811%\n",
      "total_backward_count 1909050 real_backward_count 145049   7.598%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.374044/  1.643378, val:  64.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8587%\n",
      "layer   2  Sparsity: 71.6221%\n",
      "layer   3  Sparsity: 64.2175%\n",
      "total_backward_count 1918840 real_backward_count 145576   7.587%\n",
      "lif layer 1 self.abs_max_v: 17734.0\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.362586/  1.611904, val:  70.00%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 85.9305%\n",
      "layer   2  Sparsity: 71.3082%\n",
      "layer   3  Sparsity: 64.4697%\n",
      "total_backward_count 1928630 real_backward_count 146044   7.572%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.357444/  1.606389, val:  69.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.0107%\n",
      "layer   2  Sparsity: 71.1459%\n",
      "layer   3  Sparsity: 64.6969%\n",
      "total_backward_count 1938420 real_backward_count 146580   7.562%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.358392/  1.586991, val:  68.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.8939%\n",
      "layer   2  Sparsity: 71.1331%\n",
      "layer   3  Sparsity: 64.3692%\n",
      "total_backward_count 1948210 real_backward_count 147044   7.548%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.343871/  1.590968, val:  69.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 85.9117%\n",
      "layer   2  Sparsity: 71.3863%\n",
      "layer   3  Sparsity: 63.8531%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f39ddc3b9840c7a0af3a7d15708e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.34387</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.69583</td></tr><tr><td>val_loss</td><td>1.59097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-sweep-61</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcau59ik' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gcau59ik</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_050548-gcau59ik/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8xk5a6vo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_092437-8xk5a6vo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xk5a6vo' target=\"_blank\">toasty-sweep-67</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xk5a6vo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xk5a6vo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251115_092446_543', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 201.0\n",
      "lif layer 1 self.abs_max_v: 201.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 263.0\n",
      "lif layer 1 self.abs_max_v: 334.0\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "lif layer 2 self.abs_max_v: 70.0\n",
      "fc layer 1 self.abs_max_out: 344.0\n",
      "lif layer 1 self.abs_max_v: 466.5\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 166.0\n",
      "fc layer 1 self.abs_max_out: 390.0\n",
      "fc layer 2 self.abs_max_out: 159.0\n",
      "lif layer 2 self.abs_max_v: 193.0\n",
      "fc layer 1 self.abs_max_out: 722.0\n",
      "lif layer 1 self.abs_max_v: 722.0\n",
      "fc layer 2 self.abs_max_out: 269.0\n",
      "lif layer 2 self.abs_max_v: 330.0\n",
      "fc layer 3 self.abs_max_out: 15.0\n",
      "fc layer 1 self.abs_max_out: 951.0\n",
      "lif layer 1 self.abs_max_v: 961.5\n",
      "fc layer 2 self.abs_max_out: 319.0\n",
      "lif layer 2 self.abs_max_v: 424.0\n",
      "fc layer 3 self.abs_max_out: 49.0\n",
      "fc layer 1 self.abs_max_out: 1409.0\n",
      "lif layer 1 self.abs_max_v: 1446.5\n",
      "fc layer 2 self.abs_max_out: 365.0\n",
      "lif layer 2 self.abs_max_v: 577.0\n",
      "lif layer 1 self.abs_max_v: 1487.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 1 self.abs_max_out: 1593.0\n",
      "lif layer 1 self.abs_max_v: 1673.5\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 2 self.abs_max_out: 405.0\n",
      "fc layer 2 self.abs_max_out: 423.0\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "lif layer 2 self.abs_max_v: 593.5\n",
      "lif layer 2 self.abs_max_v: 640.0\n",
      "lif layer 1 self.abs_max_v: 1737.5\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 1833.0\n",
      "lif layer 1 self.abs_max_v: 1957.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "fc layer 1 self.abs_max_out: 1923.0\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "lif layer 2 self.abs_max_v: 661.0\n",
      "fc layer 2 self.abs_max_out: 621.0\n",
      "fc layer 1 self.abs_max_out: 1925.0\n",
      "lif layer 2 self.abs_max_v: 715.5\n",
      "lif layer 2 self.abs_max_v: 823.0\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 733.0\n",
      "fc layer 1 self.abs_max_out: 2120.0\n",
      "lif layer 1 self.abs_max_v: 2120.0\n",
      "fc layer 2 self.abs_max_out: 737.0\n",
      "lif layer 2 self.abs_max_v: 930.5\n",
      "fc layer 2 self.abs_max_out: 822.0\n",
      "fc layer 2 self.abs_max_out: 871.0\n",
      "lif layer 2 self.abs_max_v: 981.0\n",
      "lif layer 2 self.abs_max_v: 1056.5\n",
      "lif layer 2 self.abs_max_v: 1124.5\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 1 self.abs_max_out: 2196.0\n",
      "lif layer 1 self.abs_max_v: 2196.0\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "fc layer 2 self.abs_max_out: 923.0\n",
      "lif layer 2 self.abs_max_v: 1142.0\n",
      "fc layer 2 self.abs_max_out: 953.0\n",
      "lif layer 2 self.abs_max_v: 1183.5\n",
      "fc layer 3 self.abs_max_out: 180.0\n",
      "lif layer 2 self.abs_max_v: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1248.0\n",
      "lif layer 2 self.abs_max_v: 1283.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "lif layer 2 self.abs_max_v: 1340.5\n",
      "lif layer 2 self.abs_max_v: 1357.5\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "lif layer 2 self.abs_max_v: 1365.0\n",
      "lif layer 2 self.abs_max_v: 1378.5\n",
      "lif layer 1 self.abs_max_v: 2239.5\n",
      "lif layer 1 self.abs_max_v: 2314.0\n",
      "fc layer 2 self.abs_max_out: 1146.0\n",
      "fc layer 1 self.abs_max_out: 2586.0\n",
      "lif layer 1 self.abs_max_v: 2586.0\n",
      "lif layer 2 self.abs_max_v: 1392.0\n",
      "lif layer 2 self.abs_max_v: 1415.0\n",
      "lif layer 2 self.abs_max_v: 1422.5\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "fc layer 2 self.abs_max_out: 1280.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "lif layer 1 self.abs_max_v: 2731.0\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "lif layer 2 self.abs_max_v: 1647.0\n",
      "fc layer 2 self.abs_max_out: 1302.0\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "fc layer 2 self.abs_max_out: 1370.0\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "lif layer 2 self.abs_max_v: 1670.0\n",
      "lif layer 2 self.abs_max_v: 1694.5\n",
      "fc layer 2 self.abs_max_out: 1439.0\n",
      "lif layer 2 self.abs_max_v: 1737.0\n",
      "fc layer 2 self.abs_max_out: 1453.0\n",
      "lif layer 2 self.abs_max_v: 1834.5\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "lif layer 2 self.abs_max_v: 1898.5\n",
      "lif layer 2 self.abs_max_v: 1975.5\n",
      "fc layer 3 self.abs_max_out: 315.0\n",
      "fc layer 3 self.abs_max_out: 319.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "lif layer 2 self.abs_max_v: 1978.0\n",
      "lif layer 2 self.abs_max_v: 1990.0\n",
      "lif layer 2 self.abs_max_v: 2086.0\n",
      "fc layer 2 self.abs_max_out: 1608.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 2 self.abs_max_out: 1696.0\n",
      "fc layer 2 self.abs_max_out: 1702.0\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "fc layer 2 self.abs_max_out: 1823.0\n",
      "lif layer 2 self.abs_max_v: 2092.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 2254.0\n",
      "lif layer 1 self.abs_max_v: 2913.5\n",
      "lif layer 2 self.abs_max_v: 2283.0\n",
      "lif layer 2 self.abs_max_v: 2370.5\n",
      "lif layer 2 self.abs_max_v: 2414.5\n",
      "lif layer 2 self.abs_max_v: 2424.0\n",
      "fc layer 2 self.abs_max_out: 1887.0\n",
      "fc layer 1 self.abs_max_out: 2650.0\n",
      "fc layer 1 self.abs_max_out: 2662.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 2472.5\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "fc layer 1 self.abs_max_out: 3180.0\n",
      "lif layer 1 self.abs_max_v: 3180.0\n",
      "fc layer 1 self.abs_max_out: 3241.0\n",
      "lif layer 1 self.abs_max_v: 3241.5\n",
      "lif layer 1 self.abs_max_v: 3338.0\n",
      "lif layer 1 self.abs_max_v: 3932.5\n",
      "fc layer 2 self.abs_max_out: 1904.0\n",
      "fc layer 2 self.abs_max_out: 1925.0\n",
      "fc layer 2 self.abs_max_out: 1985.0\n",
      "lif layer 1 self.abs_max_v: 3967.0\n",
      "lif layer 1 self.abs_max_v: 4034.5\n",
      "lif layer 1 self.abs_max_v: 4227.5\n",
      "fc layer 2 self.abs_max_out: 2072.0\n",
      "lif layer 2 self.abs_max_v: 2507.5\n",
      "lif layer 2 self.abs_max_v: 2512.0\n",
      "lif layer 2 self.abs_max_v: 2537.0\n",
      "lif layer 1 self.abs_max_v: 4228.0\n",
      "lif layer 2 self.abs_max_v: 2570.5\n",
      "lif layer 2 self.abs_max_v: 2608.0\n",
      "lif layer 1 self.abs_max_v: 4302.5\n",
      "fc layer 1 self.abs_max_out: 3252.0\n",
      "fc layer 1 self.abs_max_out: 3522.0\n",
      "lif layer 2 self.abs_max_v: 2626.0\n",
      "lif layer 1 self.abs_max_v: 4350.0\n",
      "lif layer 1 self.abs_max_v: 4545.0\n",
      "lif layer 1 self.abs_max_v: 4647.5\n",
      "lif layer 1 self.abs_max_v: 5153.5\n",
      "lif layer 1 self.abs_max_v: 5336.0\n",
      "lif layer 1 self.abs_max_v: 5900.0\n",
      "lif layer 2 self.abs_max_v: 2680.5\n",
      "lif layer 2 self.abs_max_v: 2709.5\n",
      "lif layer 2 self.abs_max_v: 2732.0\n",
      "lif layer 2 self.abs_max_v: 2784.0\n",
      "fc layer 2 self.abs_max_out: 2086.0\n",
      "fc layer 2 self.abs_max_out: 2119.0\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "lif layer 2 self.abs_max_v: 3000.0\n",
      "lif layer 2 self.abs_max_v: 3003.0\n",
      "lif layer 1 self.abs_max_v: 6074.5\n",
      "fc layer 1 self.abs_max_out: 3602.0\n",
      "fc layer 1 self.abs_max_out: 3626.0\n",
      "lif layer 1 self.abs_max_v: 6078.5\n",
      "lif layer 1 self.abs_max_v: 6613.5\n",
      "lif layer 1 self.abs_max_v: 6676.0\n",
      "lif layer 1 self.abs_max_v: 6798.0\n",
      "fc layer 1 self.abs_max_out: 3673.0\n",
      "fc layer 1 self.abs_max_out: 3725.0\n",
      "fc layer 1 self.abs_max_out: 3834.0\n",
      "fc layer 1 self.abs_max_out: 3957.0\n",
      "lif layer 2 self.abs_max_v: 3025.0\n",
      "lif layer 2 self.abs_max_v: 3141.0\n",
      "lif layer 1 self.abs_max_v: 6846.5\n",
      "lif layer 1 self.abs_max_v: 7094.5\n",
      "fc layer 2 self.abs_max_out: 2245.0\n",
      "fc layer 1 self.abs_max_out: 4110.0\n",
      "fc layer 1 self.abs_max_out: 4130.0\n",
      "lif layer 1 self.abs_max_v: 7516.0\n",
      "lif layer 1 self.abs_max_v: 7593.0\n",
      "lif layer 2 self.abs_max_v: 3186.0\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 2 self.abs_max_out: 2254.0\n",
      "fc layer 1 self.abs_max_out: 4377.0\n",
      "fc layer 1 self.abs_max_out: 4575.0\n",
      "lif layer 1 self.abs_max_v: 7814.0\n",
      "lif layer 1 self.abs_max_v: 8340.5\n",
      "lif layer 1 self.abs_max_v: 8629.5\n",
      "lif layer 2 self.abs_max_v: 3327.0\n",
      "fc layer 2 self.abs_max_out: 2319.0\n",
      "fc layer 2 self.abs_max_out: 2340.0\n",
      "fc layer 2 self.abs_max_out: 2469.0\n",
      "fc layer 2 self.abs_max_out: 2578.0\n",
      "lif layer 2 self.abs_max_v: 3329.0\n",
      "lif layer 2 self.abs_max_v: 3357.5\n",
      "lif layer 2 self.abs_max_v: 3424.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.535375/  1.873405, val:  33.33%, val_best:  33.33%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2441%\n",
      "layer   3  Sparsity: 71.2825%\n",
      "total_backward_count 9790 real_backward_count 1649  16.844%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3436.5\n",
      "lif layer 2 self.abs_max_v: 3646.0\n",
      "lif layer 2 self.abs_max_v: 3698.0\n",
      "lif layer 2 self.abs_max_v: 3890.5\n",
      "fc layer 1 self.abs_max_out: 4640.0\n",
      "fc layer 2 self.abs_max_out: 2585.0\n",
      "fc layer 2 self.abs_max_out: 2669.0\n",
      "fc layer 2 self.abs_max_out: 2761.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 3959.0\n",
      "fc layer 2 self.abs_max_out: 2788.0\n",
      "fc layer 2 self.abs_max_out: 2831.0\n",
      "fc layer 2 self.abs_max_out: 2949.0\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "fc layer 3 self.abs_max_out: 607.0\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "fc layer 3 self.abs_max_out: 687.0\n",
      "fc layer 1 self.abs_max_out: 5061.0\n",
      "lif layer 1 self.abs_max_v: 8651.5\n",
      "lif layer 1 self.abs_max_v: 8885.5\n",
      "fc layer 1 self.abs_max_out: 5165.0\n",
      "fc layer 1 self.abs_max_out: 5176.0\n",
      "lif layer 1 self.abs_max_v: 9078.5\n",
      "fc layer 2 self.abs_max_out: 3350.0\n",
      "fc layer 2 self.abs_max_out: 3476.0\n",
      "fc layer 2 self.abs_max_out: 3529.0\n",
      "fc layer 2 self.abs_max_out: 3549.0\n",
      "fc layer 2 self.abs_max_out: 3596.0\n",
      "fc layer 2 self.abs_max_out: 3603.0\n",
      "fc layer 2 self.abs_max_out: 3652.0\n",
      "fc layer 2 self.abs_max_out: 3714.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.407571/  1.765718, val:  46.25%, val_best:  46.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 75.8642%\n",
      "layer   3  Sparsity: 71.6582%\n",
      "total_backward_count 19580 real_backward_count 3078  15.720%\n",
      "fc layer 3 self.abs_max_out: 695.0\n",
      "fc layer 1 self.abs_max_out: 5242.0\n",
      "fc layer 1 self.abs_max_out: 5535.0\n",
      "fc layer 1 self.abs_max_out: 6090.0\n",
      "fc layer 1 self.abs_max_out: 6098.0\n",
      "lif layer 1 self.abs_max_v: 9585.5\n",
      "fc layer 1 self.abs_max_out: 6135.0\n",
      "lif layer 1 self.abs_max_v: 9621.0\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "fc layer 3 self.abs_max_out: 724.0\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "lif layer 1 self.abs_max_v: 10203.5\n",
      "lif layer 2 self.abs_max_v: 4131.0\n",
      "lif layer 2 self.abs_max_v: 4309.5\n",
      "lif layer 2 self.abs_max_v: 4325.0\n",
      "lif layer 1 self.abs_max_v: 11013.0\n",
      "lif layer 2 self.abs_max_v: 4508.0\n",
      "lif layer 2 self.abs_max_v: 4760.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.314610/  1.747644, val:  44.17%, val_best:  46.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 75.3058%\n",
      "layer   3  Sparsity: 72.4966%\n",
      "total_backward_count 29370 real_backward_count 4544  15.472%\n",
      "fc layer 1 self.abs_max_out: 6171.0\n",
      "fc layer 1 self.abs_max_out: 6204.0\n",
      "fc layer 1 self.abs_max_out: 6619.0\n",
      "lif layer 1 self.abs_max_v: 11023.0\n",
      "lif layer 1 self.abs_max_v: 11170.5\n",
      "lif layer 1 self.abs_max_v: 11298.5\n",
      "lif layer 1 self.abs_max_v: 11465.5\n",
      "lif layer 1 self.abs_max_v: 11819.0\n",
      "lif layer 1 self.abs_max_v: 11890.5\n",
      "fc layer 1 self.abs_max_out: 6832.0\n",
      "fc layer 1 self.abs_max_out: 6963.0\n",
      "fc layer 1 self.abs_max_out: 7236.0\n",
      "fc layer 1 self.abs_max_out: 7437.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.325684/  1.820032, val:  35.00%, val_best:  46.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 76.7502%\n",
      "layer   3  Sparsity: 73.2237%\n",
      "total_backward_count 39160 real_backward_count 5894  15.051%\n",
      "fc layer 1 self.abs_max_out: 7823.0\n",
      "lif layer 2 self.abs_max_v: 4951.0\n",
      "fc layer 2 self.abs_max_out: 3732.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.319079/  1.767836, val:  44.58%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 76.5335%\n",
      "layer   3  Sparsity: 73.6001%\n",
      "total_backward_count 48950 real_backward_count 7202  14.713%\n",
      "lif layer 1 self.abs_max_v: 12584.0\n",
      "lif layer 2 self.abs_max_v: 5052.5\n",
      "lif layer 2 self.abs_max_v: 5337.0\n",
      "lif layer 2 self.abs_max_v: 5417.5\n",
      "lif layer 2 self.abs_max_v: 5455.0\n",
      "lif layer 2 self.abs_max_v: 5516.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.303200/  1.720073, val:  38.75%, val_best:  46.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 75.6020%\n",
      "layer   3  Sparsity: 73.5395%\n",
      "total_backward_count 58740 real_backward_count 8477  14.431%\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "lif layer 2 self.abs_max_v: 5516.5\n",
      "lif layer 2 self.abs_max_v: 5616.0\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "fc layer 1 self.abs_max_out: 7888.0\n",
      "lif layer 1 self.abs_max_v: 13093.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.253972/  1.694716, val:  58.75%, val_best:  58.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 75.1879%\n",
      "layer   3  Sparsity: 73.6803%\n",
      "total_backward_count 68530 real_backward_count 9678  14.122%\n",
      "fc layer 1 self.abs_max_out: 8224.0\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "fc layer 3 self.abs_max_out: 858.0\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.247242/  1.632064, val:  50.42%, val_best:  58.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 75.2203%\n",
      "layer   3  Sparsity: 74.2777%\n",
      "total_backward_count 78320 real_backward_count 10868  13.876%\n",
      "lif layer 2 self.abs_max_v: 5627.0\n",
      "lif layer 2 self.abs_max_v: 5748.5\n",
      "lif layer 2 self.abs_max_v: 6024.5\n",
      "lif layer 2 self.abs_max_v: 6105.0\n",
      "lif layer 1 self.abs_max_v: 13341.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.197892/  1.586817, val:  50.83%, val_best:  58.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 75.7950%\n",
      "layer   3  Sparsity: 73.9288%\n",
      "total_backward_count 88110 real_backward_count 12078  13.708%\n",
      "fc layer 1 self.abs_max_out: 8651.0\n",
      "lif layer 1 self.abs_max_v: 14388.5\n",
      "fc layer 1 self.abs_max_out: 9079.0\n",
      "lif layer 2 self.abs_max_v: 6410.5\n",
      "fc layer 3 self.abs_max_out: 923.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.183702/  1.585885, val:  49.17%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 76.3020%\n",
      "layer   3  Sparsity: 73.3459%\n",
      "total_backward_count 97900 real_backward_count 13255  13.539%\n",
      "fc layer 2 self.abs_max_out: 3861.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.107648/  1.569003, val:  52.08%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.0618%\n",
      "layer   3  Sparsity: 73.1450%\n",
      "total_backward_count 107690 real_backward_count 14408  13.379%\n",
      "lif layer 1 self.abs_max_v: 14704.0\n",
      "lif layer 2 self.abs_max_v: 6511.5\n",
      "lif layer 2 self.abs_max_v: 6743.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.158272/  1.551325, val:  49.58%, val_best:  58.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.9408%\n",
      "layer   3  Sparsity: 74.4477%\n",
      "total_backward_count 117480 real_backward_count 15537  13.225%\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.069076/  1.545543, val:  45.83%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.4752%\n",
      "layer   3  Sparsity: 72.8077%\n",
      "total_backward_count 127270 real_backward_count 16629  13.066%\n",
      "lif layer 1 self.abs_max_v: 14840.5\n",
      "lif layer 1 self.abs_max_v: 15033.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.036283/  1.587788, val:  44.17%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.5917%\n",
      "layer   3  Sparsity: 72.9494%\n",
      "total_backward_count 137060 real_backward_count 17701  12.915%\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.048834/  1.499500, val:  57.08%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.0036%\n",
      "layer   3  Sparsity: 73.0727%\n",
      "total_backward_count 146850 real_backward_count 18748  12.767%\n",
      "lif layer 1 self.abs_max_v: 15172.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.000194/  1.477681, val:  54.17%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.1048%\n",
      "layer   3  Sparsity: 73.2943%\n",
      "total_backward_count 156640 real_backward_count 19794  12.637%\n",
      "lif layer 1 self.abs_max_v: 15198.0\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "fc layer 3 self.abs_max_out: 959.0\n",
      "lif layer 1 self.abs_max_v: 15300.0\n",
      "lif layer 1 self.abs_max_v: 15411.0\n",
      "fc layer 1 self.abs_max_out: 9983.0\n",
      "lif layer 1 self.abs_max_v: 17212.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.973164/  1.435735, val:  59.58%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.4707%\n",
      "layer   3  Sparsity: 74.2495%\n",
      "total_backward_count 166430 real_backward_count 20786  12.489%\n",
      "fc layer 1 self.abs_max_out: 10159.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.991664/  1.421695, val:  59.17%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 74.4317%\n",
      "layer   3  Sparsity: 72.7437%\n",
      "total_backward_count 176220 real_backward_count 21767  12.352%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.964338/  1.374908, val:  58.75%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 73.9381%\n",
      "layer   3  Sparsity: 72.6735%\n",
      "total_backward_count 186010 real_backward_count 22750  12.231%\n",
      "fc layer 3 self.abs_max_out: 986.0\n",
      "lif layer 1 self.abs_max_v: 17330.0\n",
      "lif layer 1 self.abs_max_v: 18116.5\n",
      "fc layer 1 self.abs_max_out: 10164.0\n",
      "fc layer 1 self.abs_max_out: 10454.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.905126/  1.474919, val:  53.33%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 73.4189%\n",
      "layer   3  Sparsity: 73.2936%\n",
      "total_backward_count 195800 real_backward_count 23667  12.087%\n",
      "lif layer 1 self.abs_max_v: 18641.5\n",
      "fc layer 1 self.abs_max_out: 10490.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.882413/  1.374423, val:  62.92%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 73.2947%\n",
      "layer   3  Sparsity: 72.6884%\n",
      "total_backward_count 205590 real_backward_count 24572  11.952%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.872064/  1.388648, val:  60.42%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.7089%\n",
      "layer   3  Sparsity: 72.1530%\n",
      "total_backward_count 215380 real_backward_count 25535  11.856%\n",
      "fc layer 3 self.abs_max_out: 1043.0\n",
      "fc layer 3 self.abs_max_out: 1052.0\n",
      "fc layer 3 self.abs_max_out: 1087.0\n",
      "fc layer 1 self.abs_max_out: 10673.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.856953/  1.332391, val:  60.00%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.9836%\n",
      "layer   3  Sparsity: 70.9502%\n",
      "total_backward_count 225170 real_backward_count 26491  11.765%\n",
      "lif layer 1 self.abs_max_v: 19108.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.805942/  1.328368, val:  58.75%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.4380%\n",
      "layer   3  Sparsity: 71.1596%\n",
      "total_backward_count 234960 real_backward_count 27393  11.659%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.774063/  1.237865, val:  69.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.3687%\n",
      "layer   3  Sparsity: 71.1060%\n",
      "total_backward_count 244750 real_backward_count 28262  11.547%\n",
      "fc layer 3 self.abs_max_out: 1112.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.786049/  1.272169, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.5025%\n",
      "layer   3  Sparsity: 71.5183%\n",
      "total_backward_count 254540 real_backward_count 29182  11.465%\n",
      "fc layer 1 self.abs_max_out: 10791.0\n",
      "lif layer 1 self.abs_max_v: 19755.5\n",
      "fc layer 1 self.abs_max_out: 11027.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.762922/  1.278272, val:  58.33%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.9228%\n",
      "layer   3  Sparsity: 72.2498%\n",
      "total_backward_count 264330 real_backward_count 30046  11.367%\n",
      "lif layer 1 self.abs_max_v: 20128.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.774111/  1.286891, val:  65.00%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.8961%\n",
      "layer   3  Sparsity: 71.8412%\n",
      "total_backward_count 274120 real_backward_count 30938  11.286%\n",
      "fc layer 3 self.abs_max_out: 1164.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.716322/  1.278235, val:  66.25%, val_best:  69.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.8783%\n",
      "layer   3  Sparsity: 71.0990%\n",
      "total_backward_count 283910 real_backward_count 31781  11.194%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.714541/  1.335230, val:  53.33%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7605%\n",
      "layer   3  Sparsity: 71.0849%\n",
      "total_backward_count 293700 real_backward_count 32605  11.101%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.717395/  1.230295, val:  66.67%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.3436%\n",
      "layer   3  Sparsity: 71.5378%\n",
      "total_backward_count 303490 real_backward_count 33404  11.007%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.682619/  1.253078, val:  61.67%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.5437%\n",
      "layer   3  Sparsity: 70.5944%\n",
      "total_backward_count 313280 real_backward_count 34192  10.914%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.665931/  1.312735, val:  56.25%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6349%\n",
      "layer   3  Sparsity: 70.7405%\n",
      "total_backward_count 323070 real_backward_count 34963  10.822%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.655787/  1.167830, val:  72.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.8565%\n",
      "layer   3  Sparsity: 71.7030%\n",
      "total_backward_count 332860 real_backward_count 35792  10.753%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.636241/  1.161798, val:  65.00%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.9911%\n",
      "layer   3  Sparsity: 70.9009%\n",
      "total_backward_count 342650 real_backward_count 36582  10.676%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.629361/  1.272162, val:  55.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6302%\n",
      "layer   3  Sparsity: 70.7610%\n",
      "total_backward_count 352440 real_backward_count 37346  10.596%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.603308/  1.180630, val:  67.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3158%\n",
      "layer   3  Sparsity: 70.4082%\n",
      "total_backward_count 362230 real_backward_count 38086  10.514%\n",
      "fc layer 3 self.abs_max_out: 1208.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.599476/  1.226741, val:  61.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3072%\n",
      "layer   3  Sparsity: 70.0023%\n",
      "total_backward_count 372020 real_backward_count 38791  10.427%\n",
      "fc layer 3 self.abs_max_out: 1221.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.599451/  1.147170, val:  63.75%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.9665%\n",
      "layer   3  Sparsity: 70.2733%\n",
      "total_backward_count 381810 real_backward_count 39524  10.352%\n",
      "fc layer 3 self.abs_max_out: 1238.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.583917/  1.301298, val:  60.42%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.2441%\n",
      "layer   3  Sparsity: 70.5996%\n",
      "total_backward_count 391600 real_backward_count 40252  10.279%\n",
      "lif layer 2 self.abs_max_v: 6818.0\n",
      "lif layer 2 self.abs_max_v: 6860.0\n",
      "fc layer 3 self.abs_max_out: 1249.0\n",
      "fc layer 3 self.abs_max_out: 1252.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.585698/  1.165045, val:  66.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6479%\n",
      "layer   3  Sparsity: 69.4720%\n",
      "total_backward_count 401390 real_backward_count 41015  10.218%\n",
      "fc layer 3 self.abs_max_out: 1290.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.550477/  1.189205, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5142%\n",
      "layer   3  Sparsity: 69.7207%\n",
      "total_backward_count 411180 real_backward_count 41722  10.147%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.527912/  1.112248, val:  65.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4563%\n",
      "layer   3  Sparsity: 69.5894%\n",
      "total_backward_count 420970 real_backward_count 42393  10.070%\n",
      "lif layer 2 self.abs_max_v: 6868.0\n",
      "lif layer 2 self.abs_max_v: 7108.0\n",
      "fc layer 3 self.abs_max_out: 1305.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.532571/  1.178245, val:  67.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5526%\n",
      "layer   3  Sparsity: 70.0808%\n",
      "total_backward_count 430760 real_backward_count 43105  10.007%\n",
      "fc layer 3 self.abs_max_out: 1325.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.519316/  1.193437, val:  62.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2428%\n",
      "layer   3  Sparsity: 69.7434%\n",
      "total_backward_count 440550 real_backward_count 43746   9.930%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.492314/  1.085330, val:  67.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3342%\n",
      "layer   3  Sparsity: 69.6147%\n",
      "total_backward_count 450340 real_backward_count 44414   9.862%\n",
      "fc layer 3 self.abs_max_out: 1396.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.495573/  1.182168, val:  61.67%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4557%\n",
      "layer   3  Sparsity: 69.1087%\n",
      "total_backward_count 460130 real_backward_count 45051   9.791%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.484672/  1.149780, val:  62.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7382%\n",
      "layer   3  Sparsity: 69.6632%\n",
      "total_backward_count 469920 real_backward_count 45703   9.726%\n",
      "fc layer 3 self.abs_max_out: 1399.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.458178/  1.080893, val:  70.83%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4110%\n",
      "layer   3  Sparsity: 69.9726%\n",
      "total_backward_count 479710 real_backward_count 46293   9.650%\n",
      "fc layer 2 self.abs_max_out: 3906.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.464055/  1.101538, val:  66.67%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0540%\n",
      "layer   3  Sparsity: 70.0149%\n",
      "total_backward_count 489500 real_backward_count 46959   9.593%\n",
      "fc layer 3 self.abs_max_out: 1401.0\n",
      "fc layer 3 self.abs_max_out: 1424.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.467357/  1.140148, val:  67.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.6897%\n",
      "layer   3  Sparsity: 69.7261%\n",
      "total_backward_count 499290 real_backward_count 47579   9.529%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.451715/  1.147325, val:  66.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9027%\n",
      "layer   3  Sparsity: 69.5533%\n",
      "total_backward_count 509080 real_backward_count 48145   9.457%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.460884/  1.121302, val:  66.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.8785%\n",
      "layer   3  Sparsity: 69.0945%\n",
      "total_backward_count 518870 real_backward_count 48744   9.394%\n",
      "fc layer 3 self.abs_max_out: 1543.0\n",
      "fc layer 3 self.abs_max_out: 1570.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.449300/  1.217256, val:  60.00%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0253%\n",
      "layer   3  Sparsity: 69.0804%\n",
      "total_backward_count 528660 real_backward_count 49367   9.338%\n",
      "fc layer 2 self.abs_max_out: 3927.0\n",
      "lif layer 2 self.abs_max_v: 7193.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.435535/  1.180807, val:  63.33%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3307%\n",
      "layer   3  Sparsity: 68.9694%\n",
      "total_backward_count 538450 real_backward_count 49935   9.274%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.429718/  1.193223, val:  61.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3299%\n",
      "layer   3  Sparsity: 69.3483%\n",
      "total_backward_count 548240 real_backward_count 50536   9.218%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.418730/  1.150836, val:  65.83%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0722%\n",
      "layer   3  Sparsity: 69.3750%\n",
      "total_backward_count 558030 real_backward_count 51128   9.162%\n",
      "fc layer 2 self.abs_max_out: 3979.0\n",
      "lif layer 2 self.abs_max_v: 7530.5\n",
      "lif layer 2 self.abs_max_v: 7586.5\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.409557/  1.086447, val:  68.75%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9232%\n",
      "layer   3  Sparsity: 69.3394%\n",
      "total_backward_count 567820 real_backward_count 51682   9.102%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.405649/  1.049808, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1481%\n",
      "layer   3  Sparsity: 67.9348%\n",
      "total_backward_count 577610 real_backward_count 52168   9.032%\n",
      "fc layer 2 self.abs_max_out: 4062.0\n",
      "lif layer 2 self.abs_max_v: 7603.5\n",
      "lif layer 2 self.abs_max_v: 7826.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.386728/  1.217641, val:  61.25%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4244%\n",
      "layer   3  Sparsity: 68.5560%\n",
      "total_backward_count 587400 real_backward_count 52675   8.967%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.380001/  1.173382, val:  66.25%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3489%\n",
      "layer   3  Sparsity: 68.5534%\n",
      "total_backward_count 597190 real_backward_count 53172   8.904%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.396545/  1.084840, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.6936%\n",
      "layer   3  Sparsity: 69.1191%\n",
      "total_backward_count 606980 real_backward_count 53694   8.846%\n",
      "fc layer 2 self.abs_max_out: 4101.0\n",
      "fc layer 2 self.abs_max_out: 4119.0\n",
      "lif layer 2 self.abs_max_v: 7863.5\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.401729/  1.093558, val:  67.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.8993%\n",
      "layer   3  Sparsity: 69.8409%\n",
      "total_backward_count 616770 real_backward_count 54223   8.791%\n",
      "fc layer 2 self.abs_max_out: 4193.0\n",
      "lif layer 2 self.abs_max_v: 7995.5\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.383574/  1.142852, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9388%\n",
      "layer   3  Sparsity: 68.9505%\n",
      "total_backward_count 626560 real_backward_count 54687   8.728%\n",
      "fc layer 2 self.abs_max_out: 4313.0\n",
      "lif layer 2 self.abs_max_v: 8122.0\n",
      "fc layer 3 self.abs_max_out: 1584.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.387970/  1.120034, val:  65.00%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.5571%\n",
      "layer   3  Sparsity: 69.4622%\n",
      "total_backward_count 636350 real_backward_count 55172   8.670%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.375812/  1.073213, val:  67.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9542%\n",
      "layer   3  Sparsity: 69.1468%\n",
      "total_backward_count 646140 real_backward_count 55633   8.610%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.379800/  1.071926, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0299%\n",
      "layer   3  Sparsity: 68.3250%\n",
      "total_backward_count 655930 real_backward_count 56129   8.557%\n",
      "fc layer 3 self.abs_max_out: 1597.0\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.365270/  1.096638, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2741%\n",
      "layer   3  Sparsity: 68.3551%\n",
      "total_backward_count 665720 real_backward_count 56572   8.498%\n",
      "fc layer 2 self.abs_max_out: 4790.0\n",
      "lif layer 2 self.abs_max_v: 8488.5\n",
      "lif layer 2 self.abs_max_v: 8751.5\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.355643/  1.044109, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1072%\n",
      "layer   3  Sparsity: 67.9716%\n",
      "total_backward_count 675510 real_backward_count 57021   8.441%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.342358/  1.158560, val:  67.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.7559%\n",
      "layer   3  Sparsity: 68.3562%\n",
      "total_backward_count 685300 real_backward_count 57438   8.381%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.349845/  1.106788, val:  68.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0080%\n",
      "layer   3  Sparsity: 67.8535%\n",
      "total_backward_count 695090 real_backward_count 57886   8.328%\n",
      "fc layer 3 self.abs_max_out: 1601.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.344408/  1.089533, val:  70.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.7025%\n",
      "layer   3  Sparsity: 67.3790%\n",
      "total_backward_count 704880 real_backward_count 58305   8.272%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.343972/  1.100752, val:  67.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3281%\n",
      "layer   3  Sparsity: 67.3818%\n",
      "total_backward_count 714670 real_backward_count 58723   8.217%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.332863/  1.056792, val:  75.00%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5458%\n",
      "layer   3  Sparsity: 66.7750%\n",
      "total_backward_count 724460 real_backward_count 59125   8.161%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.320891/  1.072977, val:  73.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4368%\n",
      "layer   3  Sparsity: 67.8436%\n",
      "total_backward_count 734250 real_backward_count 59532   8.108%\n",
      "fc layer 3 self.abs_max_out: 1661.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.317347/  1.063673, val:  70.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3758%\n",
      "layer   3  Sparsity: 66.5860%\n",
      "total_backward_count 744040 real_backward_count 59920   8.053%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.330149/  1.154036, val:  63.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9900%\n",
      "layer   3  Sparsity: 67.5225%\n",
      "total_backward_count 753830 real_backward_count 60311   8.001%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.329360/  1.159498, val:  63.75%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2665%\n",
      "layer   3  Sparsity: 67.3787%\n",
      "total_backward_count 763620 real_backward_count 60687   7.947%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.320357/  1.057546, val:  71.25%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1879%\n",
      "layer   3  Sparsity: 67.2410%\n",
      "total_backward_count 773410 real_backward_count 61078   7.897%\n",
      "fc layer 3 self.abs_max_out: 1705.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.341392/  1.002904, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2503%\n",
      "layer   3  Sparsity: 68.3955%\n",
      "total_backward_count 783200 real_backward_count 61470   7.849%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.336055/  1.071479, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0719%\n",
      "layer   3  Sparsity: 67.3587%\n",
      "total_backward_count 792990 real_backward_count 61850   7.800%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.322406/  1.104886, val:  71.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 72.0986%\n",
      "layer   3  Sparsity: 67.7619%\n",
      "total_backward_count 802780 real_backward_count 62252   7.755%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.313544/  1.045983, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7454%\n",
      "layer   3  Sparsity: 68.0912%\n",
      "total_backward_count 812570 real_backward_count 62590   7.703%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.311342/  1.089375, val:  71.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5007%\n",
      "layer   3  Sparsity: 66.9264%\n",
      "total_backward_count 822360 real_backward_count 62984   7.659%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.315745/  1.049790, val:  72.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6800%\n",
      "layer   3  Sparsity: 67.6465%\n",
      "total_backward_count 832150 real_backward_count 63330   7.610%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.306727/  1.097921, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6200%\n",
      "layer   3  Sparsity: 67.5825%\n",
      "total_backward_count 841940 real_backward_count 63680   7.563%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.299446/  1.015788, val:  75.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6368%\n",
      "layer   3  Sparsity: 67.1478%\n",
      "total_backward_count 851730 real_backward_count 64006   7.515%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.291761/  1.083700, val:  70.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4073%\n",
      "layer   3  Sparsity: 68.2698%\n",
      "total_backward_count 861520 real_backward_count 64329   7.467%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.300622/  1.031351, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3494%\n",
      "layer   3  Sparsity: 68.3158%\n",
      "total_backward_count 871310 real_backward_count 64685   7.424%\n",
      "fc layer 3 self.abs_max_out: 1707.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.301502/  1.029764, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5650%\n",
      "layer   3  Sparsity: 67.7485%\n",
      "total_backward_count 881100 real_backward_count 65041   7.382%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.286807/  1.058642, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5959%\n",
      "layer   3  Sparsity: 67.9169%\n",
      "total_backward_count 890890 real_backward_count 65376   7.338%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.281123/  1.099678, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3834%\n",
      "layer   3  Sparsity: 67.8908%\n",
      "total_backward_count 900680 real_backward_count 65689   7.293%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.277828/  1.079377, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2377%\n",
      "layer   3  Sparsity: 68.3785%\n",
      "total_backward_count 910470 real_backward_count 65994   7.248%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.272713/  1.093756, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0556%\n",
      "layer   3  Sparsity: 67.9768%\n",
      "total_backward_count 920260 real_backward_count 66282   7.203%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.282587/  1.048059, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1118%\n",
      "layer   3  Sparsity: 67.0850%\n",
      "total_backward_count 930050 real_backward_count 66573   7.158%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.274668/  1.061952, val:  67.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9084%\n",
      "layer   3  Sparsity: 67.3094%\n",
      "total_backward_count 939840 real_backward_count 66855   7.113%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.271801/  1.098441, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0659%\n",
      "layer   3  Sparsity: 66.6744%\n",
      "total_backward_count 949630 real_backward_count 67159   7.072%\n",
      "fc layer 3 self.abs_max_out: 1726.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.272395/  1.035742, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3297%\n",
      "layer   3  Sparsity: 66.5651%\n",
      "total_backward_count 959420 real_backward_count 67405   7.026%\n",
      "fc layer 3 self.abs_max_out: 1730.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.267871/  1.015133, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.7355%\n",
      "layer   3  Sparsity: 66.7941%\n",
      "total_backward_count 969210 real_backward_count 67678   6.983%\n",
      "fc layer 3 self.abs_max_out: 1766.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.262859/  1.097966, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9943%\n",
      "layer   3  Sparsity: 67.7272%\n",
      "total_backward_count 979000 real_backward_count 67960   6.942%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.260271/  1.078090, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1735%\n",
      "layer   3  Sparsity: 67.8420%\n",
      "total_backward_count 988790 real_backward_count 68217   6.899%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.240022/  1.054106, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3710%\n",
      "layer   3  Sparsity: 67.5263%\n",
      "total_backward_count 998580 real_backward_count 68440   6.854%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.259448/  1.097509, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3323%\n",
      "layer   3  Sparsity: 67.6953%\n",
      "total_backward_count 1008370 real_backward_count 68733   6.816%\n",
      "fc layer 3 self.abs_max_out: 1786.0\n",
      "fc layer 3 self.abs_max_out: 1853.0\n",
      "fc layer 3 self.abs_max_out: 1872.0\n",
      "fc layer 3 self.abs_max_out: 1878.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.248934/  1.151325, val:  67.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1749%\n",
      "layer   3  Sparsity: 68.2754%\n",
      "total_backward_count 1018160 real_backward_count 68999   6.777%\n",
      "fc layer 3 self.abs_max_out: 1910.0\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.244471/  1.020126, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3983%\n",
      "layer   3  Sparsity: 68.2398%\n",
      "total_backward_count 1027950 real_backward_count 69215   6.733%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.248174/  1.075727, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5905%\n",
      "layer   3  Sparsity: 67.8975%\n",
      "total_backward_count 1037740 real_backward_count 69453   6.693%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.251802/  1.063164, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3350%\n",
      "layer   3  Sparsity: 67.7086%\n",
      "total_backward_count 1047530 real_backward_count 69694   6.653%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.248195/  1.155178, val:  68.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5306%\n",
      "layer   3  Sparsity: 67.8685%\n",
      "total_backward_count 1057320 real_backward_count 69960   6.617%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.243256/  1.069283, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6748%\n",
      "layer   3  Sparsity: 68.0355%\n",
      "total_backward_count 1067110 real_backward_count 70209   6.579%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.233087/  1.048235, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2308%\n",
      "layer   3  Sparsity: 67.9502%\n",
      "total_backward_count 1076900 real_backward_count 70407   6.538%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.234500/  1.138368, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4275%\n",
      "layer   3  Sparsity: 67.2936%\n",
      "total_backward_count 1086690 real_backward_count 70606   6.497%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.233973/  1.084227, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1378%\n",
      "layer   3  Sparsity: 67.1044%\n",
      "total_backward_count 1096480 real_backward_count 70830   6.460%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.238414/  1.058841, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.8626%\n",
      "layer   3  Sparsity: 67.1458%\n",
      "total_backward_count 1106270 real_backward_count 71067   6.424%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.232584/  1.017375, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.9268%\n",
      "layer   3  Sparsity: 67.5571%\n",
      "total_backward_count 1116060 real_backward_count 71304   6.389%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.227813/  1.097804, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0374%\n",
      "layer   3  Sparsity: 67.5975%\n",
      "total_backward_count 1125850 real_backward_count 71528   6.353%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.227554/  1.072716, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2172%\n",
      "layer   3  Sparsity: 67.8121%\n",
      "total_backward_count 1135640 real_backward_count 71740   6.317%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.229801/  1.131364, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2753%\n",
      "layer   3  Sparsity: 67.5174%\n",
      "total_backward_count 1145430 real_backward_count 71940   6.281%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.223345/  0.996479, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0804%\n",
      "layer   3  Sparsity: 67.5819%\n",
      "total_backward_count 1155220 real_backward_count 72132   6.244%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.224548/  1.055506, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0951%\n",
      "layer   3  Sparsity: 68.2626%\n",
      "total_backward_count 1165010 real_backward_count 72323   6.208%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.222009/  1.065786, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1786%\n",
      "layer   3  Sparsity: 67.8314%\n",
      "total_backward_count 1174800 real_backward_count 72519   6.173%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.226629/  1.011782, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1693%\n",
      "layer   3  Sparsity: 67.3550%\n",
      "total_backward_count 1184590 real_backward_count 72705   6.138%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.221703/  1.090482, val:  69.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2870%\n",
      "layer   3  Sparsity: 67.2221%\n",
      "total_backward_count 1194380 real_backward_count 72896   6.103%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.208559/  1.095812, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5484%\n",
      "layer   3  Sparsity: 67.1210%\n",
      "total_backward_count 1204170 real_backward_count 73069   6.068%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.214721/  1.065309, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1457%\n",
      "layer   3  Sparsity: 67.2904%\n",
      "total_backward_count 1213960 real_backward_count 73248   6.034%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.208154/  1.066027, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1938%\n",
      "layer   3  Sparsity: 67.9610%\n",
      "total_backward_count 1223750 real_backward_count 73406   5.998%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.205843/  1.059249, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1276%\n",
      "layer   3  Sparsity: 68.5957%\n",
      "total_backward_count 1233540 real_backward_count 73574   5.964%\n",
      "fc layer 3 self.abs_max_out: 1945.0\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.196030/  1.087015, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4566%\n",
      "layer   3  Sparsity: 68.6978%\n",
      "total_backward_count 1243330 real_backward_count 73700   5.928%\n",
      "fc layer 3 self.abs_max_out: 1957.0\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.195396/  1.149920, val:  67.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6543%\n",
      "layer   3  Sparsity: 68.4690%\n",
      "total_backward_count 1253120 real_backward_count 73838   5.892%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.202584/  1.108964, val:  69.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2957%\n",
      "layer   3  Sparsity: 67.8999%\n",
      "total_backward_count 1262910 real_backward_count 74008   5.860%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.199263/  1.163729, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3652%\n",
      "layer   3  Sparsity: 66.9265%\n",
      "total_backward_count 1272700 real_backward_count 74166   5.827%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.206000/  1.088022, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4621%\n",
      "layer   3  Sparsity: 67.1594%\n",
      "total_backward_count 1282490 real_backward_count 74350   5.797%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.208538/  1.091660, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5534%\n",
      "layer   3  Sparsity: 67.7588%\n",
      "total_backward_count 1292280 real_backward_count 74507   5.766%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.199353/  1.053140, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3876%\n",
      "layer   3  Sparsity: 68.3742%\n",
      "total_backward_count 1302070 real_backward_count 74628   5.731%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.194687/  1.070131, val:  70.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4846%\n",
      "layer   3  Sparsity: 68.2308%\n",
      "total_backward_count 1311860 real_backward_count 74764   5.699%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.187989/  1.097790, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6248%\n",
      "layer   3  Sparsity: 68.0077%\n",
      "total_backward_count 1321650 real_backward_count 74890   5.666%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.182855/  1.093185, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3532%\n",
      "layer   3  Sparsity: 68.7407%\n",
      "total_backward_count 1331440 real_backward_count 75016   5.634%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.182929/  1.046427, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3831%\n",
      "layer   3  Sparsity: 68.2904%\n",
      "total_backward_count 1341230 real_backward_count 75144   5.603%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.184834/  1.093065, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5069%\n",
      "layer   3  Sparsity: 67.1619%\n",
      "total_backward_count 1351020 real_backward_count 75293   5.573%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.183524/  1.133635, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4162%\n",
      "layer   3  Sparsity: 67.4413%\n",
      "total_backward_count 1360810 real_backward_count 75421   5.542%\n",
      "fc layer 3 self.abs_max_out: 1961.0\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.187547/  1.142429, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5523%\n",
      "layer   3  Sparsity: 67.1279%\n",
      "total_backward_count 1370600 real_backward_count 75544   5.512%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.177226/  1.069068, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5184%\n",
      "layer   3  Sparsity: 67.4772%\n",
      "total_backward_count 1380390 real_backward_count 75646   5.480%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.171330/  1.046580, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6300%\n",
      "layer   3  Sparsity: 67.6532%\n",
      "total_backward_count 1390180 real_backward_count 75753   5.449%\n",
      "fc layer 3 self.abs_max_out: 1994.0\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.184170/  1.111563, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2416%\n",
      "layer   3  Sparsity: 67.4688%\n",
      "total_backward_count 1399970 real_backward_count 75879   5.420%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.170688/  1.065036, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4011%\n",
      "layer   3  Sparsity: 67.1474%\n",
      "total_backward_count 1409760 real_backward_count 76003   5.391%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.173855/  1.076990, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2200%\n",
      "layer   3  Sparsity: 67.1999%\n",
      "total_backward_count 1419550 real_backward_count 76132   5.363%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.166044/  1.141453, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5158%\n",
      "layer   3  Sparsity: 67.3249%\n",
      "total_backward_count 1429340 real_backward_count 76260   5.335%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.167036/  1.146121, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2213%\n",
      "layer   3  Sparsity: 67.8019%\n",
      "total_backward_count 1439130 real_backward_count 76377   5.307%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.159207/  1.120192, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2098%\n",
      "layer   3  Sparsity: 68.0594%\n",
      "total_backward_count 1448920 real_backward_count 76498   5.280%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.165060/  1.130175, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2511%\n",
      "layer   3  Sparsity: 68.0050%\n",
      "total_backward_count 1458710 real_backward_count 76605   5.252%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.159001/  1.109346, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4536%\n",
      "layer   3  Sparsity: 68.1114%\n",
      "total_backward_count 1468500 real_backward_count 76709   5.224%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.163401/  1.077546, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.0716%\n",
      "layer   3  Sparsity: 67.9364%\n",
      "total_backward_count 1478290 real_backward_count 76804   5.195%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.156445/  1.123785, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1953%\n",
      "layer   3  Sparsity: 67.3164%\n",
      "total_backward_count 1488080 real_backward_count 76900   5.168%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.144633/  1.108139, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3309%\n",
      "layer   3  Sparsity: 67.9712%\n",
      "total_backward_count 1497870 real_backward_count 76975   5.139%\n",
      "fc layer 3 self.abs_max_out: 2030.0\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.149972/  1.102936, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3769%\n",
      "layer   3  Sparsity: 67.5680%\n",
      "total_backward_count 1507660 real_backward_count 77072   5.112%\n",
      "fc layer 3 self.abs_max_out: 2055.0\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.144041/  1.130855, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5415%\n",
      "layer   3  Sparsity: 68.0456%\n",
      "total_backward_count 1517450 real_backward_count 77167   5.085%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.152870/  1.087077, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5317%\n",
      "layer   3  Sparsity: 68.1437%\n",
      "total_backward_count 1527240 real_backward_count 77257   5.059%\n",
      "fc layer 3 self.abs_max_out: 2061.0\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.152182/  1.141047, val:  70.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6818%\n",
      "layer   3  Sparsity: 68.0193%\n",
      "total_backward_count 1537030 real_backward_count 77351   5.032%\n",
      "fc layer 3 self.abs_max_out: 2136.0\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.160449/  1.134035, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7146%\n",
      "layer   3  Sparsity: 67.7608%\n",
      "total_backward_count 1546820 real_backward_count 77458   5.008%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.159837/  1.132871, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5899%\n",
      "layer   3  Sparsity: 67.7831%\n",
      "total_backward_count 1556610 real_backward_count 77563   4.983%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.157359/  1.102473, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6696%\n",
      "layer   3  Sparsity: 67.3897%\n",
      "total_backward_count 1566400 real_backward_count 77640   4.957%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.156361/  1.144871, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6742%\n",
      "layer   3  Sparsity: 67.2558%\n",
      "total_backward_count 1576190 real_backward_count 77724   4.931%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.159350/  1.094779, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2462%\n",
      "layer   3  Sparsity: 66.8228%\n",
      "total_backward_count 1585980 real_backward_count 77817   4.907%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.152569/  1.113559, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1434%\n",
      "layer   3  Sparsity: 67.4783%\n",
      "total_backward_count 1595770 real_backward_count 77909   4.882%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.156098/  1.103674, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3091%\n",
      "layer   3  Sparsity: 67.7016%\n",
      "total_backward_count 1605560 real_backward_count 77983   4.857%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.160454/  1.127309, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2652%\n",
      "layer   3  Sparsity: 67.8817%\n",
      "total_backward_count 1615350 real_backward_count 78078   4.834%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.153901/  1.106981, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.8326%\n",
      "layer   3  Sparsity: 68.7421%\n",
      "total_backward_count 1625140 real_backward_count 78146   4.809%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.153591/  1.101671, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 70.8896%\n",
      "layer   3  Sparsity: 68.2201%\n",
      "total_backward_count 1634930 real_backward_count 78232   4.785%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.156753/  1.077038, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3194%\n",
      "layer   3  Sparsity: 68.0422%\n",
      "total_backward_count 1644720 real_backward_count 78309   4.761%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.153799/  1.058060, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4076%\n",
      "layer   3  Sparsity: 67.7305%\n",
      "total_backward_count 1654510 real_backward_count 78383   4.738%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.158158/  1.070939, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2884%\n",
      "layer   3  Sparsity: 67.5621%\n",
      "total_backward_count 1664300 real_backward_count 78450   4.714%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.158320/  1.123950, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3122%\n",
      "layer   3  Sparsity: 67.5315%\n",
      "total_backward_count 1674090 real_backward_count 78551   4.692%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.155138/  1.124278, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2100%\n",
      "layer   3  Sparsity: 67.1607%\n",
      "total_backward_count 1683880 real_backward_count 78615   4.669%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.159702/  1.047159, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1730%\n",
      "layer   3  Sparsity: 67.1192%\n",
      "total_backward_count 1693670 real_backward_count 78705   4.647%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.155152/  1.094706, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2776%\n",
      "layer   3  Sparsity: 67.2704%\n",
      "total_backward_count 1703460 real_backward_count 78775   4.624%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.153917/  1.121215, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.1740%\n",
      "layer   3  Sparsity: 67.5474%\n",
      "total_backward_count 1713250 real_backward_count 78864   4.603%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.147044/  1.148049, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3698%\n",
      "layer   3  Sparsity: 67.6126%\n",
      "total_backward_count 1723040 real_backward_count 78931   4.581%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.149738/  1.099453, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5379%\n",
      "layer   3  Sparsity: 67.7227%\n",
      "total_backward_count 1732830 real_backward_count 79017   4.560%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.147853/  1.105660, val:  75.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2653%\n",
      "layer   3  Sparsity: 68.0713%\n",
      "total_backward_count 1742620 real_backward_count 79093   4.539%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.153512/  1.109874, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3507%\n",
      "layer   3  Sparsity: 67.5837%\n",
      "total_backward_count 1752410 real_backward_count 79176   4.518%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.155518/  1.125855, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3896%\n",
      "layer   3  Sparsity: 66.8407%\n",
      "total_backward_count 1762200 real_backward_count 79233   4.496%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.147690/  1.134045, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4527%\n",
      "layer   3  Sparsity: 67.0528%\n",
      "total_backward_count 1771990 real_backward_count 79311   4.476%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.139153/  1.132959, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5224%\n",
      "layer   3  Sparsity: 67.2626%\n",
      "total_backward_count 1781780 real_backward_count 79364   4.454%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.142246/  1.121355, val:  71.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.2650%\n",
      "layer   3  Sparsity: 67.2990%\n",
      "total_backward_count 1791570 real_backward_count 79392   4.431%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.144109/  1.233201, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7500%\n",
      "layer   3  Sparsity: 67.4202%\n",
      "total_backward_count 1801360 real_backward_count 79469   4.412%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.148779/  1.156128, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7507%\n",
      "layer   3  Sparsity: 67.0783%\n",
      "total_backward_count 1811150 real_backward_count 79516   4.390%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.144441/  1.133134, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7296%\n",
      "layer   3  Sparsity: 67.9751%\n",
      "total_backward_count 1820940 real_backward_count 79567   4.370%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.150100/  1.132437, val:  71.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5091%\n",
      "layer   3  Sparsity: 67.2517%\n",
      "total_backward_count 1830730 real_backward_count 79657   4.351%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.141211/  1.123112, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.3856%\n",
      "layer   3  Sparsity: 67.1041%\n",
      "total_backward_count 1840520 real_backward_count 79720   4.331%\n",
      "fc layer 3 self.abs_max_out: 2138.0\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.141023/  1.127535, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5095%\n",
      "layer   3  Sparsity: 66.7888%\n",
      "total_backward_count 1850310 real_backward_count 79777   4.312%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.139617/  1.125636, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6081%\n",
      "layer   3  Sparsity: 66.4216%\n",
      "total_backward_count 1860100 real_backward_count 79831   4.292%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.143173/  1.107950, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.4894%\n",
      "layer   3  Sparsity: 66.5481%\n",
      "total_backward_count 1869890 real_backward_count 79886   4.272%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.142324/  1.167332, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.5856%\n",
      "layer   3  Sparsity: 67.2245%\n",
      "total_backward_count 1879680 real_backward_count 79967   4.254%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.128768/  1.139250, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.8233%\n",
      "layer   3  Sparsity: 67.5462%\n",
      "total_backward_count 1889470 real_backward_count 80005   4.234%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.127380/  1.143028, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.9427%\n",
      "layer   3  Sparsity: 67.6624%\n",
      "total_backward_count 1899260 real_backward_count 80040   4.214%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.129077/  1.160852, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7441%\n",
      "layer   3  Sparsity: 67.4953%\n",
      "total_backward_count 1909050 real_backward_count 80081   4.195%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.130242/  1.153548, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.7770%\n",
      "layer   3  Sparsity: 67.8934%\n",
      "total_backward_count 1918840 real_backward_count 80129   4.176%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.129664/  1.121677, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6749%\n",
      "layer   3  Sparsity: 67.7892%\n",
      "total_backward_count 1928630 real_backward_count 80174   4.157%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.136935/  1.108640, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6809%\n",
      "layer   3  Sparsity: 67.1726%\n",
      "total_backward_count 1938420 real_backward_count 80235   4.139%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.133768/  1.103793, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.8419%\n",
      "layer   3  Sparsity: 67.4210%\n",
      "total_backward_count 1948210 real_backward_count 80287   4.121%\n",
      "fc layer 3 self.abs_max_out: 2143.0\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.131646/  1.131549, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.7787%\n",
      "layer   2  Sparsity: 71.6906%\n",
      "layer   3  Sparsity: 66.8409%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4109b4187845088bec3294d71c0362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÇ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.13165</td></tr><tr><td>val_acc_best</td><td>0.78333</td></tr><tr><td>val_acc_now</td><td>0.72917</td></tr><tr><td>val_loss</td><td>1.13155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-67</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xk5a6vo' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xk5a6vo</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_092437-8xk5a6vo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 512y20wp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_134332-512y20wp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/512y20wp' target=\"_blank\">soft-sweep-73</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/512y20wp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/512y20wp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251115_134341_056', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "lif layer 1 self.abs_max_v: 348.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 450.0\n",
      "lif layer 2 self.abs_max_v: 450.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "lif layer 1 self.abs_max_v: 355.0\n",
      "fc layer 2 self.abs_max_out: 689.0\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "lif layer 1 self.abs_max_v: 361.5\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "lif layer 2 self.abs_max_v: 1006.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "lif layer 1 self.abs_max_v: 387.0\n",
      "lif layer 1 self.abs_max_v: 449.5\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 1 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 498.0\n",
      "fc layer 2 self.abs_max_out: 890.0\n",
      "lif layer 2 self.abs_max_v: 1184.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 535.0\n",
      "lif layer 2 self.abs_max_v: 1258.5\n",
      "fc layer 1 self.abs_max_out: 569.0\n",
      "lif layer 1 self.abs_max_v: 569.0\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "lif layer 1 self.abs_max_v: 711.5\n",
      "lif layer 1 self.abs_max_v: 836.5\n",
      "fc layer 1 self.abs_max_out: 599.0\n",
      "fc layer 2 self.abs_max_out: 1055.0\n",
      "lif layer 2 self.abs_max_v: 1425.0\n",
      "fc layer 1 self.abs_max_out: 837.0\n",
      "lif layer 1 self.abs_max_v: 915.5\n",
      "fc layer 1 self.abs_max_out: 840.0\n",
      "lif layer 1 self.abs_max_v: 996.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "lif layer 1 self.abs_max_v: 1014.5\n",
      "lif layer 2 self.abs_max_v: 1534.0\n",
      "fc layer 2 self.abs_max_out: 1059.0\n",
      "fc layer 1 self.abs_max_out: 845.0\n",
      "fc layer 1 self.abs_max_out: 918.0\n",
      "lif layer 2 self.abs_max_v: 1615.0\n",
      "lif layer 2 self.abs_max_v: 1703.5\n",
      "lif layer 2 self.abs_max_v: 1898.5\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "lif layer 1 self.abs_max_v: 1076.0\n",
      "fc layer 2 self.abs_max_out: 1149.0\n",
      "fc layer 1 self.abs_max_out: 947.0\n",
      "lif layer 1 self.abs_max_v: 1181.0\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 2 self.abs_max_out: 1191.0\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "lif layer 2 self.abs_max_v: 1977.5\n",
      "lif layer 2 self.abs_max_v: 2075.0\n",
      "lif layer 1 self.abs_max_v: 1216.0\n",
      "fc layer 2 self.abs_max_out: 1196.0\n",
      "fc layer 1 self.abs_max_out: 1058.0\n",
      "fc layer 2 self.abs_max_out: 1268.0\n",
      "fc layer 1 self.abs_max_out: 1250.0\n",
      "lif layer 1 self.abs_max_v: 1280.0\n",
      "fc layer 1 self.abs_max_out: 1299.0\n",
      "lif layer 1 self.abs_max_v: 1413.0\n",
      "lif layer 1 self.abs_max_v: 1440.5\n",
      "lif layer 2 self.abs_max_v: 2082.0\n",
      "fc layer 1 self.abs_max_out: 1344.0\n",
      "fc layer 1 self.abs_max_out: 1377.0\n",
      "lif layer 1 self.abs_max_v: 1919.0\n",
      "lif layer 2 self.abs_max_v: 2179.5\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "lif layer 2 self.abs_max_v: 2520.0\n",
      "fc layer 1 self.abs_max_out: 1422.0\n",
      "lif layer 1 self.abs_max_v: 2282.0\n",
      "fc layer 1 self.abs_max_out: 1424.0\n",
      "fc layer 2 self.abs_max_out: 1539.0\n",
      "fc layer 1 self.abs_max_out: 1464.0\n",
      "fc layer 1 self.abs_max_out: 1770.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "lif layer 2 self.abs_max_v: 2641.5\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "lif layer 2 self.abs_max_v: 2673.5\n",
      "fc layer 2 self.abs_max_out: 1703.0\n",
      "lif layer 2 self.abs_max_v: 2821.5\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 3 self.abs_max_out: 753.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "lif layer 1 self.abs_max_v: 2378.0\n",
      "fc layer 2 self.abs_max_out: 1838.0\n",
      "lif layer 2 self.abs_max_v: 2929.5\n",
      "fc layer 3 self.abs_max_out: 765.0\n",
      "fc layer 2 self.abs_max_out: 1899.0\n",
      "fc layer 1 self.abs_max_out: 1901.0\n",
      "fc layer 2 self.abs_max_out: 1931.0\n",
      "fc layer 1 self.abs_max_out: 2045.0\n",
      "lif layer 1 self.abs_max_v: 2515.0\n",
      "fc layer 3 self.abs_max_out: 782.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 795.0\n",
      "lif layer 1 self.abs_max_v: 2703.0\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "fc layer 2 self.abs_max_out: 1969.0\n",
      "fc layer 2 self.abs_max_out: 1970.0\n",
      "fc layer 2 self.abs_max_out: 2018.0\n",
      "fc layer 2 self.abs_max_out: 2051.0\n",
      "fc layer 1 self.abs_max_out: 2073.0\n",
      "lif layer 1 self.abs_max_v: 2842.5\n",
      "fc layer 1 self.abs_max_out: 2270.0\n",
      "lif layer 1 self.abs_max_v: 3001.5\n",
      "lif layer 2 self.abs_max_v: 2958.0\n",
      "lif layer 2 self.abs_max_v: 2969.5\n",
      "lif layer 2 self.abs_max_v: 3053.0\n",
      "lif layer 2 self.abs_max_v: 3097.0\n",
      "lif layer 2 self.abs_max_v: 3246.5\n",
      "lif layer 2 self.abs_max_v: 3257.5\n",
      "fc layer 2 self.abs_max_out: 2100.0\n",
      "fc layer 2 self.abs_max_out: 2150.0\n",
      "lif layer 1 self.abs_max_v: 3345.0\n",
      "lif layer 1 self.abs_max_v: 3493.5\n",
      "lif layer 2 self.abs_max_v: 3272.0\n",
      "fc layer 2 self.abs_max_out: 2187.0\n",
      "fc layer 2 self.abs_max_out: 2227.0\n",
      "fc layer 1 self.abs_max_out: 2638.0\n",
      "fc layer 2 self.abs_max_out: 2265.0\n",
      "lif layer 2 self.abs_max_v: 3274.5\n",
      "lif layer 2 self.abs_max_v: 3558.5\n",
      "lif layer 2 self.abs_max_v: 3693.5\n",
      "lif layer 2 self.abs_max_v: 3700.0\n",
      "fc layer 3 self.abs_max_out: 861.0\n",
      "fc layer 2 self.abs_max_out: 2280.0\n",
      "lif layer 2 self.abs_max_v: 3855.0\n",
      "fc layer 2 self.abs_max_out: 2295.0\n",
      "fc layer 2 self.abs_max_out: 2297.0\n",
      "fc layer 2 self.abs_max_out: 2348.0\n",
      "fc layer 2 self.abs_max_out: 2358.0\n",
      "lif layer 1 self.abs_max_v: 3695.5\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "lif layer 2 self.abs_max_v: 3857.0\n",
      "fc layer 2 self.abs_max_out: 2383.0\n",
      "lif layer 2 self.abs_max_v: 4170.5\n",
      "lif layer 1 self.abs_max_v: 3824.5\n",
      "lif layer 1 self.abs_max_v: 3968.0\n",
      "lif layer 1 self.abs_max_v: 4304.0\n",
      "lif layer 1 self.abs_max_v: 4708.0\n",
      "fc layer 2 self.abs_max_out: 2400.0\n",
      "fc layer 2 self.abs_max_out: 2508.0\n",
      "fc layer 2 self.abs_max_out: 2571.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.709481/  1.965192, val:  32.08%, val_best:  32.08%, tr:  98.57%, tr_best:  98.57%, epoch time: 81.29 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 94.1459%\n",
      "layer   2  Sparsity: 73.5517%\n",
      "layer   3  Sparsity: 66.8238%\n",
      "total_backward_count 9790 real_backward_count 1880  19.203%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "fc layer 3 self.abs_max_out: 965.0\n",
      "fc layer 2 self.abs_max_out: 2738.0\n",
      "fc layer 1 self.abs_max_out: 3350.0\n",
      "fc layer 2 self.abs_max_out: 2888.0\n",
      "fc layer 2 self.abs_max_out: 2957.0\n",
      "lif layer 1 self.abs_max_v: 5320.5\n",
      "lif layer 1 self.abs_max_v: 5714.5\n",
      "lif layer 1 self.abs_max_v: 6040.5\n",
      "fc layer 1 self.abs_max_out: 3363.0\n",
      "lif layer 1 self.abs_max_v: 6178.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.631791/  1.920668, val:  39.58%, val_best:  39.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 94.1546%\n",
      "layer   2  Sparsity: 74.3559%\n",
      "layer   3  Sparsity: 65.5789%\n",
      "total_backward_count 19580 real_backward_count 3387  17.298%\n",
      "fc layer 2 self.abs_max_out: 3056.0\n",
      "fc layer 2 self.abs_max_out: 3072.0\n",
      "fc layer 2 self.abs_max_out: 3163.0\n",
      "lif layer 2 self.abs_max_v: 4189.0\n",
      "lif layer 2 self.abs_max_v: 4212.5\n",
      "lif layer 2 self.abs_max_v: 4259.5\n",
      "lif layer 2 self.abs_max_v: 4399.5\n",
      "lif layer 2 self.abs_max_v: 4422.5\n",
      "fc layer 3 self.abs_max_out: 981.0\n",
      "fc layer 1 self.abs_max_out: 3377.0\n",
      "fc layer 1 self.abs_max_out: 3571.0\n",
      "lif layer 1 self.abs_max_v: 6372.5\n",
      "lif layer 1 self.abs_max_v: 6487.0\n",
      "lif layer 1 self.abs_max_v: 6730.0\n",
      "fc layer 1 self.abs_max_out: 3614.0\n",
      "fc layer 1 self.abs_max_out: 3986.0\n",
      "lif layer 1 self.abs_max_v: 6819.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.594102/  1.886431, val:  36.67%, val_best:  39.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1670%\n",
      "layer   2  Sparsity: 74.6514%\n",
      "layer   3  Sparsity: 63.8823%\n",
      "total_backward_count 29370 real_backward_count 4747  16.163%\n",
      "fc layer 3 self.abs_max_out: 1017.0\n",
      "lif layer 2 self.abs_max_v: 4504.0\n",
      "lif layer 2 self.abs_max_v: 4620.0\n",
      "lif layer 2 self.abs_max_v: 4845.0\n",
      "lif layer 2 self.abs_max_v: 4848.5\n",
      "lif layer 2 self.abs_max_v: 4916.5\n",
      "lif layer 2 self.abs_max_v: 5142.0\n",
      "lif layer 1 self.abs_max_v: 6866.5\n",
      "fc layer 1 self.abs_max_out: 4009.0\n",
      "lif layer 1 self.abs_max_v: 7190.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.539983/  1.875066, val:  47.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1543%\n",
      "layer   2  Sparsity: 74.8781%\n",
      "layer   3  Sparsity: 62.8221%\n",
      "total_backward_count 39160 real_backward_count 6041  15.426%\n",
      "fc layer 3 self.abs_max_out: 1083.0\n",
      "fc layer 2 self.abs_max_out: 3189.0\n",
      "fc layer 2 self.abs_max_out: 3213.0\n",
      "fc layer 2 self.abs_max_out: 3222.0\n",
      "lif layer 2 self.abs_max_v: 5193.0\n",
      "fc layer 3 self.abs_max_out: 1154.0\n",
      "fc layer 3 self.abs_max_out: 1160.0\n",
      "fc layer 3 self.abs_max_out: 1218.0\n",
      "lif layer 2 self.abs_max_v: 5515.0\n",
      "lif layer 1 self.abs_max_v: 7218.5\n",
      "lif layer 1 self.abs_max_v: 7323.5\n",
      "lif layer 2 self.abs_max_v: 5589.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.515368/  1.768056, val:  57.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1383%\n",
      "layer   2  Sparsity: 73.5267%\n",
      "layer   3  Sparsity: 62.4489%\n",
      "total_backward_count 48950 real_backward_count 7281  14.874%\n",
      "lif layer 2 self.abs_max_v: 5674.0\n",
      "fc layer 1 self.abs_max_out: 4060.0\n",
      "lif layer 1 self.abs_max_v: 7330.5\n",
      "fc layer 1 self.abs_max_out: 4660.0\n",
      "lif layer 1 self.abs_max_v: 8299.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.464493/  1.797583, val:  50.00%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1483%\n",
      "layer   2  Sparsity: 73.3858%\n",
      "layer   3  Sparsity: 62.2842%\n",
      "total_backward_count 58740 real_backward_count 8529  14.520%\n",
      "fc layer 2 self.abs_max_out: 3411.0\n",
      "fc layer 3 self.abs_max_out: 1233.0\n",
      "fc layer 2 self.abs_max_out: 3412.0\n",
      "fc layer 2 self.abs_max_out: 3742.0\n",
      "lif layer 2 self.abs_max_v: 5831.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.455259/  1.780789, val:  55.00%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1570%\n",
      "layer   2  Sparsity: 73.4393%\n",
      "layer   3  Sparsity: 62.1249%\n",
      "total_backward_count 68530 real_backward_count 9748  14.224%\n",
      "fc layer 2 self.abs_max_out: 3747.0\n",
      "fc layer 2 self.abs_max_out: 3793.0\n",
      "fc layer 2 self.abs_max_out: 3922.0\n",
      "lif layer 2 self.abs_max_v: 6212.0\n",
      "lif layer 2 self.abs_max_v: 6245.5\n",
      "fc layer 1 self.abs_max_out: 4801.0\n",
      "lif layer 1 self.abs_max_v: 8831.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.430461/  1.755330, val:  49.58%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1528%\n",
      "layer   2  Sparsity: 72.6041%\n",
      "layer   3  Sparsity: 62.0643%\n",
      "total_backward_count 78320 real_backward_count 10931  13.957%\n",
      "fc layer 2 self.abs_max_out: 4029.0\n",
      "fc layer 1 self.abs_max_out: 5509.0\n",
      "lif layer 1 self.abs_max_v: 9713.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.432352/  1.740138, val:  55.83%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1518%\n",
      "layer   2  Sparsity: 72.5972%\n",
      "layer   3  Sparsity: 62.7304%\n",
      "total_backward_count 88110 real_backward_count 12162  13.803%\n",
      "lif layer 2 self.abs_max_v: 6287.0\n",
      "fc layer 2 self.abs_max_out: 4071.0\n",
      "fc layer 2 self.abs_max_out: 4092.0\n",
      "fc layer 2 self.abs_max_out: 4146.0\n",
      "lif layer 1 self.abs_max_v: 9796.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.423409/  1.741547, val:  45.42%, val_best:  57.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1444%\n",
      "layer   2  Sparsity: 72.0516%\n",
      "layer   3  Sparsity: 63.2803%\n",
      "total_backward_count 97900 real_backward_count 13312  13.598%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.393100/  1.718953, val:  55.00%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1367%\n",
      "layer   2  Sparsity: 71.8975%\n",
      "layer   3  Sparsity: 63.1121%\n",
      "total_backward_count 107690 real_backward_count 14427  13.397%\n",
      "lif layer 1 self.abs_max_v: 9875.0\n",
      "fc layer 1 self.abs_max_out: 5840.0\n",
      "lif layer 1 self.abs_max_v: 10777.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.383163/  1.667032, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1299%\n",
      "layer   2  Sparsity: 72.0509%\n",
      "layer   3  Sparsity: 62.5667%\n",
      "total_backward_count 117480 real_backward_count 15562  13.247%\n",
      "lif layer 2 self.abs_max_v: 6300.0\n",
      "fc layer 3 self.abs_max_out: 1261.0\n",
      "fc layer 1 self.abs_max_out: 5881.0\n",
      "lif layer 1 self.abs_max_v: 11217.5\n",
      "fc layer 1 self.abs_max_out: 6546.0\n",
      "lif layer 1 self.abs_max_v: 12155.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.354802/  1.671925, val:  51.25%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1443%\n",
      "layer   2  Sparsity: 71.5400%\n",
      "layer   3  Sparsity: 62.6823%\n",
      "total_backward_count 127270 real_backward_count 16599  13.042%\n",
      "fc layer 3 self.abs_max_out: 1361.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.344764/  1.667241, val:  51.67%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1187%\n",
      "layer   2  Sparsity: 71.6033%\n",
      "layer   3  Sparsity: 62.3747%\n",
      "total_backward_count 137060 real_backward_count 17670  12.892%\n",
      "fc layer 3 self.abs_max_out: 1382.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.316939/  1.636485, val:  55.00%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1255%\n",
      "layer   2  Sparsity: 71.7827%\n",
      "layer   3  Sparsity: 62.9005%\n",
      "total_backward_count 146850 real_backward_count 18665  12.710%\n",
      "lif layer 2 self.abs_max_v: 6535.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.317573/  1.649913, val:  52.50%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1265%\n",
      "layer   2  Sparsity: 71.2692%\n",
      "layer   3  Sparsity: 64.8066%\n",
      "total_backward_count 156640 real_backward_count 19675  12.561%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.318509/  1.620092, val:  63.33%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1383%\n",
      "layer   2  Sparsity: 71.2844%\n",
      "layer   3  Sparsity: 65.3532%\n",
      "total_backward_count 166430 real_backward_count 20617  12.388%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.305274/  1.575605, val:  85.00%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1540%\n",
      "layer   2  Sparsity: 71.3190%\n",
      "layer   3  Sparsity: 64.2893%\n",
      "total_backward_count 176220 real_backward_count 21545  12.226%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.307828/  1.604029, val:  59.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1424%\n",
      "layer   2  Sparsity: 71.1029%\n",
      "layer   3  Sparsity: 63.5908%\n",
      "total_backward_count 186010 real_backward_count 22477  12.084%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.267814/  1.629518, val:  45.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 70.6356%\n",
      "layer   3  Sparsity: 64.0064%\n",
      "total_backward_count 195800 real_backward_count 23394  11.948%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.262129/  1.587953, val:  62.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1615%\n",
      "layer   2  Sparsity: 70.7317%\n",
      "layer   3  Sparsity: 64.1122%\n",
      "total_backward_count 205590 real_backward_count 24271  11.806%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.276654/  1.547171, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1483%\n",
      "layer   2  Sparsity: 70.6204%\n",
      "layer   3  Sparsity: 64.4680%\n",
      "total_backward_count 215380 real_backward_count 25154  11.679%\n",
      "fc layer 3 self.abs_max_out: 1433.0\n",
      "fc layer 3 self.abs_max_out: 1473.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.250845/  1.522931, val:  74.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1396%\n",
      "layer   2  Sparsity: 70.7153%\n",
      "layer   3  Sparsity: 65.0560%\n",
      "total_backward_count 225170 real_backward_count 26017  11.554%\n",
      "fc layer 2 self.abs_max_out: 4194.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.247354/  1.500489, val:  70.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1523%\n",
      "layer   2  Sparsity: 70.7936%\n",
      "layer   3  Sparsity: 64.8447%\n",
      "total_backward_count 234960 real_backward_count 26874  11.438%\n",
      "lif layer 2 self.abs_max_v: 6537.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.239441/  1.517452, val:  72.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1539%\n",
      "layer   2  Sparsity: 70.7928%\n",
      "layer   3  Sparsity: 65.8840%\n",
      "total_backward_count 244750 real_backward_count 27704  11.319%\n",
      "fc layer 2 self.abs_max_out: 4228.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.248731/  1.500134, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1350%\n",
      "layer   2  Sparsity: 70.4667%\n",
      "layer   3  Sparsity: 65.0470%\n",
      "total_backward_count 254540 real_backward_count 28530  11.208%\n",
      "fc layer 1 self.abs_max_out: 6627.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.231021/  1.477510, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1489%\n",
      "layer   2  Sparsity: 70.4604%\n",
      "layer   3  Sparsity: 65.4322%\n",
      "total_backward_count 264330 real_backward_count 29349  11.103%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.224225/  1.485134, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1414%\n",
      "layer   2  Sparsity: 69.7850%\n",
      "layer   3  Sparsity: 64.8823%\n",
      "total_backward_count 274120 real_backward_count 30127  10.990%\n",
      "lif layer 2 self.abs_max_v: 6577.5\n",
      "lif layer 2 self.abs_max_v: 6793.0\n",
      "lif layer 2 self.abs_max_v: 7014.5\n",
      "fc layer 3 self.abs_max_out: 1492.0\n",
      "fc layer 1 self.abs_max_out: 6949.0\n",
      "lif layer 1 self.abs_max_v: 12813.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.219942/  1.520941, val:  57.50%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1453%\n",
      "layer   2  Sparsity: 69.9427%\n",
      "layer   3  Sparsity: 65.4015%\n",
      "total_backward_count 283910 real_backward_count 30882  10.877%\n",
      "fc layer 3 self.abs_max_out: 1498.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.189145/  1.475885, val:  76.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1285%\n",
      "layer   2  Sparsity: 69.9089%\n",
      "layer   3  Sparsity: 64.6512%\n",
      "total_backward_count 293700 real_backward_count 31642  10.774%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.195217/  1.485541, val:  79.17%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1313%\n",
      "layer   2  Sparsity: 69.7287%\n",
      "layer   3  Sparsity: 65.0188%\n",
      "total_backward_count 303490 real_backward_count 32315  10.648%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.201679/  1.478017, val:  66.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1351%\n",
      "layer   2  Sparsity: 69.7739%\n",
      "layer   3  Sparsity: 64.8845%\n",
      "total_backward_count 313280 real_backward_count 33095  10.564%\n",
      "fc layer 1 self.abs_max_out: 7247.0\n",
      "lif layer 1 self.abs_max_v: 13435.0\n",
      "fc layer 3 self.abs_max_out: 1521.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.178753/  1.464765, val:  70.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1375%\n",
      "layer   2  Sparsity: 69.9275%\n",
      "layer   3  Sparsity: 65.7722%\n",
      "total_backward_count 323070 real_backward_count 33775  10.454%\n",
      "lif layer 2 self.abs_max_v: 7104.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.196720/  1.486637, val:  68.33%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1465%\n",
      "layer   2  Sparsity: 69.9132%\n",
      "layer   3  Sparsity: 65.9913%\n",
      "total_backward_count 332860 real_backward_count 34501  10.365%\n",
      "fc layer 2 self.abs_max_out: 4336.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.176278/  1.448730, val:  70.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1241%\n",
      "layer   2  Sparsity: 69.9459%\n",
      "layer   3  Sparsity: 65.4581%\n",
      "total_backward_count 342650 real_backward_count 35184  10.268%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.159445/  1.407477, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1396%\n",
      "layer   2  Sparsity: 69.9717%\n",
      "layer   3  Sparsity: 66.2257%\n",
      "total_backward_count 352440 real_backward_count 35846  10.171%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.142882/  1.428793, val:  77.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1421%\n",
      "layer   2  Sparsity: 69.7901%\n",
      "layer   3  Sparsity: 65.7947%\n",
      "total_backward_count 362230 real_backward_count 36457  10.065%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.138143/  1.420733, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1491%\n",
      "layer   2  Sparsity: 69.8952%\n",
      "layer   3  Sparsity: 64.9528%\n",
      "total_backward_count 372020 real_backward_count 37100   9.973%\n",
      "fc layer 3 self.abs_max_out: 1532.0\n",
      "fc layer 3 self.abs_max_out: 1534.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.124622/  1.403823, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1671%\n",
      "layer   2  Sparsity: 69.7711%\n",
      "layer   3  Sparsity: 65.6062%\n",
      "total_backward_count 381810 real_backward_count 37749   9.887%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.107136/  1.410676, val:  72.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1501%\n",
      "layer   2  Sparsity: 69.8265%\n",
      "layer   3  Sparsity: 65.7852%\n",
      "total_backward_count 391600 real_backward_count 38380   9.801%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.108984/  1.381416, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1510%\n",
      "layer   2  Sparsity: 69.6183%\n",
      "layer   3  Sparsity: 66.2442%\n",
      "total_backward_count 401390 real_backward_count 39008   9.718%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.098946/  1.388975, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1339%\n",
      "layer   2  Sparsity: 69.7444%\n",
      "layer   3  Sparsity: 65.9323%\n",
      "total_backward_count 411180 real_backward_count 39645   9.642%\n",
      "fc layer 3 self.abs_max_out: 1537.0\n",
      "lif layer 2 self.abs_max_v: 7186.5\n",
      "fc layer 2 self.abs_max_out: 4588.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.112287/  1.376296, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1395%\n",
      "layer   2  Sparsity: 70.1568%\n",
      "layer   3  Sparsity: 65.8677%\n",
      "total_backward_count 420970 real_backward_count 40227   9.556%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.106758/  1.379460, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1599%\n",
      "layer   2  Sparsity: 69.9328%\n",
      "layer   3  Sparsity: 66.0327%\n",
      "total_backward_count 430760 real_backward_count 40816   9.475%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.081840/  1.372572, val:  80.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1571%\n",
      "layer   2  Sparsity: 69.8299%\n",
      "layer   3  Sparsity: 66.3619%\n",
      "total_backward_count 440550 real_backward_count 41410   9.400%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.083631/  1.336357, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1518%\n",
      "layer   2  Sparsity: 69.4495%\n",
      "layer   3  Sparsity: 66.6249%\n",
      "total_backward_count 450340 real_backward_count 42022   9.331%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.091955/  1.336691, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1506%\n",
      "layer   2  Sparsity: 69.6522%\n",
      "layer   3  Sparsity: 66.6694%\n",
      "total_backward_count 460130 real_backward_count 42619   9.262%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.078062/  1.361223, val:  78.33%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1423%\n",
      "layer   2  Sparsity: 69.5321%\n",
      "layer   3  Sparsity: 66.4763%\n",
      "total_backward_count 469920 real_backward_count 43150   9.182%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.068369/  1.293886, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1530%\n",
      "layer   2  Sparsity: 69.3114%\n",
      "layer   3  Sparsity: 66.8035%\n",
      "total_backward_count 479710 real_backward_count 43695   9.109%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.032226/  1.298057, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1364%\n",
      "layer   2  Sparsity: 69.5805%\n",
      "layer   3  Sparsity: 67.3498%\n",
      "total_backward_count 489500 real_backward_count 44231   9.036%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.071531/  1.359403, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 69.3316%\n",
      "layer   3  Sparsity: 67.4274%\n",
      "total_backward_count 499290 real_backward_count 44805   8.974%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.068338/  1.343740, val:  77.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1263%\n",
      "layer   2  Sparsity: 69.2133%\n",
      "layer   3  Sparsity: 67.7493%\n",
      "total_backward_count 509080 real_backward_count 45344   8.907%\n",
      "fc layer 3 self.abs_max_out: 1566.0\n",
      "fc layer 3 self.abs_max_out: 1571.0\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.049442/  1.304325, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1464%\n",
      "layer   2  Sparsity: 69.3263%\n",
      "layer   3  Sparsity: 67.4122%\n",
      "total_backward_count 518870 real_backward_count 45816   8.830%\n",
      "fc layer 3 self.abs_max_out: 1578.0\n",
      "fc layer 3 self.abs_max_out: 1592.0\n",
      "fc layer 3 self.abs_max_out: 1594.0\n",
      "fc layer 3 self.abs_max_out: 1654.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.061377/  1.321422, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1439%\n",
      "layer   2  Sparsity: 69.3246%\n",
      "layer   3  Sparsity: 68.2505%\n",
      "total_backward_count 528660 real_backward_count 46346   8.767%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.045153/  1.308568, val:  80.42%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1540%\n",
      "layer   2  Sparsity: 69.3498%\n",
      "layer   3  Sparsity: 67.9237%\n",
      "total_backward_count 538450 real_backward_count 46878   8.706%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.043013/  1.320983, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1491%\n",
      "layer   2  Sparsity: 69.3856%\n",
      "layer   3  Sparsity: 67.1104%\n",
      "total_backward_count 548240 real_backward_count 47408   8.647%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.043245/  1.297785, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1494%\n",
      "layer   2  Sparsity: 69.2799%\n",
      "layer   3  Sparsity: 67.1840%\n",
      "total_backward_count 558030 real_backward_count 47916   8.587%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.047477/  1.317154, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1349%\n",
      "layer   2  Sparsity: 69.5588%\n",
      "layer   3  Sparsity: 67.3376%\n",
      "total_backward_count 567820 real_backward_count 48369   8.518%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.036909/  1.299518, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1354%\n",
      "layer   2  Sparsity: 69.4050%\n",
      "layer   3  Sparsity: 67.4587%\n",
      "total_backward_count 577610 real_backward_count 48861   8.459%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.037322/  1.359561, val:  67.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1281%\n",
      "layer   2  Sparsity: 69.3066%\n",
      "layer   3  Sparsity: 67.3854%\n",
      "total_backward_count 587400 real_backward_count 49310   8.395%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.039397/  1.308360, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1564%\n",
      "layer   2  Sparsity: 69.3681%\n",
      "layer   3  Sparsity: 68.2299%\n",
      "total_backward_count 597190 real_backward_count 49752   8.331%\n",
      "fc layer 3 self.abs_max_out: 1674.0\n",
      "fc layer 3 self.abs_max_out: 1702.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.029601/  1.262081, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1373%\n",
      "layer   2  Sparsity: 69.0967%\n",
      "layer   3  Sparsity: 68.2669%\n",
      "total_backward_count 606980 real_backward_count 50225   8.275%\n",
      "fc layer 1 self.abs_max_out: 7355.0\n",
      "lif layer 1 self.abs_max_v: 13670.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.003394/  1.281685, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1314%\n",
      "layer   2  Sparsity: 69.0930%\n",
      "layer   3  Sparsity: 67.2472%\n",
      "total_backward_count 616770 real_backward_count 50704   8.221%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.011698/  1.299830, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1441%\n",
      "layer   2  Sparsity: 69.1400%\n",
      "layer   3  Sparsity: 67.6293%\n",
      "total_backward_count 626560 real_backward_count 51222   8.175%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.024021/  1.289505, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1330%\n",
      "layer   2  Sparsity: 68.9864%\n",
      "layer   3  Sparsity: 67.3154%\n",
      "total_backward_count 636350 real_backward_count 51702   8.125%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.018356/  1.298914, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1358%\n",
      "layer   2  Sparsity: 69.1954%\n",
      "layer   3  Sparsity: 67.5265%\n",
      "total_backward_count 646140 real_backward_count 52193   8.078%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.019354/  1.318112, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1424%\n",
      "layer   2  Sparsity: 69.4272%\n",
      "layer   3  Sparsity: 68.5517%\n",
      "total_backward_count 655930 real_backward_count 52622   8.023%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.022520/  1.270274, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1483%\n",
      "layer   2  Sparsity: 69.2135%\n",
      "layer   3  Sparsity: 68.1712%\n",
      "total_backward_count 665720 real_backward_count 53081   7.973%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.011855/  1.268604, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1296%\n",
      "layer   2  Sparsity: 69.3523%\n",
      "layer   3  Sparsity: 68.5749%\n",
      "total_backward_count 675510 real_backward_count 53504   7.921%\n",
      "lif layer 2 self.abs_max_v: 7196.5\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.010111/  1.309429, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1562%\n",
      "layer   2  Sparsity: 69.3359%\n",
      "layer   3  Sparsity: 67.7314%\n",
      "total_backward_count 685300 real_backward_count 53970   7.875%\n",
      "lif layer 2 self.abs_max_v: 7198.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.011287/  1.289741, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1430%\n",
      "layer   2  Sparsity: 69.1855%\n",
      "layer   3  Sparsity: 68.0271%\n",
      "total_backward_count 695090 real_backward_count 54417   7.829%\n",
      "lif layer 2 self.abs_max_v: 7282.5\n",
      "lif layer 2 self.abs_max_v: 7429.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.013577/  1.353526, val:  73.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1387%\n",
      "layer   2  Sparsity: 69.2494%\n",
      "layer   3  Sparsity: 68.2438%\n",
      "total_backward_count 704880 real_backward_count 54834   7.779%\n",
      "lif layer 2 self.abs_max_v: 7503.5\n",
      "lif layer 2 self.abs_max_v: 7603.5\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.020813/  1.291117, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1447%\n",
      "layer   2  Sparsity: 69.2758%\n",
      "layer   3  Sparsity: 68.6326%\n",
      "total_backward_count 714670 real_backward_count 55260   7.732%\n",
      "lif layer 2 self.abs_max_v: 7701.5\n",
      "lif layer 2 self.abs_max_v: 7839.5\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.998146/  1.320883, val:  73.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1507%\n",
      "layer   2  Sparsity: 69.0485%\n",
      "layer   3  Sparsity: 69.0634%\n",
      "total_backward_count 724460 real_backward_count 55657   7.683%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.985576/  1.278958, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1254%\n",
      "layer   2  Sparsity: 69.1199%\n",
      "layer   3  Sparsity: 68.6149%\n",
      "total_backward_count 734250 real_backward_count 56067   7.636%\n",
      "lif layer 2 self.abs_max_v: 8035.5\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.996125/  1.243811, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1420%\n",
      "layer   2  Sparsity: 69.0641%\n",
      "layer   3  Sparsity: 68.0932%\n",
      "total_backward_count 744040 real_backward_count 56476   7.590%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.005597/  1.303244, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1370%\n",
      "layer   2  Sparsity: 69.0188%\n",
      "layer   3  Sparsity: 69.1772%\n",
      "total_backward_count 753830 real_backward_count 56887   7.546%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.990074/  1.311160, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1549%\n",
      "layer   2  Sparsity: 69.1094%\n",
      "layer   3  Sparsity: 68.8758%\n",
      "total_backward_count 763620 real_backward_count 57280   7.501%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.967992/  1.237369, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1351%\n",
      "layer   2  Sparsity: 69.2609%\n",
      "layer   3  Sparsity: 68.6064%\n",
      "total_backward_count 773410 real_backward_count 57678   7.458%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.972000/  1.256530, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1521%\n",
      "layer   2  Sparsity: 69.3078%\n",
      "layer   3  Sparsity: 67.7821%\n",
      "total_backward_count 783200 real_backward_count 58080   7.416%\n",
      "lif layer 2 self.abs_max_v: 8182.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.982271/  1.231631, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1510%\n",
      "layer   2  Sparsity: 69.2578%\n",
      "layer   3  Sparsity: 68.4484%\n",
      "total_backward_count 792990 real_backward_count 58469   7.373%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.984100/  1.291705, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1502%\n",
      "layer   2  Sparsity: 69.2880%\n",
      "layer   3  Sparsity: 68.8577%\n",
      "total_backward_count 802780 real_backward_count 58836   7.329%\n",
      "lif layer 2 self.abs_max_v: 8203.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.964402/  1.246865, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1397%\n",
      "layer   2  Sparsity: 69.1711%\n",
      "layer   3  Sparsity: 68.8642%\n",
      "total_backward_count 812570 real_backward_count 59201   7.286%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.965421/  1.260063, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1499%\n",
      "layer   2  Sparsity: 69.1451%\n",
      "layer   3  Sparsity: 68.1594%\n",
      "total_backward_count 822360 real_backward_count 59583   7.245%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.971498/  1.238928, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1375%\n",
      "layer   2  Sparsity: 69.2331%\n",
      "layer   3  Sparsity: 68.3128%\n",
      "total_backward_count 832150 real_backward_count 59929   7.202%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.974560/  1.258677, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1608%\n",
      "layer   2  Sparsity: 69.2383%\n",
      "layer   3  Sparsity: 68.2305%\n",
      "total_backward_count 841940 real_backward_count 60305   7.163%\n",
      "fc layer 3 self.abs_max_out: 1749.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.944785/  1.235367, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1555%\n",
      "layer   2  Sparsity: 69.3129%\n",
      "layer   3  Sparsity: 67.6253%\n",
      "total_backward_count 851730 real_backward_count 60670   7.123%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.954326/  1.240148, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1463%\n",
      "layer   2  Sparsity: 69.3405%\n",
      "layer   3  Sparsity: 68.4783%\n",
      "total_backward_count 861520 real_backward_count 61055   7.087%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.943709/  1.217880, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1460%\n",
      "layer   2  Sparsity: 69.0904%\n",
      "layer   3  Sparsity: 67.9395%\n",
      "total_backward_count 871310 real_backward_count 61446   7.052%\n",
      "fc layer 3 self.abs_max_out: 1780.0\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.942968/  1.224900, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1420%\n",
      "layer   2  Sparsity: 69.0061%\n",
      "layer   3  Sparsity: 68.4019%\n",
      "total_backward_count 881100 real_backward_count 61817   7.016%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.929575/  1.257892, val:  81.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 94.1614%\n",
      "layer   2  Sparsity: 68.9816%\n",
      "layer   3  Sparsity: 68.8275%\n",
      "total_backward_count 890890 real_backward_count 62192   6.981%\n",
      "fc layer 2 self.abs_max_out: 4625.0\n",
      "lif layer 2 self.abs_max_v: 8393.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.945824/  1.250988, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1448%\n",
      "layer   2  Sparsity: 69.0017%\n",
      "layer   3  Sparsity: 68.8412%\n",
      "total_backward_count 900680 real_backward_count 62544   6.944%\n",
      "fc layer 2 self.abs_max_out: 4707.0\n",
      "lif layer 2 self.abs_max_v: 8529.0\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.954096/  1.240793, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1457%\n",
      "layer   2  Sparsity: 69.1316%\n",
      "layer   3  Sparsity: 69.3776%\n",
      "total_backward_count 910470 real_backward_count 62903   6.909%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.954575/  1.224946, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1377%\n",
      "layer   2  Sparsity: 69.3266%\n",
      "layer   3  Sparsity: 69.3899%\n",
      "total_backward_count 920260 real_backward_count 63261   6.874%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.940030/  1.196693, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1374%\n",
      "layer   2  Sparsity: 69.4445%\n",
      "layer   3  Sparsity: 69.5783%\n",
      "total_backward_count 930050 real_backward_count 63604   6.839%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.932674/  1.211433, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1389%\n",
      "layer   2  Sparsity: 69.3238%\n",
      "layer   3  Sparsity: 69.6734%\n",
      "total_backward_count 939840 real_backward_count 63948   6.804%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.932063/  1.232413, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1515%\n",
      "layer   2  Sparsity: 69.2624%\n",
      "layer   3  Sparsity: 69.8608%\n",
      "total_backward_count 949630 real_backward_count 64264   6.767%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.932619/  1.212936, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1301%\n",
      "layer   2  Sparsity: 69.0851%\n",
      "layer   3  Sparsity: 70.0446%\n",
      "total_backward_count 959420 real_backward_count 64619   6.735%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.933937/  1.210774, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1332%\n",
      "layer   2  Sparsity: 68.9074%\n",
      "layer   3  Sparsity: 69.4855%\n",
      "total_backward_count 969210 real_backward_count 64978   6.704%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.933526/  1.215704, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1287%\n",
      "layer   2  Sparsity: 68.8687%\n",
      "layer   3  Sparsity: 69.2637%\n",
      "total_backward_count 979000 real_backward_count 65351   6.675%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.930797/  1.213017, val:  82.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1440%\n",
      "layer   2  Sparsity: 69.0461%\n",
      "layer   3  Sparsity: 69.3355%\n",
      "total_backward_count 988790 real_backward_count 65689   6.643%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.927695/  1.196713, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1571%\n",
      "layer   2  Sparsity: 69.0856%\n",
      "layer   3  Sparsity: 70.1208%\n",
      "total_backward_count 998580 real_backward_count 66024   6.612%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.927805/  1.196766, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1470%\n",
      "layer   2  Sparsity: 69.0047%\n",
      "layer   3  Sparsity: 69.9183%\n",
      "total_backward_count 1008370 real_backward_count 66378   6.583%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.925177/  1.217418, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1296%\n",
      "layer   2  Sparsity: 68.8361%\n",
      "layer   3  Sparsity: 69.2333%\n",
      "total_backward_count 1018160 real_backward_count 66707   6.552%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.917089/  1.190491, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1357%\n",
      "layer   2  Sparsity: 68.8752%\n",
      "layer   3  Sparsity: 69.3982%\n",
      "total_backward_count 1027950 real_backward_count 67041   6.522%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.923986/  1.221520, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1387%\n",
      "layer   2  Sparsity: 68.8689%\n",
      "layer   3  Sparsity: 69.9265%\n",
      "total_backward_count 1037740 real_backward_count 67370   6.492%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.930920/  1.221465, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1653%\n",
      "layer   2  Sparsity: 68.8248%\n",
      "layer   3  Sparsity: 69.0080%\n",
      "total_backward_count 1047530 real_backward_count 67675   6.460%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.921586/  1.220846, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1307%\n",
      "layer   2  Sparsity: 68.8875%\n",
      "layer   3  Sparsity: 69.0547%\n",
      "total_backward_count 1057320 real_backward_count 67998   6.431%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.907965/  1.182879, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1475%\n",
      "layer   2  Sparsity: 68.8775%\n",
      "layer   3  Sparsity: 68.5603%\n",
      "total_backward_count 1067110 real_backward_count 68284   6.399%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.899089/  1.183941, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1474%\n",
      "layer   2  Sparsity: 69.0923%\n",
      "layer   3  Sparsity: 68.8229%\n",
      "total_backward_count 1076900 real_backward_count 68585   6.369%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.906993/  1.179705, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1394%\n",
      "layer   2  Sparsity: 69.1713%\n",
      "layer   3  Sparsity: 68.8598%\n",
      "total_backward_count 1086690 real_backward_count 68886   6.339%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.911232/  1.238687, val:  79.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1400%\n",
      "layer   2  Sparsity: 68.9919%\n",
      "layer   3  Sparsity: 69.0672%\n",
      "total_backward_count 1096480 real_backward_count 69168   6.308%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.908897/  1.218336, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1280%\n",
      "layer   2  Sparsity: 68.9295%\n",
      "layer   3  Sparsity: 69.3788%\n",
      "total_backward_count 1106270 real_backward_count 69511   6.283%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.917882/  1.216573, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1382%\n",
      "layer   2  Sparsity: 69.0234%\n",
      "layer   3  Sparsity: 69.1046%\n",
      "total_backward_count 1116060 real_backward_count 69817   6.256%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.914539/  1.225515, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1368%\n",
      "layer   2  Sparsity: 68.9885%\n",
      "layer   3  Sparsity: 68.5162%\n",
      "total_backward_count 1125850 real_backward_count 70129   6.229%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.912439/  1.173939, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1422%\n",
      "layer   2  Sparsity: 68.9433%\n",
      "layer   3  Sparsity: 68.8347%\n",
      "total_backward_count 1135640 real_backward_count 70482   6.206%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.900365/  1.176654, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.1403%\n",
      "layer   2  Sparsity: 69.0178%\n",
      "layer   3  Sparsity: 69.0652%\n",
      "total_backward_count 1145430 real_backward_count 70761   6.178%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.893548/  1.167569, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1440%\n",
      "layer   2  Sparsity: 69.1080%\n",
      "layer   3  Sparsity: 69.0229%\n",
      "total_backward_count 1155220 real_backward_count 71038   6.149%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.900607/  1.174661, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1527%\n",
      "layer   2  Sparsity: 69.0346%\n",
      "layer   3  Sparsity: 69.2502%\n",
      "total_backward_count 1165010 real_backward_count 71317   6.122%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.892642/  1.177048, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1428%\n",
      "layer   2  Sparsity: 68.9690%\n",
      "layer   3  Sparsity: 69.6012%\n",
      "total_backward_count 1174800 real_backward_count 71571   6.092%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.901707/  1.198587, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1561%\n",
      "layer   2  Sparsity: 68.8549%\n",
      "layer   3  Sparsity: 69.6804%\n",
      "total_backward_count 1184590 real_backward_count 71878   6.068%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.894969/  1.179783, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1484%\n",
      "layer   2  Sparsity: 68.6752%\n",
      "layer   3  Sparsity: 69.0783%\n",
      "total_backward_count 1194380 real_backward_count 72139   6.040%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.900307/  1.180984, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1584%\n",
      "layer   2  Sparsity: 68.7132%\n",
      "layer   3  Sparsity: 68.9114%\n",
      "total_backward_count 1204170 real_backward_count 72418   6.014%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.890547/  1.160747, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.1594%\n",
      "layer   2  Sparsity: 68.8606%\n",
      "layer   3  Sparsity: 69.1005%\n",
      "total_backward_count 1213960 real_backward_count 72665   5.986%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.877887/  1.128527, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1659%\n",
      "layer   2  Sparsity: 68.9556%\n",
      "layer   3  Sparsity: 69.7919%\n",
      "total_backward_count 1223750 real_backward_count 72929   5.959%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.869042/  1.185395, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1557%\n",
      "layer   2  Sparsity: 68.8686%\n",
      "layer   3  Sparsity: 70.3646%\n",
      "total_backward_count 1233540 real_backward_count 73220   5.936%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.877160/  1.177568, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1500%\n",
      "layer   2  Sparsity: 68.8865%\n",
      "layer   3  Sparsity: 69.4686%\n",
      "total_backward_count 1243330 real_backward_count 73515   5.913%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.872186/  1.183798, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1258%\n",
      "layer   2  Sparsity: 68.7957%\n",
      "layer   3  Sparsity: 68.9129%\n",
      "total_backward_count 1253120 real_backward_count 73781   5.888%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.886335/  1.186809, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1486%\n",
      "layer   2  Sparsity: 68.8492%\n",
      "layer   3  Sparsity: 69.2238%\n",
      "total_backward_count 1262910 real_backward_count 74067   5.865%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.884056/  1.181181, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1501%\n",
      "layer   2  Sparsity: 68.6493%\n",
      "layer   3  Sparsity: 69.0835%\n",
      "total_backward_count 1272700 real_backward_count 74346   5.842%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.877372/  1.144835, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1564%\n",
      "layer   2  Sparsity: 68.7025%\n",
      "layer   3  Sparsity: 69.0439%\n",
      "total_backward_count 1282490 real_backward_count 74611   5.818%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.870111/  1.163591, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1390%\n",
      "layer   2  Sparsity: 68.6105%\n",
      "layer   3  Sparsity: 69.2774%\n",
      "total_backward_count 1292280 real_backward_count 74897   5.796%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.869041/  1.167056, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1520%\n",
      "layer   2  Sparsity: 68.7258%\n",
      "layer   3  Sparsity: 69.1760%\n",
      "total_backward_count 1302070 real_backward_count 75152   5.772%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.860395/  1.181801, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1533%\n",
      "layer   2  Sparsity: 68.6783%\n",
      "layer   3  Sparsity: 69.5633%\n",
      "total_backward_count 1311860 real_backward_count 75403   5.748%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.883454/  1.198084, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1417%\n",
      "layer   2  Sparsity: 68.7995%\n",
      "layer   3  Sparsity: 69.7164%\n",
      "total_backward_count 1321650 real_backward_count 75669   5.725%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.893092/  1.182678, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1406%\n",
      "layer   2  Sparsity: 68.7308%\n",
      "layer   3  Sparsity: 69.9112%\n",
      "total_backward_count 1331440 real_backward_count 75941   5.704%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.888249/  1.176637, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1238%\n",
      "layer   2  Sparsity: 68.6777%\n",
      "layer   3  Sparsity: 69.5785%\n",
      "total_backward_count 1341230 real_backward_count 76215   5.682%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.875192/  1.145812, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1273%\n",
      "layer   2  Sparsity: 68.7476%\n",
      "layer   3  Sparsity: 69.6896%\n",
      "total_backward_count 1351020 real_backward_count 76489   5.662%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.867658/  1.192495, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1523%\n",
      "layer   2  Sparsity: 68.5450%\n",
      "layer   3  Sparsity: 70.0660%\n",
      "total_backward_count 1360810 real_backward_count 76740   5.639%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.858227/  1.176671, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1323%\n",
      "layer   2  Sparsity: 68.7852%\n",
      "layer   3  Sparsity: 70.9107%\n",
      "total_backward_count 1370600 real_backward_count 76991   5.617%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.855188/  1.121576, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1458%\n",
      "layer   2  Sparsity: 68.8345%\n",
      "layer   3  Sparsity: 70.4281%\n",
      "total_backward_count 1380390 real_backward_count 77241   5.596%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.861145/  1.144102, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1482%\n",
      "layer   2  Sparsity: 68.7707%\n",
      "layer   3  Sparsity: 70.0280%\n",
      "total_backward_count 1390180 real_backward_count 77476   5.573%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.858281/  1.131440, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1504%\n",
      "layer   2  Sparsity: 68.8558%\n",
      "layer   3  Sparsity: 69.7169%\n",
      "total_backward_count 1399970 real_backward_count 77698   5.550%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.835470/  1.133812, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1470%\n",
      "layer   2  Sparsity: 68.8694%\n",
      "layer   3  Sparsity: 70.5631%\n",
      "total_backward_count 1409760 real_backward_count 77924   5.527%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.824726/  1.128822, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1242%\n",
      "layer   2  Sparsity: 68.6555%\n",
      "layer   3  Sparsity: 70.7034%\n",
      "total_backward_count 1419550 real_backward_count 78137   5.504%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.840350/  1.125512, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1474%\n",
      "layer   2  Sparsity: 68.6511%\n",
      "layer   3  Sparsity: 70.3309%\n",
      "total_backward_count 1429340 real_backward_count 78385   5.484%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.851581/  1.130140, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1265%\n",
      "layer   2  Sparsity: 68.7213%\n",
      "layer   3  Sparsity: 69.8029%\n",
      "total_backward_count 1439130 real_backward_count 78640   5.464%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.828498/  1.152148, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1268%\n",
      "layer   2  Sparsity: 68.5748%\n",
      "layer   3  Sparsity: 70.3474%\n",
      "total_backward_count 1448920 real_backward_count 78871   5.443%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.842310/  1.126117, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1601%\n",
      "layer   2  Sparsity: 68.6696%\n",
      "layer   3  Sparsity: 69.4799%\n",
      "total_backward_count 1458710 real_backward_count 79076   5.421%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.830483/  1.125135, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1439%\n",
      "layer   2  Sparsity: 68.8962%\n",
      "layer   3  Sparsity: 69.6292%\n",
      "total_backward_count 1468500 real_backward_count 79302   5.400%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.828596/  1.182258, val:  79.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1414%\n",
      "layer   2  Sparsity: 68.8317%\n",
      "layer   3  Sparsity: 69.1256%\n",
      "total_backward_count 1478290 real_backward_count 79542   5.381%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.835107/  1.133220, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 68.9402%\n",
      "layer   3  Sparsity: 69.6517%\n",
      "total_backward_count 1488080 real_backward_count 79783   5.361%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.831127/  1.126519, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1378%\n",
      "layer   2  Sparsity: 68.8753%\n",
      "layer   3  Sparsity: 70.2607%\n",
      "total_backward_count 1497870 real_backward_count 80024   5.343%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.832885/  1.101197, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1294%\n",
      "layer   2  Sparsity: 68.6923%\n",
      "layer   3  Sparsity: 70.3119%\n",
      "total_backward_count 1507660 real_backward_count 80239   5.322%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.832163/  1.137719, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1363%\n",
      "layer   2  Sparsity: 68.7147%\n",
      "layer   3  Sparsity: 69.6653%\n",
      "total_backward_count 1517450 real_backward_count 80479   5.304%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.831903/  1.137271, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1538%\n",
      "layer   2  Sparsity: 68.5828%\n",
      "layer   3  Sparsity: 69.5051%\n",
      "total_backward_count 1527240 real_backward_count 80705   5.284%\n",
      "fc layer 3 self.abs_max_out: 1789.0\n",
      "fc layer 3 self.abs_max_out: 1805.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.844530/  1.141471, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1209%\n",
      "layer   2  Sparsity: 68.4431%\n",
      "layer   3  Sparsity: 69.6836%\n",
      "total_backward_count 1537030 real_backward_count 80928   5.265%\n",
      "fc layer 3 self.abs_max_out: 1856.0\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.834267/  1.127579, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1751%\n",
      "layer   2  Sparsity: 68.6840%\n",
      "layer   3  Sparsity: 69.4050%\n",
      "total_backward_count 1546820 real_backward_count 81172   5.248%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.815829/  1.123166, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.1462%\n",
      "layer   2  Sparsity: 68.5406%\n",
      "layer   3  Sparsity: 69.7177%\n",
      "total_backward_count 1556610 real_backward_count 81380   5.228%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.818541/  1.129738, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.1576%\n",
      "layer   2  Sparsity: 68.6277%\n",
      "layer   3  Sparsity: 70.0308%\n",
      "total_backward_count 1566400 real_backward_count 81608   5.210%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.803862/  1.152562, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1480%\n",
      "layer   2  Sparsity: 68.8209%\n",
      "layer   3  Sparsity: 70.4000%\n",
      "total_backward_count 1576190 real_backward_count 81821   5.191%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.812203/  1.103681, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1408%\n",
      "layer   2  Sparsity: 68.9864%\n",
      "layer   3  Sparsity: 70.1550%\n",
      "total_backward_count 1585980 real_backward_count 82034   5.172%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.818004/  1.128749, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1291%\n",
      "layer   2  Sparsity: 68.8113%\n",
      "layer   3  Sparsity: 70.5783%\n",
      "total_backward_count 1595770 real_backward_count 82254   5.155%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.831520/  1.143196, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1363%\n",
      "layer   2  Sparsity: 68.8080%\n",
      "layer   3  Sparsity: 70.9522%\n",
      "total_backward_count 1605560 real_backward_count 82445   5.135%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.820863/  1.154783, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1664%\n",
      "layer   2  Sparsity: 68.7845%\n",
      "layer   3  Sparsity: 70.7322%\n",
      "total_backward_count 1615350 real_backward_count 82636   5.116%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.818901/  1.133616, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1471%\n",
      "layer   2  Sparsity: 68.6691%\n",
      "layer   3  Sparsity: 70.9938%\n",
      "total_backward_count 1625140 real_backward_count 82869   5.099%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.817948/  1.094434, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1381%\n",
      "layer   2  Sparsity: 68.6768%\n",
      "layer   3  Sparsity: 70.4794%\n",
      "total_backward_count 1634930 real_backward_count 83038   5.079%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.807533/  1.140162, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1349%\n",
      "layer   2  Sparsity: 68.6267%\n",
      "layer   3  Sparsity: 70.0306%\n",
      "total_backward_count 1644720 real_backward_count 83257   5.062%\n",
      "fc layer 1 self.abs_max_out: 7431.0\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.806770/  1.106318, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1424%\n",
      "layer   2  Sparsity: 68.8447%\n",
      "layer   3  Sparsity: 70.0255%\n",
      "total_backward_count 1654510 real_backward_count 83472   5.045%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.799541/  1.114795, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1577%\n",
      "layer   2  Sparsity: 68.7600%\n",
      "layer   3  Sparsity: 69.8940%\n",
      "total_backward_count 1664300 real_backward_count 83711   5.030%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.804207/  1.127695, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1283%\n",
      "layer   2  Sparsity: 68.8021%\n",
      "layer   3  Sparsity: 69.8345%\n",
      "total_backward_count 1674090 real_backward_count 83904   5.012%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.799901/  1.123587, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1280%\n",
      "layer   2  Sparsity: 68.6482%\n",
      "layer   3  Sparsity: 69.9700%\n",
      "total_backward_count 1683880 real_backward_count 84108   4.995%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.810200/  1.110795, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1463%\n",
      "layer   2  Sparsity: 68.6632%\n",
      "layer   3  Sparsity: 69.4894%\n",
      "total_backward_count 1693670 real_backward_count 84302   4.977%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.806957/  1.099919, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1499%\n",
      "layer   2  Sparsity: 68.8299%\n",
      "layer   3  Sparsity: 69.7932%\n",
      "total_backward_count 1703460 real_backward_count 84489   4.960%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.803184/  1.137475, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1359%\n",
      "layer   2  Sparsity: 68.7604%\n",
      "layer   3  Sparsity: 70.1306%\n",
      "total_backward_count 1713250 real_backward_count 84674   4.942%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.809899/  1.122115, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1480%\n",
      "layer   2  Sparsity: 68.8405%\n",
      "layer   3  Sparsity: 70.1591%\n",
      "total_backward_count 1723040 real_backward_count 84854   4.925%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.804316/  1.094373, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1503%\n",
      "layer   2  Sparsity: 68.7902%\n",
      "layer   3  Sparsity: 70.7619%\n",
      "total_backward_count 1732830 real_backward_count 85034   4.907%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.803708/  1.105550, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1360%\n",
      "layer   2  Sparsity: 68.8232%\n",
      "layer   3  Sparsity: 70.5381%\n",
      "total_backward_count 1742620 real_backward_count 85223   4.891%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.806558/  1.111494, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1382%\n",
      "layer   2  Sparsity: 68.7406%\n",
      "layer   3  Sparsity: 69.9067%\n",
      "total_backward_count 1752410 real_backward_count 85438   4.875%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.798778/  1.103821, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1268%\n",
      "layer   2  Sparsity: 68.7230%\n",
      "layer   3  Sparsity: 70.0910%\n",
      "total_backward_count 1762200 real_backward_count 85620   4.859%\n",
      "fc layer 3 self.abs_max_out: 1901.0\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.790318/  1.120585, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1410%\n",
      "layer   2  Sparsity: 68.8599%\n",
      "layer   3  Sparsity: 70.5745%\n",
      "total_backward_count 1771990 real_backward_count 85812   4.843%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.796350/  1.097742, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1261%\n",
      "layer   2  Sparsity: 68.9779%\n",
      "layer   3  Sparsity: 70.4304%\n",
      "total_backward_count 1781780 real_backward_count 86008   4.827%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.801370/  1.098177, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1428%\n",
      "layer   2  Sparsity: 68.7881%\n",
      "layer   3  Sparsity: 70.6852%\n",
      "total_backward_count 1791570 real_backward_count 86192   4.811%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.801980/  1.120486, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1275%\n",
      "layer   2  Sparsity: 68.6481%\n",
      "layer   3  Sparsity: 70.7257%\n",
      "total_backward_count 1801360 real_backward_count 86401   4.796%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.799162/  1.106040, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1502%\n",
      "layer   2  Sparsity: 68.6753%\n",
      "layer   3  Sparsity: 71.0776%\n",
      "total_backward_count 1811150 real_backward_count 86572   4.780%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.789818/  1.090644, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1523%\n",
      "layer   2  Sparsity: 69.0124%\n",
      "layer   3  Sparsity: 70.5141%\n",
      "total_backward_count 1820940 real_backward_count 86748   4.764%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.794666/  1.079117, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1422%\n",
      "layer   2  Sparsity: 69.0531%\n",
      "layer   3  Sparsity: 70.4035%\n",
      "total_backward_count 1830730 real_backward_count 86940   4.749%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.783907/  1.100747, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1480%\n",
      "layer   2  Sparsity: 69.0610%\n",
      "layer   3  Sparsity: 71.3409%\n",
      "total_backward_count 1840520 real_backward_count 87117   4.733%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.789343/  1.089208, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1542%\n",
      "layer   2  Sparsity: 68.9252%\n",
      "layer   3  Sparsity: 71.3475%\n",
      "total_backward_count 1850310 real_backward_count 87292   4.718%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.788677/  1.104718, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1358%\n",
      "layer   2  Sparsity: 68.8817%\n",
      "layer   3  Sparsity: 71.0338%\n",
      "total_backward_count 1860100 real_backward_count 87446   4.701%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.781738/  1.105557, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1449%\n",
      "layer   2  Sparsity: 69.0036%\n",
      "layer   3  Sparsity: 70.6784%\n",
      "total_backward_count 1869890 real_backward_count 87609   4.685%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.783531/  1.108379, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1516%\n",
      "layer   2  Sparsity: 69.0530%\n",
      "layer   3  Sparsity: 70.7737%\n",
      "total_backward_count 1879680 real_backward_count 87780   4.670%\n",
      "fc layer 2 self.abs_max_out: 4713.0\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.764293/  1.075866, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1623%\n",
      "layer   2  Sparsity: 69.0085%\n",
      "layer   3  Sparsity: 70.9648%\n",
      "total_backward_count 1889470 real_backward_count 87973   4.656%\n",
      "fc layer 2 self.abs_max_out: 4809.0\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.773552/  1.109471, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 69.2338%\n",
      "layer   3  Sparsity: 70.7430%\n",
      "total_backward_count 1899260 real_backward_count 88142   4.641%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.770686/  1.056713, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 94.1399%\n",
      "layer   2  Sparsity: 69.4057%\n",
      "layer   3  Sparsity: 71.3272%\n",
      "total_backward_count 1909050 real_backward_count 88297   4.625%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.752933/  1.081371, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1496%\n",
      "layer   2  Sparsity: 69.2659%\n",
      "layer   3  Sparsity: 71.1102%\n",
      "total_backward_count 1918840 real_backward_count 88475   4.611%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.765959/  1.062269, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1495%\n",
      "layer   2  Sparsity: 69.2122%\n",
      "layer   3  Sparsity: 72.2078%\n",
      "total_backward_count 1928630 real_backward_count 88624   4.595%\n",
      "lif layer 1 self.abs_max_v: 13800.0\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.765762/  1.065856, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1349%\n",
      "layer   2  Sparsity: 69.0334%\n",
      "layer   3  Sparsity: 71.2752%\n",
      "total_backward_count 1938420 real_backward_count 88790   4.581%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.761933/  1.072078, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1401%\n",
      "layer   2  Sparsity: 68.7610%\n",
      "layer   3  Sparsity: 71.1336%\n",
      "total_backward_count 1948210 real_backward_count 88959   4.566%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.767165/  1.079600, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1328%\n",
      "layer   2  Sparsity: 68.6737%\n",
      "layer   3  Sparsity: 70.7350%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0052e16483cd45c39738895d2305b0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.76717</td></tr><tr><td>val_acc_best</td><td>0.89583</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>1.0796</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-73</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/512y20wp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/512y20wp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_134332-512y20wp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e9amhexb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_180210-e9amhexb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9amhexb' target=\"_blank\">pretty-sweep-80</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9amhexb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9amhexb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251115_180218_938', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 712.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 525.5\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "lif layer 2 self.abs_max_v: 1078.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "fc layer 1 self.abs_max_out: 538.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "lif layer 2 self.abs_max_v: 1503.0\n",
      "fc layer 1 self.abs_max_out: 693.0\n",
      "lif layer 1 self.abs_max_v: 693.0\n",
      "fc layer 2 self.abs_max_out: 1049.0\n",
      "lif layer 2 self.abs_max_v: 1800.5\n",
      "lif layer 1 self.abs_max_v: 821.5\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 1 self.abs_max_out: 887.0\n",
      "lif layer 1 self.abs_max_v: 887.0\n",
      "lif layer 1 self.abs_max_v: 1007.5\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 1 self.abs_max_out: 1134.0\n",
      "lif layer 1 self.abs_max_v: 1134.0\n",
      "fc layer 2 self.abs_max_out: 1109.0\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "lif layer 1 self.abs_max_v: 1153.5\n",
      "lif layer 1 self.abs_max_v: 1184.0\n",
      "fc layer 2 self.abs_max_out: 1119.0\n",
      "fc layer 1 self.abs_max_out: 1160.0\n",
      "lif layer 1 self.abs_max_v: 1369.0\n",
      "fc layer 2 self.abs_max_out: 1140.0\n",
      "fc layer 1 self.abs_max_out: 1188.0\n",
      "fc layer 1 self.abs_max_out: 1290.0\n",
      "lif layer 1 self.abs_max_v: 1458.5\n",
      "lif layer 1 self.abs_max_v: 1532.0\n",
      "lif layer 1 self.abs_max_v: 1596.0\n",
      "lif layer 1 self.abs_max_v: 1624.0\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "lif layer 2 self.abs_max_v: 1892.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "fc layer 1 self.abs_max_out: 1624.0\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "lif layer 1 self.abs_max_v: 1647.0\n",
      "fc layer 2 self.abs_max_out: 1426.0\n",
      "lif layer 2 self.abs_max_v: 2244.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 1673.0\n",
      "lif layer 1 self.abs_max_v: 1723.5\n",
      "lif layer 2 self.abs_max_v: 2324.0\n",
      "lif layer 1 self.abs_max_v: 1777.0\n",
      "lif layer 1 self.abs_max_v: 1944.5\n",
      "lif layer 1 self.abs_max_v: 2012.0\n",
      "lif layer 1 self.abs_max_v: 2324.0\n",
      "lif layer 1 self.abs_max_v: 2344.0\n",
      "lif layer 1 self.abs_max_v: 2420.0\n",
      "lif layer 1 self.abs_max_v: 2507.0\n",
      "lif layer 1 self.abs_max_v: 2603.0\n",
      "lif layer 1 self.abs_max_v: 2743.5\n",
      "fc layer 1 self.abs_max_out: 1682.0\n",
      "lif layer 1 self.abs_max_v: 3033.0\n",
      "fc layer 2 self.abs_max_out: 1434.0\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "lif layer 1 self.abs_max_v: 3149.0\n",
      "lif layer 1 self.abs_max_v: 3186.5\n",
      "fc layer 1 self.abs_max_out: 1944.0\n",
      "fc layer 1 self.abs_max_out: 1959.0\n",
      "fc layer 1 self.abs_max_out: 1994.0\n",
      "lif layer 1 self.abs_max_v: 3392.0\n",
      "lif layer 1 self.abs_max_v: 3446.5\n",
      "fc layer 1 self.abs_max_out: 2006.0\n",
      "lif layer 2 self.abs_max_v: 2349.5\n",
      "lif layer 2 self.abs_max_v: 2368.0\n",
      "fc layer 2 self.abs_max_out: 1545.0\n",
      "fc layer 1 self.abs_max_out: 2232.0\n",
      "lif layer 2 self.abs_max_v: 2460.0\n",
      "lif layer 2 self.abs_max_v: 2478.5\n",
      "lif layer 1 self.abs_max_v: 3592.0\n",
      "lif layer 1 self.abs_max_v: 3705.0\n",
      "fc layer 1 self.abs_max_out: 2297.0\n",
      "fc layer 1 self.abs_max_out: 2374.0\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "fc layer 2 self.abs_max_out: 1571.0\n",
      "lif layer 1 self.abs_max_v: 4007.5\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "lif layer 2 self.abs_max_v: 2526.0\n",
      "lif layer 2 self.abs_max_v: 2532.5\n",
      "fc layer 1 self.abs_max_out: 2450.0\n",
      "lif layer 1 self.abs_max_v: 4252.5\n",
      "fc layer 1 self.abs_max_out: 2678.0\n",
      "lif layer 1 self.abs_max_v: 4601.0\n",
      "fc layer 1 self.abs_max_out: 2863.0\n",
      "lif layer 1 self.abs_max_v: 4969.5\n",
      "lif layer 2 self.abs_max_v: 2570.0\n",
      "lif layer 2 self.abs_max_v: 2791.5\n",
      "fc layer 1 self.abs_max_out: 2905.0\n",
      "fc layer 1 self.abs_max_out: 2964.0\n",
      "lif layer 1 self.abs_max_v: 5097.0\n",
      "lif layer 1 self.abs_max_v: 5107.5\n",
      "lif layer 1 self.abs_max_v: 5192.0\n",
      "lif layer 1 self.abs_max_v: 5199.0\n",
      "lif layer 1 self.abs_max_v: 5323.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.895674/  2.035502, val:  33.75%, val_best:  33.75%, tr:  93.56%, tr_best:  93.56%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9369%\n",
      "layer   2  Sparsity: 73.1154%\n",
      "layer   3  Sparsity: 70.0364%\n",
      "total_backward_count 9790 real_backward_count 2624  26.803%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2794.5\n",
      "fc layer 1 self.abs_max_out: 3039.0\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "fc layer 1 self.abs_max_out: 3128.0\n",
      "lif layer 1 self.abs_max_v: 5332.0\n",
      "lif layer 2 self.abs_max_v: 2801.5\n",
      "lif layer 2 self.abs_max_v: 2838.0\n",
      "lif layer 2 self.abs_max_v: 2953.0\n",
      "fc layer 1 self.abs_max_out: 3167.0\n",
      "fc layer 1 self.abs_max_out: 3228.0\n",
      "lif layer 1 self.abs_max_v: 5417.0\n",
      "lif layer 1 self.abs_max_v: 5491.5\n",
      "lif layer 1 self.abs_max_v: 5570.0\n",
      "lif layer 1 self.abs_max_v: 5613.0\n",
      "lif layer 1 self.abs_max_v: 5676.5\n",
      "lif layer 2 self.abs_max_v: 2956.5\n",
      "lif layer 2 self.abs_max_v: 3112.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.864980/  2.027432, val:  42.50%, val_best:  42.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9249%\n",
      "layer   2  Sparsity: 75.7566%\n",
      "layer   3  Sparsity: 71.5213%\n",
      "total_backward_count 19580 real_backward_count 4466  22.809%\n",
      "fc layer 2 self.abs_max_out: 1835.0\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "lif layer 2 self.abs_max_v: 3136.5\n",
      "lif layer 2 self.abs_max_v: 3248.0\n",
      "lif layer 2 self.abs_max_v: 3253.5\n",
      "lif layer 2 self.abs_max_v: 3259.5\n",
      "lif layer 2 self.abs_max_v: 3314.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "lif layer 2 self.abs_max_v: 3315.5\n",
      "fc layer 1 self.abs_max_out: 3352.0\n",
      "fc layer 1 self.abs_max_out: 3378.0\n",
      "lif layer 1 self.abs_max_v: 5973.5\n",
      "lif layer 2 self.abs_max_v: 3317.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.848387/  1.999652, val:  47.08%, val_best:  47.08%, tr:  99.08%, tr_best:  99.08%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 75.7302%\n",
      "layer   3  Sparsity: 71.1104%\n",
      "total_backward_count 29370 real_backward_count 6126  20.858%\n",
      "lif layer 2 self.abs_max_v: 3407.5\n",
      "lif layer 2 self.abs_max_v: 3563.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "fc layer 1 self.abs_max_out: 3706.0\n",
      "fc layer 2 self.abs_max_out: 2002.0\n",
      "lif layer 1 self.abs_max_v: 5995.0\n",
      "lif layer 1 self.abs_max_v: 6130.0\n",
      "fc layer 2 self.abs_max_out: 2027.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.847475/  1.985849, val:  36.25%, val_best:  47.08%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 75.8519%\n",
      "layer   3  Sparsity: 70.1215%\n",
      "total_backward_count 39160 real_backward_count 7625  19.471%\n",
      "fc layer 2 self.abs_max_out: 2045.0\n",
      "lif layer 1 self.abs_max_v: 6239.0\n",
      "fc layer 1 self.abs_max_out: 3779.0\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.829486/  1.978368, val:  51.25%, val_best:  51.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 75.3717%\n",
      "layer   3  Sparsity: 68.5856%\n",
      "total_backward_count 48950 real_backward_count 9052  18.492%\n",
      "lif layer 1 self.abs_max_v: 6317.5\n",
      "fc layer 3 self.abs_max_out: 723.0\n",
      "fc layer 2 self.abs_max_out: 2087.0\n",
      "fc layer 2 self.abs_max_out: 2088.0\n",
      "fc layer 1 self.abs_max_out: 4331.0\n",
      "lif layer 1 self.abs_max_v: 6570.0\n",
      "lif layer 1 self.abs_max_v: 6718.0\n",
      "lif layer 1 self.abs_max_v: 6756.0\n",
      "lif layer 1 self.abs_max_v: 6825.0\n",
      "lif layer 1 self.abs_max_v: 6983.5\n",
      "fc layer 2 self.abs_max_out: 2189.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.826056/  1.978735, val:  60.42%, val_best:  60.42%, tr:  99.49%, tr_best:  99.59%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9070%\n",
      "layer   2  Sparsity: 74.5039%\n",
      "layer   3  Sparsity: 68.9055%\n",
      "total_backward_count 58740 real_backward_count 10376  17.664%\n",
      "fc layer 1 self.abs_max_out: 4399.0\n",
      "lif layer 2 self.abs_max_v: 3583.5\n",
      "lif layer 2 self.abs_max_v: 3674.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.814920/  1.961184, val:  58.75%, val_best:  60.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9030%\n",
      "layer   2  Sparsity: 74.1899%\n",
      "layer   3  Sparsity: 67.5343%\n",
      "total_backward_count 68530 real_backward_count 11602  16.930%\n",
      "fc layer 2 self.abs_max_out: 2229.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.812644/  1.947643, val:  49.58%, val_best:  60.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9136%\n",
      "layer   2  Sparsity: 74.6804%\n",
      "layer   3  Sparsity: 68.1169%\n",
      "total_backward_count 78320 real_backward_count 12783  16.322%\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.795294/  1.927848, val:  56.67%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 74.7534%\n",
      "layer   3  Sparsity: 67.9823%\n",
      "total_backward_count 88110 real_backward_count 14029  15.922%\n",
      "lif layer 1 self.abs_max_v: 7124.5\n",
      "lif layer 1 self.abs_max_v: 7147.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.795871/  1.941800, val:  56.67%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 75.0145%\n",
      "layer   3  Sparsity: 68.9979%\n",
      "total_backward_count 97900 real_backward_count 15228  15.555%\n",
      "lif layer 2 self.abs_max_v: 3741.0\n",
      "lif layer 2 self.abs_max_v: 3820.0\n",
      "lif layer 2 self.abs_max_v: 3996.0\n",
      "lif layer 1 self.abs_max_v: 7188.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.787286/  1.943630, val:  50.00%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9415%\n",
      "layer   2  Sparsity: 75.1033%\n",
      "layer   3  Sparsity: 70.2578%\n",
      "total_backward_count 107690 real_backward_count 16383  15.213%\n",
      "lif layer 1 self.abs_max_v: 7580.0\n",
      "fc layer 2 self.abs_max_out: 2285.0\n",
      "lif layer 2 self.abs_max_v: 4169.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.797280/  1.929896, val:  55.83%, val_best:  60.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9446%\n",
      "layer   2  Sparsity: 75.2651%\n",
      "layer   3  Sparsity: 69.8505%\n",
      "total_backward_count 117480 real_backward_count 17550  14.939%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.787779/  1.934124, val:  47.50%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9206%\n",
      "layer   2  Sparsity: 75.0670%\n",
      "layer   3  Sparsity: 70.2232%\n",
      "total_backward_count 127270 real_backward_count 18607  14.620%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.797630/  1.957071, val:  46.25%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9257%\n",
      "layer   2  Sparsity: 75.1553%\n",
      "layer   3  Sparsity: 69.6596%\n",
      "total_backward_count 137060 real_backward_count 19678  14.357%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.787422/  1.929621, val:  54.17%, val_best:  60.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9279%\n",
      "layer   2  Sparsity: 74.3652%\n",
      "layer   3  Sparsity: 69.7189%\n",
      "total_backward_count 146850 real_backward_count 20737  14.121%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.786035/  1.922696, val:  56.25%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9142%\n",
      "layer   2  Sparsity: 74.5470%\n",
      "layer   3  Sparsity: 69.5036%\n",
      "total_backward_count 156640 real_backward_count 21820  13.930%\n",
      "fc layer 2 self.abs_max_out: 2321.0\n",
      "lif layer 2 self.abs_max_v: 4294.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.773485/  1.908333, val:  53.75%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8762%\n",
      "layer   2  Sparsity: 74.2673%\n",
      "layer   3  Sparsity: 70.7963%\n",
      "total_backward_count 166430 real_backward_count 22875  13.745%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.763508/  1.901671, val:  68.33%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9105%\n",
      "layer   2  Sparsity: 73.9989%\n",
      "layer   3  Sparsity: 70.7642%\n",
      "total_backward_count 176220 real_backward_count 23902  13.564%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.750118/  1.874194, val:  56.25%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 74.7443%\n",
      "layer   3  Sparsity: 69.2852%\n",
      "total_backward_count 186010 real_backward_count 25001  13.441%\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.733417/  1.894001, val:  54.58%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 75.5032%\n",
      "layer   3  Sparsity: 69.4524%\n",
      "total_backward_count 195800 real_backward_count 25973  13.265%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.742710/  1.884545, val:  58.75%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8994%\n",
      "layer   2  Sparsity: 75.4718%\n",
      "layer   3  Sparsity: 70.1564%\n",
      "total_backward_count 205590 real_backward_count 26919  13.094%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.745652/  1.889464, val:  60.00%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9562%\n",
      "layer   2  Sparsity: 75.7156%\n",
      "layer   3  Sparsity: 69.7126%\n",
      "total_backward_count 215380 real_backward_count 27946  12.975%\n",
      "fc layer 1 self.abs_max_out: 4462.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.744080/  1.854254, val:  61.25%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9311%\n",
      "layer   2  Sparsity: 74.8426%\n",
      "layer   3  Sparsity: 70.4159%\n",
      "total_backward_count 225170 real_backward_count 28935  12.850%\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 741.0\n",
      "fc layer 1 self.abs_max_out: 4552.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.730796/  1.873431, val:  63.33%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9098%\n",
      "layer   2  Sparsity: 74.5873%\n",
      "layer   3  Sparsity: 71.1538%\n",
      "total_backward_count 234960 real_backward_count 29871  12.713%\n",
      "fc layer 1 self.abs_max_out: 4589.0\n",
      "fc layer 1 self.abs_max_out: 5131.0\n",
      "lif layer 1 self.abs_max_v: 7669.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.728869/  1.855456, val:  66.67%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9511%\n",
      "layer   2  Sparsity: 73.5161%\n",
      "layer   3  Sparsity: 70.5725%\n",
      "total_backward_count 244750 real_backward_count 30815  12.590%\n",
      "fc layer 1 self.abs_max_out: 5263.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.722141/  1.860801, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 73.6898%\n",
      "layer   3  Sparsity: 70.0694%\n",
      "total_backward_count 254540 real_backward_count 31768  12.481%\n",
      "fc layer 1 self.abs_max_out: 5574.0\n",
      "lif layer 1 self.abs_max_v: 7804.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.715587/  1.851442, val:  65.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9275%\n",
      "layer   2  Sparsity: 73.8760%\n",
      "layer   3  Sparsity: 70.2028%\n",
      "total_backward_count 264330 real_backward_count 32670  12.360%\n",
      "fc layer 3 self.abs_max_out: 758.0\n",
      "lif layer 1 self.abs_max_v: 7859.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.715609/  1.846045, val:  70.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9239%\n",
      "layer   2  Sparsity: 73.8755%\n",
      "layer   3  Sparsity: 70.9637%\n",
      "total_backward_count 274120 real_backward_count 33561  12.243%\n",
      "lif layer 1 self.abs_max_v: 8030.0\n",
      "lif layer 1 self.abs_max_v: 8077.0\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.703920/  1.860067, val:  60.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9076%\n",
      "layer   2  Sparsity: 73.8302%\n",
      "layer   3  Sparsity: 71.0664%\n",
      "total_backward_count 283910 real_backward_count 34379  12.109%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.707918/  1.833315, val:  59.58%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9285%\n",
      "layer   2  Sparsity: 74.0771%\n",
      "layer   3  Sparsity: 70.6573%\n",
      "total_backward_count 293700 real_backward_count 35248  12.001%\n",
      "fc layer 1 self.abs_max_out: 5644.0\n",
      "lif layer 1 self.abs_max_v: 8183.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.689441/  1.820193, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 73.6938%\n",
      "layer   3  Sparsity: 70.3022%\n",
      "total_backward_count 303490 real_backward_count 36081  11.889%\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "fc layer 3 self.abs_max_out: 851.0\n",
      "lif layer 1 self.abs_max_v: 8579.0\n",
      "fc layer 1 self.abs_max_out: 5832.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.680762/  1.823101, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9590%\n",
      "layer   2  Sparsity: 73.6313%\n",
      "layer   3  Sparsity: 70.6769%\n",
      "total_backward_count 313280 real_backward_count 36947  11.794%\n",
      "fc layer 2 self.abs_max_out: 2346.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.667968/  1.837686, val:  62.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.8996%\n",
      "layer   2  Sparsity: 73.5873%\n",
      "layer   3  Sparsity: 69.9414%\n",
      "total_backward_count 323070 real_backward_count 37742  11.682%\n",
      "fc layer 2 self.abs_max_out: 2510.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.677134/  1.805265, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9276%\n",
      "layer   2  Sparsity: 74.0883%\n",
      "layer   3  Sparsity: 69.8896%\n",
      "total_backward_count 332860 real_backward_count 38538  11.578%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.659499/  1.804871, val:  73.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9063%\n",
      "layer   2  Sparsity: 74.5865%\n",
      "layer   3  Sparsity: 70.4582%\n",
      "total_backward_count 342650 real_backward_count 39313  11.473%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.663365/  1.796142, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9461%\n",
      "layer   2  Sparsity: 74.8231%\n",
      "layer   3  Sparsity: 70.6858%\n",
      "total_backward_count 352440 real_backward_count 40077  11.371%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.653420/  1.796451, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 75.1653%\n",
      "layer   3  Sparsity: 69.5831%\n",
      "total_backward_count 362230 real_backward_count 40791  11.261%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.636845/  1.802278, val:  65.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9292%\n",
      "layer   2  Sparsity: 75.3450%\n",
      "layer   3  Sparsity: 70.1435%\n",
      "total_backward_count 372020 real_backward_count 41452  11.142%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.640246/  1.780929, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9087%\n",
      "layer   2  Sparsity: 74.6789%\n",
      "layer   3  Sparsity: 70.3370%\n",
      "total_backward_count 381810 real_backward_count 42144  11.038%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.636145/  1.797652, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9210%\n",
      "layer   2  Sparsity: 74.5798%\n",
      "layer   3  Sparsity: 70.0772%\n",
      "total_backward_count 391600 real_backward_count 42859  10.945%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.637630/  1.786862, val:  63.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9477%\n",
      "layer   2  Sparsity: 74.7516%\n",
      "layer   3  Sparsity: 69.9034%\n",
      "total_backward_count 401390 real_backward_count 43601  10.863%\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 3 self.abs_max_out: 890.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.625923/  1.779164, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9231%\n",
      "layer   2  Sparsity: 74.5071%\n",
      "layer   3  Sparsity: 70.7091%\n",
      "total_backward_count 411180 real_backward_count 44289  10.771%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.630650/  1.781182, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9254%\n",
      "layer   2  Sparsity: 74.2336%\n",
      "layer   3  Sparsity: 70.9313%\n",
      "total_backward_count 420970 real_backward_count 44983  10.686%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.630105/  1.775558, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9274%\n",
      "layer   2  Sparsity: 74.2625%\n",
      "layer   3  Sparsity: 70.6944%\n",
      "total_backward_count 430760 real_backward_count 45612  10.589%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.616313/  1.772810, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9248%\n",
      "layer   2  Sparsity: 74.4308%\n",
      "layer   3  Sparsity: 70.7802%\n",
      "total_backward_count 440550 real_backward_count 46259  10.500%\n",
      "lif layer 1 self.abs_max_v: 8750.0\n",
      "lif layer 1 self.abs_max_v: 8815.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.616505/  1.781018, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 73.9147%\n",
      "layer   3  Sparsity: 71.3097%\n",
      "total_backward_count 450340 real_backward_count 46870  10.408%\n",
      "lif layer 1 self.abs_max_v: 8958.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.618526/  1.783698, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 73.9802%\n",
      "layer   3  Sparsity: 71.7536%\n",
      "total_backward_count 460130 real_backward_count 47498  10.323%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.614281/  1.768628, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9233%\n",
      "layer   2  Sparsity: 74.2504%\n",
      "layer   3  Sparsity: 71.3065%\n",
      "total_backward_count 469920 real_backward_count 48109  10.238%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.606296/  1.780429, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9425%\n",
      "layer   2  Sparsity: 74.1438%\n",
      "layer   3  Sparsity: 71.4580%\n",
      "total_backward_count 479710 real_backward_count 48679  10.148%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.607910/  1.769855, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9284%\n",
      "layer   2  Sparsity: 74.4863%\n",
      "layer   3  Sparsity: 71.6045%\n",
      "total_backward_count 489500 real_backward_count 49262  10.064%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.597162/  1.742887, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 74.6054%\n",
      "layer   3  Sparsity: 71.3420%\n",
      "total_backward_count 499290 real_backward_count 49832   9.981%\n",
      "lif layer 1 self.abs_max_v: 9481.0\n",
      "fc layer 1 self.abs_max_out: 5974.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.584477/  1.745617, val:  82.50%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 74.2088%\n",
      "layer   3  Sparsity: 70.9811%\n",
      "total_backward_count 509080 real_backward_count 50376   9.895%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.584298/  1.748091, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9299%\n",
      "layer   2  Sparsity: 74.2384%\n",
      "layer   3  Sparsity: 71.1701%\n",
      "total_backward_count 518870 real_backward_count 50966   9.822%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.580654/  1.728316, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9044%\n",
      "layer   2  Sparsity: 73.7443%\n",
      "layer   3  Sparsity: 71.0252%\n",
      "total_backward_count 528660 real_backward_count 51475   9.737%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.572234/  1.728607, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9200%\n",
      "layer   2  Sparsity: 73.9035%\n",
      "layer   3  Sparsity: 70.5610%\n",
      "total_backward_count 538450 real_backward_count 51996   9.657%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.570133/  1.723985, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9145%\n",
      "layer   2  Sparsity: 73.8542%\n",
      "layer   3  Sparsity: 70.2295%\n",
      "total_backward_count 548240 real_backward_count 52528   9.581%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.561447/  1.716143, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 73.7556%\n",
      "layer   3  Sparsity: 70.5901%\n",
      "total_backward_count 558030 real_backward_count 53061   9.509%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.559548/  1.727000, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9417%\n",
      "layer   2  Sparsity: 73.6687%\n",
      "layer   3  Sparsity: 70.2471%\n",
      "total_backward_count 567820 real_backward_count 53568   9.434%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.566718/  1.709297, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9237%\n",
      "layer   2  Sparsity: 73.1861%\n",
      "layer   3  Sparsity: 70.5078%\n",
      "total_backward_count 577610 real_backward_count 54069   9.361%\n",
      "lif layer 1 self.abs_max_v: 9518.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.559001/  1.740038, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 73.1803%\n",
      "layer   3  Sparsity: 71.2088%\n",
      "total_backward_count 587400 real_backward_count 54569   9.290%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.556809/  1.726893, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9277%\n",
      "layer   2  Sparsity: 73.3425%\n",
      "layer   3  Sparsity: 71.2239%\n",
      "total_backward_count 597190 real_backward_count 55049   9.218%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.550343/  1.732696, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9349%\n",
      "layer   2  Sparsity: 73.0955%\n",
      "layer   3  Sparsity: 70.8842%\n",
      "total_backward_count 606980 real_backward_count 55532   9.149%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.558050/  1.741470, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8898%\n",
      "layer   2  Sparsity: 72.8582%\n",
      "layer   3  Sparsity: 71.2151%\n",
      "total_backward_count 616770 real_backward_count 56018   9.082%\n",
      "fc layer 1 self.abs_max_out: 6137.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.553908/  1.715817, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 72.6779%\n",
      "layer   3  Sparsity: 71.2862%\n",
      "total_backward_count 626560 real_backward_count 56433   9.007%\n",
      "fc layer 3 self.abs_max_out: 899.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.549109/  1.713614, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9546%\n",
      "layer   2  Sparsity: 72.8907%\n",
      "layer   3  Sparsity: 70.9500%\n",
      "total_backward_count 636350 real_backward_count 56877   8.938%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.531387/  1.700047, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9114%\n",
      "layer   2  Sparsity: 72.9961%\n",
      "layer   3  Sparsity: 70.6961%\n",
      "total_backward_count 646140 real_backward_count 57287   8.866%\n",
      "fc layer 3 self.abs_max_out: 906.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.532367/  1.701670, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 73.0624%\n",
      "layer   3  Sparsity: 70.6974%\n",
      "total_backward_count 655930 real_backward_count 57704   8.797%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.531453/  1.688655, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8998%\n",
      "layer   2  Sparsity: 72.9929%\n",
      "layer   3  Sparsity: 70.8606%\n",
      "total_backward_count 665720 real_backward_count 58097   8.727%\n",
      "fc layer 1 self.abs_max_out: 6165.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.523133/  1.699637, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9334%\n",
      "layer   2  Sparsity: 73.0498%\n",
      "layer   3  Sparsity: 71.2513%\n",
      "total_backward_count 675510 real_backward_count 58475   8.656%\n",
      "fc layer 1 self.abs_max_out: 6177.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.520616/  1.705534, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9102%\n",
      "layer   2  Sparsity: 73.3353%\n",
      "layer   3  Sparsity: 71.5897%\n",
      "total_backward_count 685300 real_backward_count 58861   8.589%\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.520222/  1.689845, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 73.0993%\n",
      "layer   3  Sparsity: 71.5469%\n",
      "total_backward_count 695090 real_backward_count 59240   8.523%\n",
      "fc layer 3 self.abs_max_out: 915.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.515069/  1.683032, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9232%\n",
      "layer   2  Sparsity: 73.1621%\n",
      "layer   3  Sparsity: 71.1699%\n",
      "total_backward_count 704880 real_backward_count 59634   8.460%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.515009/  1.679853, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9288%\n",
      "layer   2  Sparsity: 73.2179%\n",
      "layer   3  Sparsity: 71.7493%\n",
      "total_backward_count 714670 real_backward_count 60018   8.398%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.507427/  1.690408, val:  73.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8941%\n",
      "layer   2  Sparsity: 73.5460%\n",
      "layer   3  Sparsity: 72.0200%\n",
      "total_backward_count 724460 real_backward_count 60377   8.334%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.508287/  1.679817, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8986%\n",
      "layer   2  Sparsity: 72.8279%\n",
      "layer   3  Sparsity: 72.0213%\n",
      "total_backward_count 734250 real_backward_count 60738   8.272%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.512515/  1.669672, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8861%\n",
      "layer   2  Sparsity: 72.8594%\n",
      "layer   3  Sparsity: 71.9178%\n",
      "total_backward_count 744040 real_backward_count 61109   8.213%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.504541/  1.670750, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9370%\n",
      "layer   2  Sparsity: 73.0837%\n",
      "layer   3  Sparsity: 72.0299%\n",
      "total_backward_count 753830 real_backward_count 61423   8.148%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.500995/  1.680015, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9182%\n",
      "layer   2  Sparsity: 73.2506%\n",
      "layer   3  Sparsity: 71.8948%\n",
      "total_backward_count 763620 real_backward_count 61790   8.092%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.494357/  1.672889, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9424%\n",
      "layer   2  Sparsity: 73.3422%\n",
      "layer   3  Sparsity: 71.8250%\n",
      "total_backward_count 773410 real_backward_count 62128   8.033%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.493950/  1.666026, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9021%\n",
      "layer   2  Sparsity: 73.4867%\n",
      "layer   3  Sparsity: 72.0958%\n",
      "total_backward_count 783200 real_backward_count 62428   7.971%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.491854/  1.652474, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9190%\n",
      "layer   2  Sparsity: 73.1290%\n",
      "layer   3  Sparsity: 72.1072%\n",
      "total_backward_count 792990 real_backward_count 62733   7.911%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.496121/  1.664179, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9316%\n",
      "layer   2  Sparsity: 73.2444%\n",
      "layer   3  Sparsity: 72.3005%\n",
      "total_backward_count 802780 real_backward_count 63008   7.849%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.481378/  1.652811, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9199%\n",
      "layer   2  Sparsity: 73.6212%\n",
      "layer   3  Sparsity: 71.6435%\n",
      "total_backward_count 812570 real_backward_count 63276   7.787%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.487347/  1.661186, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9220%\n",
      "layer   2  Sparsity: 73.4121%\n",
      "layer   3  Sparsity: 71.6544%\n",
      "total_backward_count 822360 real_backward_count 63557   7.729%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.479013/  1.646633, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 73.4685%\n",
      "layer   3  Sparsity: 71.7750%\n",
      "total_backward_count 832150 real_backward_count 63828   7.670%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.478669/  1.649902, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9449%\n",
      "layer   2  Sparsity: 73.3720%\n",
      "layer   3  Sparsity: 72.0204%\n",
      "total_backward_count 841940 real_backward_count 64124   7.616%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.471585/  1.641964, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8963%\n",
      "layer   2  Sparsity: 73.3906%\n",
      "layer   3  Sparsity: 71.9670%\n",
      "total_backward_count 851730 real_backward_count 64393   7.560%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.469937/  1.636338, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9393%\n",
      "layer   2  Sparsity: 73.1992%\n",
      "layer   3  Sparsity: 72.0893%\n",
      "total_backward_count 861520 real_backward_count 64666   7.506%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.465932/  1.639220, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9456%\n",
      "layer   2  Sparsity: 73.1387%\n",
      "layer   3  Sparsity: 71.7585%\n",
      "total_backward_count 871310 real_backward_count 64950   7.454%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.471061/  1.652052, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9218%\n",
      "layer   2  Sparsity: 72.9327%\n",
      "layer   3  Sparsity: 72.0048%\n",
      "total_backward_count 881100 real_backward_count 65196   7.399%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.475262/  1.650450, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9225%\n",
      "layer   2  Sparsity: 72.8104%\n",
      "layer   3  Sparsity: 72.1861%\n",
      "total_backward_count 890890 real_backward_count 65436   7.345%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.482790/  1.653207, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8826%\n",
      "layer   2  Sparsity: 72.8825%\n",
      "layer   3  Sparsity: 71.8253%\n",
      "total_backward_count 900680 real_backward_count 65693   7.294%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.477288/  1.645264, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9165%\n",
      "layer   2  Sparsity: 73.0246%\n",
      "layer   3  Sparsity: 72.1546%\n",
      "total_backward_count 910470 real_backward_count 65952   7.244%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.475753/  1.657532, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 72.8397%\n",
      "layer   3  Sparsity: 72.1815%\n",
      "total_backward_count 920260 real_backward_count 66178   7.191%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.469964/  1.646261, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9118%\n",
      "layer   2  Sparsity: 72.7607%\n",
      "layer   3  Sparsity: 71.9046%\n",
      "total_backward_count 930050 real_backward_count 66403   7.140%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.458555/  1.638464, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8824%\n",
      "layer   2  Sparsity: 72.9123%\n",
      "layer   3  Sparsity: 72.1196%\n",
      "total_backward_count 939840 real_backward_count 66625   7.089%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.453154/  1.625239, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8978%\n",
      "layer   2  Sparsity: 72.9945%\n",
      "layer   3  Sparsity: 72.3422%\n",
      "total_backward_count 949630 real_backward_count 66860   7.041%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.445554/  1.620751, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 87.9068%\n",
      "layer   2  Sparsity: 73.2591%\n",
      "layer   3  Sparsity: 72.5227%\n",
      "total_backward_count 959420 real_backward_count 67073   6.991%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.444836/  1.634065, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8816%\n",
      "layer   2  Sparsity: 73.1499%\n",
      "layer   3  Sparsity: 71.5792%\n",
      "total_backward_count 969210 real_backward_count 67296   6.943%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.447110/  1.620921, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9152%\n",
      "layer   2  Sparsity: 73.1935%\n",
      "layer   3  Sparsity: 71.4274%\n",
      "total_backward_count 979000 real_backward_count 67527   6.898%\n",
      "fc layer 3 self.abs_max_out: 963.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.440013/  1.624645, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9453%\n",
      "layer   2  Sparsity: 73.2326%\n",
      "layer   3  Sparsity: 71.9016%\n",
      "total_backward_count 988790 real_backward_count 67744   6.851%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.446166/  1.630517, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9125%\n",
      "layer   2  Sparsity: 73.2447%\n",
      "layer   3  Sparsity: 71.9210%\n",
      "total_backward_count 998580 real_backward_count 67968   6.806%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.439025/  1.615657, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9186%\n",
      "layer   2  Sparsity: 73.3743%\n",
      "layer   3  Sparsity: 72.1834%\n",
      "total_backward_count 1008370 real_backward_count 68155   6.759%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.427352/  1.612680, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9054%\n",
      "layer   2  Sparsity: 73.5890%\n",
      "layer   3  Sparsity: 72.2774%\n",
      "total_backward_count 1018160 real_backward_count 68329   6.711%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.430441/  1.619175, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9300%\n",
      "layer   2  Sparsity: 73.4402%\n",
      "layer   3  Sparsity: 72.4414%\n",
      "total_backward_count 1027950 real_backward_count 68504   6.664%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.429017/  1.619076, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9371%\n",
      "layer   2  Sparsity: 72.9834%\n",
      "layer   3  Sparsity: 72.1183%\n",
      "total_backward_count 1037740 real_backward_count 68713   6.621%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.431718/  1.632591, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9051%\n",
      "layer   2  Sparsity: 73.1208%\n",
      "layer   3  Sparsity: 72.4023%\n",
      "total_backward_count 1047530 real_backward_count 68884   6.576%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.428949/  1.624891, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9208%\n",
      "layer   2  Sparsity: 73.2981%\n",
      "layer   3  Sparsity: 72.6240%\n",
      "total_backward_count 1057320 real_backward_count 69052   6.531%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.428161/  1.619562, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.8958%\n",
      "layer   2  Sparsity: 73.3677%\n",
      "layer   3  Sparsity: 72.9130%\n",
      "total_backward_count 1067110 real_backward_count 69216   6.486%\n",
      "fc layer 2 self.abs_max_out: 2521.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.434625/  1.633328, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9002%\n",
      "layer   2  Sparsity: 73.0683%\n",
      "layer   3  Sparsity: 72.9838%\n",
      "total_backward_count 1076900 real_backward_count 69387   6.443%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.437093/  1.624233, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9488%\n",
      "layer   2  Sparsity: 73.0388%\n",
      "layer   3  Sparsity: 72.9057%\n",
      "total_backward_count 1086690 real_backward_count 69572   6.402%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.437628/  1.622357, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8934%\n",
      "layer   2  Sparsity: 73.1789%\n",
      "layer   3  Sparsity: 72.8681%\n",
      "total_backward_count 1096480 real_backward_count 69725   6.359%\n",
      "lif layer 1 self.abs_max_v: 9622.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.436740/  1.610060, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9416%\n",
      "layer   2  Sparsity: 73.2614%\n",
      "layer   3  Sparsity: 72.8553%\n",
      "total_backward_count 1106270 real_backward_count 69900   6.319%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.432085/  1.616870, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9226%\n",
      "layer   2  Sparsity: 73.3934%\n",
      "layer   3  Sparsity: 73.0078%\n",
      "total_backward_count 1116060 real_backward_count 70061   6.278%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.434346/  1.630207, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 73.3659%\n",
      "layer   3  Sparsity: 73.1577%\n",
      "total_backward_count 1125850 real_backward_count 70219   6.237%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.431291/  1.604112, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8946%\n",
      "layer   2  Sparsity: 73.1445%\n",
      "layer   3  Sparsity: 72.9105%\n",
      "total_backward_count 1135640 real_backward_count 70365   6.196%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.421452/  1.605643, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9046%\n",
      "layer   2  Sparsity: 73.1892%\n",
      "layer   3  Sparsity: 72.6250%\n",
      "total_backward_count 1145430 real_backward_count 70515   6.156%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.415866/  1.612798, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9238%\n",
      "layer   2  Sparsity: 73.3046%\n",
      "layer   3  Sparsity: 72.8657%\n",
      "total_backward_count 1155220 real_backward_count 70639   6.115%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.416666/  1.618202, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9164%\n",
      "layer   2  Sparsity: 73.2183%\n",
      "layer   3  Sparsity: 73.3605%\n",
      "total_backward_count 1165010 real_backward_count 70766   6.074%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.417418/  1.622856, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9637%\n",
      "layer   2  Sparsity: 73.3614%\n",
      "layer   3  Sparsity: 73.3388%\n",
      "total_backward_count 1174800 real_backward_count 70909   6.036%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.424298/  1.607647, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9203%\n",
      "layer   2  Sparsity: 73.5977%\n",
      "layer   3  Sparsity: 73.0116%\n",
      "total_backward_count 1184590 real_backward_count 71036   5.997%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.416328/  1.617042, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9270%\n",
      "layer   2  Sparsity: 73.4573%\n",
      "layer   3  Sparsity: 72.4193%\n",
      "total_backward_count 1194380 real_backward_count 71235   5.964%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.414442/  1.609235, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9389%\n",
      "layer   2  Sparsity: 73.3401%\n",
      "layer   3  Sparsity: 72.2605%\n",
      "total_backward_count 1204170 real_backward_count 71410   5.930%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.412365/  1.600086, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9110%\n",
      "layer   2  Sparsity: 73.4022%\n",
      "layer   3  Sparsity: 72.3187%\n",
      "total_backward_count 1213960 real_backward_count 71525   5.892%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.404722/  1.600773, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 73.1835%\n",
      "layer   3  Sparsity: 72.6101%\n",
      "total_backward_count 1223750 real_backward_count 71671   5.857%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.406657/  1.598664, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9451%\n",
      "layer   2  Sparsity: 73.1941%\n",
      "layer   3  Sparsity: 72.4240%\n",
      "total_backward_count 1233540 real_backward_count 71787   5.820%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.406389/  1.600946, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9661%\n",
      "layer   2  Sparsity: 73.2521%\n",
      "layer   3  Sparsity: 72.4977%\n",
      "total_backward_count 1243330 real_backward_count 71934   5.786%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.402252/  1.599865, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9169%\n",
      "layer   2  Sparsity: 73.1732%\n",
      "layer   3  Sparsity: 73.0654%\n",
      "total_backward_count 1253120 real_backward_count 72034   5.748%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.393341/  1.589925, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9431%\n",
      "layer   2  Sparsity: 73.3730%\n",
      "layer   3  Sparsity: 73.0206%\n",
      "total_backward_count 1262910 real_backward_count 72183   5.716%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.391525/  1.593254, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9023%\n",
      "layer   2  Sparsity: 73.3973%\n",
      "layer   3  Sparsity: 73.3035%\n",
      "total_backward_count 1272700 real_backward_count 72315   5.682%\n",
      "fc layer 2 self.abs_max_out: 2658.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.397614/  1.588902, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9455%\n",
      "layer   2  Sparsity: 73.2872%\n",
      "layer   3  Sparsity: 72.9546%\n",
      "total_backward_count 1282490 real_backward_count 72438   5.648%\n",
      "fc layer 3 self.abs_max_out: 965.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.391562/  1.588782, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9222%\n",
      "layer   2  Sparsity: 73.5393%\n",
      "layer   3  Sparsity: 72.8030%\n",
      "total_backward_count 1292280 real_backward_count 72558   5.615%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.392293/  1.601080, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9112%\n",
      "layer   2  Sparsity: 73.5797%\n",
      "layer   3  Sparsity: 72.9130%\n",
      "total_backward_count 1302070 real_backward_count 72668   5.581%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.390036/  1.601274, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9221%\n",
      "layer   2  Sparsity: 73.6087%\n",
      "layer   3  Sparsity: 72.9626%\n",
      "total_backward_count 1311860 real_backward_count 72765   5.547%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.389137/  1.594046, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9441%\n",
      "layer   2  Sparsity: 73.5239%\n",
      "layer   3  Sparsity: 72.6863%\n",
      "total_backward_count 1321650 real_backward_count 72865   5.513%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.391363/  1.581679, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9201%\n",
      "layer   2  Sparsity: 73.5025%\n",
      "layer   3  Sparsity: 72.8248%\n",
      "total_backward_count 1331440 real_backward_count 72974   5.481%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.387501/  1.584429, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9308%\n",
      "layer   2  Sparsity: 73.6255%\n",
      "layer   3  Sparsity: 72.8770%\n",
      "total_backward_count 1341230 real_backward_count 73075   5.448%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.387267/  1.581510, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9331%\n",
      "layer   2  Sparsity: 73.5597%\n",
      "layer   3  Sparsity: 72.8265%\n",
      "total_backward_count 1351020 real_backward_count 73171   5.416%\n",
      "fc layer 3 self.abs_max_out: 970.0\n",
      "fc layer 3 self.abs_max_out: 991.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.388109/  1.596335, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 73.3445%\n",
      "layer   3  Sparsity: 73.3194%\n",
      "total_backward_count 1360810 real_backward_count 73279   5.385%\n",
      "fc layer 2 self.abs_max_out: 2668.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.392500/  1.581998, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9294%\n",
      "layer   2  Sparsity: 73.4264%\n",
      "layer   3  Sparsity: 73.2815%\n",
      "total_backward_count 1370600 real_backward_count 73373   5.353%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.383402/  1.600038, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9078%\n",
      "layer   2  Sparsity: 73.4218%\n",
      "layer   3  Sparsity: 73.5758%\n",
      "total_backward_count 1380390 real_backward_count 73461   5.322%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.387537/  1.587695, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9458%\n",
      "layer   2  Sparsity: 73.4150%\n",
      "layer   3  Sparsity: 73.5705%\n",
      "total_backward_count 1390180 real_backward_count 73527   5.289%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.381284/  1.581784, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9376%\n",
      "layer   2  Sparsity: 73.2284%\n",
      "layer   3  Sparsity: 73.3534%\n",
      "total_backward_count 1399970 real_backward_count 73606   5.258%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.385006/  1.587790, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8808%\n",
      "layer   2  Sparsity: 73.2450%\n",
      "layer   3  Sparsity: 73.4267%\n",
      "total_backward_count 1409760 real_backward_count 73694   5.227%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.382397/  1.575455, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9289%\n",
      "layer   2  Sparsity: 73.2167%\n",
      "layer   3  Sparsity: 73.4183%\n",
      "total_backward_count 1419550 real_backward_count 73778   5.197%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.375609/  1.572944, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9198%\n",
      "layer   2  Sparsity: 73.5537%\n",
      "layer   3  Sparsity: 73.5966%\n",
      "total_backward_count 1429340 real_backward_count 73850   5.167%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.379494/  1.580336, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9191%\n",
      "layer   2  Sparsity: 73.3558%\n",
      "layer   3  Sparsity: 73.9094%\n",
      "total_backward_count 1439130 real_backward_count 73929   5.137%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.381582/  1.578334, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9180%\n",
      "layer   2  Sparsity: 73.5076%\n",
      "layer   3  Sparsity: 73.9315%\n",
      "total_backward_count 1448920 real_backward_count 73997   5.107%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.378937/  1.584996, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9117%\n",
      "layer   2  Sparsity: 73.3587%\n",
      "layer   3  Sparsity: 73.9072%\n",
      "total_backward_count 1458710 real_backward_count 74084   5.079%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.377956/  1.586191, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9204%\n",
      "layer   2  Sparsity: 73.4639%\n",
      "layer   3  Sparsity: 74.1328%\n",
      "total_backward_count 1468500 real_backward_count 74161   5.050%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.377437/  1.586200, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9235%\n",
      "layer   2  Sparsity: 73.3213%\n",
      "layer   3  Sparsity: 73.9300%\n",
      "total_backward_count 1478290 real_backward_count 74224   5.021%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.374307/  1.580484, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 73.4773%\n",
      "layer   3  Sparsity: 73.8801%\n",
      "total_backward_count 1488080 real_backward_count 74308   4.994%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.374678/  1.580910, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9170%\n",
      "layer   2  Sparsity: 73.4704%\n",
      "layer   3  Sparsity: 74.2924%\n",
      "total_backward_count 1497870 real_backward_count 74371   4.965%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.370818/  1.573926, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9245%\n",
      "layer   2  Sparsity: 73.6939%\n",
      "layer   3  Sparsity: 73.8839%\n",
      "total_backward_count 1507660 real_backward_count 74477   4.940%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.367959/  1.581195, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9502%\n",
      "layer   2  Sparsity: 73.7808%\n",
      "layer   3  Sparsity: 73.9728%\n",
      "total_backward_count 1517450 real_backward_count 74554   4.913%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.362425/  1.571058, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8933%\n",
      "layer   2  Sparsity: 73.5474%\n",
      "layer   3  Sparsity: 73.8095%\n",
      "total_backward_count 1527240 real_backward_count 74641   4.887%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.364478/  1.573935, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9306%\n",
      "layer   2  Sparsity: 73.6705%\n",
      "layer   3  Sparsity: 73.9241%\n",
      "total_backward_count 1537030 real_backward_count 74707   4.860%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.362176/  1.568162, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8926%\n",
      "layer   2  Sparsity: 73.7015%\n",
      "layer   3  Sparsity: 74.0141%\n",
      "total_backward_count 1546820 real_backward_count 74777   4.834%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.360325/  1.566073, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9280%\n",
      "layer   2  Sparsity: 73.5678%\n",
      "layer   3  Sparsity: 73.7505%\n",
      "total_backward_count 1556610 real_backward_count 74851   4.809%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.360345/  1.579939, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9141%\n",
      "layer   2  Sparsity: 73.4136%\n",
      "layer   3  Sparsity: 73.5374%\n",
      "total_backward_count 1566400 real_backward_count 74932   4.784%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.368054/  1.578657, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9265%\n",
      "layer   2  Sparsity: 73.3071%\n",
      "layer   3  Sparsity: 73.4103%\n",
      "total_backward_count 1576190 real_backward_count 75014   4.759%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.365428/  1.577837, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8972%\n",
      "layer   2  Sparsity: 73.5308%\n",
      "layer   3  Sparsity: 73.4972%\n",
      "total_backward_count 1585980 real_backward_count 75088   4.734%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.370316/  1.569663, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9236%\n",
      "layer   2  Sparsity: 73.6382%\n",
      "layer   3  Sparsity: 73.9160%\n",
      "total_backward_count 1595770 real_backward_count 75167   4.710%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.368743/  1.570019, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9259%\n",
      "layer   2  Sparsity: 73.5089%\n",
      "layer   3  Sparsity: 74.0489%\n",
      "total_backward_count 1605560 real_backward_count 75239   4.686%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.370483/  1.571853, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8838%\n",
      "layer   2  Sparsity: 73.6276%\n",
      "layer   3  Sparsity: 74.0633%\n",
      "total_backward_count 1615350 real_backward_count 75309   4.662%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.363511/  1.578084, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.8980%\n",
      "layer   2  Sparsity: 73.7713%\n",
      "layer   3  Sparsity: 73.9594%\n",
      "total_backward_count 1625140 real_backward_count 75364   4.637%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.362140/  1.563090, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 73.7379%\n",
      "layer   3  Sparsity: 73.6725%\n",
      "total_backward_count 1634930 real_backward_count 75454   4.615%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.364773/  1.566886, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9183%\n",
      "layer   2  Sparsity: 73.6297%\n",
      "layer   3  Sparsity: 73.4319%\n",
      "total_backward_count 1644720 real_backward_count 75521   4.592%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.363177/  1.575718, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 87.9592%\n",
      "layer   2  Sparsity: 73.5883%\n",
      "layer   3  Sparsity: 73.6475%\n",
      "total_backward_count 1654510 real_backward_count 75605   4.570%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.362183/  1.573911, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9400%\n",
      "layer   2  Sparsity: 73.5202%\n",
      "layer   3  Sparsity: 73.8800%\n",
      "total_backward_count 1664300 real_backward_count 75671   4.547%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.360590/  1.576745, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9351%\n",
      "layer   2  Sparsity: 73.4224%\n",
      "layer   3  Sparsity: 74.1261%\n",
      "total_backward_count 1674090 real_backward_count 75711   4.523%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.359761/  1.571388, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9583%\n",
      "layer   2  Sparsity: 73.2685%\n",
      "layer   3  Sparsity: 74.0040%\n",
      "total_backward_count 1683880 real_backward_count 75762   4.499%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.354178/  1.570938, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9464%\n",
      "layer   2  Sparsity: 73.3610%\n",
      "layer   3  Sparsity: 73.7399%\n",
      "total_backward_count 1693670 real_backward_count 75826   4.477%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.349935/  1.558330, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9307%\n",
      "layer   2  Sparsity: 73.4551%\n",
      "layer   3  Sparsity: 73.5930%\n",
      "total_backward_count 1703460 real_backward_count 75888   4.455%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.350304/  1.557498, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9328%\n",
      "layer   2  Sparsity: 73.2003%\n",
      "layer   3  Sparsity: 73.5608%\n",
      "total_backward_count 1713250 real_backward_count 75954   4.433%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.356845/  1.557374, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8885%\n",
      "layer   2  Sparsity: 73.2495%\n",
      "layer   3  Sparsity: 73.6662%\n",
      "total_backward_count 1723040 real_backward_count 76008   4.411%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.350679/  1.564644, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9290%\n",
      "layer   2  Sparsity: 73.3506%\n",
      "layer   3  Sparsity: 73.6434%\n",
      "total_backward_count 1732830 real_backward_count 76073   4.390%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.358641/  1.572978, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9033%\n",
      "layer   2  Sparsity: 73.3752%\n",
      "layer   3  Sparsity: 73.8457%\n",
      "total_backward_count 1742620 real_backward_count 76138   4.369%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.365706/  1.574396, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9364%\n",
      "layer   2  Sparsity: 73.3380%\n",
      "layer   3  Sparsity: 73.8813%\n",
      "total_backward_count 1752410 real_backward_count 76199   4.348%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.365220/  1.584037, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9329%\n",
      "layer   2  Sparsity: 73.3899%\n",
      "layer   3  Sparsity: 73.8188%\n",
      "total_backward_count 1762200 real_backward_count 76262   4.328%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.357079/  1.572125, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9406%\n",
      "layer   2  Sparsity: 73.5312%\n",
      "layer   3  Sparsity: 73.9740%\n",
      "total_backward_count 1771990 real_backward_count 76338   4.308%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.359370/  1.575235, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9135%\n",
      "layer   2  Sparsity: 73.5852%\n",
      "layer   3  Sparsity: 73.6217%\n",
      "total_backward_count 1781780 real_backward_count 76398   4.288%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.361752/  1.583532, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9184%\n",
      "layer   2  Sparsity: 73.3584%\n",
      "layer   3  Sparsity: 73.7822%\n",
      "total_backward_count 1791570 real_backward_count 76453   4.267%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.359454/  1.571313, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9134%\n",
      "layer   2  Sparsity: 73.7753%\n",
      "layer   3  Sparsity: 74.0521%\n",
      "total_backward_count 1801360 real_backward_count 76519   4.248%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.354992/  1.573207, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9368%\n",
      "layer   2  Sparsity: 73.8472%\n",
      "layer   3  Sparsity: 73.9614%\n",
      "total_backward_count 1811150 real_backward_count 76572   4.228%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.357191/  1.573229, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9151%\n",
      "layer   2  Sparsity: 73.7282%\n",
      "layer   3  Sparsity: 74.0142%\n",
      "total_backward_count 1820940 real_backward_count 76614   4.207%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.355404/  1.569945, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9224%\n",
      "layer   2  Sparsity: 73.8435%\n",
      "layer   3  Sparsity: 74.1657%\n",
      "total_backward_count 1830730 real_backward_count 76654   4.187%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.353724/  1.569089, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9323%\n",
      "layer   2  Sparsity: 73.6463%\n",
      "layer   3  Sparsity: 74.2532%\n",
      "total_backward_count 1840520 real_backward_count 76706   4.168%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.355135/  1.567948, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9251%\n",
      "layer   2  Sparsity: 73.5626%\n",
      "layer   3  Sparsity: 74.3835%\n",
      "total_backward_count 1850310 real_backward_count 76758   4.148%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.354005/  1.574716, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9298%\n",
      "layer   2  Sparsity: 73.4294%\n",
      "layer   3  Sparsity: 74.2936%\n",
      "total_backward_count 1860100 real_backward_count 76802   4.129%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.355628/  1.568260, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9260%\n",
      "layer   2  Sparsity: 73.5767%\n",
      "layer   3  Sparsity: 74.1518%\n",
      "total_backward_count 1869890 real_backward_count 76856   4.110%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.358332/  1.570054, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 87.9039%\n",
      "layer   2  Sparsity: 73.5397%\n",
      "layer   3  Sparsity: 73.8436%\n",
      "total_backward_count 1879680 real_backward_count 76922   4.092%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.352600/  1.561200, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9403%\n",
      "layer   2  Sparsity: 73.5815%\n",
      "layer   3  Sparsity: 73.5395%\n",
      "total_backward_count 1889470 real_backward_count 76993   4.075%\n",
      "lif layer 1 self.abs_max_v: 9651.5\n",
      "fc layer 1 self.abs_max_out: 6201.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.354460/  1.571979, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9115%\n",
      "layer   2  Sparsity: 73.6195%\n",
      "layer   3  Sparsity: 73.8011%\n",
      "total_backward_count 1899260 real_backward_count 77045   4.057%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.356928/  1.566802, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9172%\n",
      "layer   2  Sparsity: 73.5316%\n",
      "layer   3  Sparsity: 73.7367%\n",
      "total_backward_count 1909050 real_backward_count 77121   4.040%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.356470/  1.573710, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9042%\n",
      "layer   2  Sparsity: 73.4755%\n",
      "layer   3  Sparsity: 73.8454%\n",
      "total_backward_count 1918840 real_backward_count 77166   4.021%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.354535/  1.564894, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9185%\n",
      "layer   2  Sparsity: 73.4630%\n",
      "layer   3  Sparsity: 74.0178%\n",
      "total_backward_count 1928630 real_backward_count 77217   4.004%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.352362/  1.569189, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9420%\n",
      "layer   2  Sparsity: 73.6776%\n",
      "layer   3  Sparsity: 73.9117%\n",
      "total_backward_count 1938420 real_backward_count 77250   3.985%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.351727/  1.565042, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.9119%\n",
      "layer   2  Sparsity: 73.5354%\n",
      "layer   3  Sparsity: 73.9052%\n",
      "total_backward_count 1948210 real_backward_count 77288   3.967%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.347761/  1.561473, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.9202%\n",
      "layer   2  Sparsity: 73.6076%\n",
      "layer   3  Sparsity: 73.7066%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2741b8c7576645e0a4adfb6a635503e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.34776</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>1.56147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-sweep-80</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9amhexb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e9amhexb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_180210-e9amhexb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kpbosxhf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_221951-kpbosxhf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kpbosxhf' target=\"_blank\">winter-sweep-86</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kpbosxhf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kpbosxhf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251115_222001_059', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 697.0\n",
      "lif layer 1 self.abs_max_v: 697.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "lif layer 2 self.abs_max_v: 872.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "lif layer 1 self.abs_max_v: 702.5\n",
      "fc layer 2 self.abs_max_out: 1354.0\n",
      "lif layer 2 self.abs_max_v: 1511.0\n",
      "lif layer 1 self.abs_max_v: 724.0\n",
      "lif layer 2 self.abs_max_v: 1839.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "lif layer 1 self.abs_max_v: 773.0\n",
      "lif layer 2 self.abs_max_v: 2118.0\n",
      "lif layer 1 self.abs_max_v: 875.5\n",
      "fc layer 1 self.abs_max_out: 871.0\n",
      "lif layer 1 self.abs_max_v: 992.0\n",
      "fc layer 2 self.abs_max_out: 1710.0\n",
      "lif layer 2 self.abs_max_v: 2166.0\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "fc layer 1 self.abs_max_out: 934.0\n",
      "lif layer 1 self.abs_max_v: 1106.0\n",
      "lif layer 2 self.abs_max_v: 2265.0\n",
      "fc layer 1 self.abs_max_out: 1090.0\n",
      "fc layer 3 self.abs_max_out: 968.0\n",
      "lif layer 1 self.abs_max_v: 1125.5\n",
      "fc layer 2 self.abs_max_out: 1867.0\n",
      "lif layer 2 self.abs_max_v: 2310.5\n",
      "fc layer 3 self.abs_max_out: 1179.0\n",
      "lif layer 1 self.abs_max_v: 1422.0\n",
      "lif layer 1 self.abs_max_v: 1669.5\n",
      "lif layer 2 self.abs_max_v: 2404.5\n",
      "fc layer 1 self.abs_max_out: 1190.0\n",
      "fc layer 2 self.abs_max_out: 2231.0\n",
      "lif layer 2 self.abs_max_v: 2941.0\n",
      "fc layer 1 self.abs_max_out: 1661.0\n",
      "lif layer 1 self.abs_max_v: 1862.0\n",
      "fc layer 1 self.abs_max_out: 1704.0\n",
      "lif layer 1 self.abs_max_v: 2002.0\n",
      "fc layer 3 self.abs_max_out: 1243.0\n",
      "lif layer 2 self.abs_max_v: 3073.0\n",
      "lif layer 2 self.abs_max_v: 3141.5\n",
      "lif layer 2 self.abs_max_v: 3192.5\n",
      "lif layer 2 self.abs_max_v: 3315.5\n",
      "lif layer 1 self.abs_max_v: 2153.5\n",
      "lif layer 2 self.abs_max_v: 3333.5\n",
      "lif layer 1 self.abs_max_v: 2183.0\n",
      "fc layer 2 self.abs_max_out: 2283.0\n",
      "lif layer 1 self.abs_max_v: 2300.5\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "lif layer 1 self.abs_max_v: 2397.0\n",
      "lif layer 2 self.abs_max_v: 3403.5\n",
      "lif layer 2 self.abs_max_v: 3812.0\n",
      "fc layer 2 self.abs_max_out: 2313.0\n",
      "fc layer 2 self.abs_max_out: 2391.0\n",
      "fc layer 1 self.abs_max_out: 2332.0\n",
      "lif layer 1 self.abs_max_v: 2554.5\n",
      "fc layer 1 self.abs_max_out: 2492.0\n",
      "lif layer 1 self.abs_max_v: 2582.0\n",
      "fc layer 1 self.abs_max_out: 2778.0\n",
      "lif layer 1 self.abs_max_v: 2778.0\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "fc layer 1 self.abs_max_out: 2943.0\n",
      "lif layer 1 self.abs_max_v: 3134.0\n",
      "fc layer 2 self.abs_max_out: 2555.0\n",
      "lif layer 2 self.abs_max_v: 4075.5\n",
      "fc layer 3 self.abs_max_out: 1245.0\n",
      "fc layer 1 self.abs_max_out: 3207.0\n",
      "lif layer 1 self.abs_max_v: 3207.0\n",
      "lif layer 2 self.abs_max_v: 4157.0\n",
      "fc layer 3 self.abs_max_out: 1305.0\n",
      "fc layer 3 self.abs_max_out: 1399.0\n",
      "lif layer 1 self.abs_max_v: 4042.0\n",
      "fc layer 2 self.abs_max_out: 2832.0\n",
      "lif layer 2 self.abs_max_v: 4164.5\n",
      "lif layer 2 self.abs_max_v: 4457.5\n",
      "fc layer 2 self.abs_max_out: 3020.0\n",
      "lif layer 2 self.abs_max_v: 4781.0\n",
      "lif layer 2 self.abs_max_v: 4909.5\n",
      "lif layer 2 self.abs_max_v: 5048.0\n",
      "lif layer 2 self.abs_max_v: 5081.0\n",
      "fc layer 2 self.abs_max_out: 3223.0\n",
      "lif layer 2 self.abs_max_v: 5726.0\n",
      "fc layer 1 self.abs_max_out: 3516.0\n",
      "lif layer 1 self.abs_max_v: 4157.5\n",
      "lif layer 1 self.abs_max_v: 4356.0\n",
      "lif layer 1 self.abs_max_v: 4420.5\n",
      "fc layer 2 self.abs_max_out: 3254.0\n",
      "fc layer 3 self.abs_max_out: 1451.0\n",
      "fc layer 3 self.abs_max_out: 1475.0\n",
      "fc layer 1 self.abs_max_out: 3595.0\n",
      "fc layer 1 self.abs_max_out: 3934.0\n",
      "fc layer 2 self.abs_max_out: 3258.0\n",
      "fc layer 3 self.abs_max_out: 1524.0\n",
      "fc layer 3 self.abs_max_out: 1588.0\n",
      "fc layer 3 self.abs_max_out: 1685.0\n",
      "fc layer 3 self.abs_max_out: 1773.0\n",
      "fc layer 3 self.abs_max_out: 1825.0\n",
      "fc layer 2 self.abs_max_out: 3280.0\n",
      "fc layer 2 self.abs_max_out: 3312.0\n",
      "fc layer 2 self.abs_max_out: 3437.0\n",
      "lif layer 1 self.abs_max_v: 4442.0\n",
      "fc layer 2 self.abs_max_out: 3682.0\n",
      "lif layer 1 self.abs_max_v: 4612.0\n",
      "fc layer 1 self.abs_max_out: 4016.0\n",
      "fc layer 1 self.abs_max_out: 4061.0\n",
      "fc layer 1 self.abs_max_out: 4481.0\n",
      "lif layer 1 self.abs_max_v: 4843.0\n",
      "lif layer 1 self.abs_max_v: 4918.5\n",
      "fc layer 2 self.abs_max_out: 3695.0\n",
      "fc layer 2 self.abs_max_out: 3713.0\n",
      "fc layer 2 self.abs_max_out: 3966.0\n",
      "lif layer 1 self.abs_max_v: 6055.0\n",
      "fc layer 1 self.abs_max_out: 4543.0\n",
      "fc layer 1 self.abs_max_out: 4879.0\n",
      "lif layer 2 self.abs_max_v: 5877.0\n",
      "lif layer 1 self.abs_max_v: 6163.5\n",
      "lif layer 1 self.abs_max_v: 6679.0\n",
      "lif layer 1 self.abs_max_v: 6830.5\n",
      "lif layer 1 self.abs_max_v: 7278.5\n",
      "lif layer 2 self.abs_max_v: 5911.0\n",
      "lif layer 2 self.abs_max_v: 5926.5\n",
      "lif layer 2 self.abs_max_v: 6200.0\n",
      "lif layer 2 self.abs_max_v: 6402.0\n",
      "fc layer 2 self.abs_max_out: 4039.0\n",
      "fc layer 1 self.abs_max_out: 5025.0\n",
      "lif layer 2 self.abs_max_v: 6506.5\n",
      "lif layer 2 self.abs_max_v: 6514.5\n",
      "lif layer 2 self.abs_max_v: 6649.5\n",
      "fc layer 1 self.abs_max_out: 5082.0\n",
      "lif layer 2 self.abs_max_v: 6810.0\n",
      "fc layer 2 self.abs_max_out: 4124.0\n",
      "lif layer 2 self.abs_max_v: 7173.5\n",
      "lif layer 2 self.abs_max_v: 7380.0\n",
      "lif layer 1 self.abs_max_v: 7897.0\n",
      "lif layer 1 self.abs_max_v: 8624.0\n",
      "lif layer 1 self.abs_max_v: 8723.0\n",
      "fc layer 1 self.abs_max_out: 5444.0\n",
      "lif layer 1 self.abs_max_v: 9626.5\n",
      "lif layer 1 self.abs_max_v: 9633.5\n",
      "fc layer 2 self.abs_max_out: 4181.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.693699/  1.962409, val:  32.08%, val_best:  32.08%, tr:  98.47%, tr_best:  98.47%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.1459%\n",
      "layer   2  Sparsity: 72.1302%\n",
      "layer   3  Sparsity: 65.0423%\n",
      "total_backward_count 9790 real_backward_count 1837  18.764%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 7389.5\n",
      "lif layer 2 self.abs_max_v: 7395.5\n",
      "lif layer 2 self.abs_max_v: 7434.0\n",
      "lif layer 2 self.abs_max_v: 7593.0\n",
      "lif layer 2 self.abs_max_v: 7709.0\n",
      "lif layer 2 self.abs_max_v: 7852.5\n",
      "lif layer 2 self.abs_max_v: 7925.5\n",
      "fc layer 2 self.abs_max_out: 4310.0\n",
      "fc layer 3 self.abs_max_out: 1895.0\n",
      "fc layer 1 self.abs_max_out: 5767.0\n",
      "lif layer 2 self.abs_max_v: 8136.0\n",
      "fc layer 1 self.abs_max_out: 5847.0\n",
      "fc layer 1 self.abs_max_out: 6187.0\n",
      "fc layer 2 self.abs_max_out: 4400.0\n",
      "fc layer 1 self.abs_max_out: 6203.0\n",
      "fc layer 2 self.abs_max_out: 4407.0\n",
      "fc layer 2 self.abs_max_out: 4547.0\n",
      "lif layer 1 self.abs_max_v: 10125.5\n",
      "lif layer 1 self.abs_max_v: 10248.0\n",
      "lif layer 1 self.abs_max_v: 10689.0\n",
      "lif layer 1 self.abs_max_v: 10929.5\n",
      "fc layer 2 self.abs_max_out: 4649.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.614513/  1.929618, val:  36.25%, val_best:  36.25%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1546%\n",
      "layer   2  Sparsity: 71.9339%\n",
      "layer   3  Sparsity: 64.3205%\n",
      "total_backward_count 19580 real_backward_count 3295  16.828%\n",
      "fc layer 2 self.abs_max_out: 4676.0\n",
      "fc layer 2 self.abs_max_out: 4699.0\n",
      "fc layer 2 self.abs_max_out: 4843.0\n",
      "fc layer 2 self.abs_max_out: 4996.0\n",
      "fc layer 2 self.abs_max_out: 5158.0\n",
      "fc layer 3 self.abs_max_out: 1944.0\n",
      "fc layer 2 self.abs_max_out: 5272.0\n",
      "fc layer 2 self.abs_max_out: 5397.0\n",
      "fc layer 1 self.abs_max_out: 6450.0\n",
      "fc layer 1 self.abs_max_out: 6559.0\n",
      "lif layer 1 self.abs_max_v: 11194.5\n",
      "fc layer 1 self.abs_max_out: 6639.0\n",
      "lif layer 1 self.abs_max_v: 12184.5\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.587862/  1.904280, val:  39.58%, val_best:  39.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1670%\n",
      "layer   2  Sparsity: 72.9242%\n",
      "layer   3  Sparsity: 63.5263%\n",
      "total_backward_count 29370 real_backward_count 4726  16.091%\n",
      "lif layer 2 self.abs_max_v: 8143.0\n",
      "lif layer 2 self.abs_max_v: 8332.5\n",
      "fc layer 3 self.abs_max_out: 1975.0\n",
      "fc layer 3 self.abs_max_out: 2016.0\n",
      "fc layer 1 self.abs_max_out: 7651.0\n",
      "lif layer 2 self.abs_max_v: 8344.5\n",
      "lif layer 2 self.abs_max_v: 8617.5\n",
      "fc layer 3 self.abs_max_out: 2027.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.565304/  1.861543, val:  42.92%, val_best:  42.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1543%\n",
      "layer   2  Sparsity: 72.8428%\n",
      "layer   3  Sparsity: 63.1949%\n",
      "total_backward_count 39160 real_backward_count 6077  15.518%\n",
      "fc layer 1 self.abs_max_out: 7812.0\n",
      "fc layer 3 self.abs_max_out: 2030.0\n",
      "fc layer 3 self.abs_max_out: 2069.0\n",
      "lif layer 1 self.abs_max_v: 12847.5\n",
      "fc layer 1 self.abs_max_out: 8008.0\n",
      "lif layer 1 self.abs_max_v: 14182.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.531796/  1.871728, val:  43.33%, val_best:  43.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1383%\n",
      "layer   2  Sparsity: 72.0584%\n",
      "layer   3  Sparsity: 62.5984%\n",
      "total_backward_count 48950 real_backward_count 7397  15.111%\n",
      "fc layer 3 self.abs_max_out: 2137.0\n",
      "lif layer 2 self.abs_max_v: 8649.5\n",
      "fc layer 1 self.abs_max_out: 8099.0\n",
      "fc layer 3 self.abs_max_out: 2156.0\n",
      "lif layer 1 self.abs_max_v: 14394.0\n",
      "fc layer 2 self.abs_max_out: 5398.0\n",
      "fc layer 2 self.abs_max_out: 5542.0\n",
      "fc layer 2 self.abs_max_out: 5587.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.514676/  1.848304, val:  55.42%, val_best:  55.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1483%\n",
      "layer   2  Sparsity: 72.3608%\n",
      "layer   3  Sparsity: 63.2670%\n",
      "total_backward_count 58740 real_backward_count 8717  14.840%\n",
      "fc layer 2 self.abs_max_out: 6131.0\n",
      "fc layer 3 self.abs_max_out: 2185.0\n",
      "fc layer 3 self.abs_max_out: 2288.0\n",
      "fc layer 3 self.abs_max_out: 2362.0\n",
      "fc layer 1 self.abs_max_out: 8581.0\n",
      "lif layer 1 self.abs_max_v: 15404.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.507600/  1.827445, val:  43.75%, val_best:  55.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1570%\n",
      "layer   2  Sparsity: 72.6476%\n",
      "layer   3  Sparsity: 63.7606%\n",
      "total_backward_count 68530 real_backward_count 9995  14.585%\n",
      "fc layer 1 self.abs_max_out: 8870.0\n",
      "lif layer 1 self.abs_max_v: 15978.5\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.499384/  1.797647, val:  56.25%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1528%\n",
      "layer   2  Sparsity: 71.8142%\n",
      "layer   3  Sparsity: 62.6280%\n",
      "total_backward_count 78320 real_backward_count 11215  14.319%\n",
      "fc layer 1 self.abs_max_out: 9295.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.472872/  1.774042, val:  52.08%, val_best:  56.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1518%\n",
      "layer   2  Sparsity: 71.5969%\n",
      "layer   3  Sparsity: 64.0506%\n",
      "total_backward_count 88110 real_backward_count 12493  14.179%\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.488064/  1.811430, val:  57.92%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1444%\n",
      "layer   2  Sparsity: 71.2431%\n",
      "layer   3  Sparsity: 65.7460%\n",
      "total_backward_count 97900 real_backward_count 13747  14.042%\n",
      "lif layer 2 self.abs_max_v: 9057.5\n",
      "lif layer 2 self.abs_max_v: 9450.5\n",
      "lif layer 2 self.abs_max_v: 9488.5\n",
      "fc layer 1 self.abs_max_out: 9450.0\n",
      "lif layer 1 self.abs_max_v: 16252.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.468057/  1.771583, val:  50.00%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1367%\n",
      "layer   2  Sparsity: 71.8581%\n",
      "layer   3  Sparsity: 66.0935%\n",
      "total_backward_count 107690 real_backward_count 15060  13.985%\n",
      "lif layer 2 self.abs_max_v: 9623.0\n",
      "lif layer 2 self.abs_max_v: 9987.5\n",
      "lif layer 2 self.abs_max_v: 10129.5\n",
      "fc layer 1 self.abs_max_out: 10077.0\n",
      "lif layer 1 self.abs_max_v: 16362.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.460535/  1.725513, val:  52.08%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1299%\n",
      "layer   2  Sparsity: 71.5537%\n",
      "layer   3  Sparsity: 65.5941%\n",
      "total_backward_count 117480 real_backward_count 16325  13.896%\n",
      "fc layer 3 self.abs_max_out: 2427.0\n",
      "fc layer 3 self.abs_max_out: 2468.0\n",
      "fc layer 3 self.abs_max_out: 2569.0\n",
      "lif layer 1 self.abs_max_v: 16389.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.423619/  1.758666, val:  49.17%, val_best:  57.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1443%\n",
      "layer   2  Sparsity: 71.5177%\n",
      "layer   3  Sparsity: 65.4989%\n",
      "total_backward_count 127270 real_backward_count 17443  13.706%\n",
      "fc layer 1 self.abs_max_out: 10312.0\n",
      "fc layer 3 self.abs_max_out: 2618.0\n",
      "lif layer 1 self.abs_max_v: 17702.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.460835/  1.795417, val:  40.83%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1187%\n",
      "layer   2  Sparsity: 71.3121%\n",
      "layer   3  Sparsity: 67.2206%\n",
      "total_backward_count 137060 real_backward_count 18653  13.609%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.458009/  1.708881, val:  47.50%, val_best:  57.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1255%\n",
      "layer   2  Sparsity: 71.6568%\n",
      "layer   3  Sparsity: 68.2901%\n",
      "total_backward_count 146850 real_backward_count 19786  13.474%\n",
      "lif layer 2 self.abs_max_v: 10542.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.444851/  1.699664, val:  50.42%, val_best:  57.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1265%\n",
      "layer   2  Sparsity: 71.2776%\n",
      "layer   3  Sparsity: 69.2259%\n",
      "total_backward_count 156640 real_backward_count 20930  13.362%\n",
      "fc layer 1 self.abs_max_out: 10716.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.451063/  1.707288, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1383%\n",
      "layer   2  Sparsity: 70.8703%\n",
      "layer   3  Sparsity: 68.7289%\n",
      "total_backward_count 166430 real_backward_count 22026  13.234%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.436040/  1.762112, val:  59.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1540%\n",
      "layer   2  Sparsity: 70.4437%\n",
      "layer   3  Sparsity: 67.9724%\n",
      "total_backward_count 176220 real_backward_count 23151  13.138%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.444430/  1.679627, val:  67.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1424%\n",
      "layer   2  Sparsity: 70.6217%\n",
      "layer   3  Sparsity: 68.8744%\n",
      "total_backward_count 186010 real_backward_count 24273  13.049%\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.433552/  1.755159, val:  41.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 70.4898%\n",
      "layer   3  Sparsity: 68.4176%\n",
      "total_backward_count 195800 real_backward_count 25329  12.936%\n",
      "lif layer 1 self.abs_max_v: 17877.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.440449/  1.756555, val:  49.17%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1615%\n",
      "layer   2  Sparsity: 70.4878%\n",
      "layer   3  Sparsity: 68.4449%\n",
      "total_backward_count 205590 real_backward_count 26414  12.848%\n",
      "fc layer 1 self.abs_max_out: 10823.0\n",
      "lif layer 1 self.abs_max_v: 18315.5\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.446999/  1.717110, val:  66.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1483%\n",
      "layer   2  Sparsity: 69.8208%\n",
      "layer   3  Sparsity: 68.1497%\n",
      "total_backward_count 215380 real_backward_count 27522  12.778%\n",
      "fc layer 1 self.abs_max_out: 10860.0\n",
      "lif layer 1 self.abs_max_v: 18434.5\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.429410/  1.676239, val:  62.92%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1396%\n",
      "layer   2  Sparsity: 69.8196%\n",
      "layer   3  Sparsity: 67.6352%\n",
      "total_backward_count 225170 real_backward_count 28620  12.710%\n",
      "fc layer 2 self.abs_max_out: 6267.0\n",
      "lif layer 1 self.abs_max_v: 18461.5\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.436583/  1.658608, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1523%\n",
      "layer   2  Sparsity: 70.0269%\n",
      "layer   3  Sparsity: 70.3187%\n",
      "total_backward_count 234960 real_backward_count 29670  12.628%\n",
      "lif layer 1 self.abs_max_v: 18720.5\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.425336/  1.672613, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1539%\n",
      "layer   2  Sparsity: 69.7933%\n",
      "layer   3  Sparsity: 68.9197%\n",
      "total_backward_count 244750 real_backward_count 30735  12.558%\n",
      "fc layer 1 self.abs_max_out: 11094.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.428073/  1.628537, val:  72.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1350%\n",
      "layer   2  Sparsity: 69.8868%\n",
      "layer   3  Sparsity: 68.3184%\n",
      "total_backward_count 254540 real_backward_count 31783  12.486%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.399543/  1.668420, val:  55.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1489%\n",
      "layer   2  Sparsity: 69.6019%\n",
      "layer   3  Sparsity: 67.7737%\n",
      "total_backward_count 264330 real_backward_count 32816  12.415%\n",
      "fc layer 1 self.abs_max_out: 11284.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.403912/  1.658077, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1414%\n",
      "layer   2  Sparsity: 69.8860%\n",
      "layer   3  Sparsity: 68.8092%\n",
      "total_backward_count 274120 real_backward_count 33791  12.327%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.419972/  1.711919, val:  62.50%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1453%\n",
      "layer   2  Sparsity: 70.1276%\n",
      "layer   3  Sparsity: 70.4362%\n",
      "total_backward_count 283910 real_backward_count 34801  12.258%\n",
      "lif layer 1 self.abs_max_v: 19289.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.435229/  1.671006, val:  67.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1285%\n",
      "layer   2  Sparsity: 69.5324%\n",
      "layer   3  Sparsity: 69.3583%\n",
      "total_backward_count 293700 real_backward_count 35761  12.176%\n",
      "lif layer 1 self.abs_max_v: 20344.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.399331/  1.696065, val:  68.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1313%\n",
      "layer   2  Sparsity: 69.7885%\n",
      "layer   3  Sparsity: 68.1214%\n",
      "total_backward_count 303490 real_backward_count 36744  12.107%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.424959/  1.697412, val:  45.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1351%\n",
      "layer   2  Sparsity: 69.9463%\n",
      "layer   3  Sparsity: 69.5305%\n",
      "total_backward_count 313280 real_backward_count 37748  12.049%\n",
      "fc layer 1 self.abs_max_out: 11289.0\n",
      "fc layer 1 self.abs_max_out: 11398.0\n",
      "lif layer 1 self.abs_max_v: 20790.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.393675/  1.691343, val:  66.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1375%\n",
      "layer   2  Sparsity: 69.9151%\n",
      "layer   3  Sparsity: 69.1010%\n",
      "total_backward_count 323070 real_backward_count 38730  11.988%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.396192/  1.672807, val:  66.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1465%\n",
      "layer   2  Sparsity: 69.9870%\n",
      "layer   3  Sparsity: 68.8228%\n",
      "total_backward_count 332860 real_backward_count 39691  11.924%\n",
      "fc layer 1 self.abs_max_out: 11617.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.390527/  1.647374, val:  69.58%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1241%\n",
      "layer   2  Sparsity: 69.7736%\n",
      "layer   3  Sparsity: 70.0632%\n",
      "total_backward_count 342650 real_backward_count 40641  11.861%\n",
      "fc layer 1 self.abs_max_out: 11643.0\n",
      "lif layer 1 self.abs_max_v: 20950.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.395503/  1.623492, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1396%\n",
      "layer   2  Sparsity: 69.4547%\n",
      "layer   3  Sparsity: 70.5132%\n",
      "total_backward_count 352440 real_backward_count 41586  11.799%\n",
      "fc layer 1 self.abs_max_out: 11676.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.376982/  1.643641, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1421%\n",
      "layer   2  Sparsity: 69.8469%\n",
      "layer   3  Sparsity: 69.9637%\n",
      "total_backward_count 362230 real_backward_count 42522  11.739%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.386626/  1.667755, val:  62.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1491%\n",
      "layer   2  Sparsity: 69.7927%\n",
      "layer   3  Sparsity: 70.5696%\n",
      "total_backward_count 372020 real_backward_count 43383  11.661%\n",
      "fc layer 1 self.abs_max_out: 11730.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.403024/  1.653049, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1671%\n",
      "layer   2  Sparsity: 69.5666%\n",
      "layer   3  Sparsity: 70.9327%\n",
      "total_backward_count 381810 real_backward_count 44265  11.593%\n",
      "fc layer 1 self.abs_max_out: 11791.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.381905/  1.692270, val:  60.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1501%\n",
      "layer   2  Sparsity: 69.5178%\n",
      "layer   3  Sparsity: 69.9464%\n",
      "total_backward_count 391600 real_backward_count 45171  11.535%\n",
      "fc layer 1 self.abs_max_out: 11807.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.398995/  1.654864, val:  60.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1510%\n",
      "layer   2  Sparsity: 69.3869%\n",
      "layer   3  Sparsity: 70.1050%\n",
      "total_backward_count 401390 real_backward_count 46101  11.485%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.395334/  1.705393, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1339%\n",
      "layer   2  Sparsity: 69.4961%\n",
      "layer   3  Sparsity: 70.9553%\n",
      "total_backward_count 411180 real_backward_count 47007  11.432%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.399058/  1.648321, val:  75.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1395%\n",
      "layer   2  Sparsity: 69.8677%\n",
      "layer   3  Sparsity: 71.2380%\n",
      "total_backward_count 420970 real_backward_count 47903  11.379%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.398969/  1.650831, val:  67.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1599%\n",
      "layer   2  Sparsity: 69.6486%\n",
      "layer   3  Sparsity: 70.8544%\n",
      "total_backward_count 430760 real_backward_count 48819  11.333%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.376448/  1.644190, val:  68.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1571%\n",
      "layer   2  Sparsity: 69.3884%\n",
      "layer   3  Sparsity: 69.9586%\n",
      "total_backward_count 440550 real_backward_count 49733  11.289%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.400809/  1.647151, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1518%\n",
      "layer   2  Sparsity: 69.3542%\n",
      "layer   3  Sparsity: 71.0596%\n",
      "total_backward_count 450340 real_backward_count 50651  11.247%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.387175/  1.659201, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1506%\n",
      "layer   2  Sparsity: 69.3324%\n",
      "layer   3  Sparsity: 70.1575%\n",
      "total_backward_count 460130 real_backward_count 51568  11.207%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.404266/  1.694399, val:  55.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1423%\n",
      "layer   2  Sparsity: 69.1941%\n",
      "layer   3  Sparsity: 70.7281%\n",
      "total_backward_count 469920 real_backward_count 52463  11.164%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.373123/  1.688025, val:  58.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1530%\n",
      "layer   2  Sparsity: 69.4606%\n",
      "layer   3  Sparsity: 70.5042%\n",
      "total_backward_count 479710 real_backward_count 53388  11.129%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.378947/  1.588136, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1364%\n",
      "layer   2  Sparsity: 69.6541%\n",
      "layer   3  Sparsity: 70.6023%\n",
      "total_backward_count 489500 real_backward_count 54218  11.076%\n",
      "lif layer 1 self.abs_max_v: 21301.5\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.388840/  1.641543, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 69.4428%\n",
      "layer   3  Sparsity: 71.4461%\n",
      "total_backward_count 499290 real_backward_count 55111  11.038%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.392265/  1.650057, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1263%\n",
      "layer   2  Sparsity: 69.3403%\n",
      "layer   3  Sparsity: 71.5388%\n",
      "total_backward_count 509080 real_backward_count 55956  10.992%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.372703/  1.633157, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1464%\n",
      "layer   2  Sparsity: 69.6328%\n",
      "layer   3  Sparsity: 70.8357%\n",
      "total_backward_count 518870 real_backward_count 56764  10.940%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.378932/  1.633438, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1439%\n",
      "layer   2  Sparsity: 69.5112%\n",
      "layer   3  Sparsity: 70.7742%\n",
      "total_backward_count 528660 real_backward_count 57642  10.903%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.384575/  1.654933, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1540%\n",
      "layer   2  Sparsity: 69.6172%\n",
      "layer   3  Sparsity: 70.2002%\n",
      "total_backward_count 538450 real_backward_count 58517  10.868%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.406878/  1.629173, val:  82.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1491%\n",
      "layer   2  Sparsity: 69.5089%\n",
      "layer   3  Sparsity: 71.3540%\n",
      "total_backward_count 548240 real_backward_count 59324  10.821%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.379445/  1.633984, val:  63.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1494%\n",
      "layer   2  Sparsity: 69.3109%\n",
      "layer   3  Sparsity: 70.2070%\n",
      "total_backward_count 558030 real_backward_count 60132  10.776%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.351724/  1.597412, val:  79.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1349%\n",
      "layer   2  Sparsity: 69.3815%\n",
      "layer   3  Sparsity: 69.0766%\n",
      "total_backward_count 567820 real_backward_count 60948  10.734%\n",
      "fc layer 1 self.abs_max_out: 11828.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.345071/  1.581720, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1354%\n",
      "layer   2  Sparsity: 69.2261%\n",
      "layer   3  Sparsity: 69.5321%\n",
      "total_backward_count 577610 real_backward_count 61761  10.693%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.356677/  1.606979, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1281%\n",
      "layer   2  Sparsity: 69.1538%\n",
      "layer   3  Sparsity: 69.8503%\n",
      "total_backward_count 587400 real_backward_count 62515  10.643%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.370467/  1.638108, val:  62.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1564%\n",
      "layer   2  Sparsity: 69.3328%\n",
      "layer   3  Sparsity: 70.1997%\n",
      "total_backward_count 597190 real_backward_count 63333  10.605%\n",
      "fc layer 1 self.abs_max_out: 11923.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.375323/  1.606248, val:  75.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1373%\n",
      "layer   2  Sparsity: 69.0240%\n",
      "layer   3  Sparsity: 69.9933%\n",
      "total_backward_count 606980 real_backward_count 64171  10.572%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.352540/  1.599285, val:  77.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1314%\n",
      "layer   2  Sparsity: 69.3298%\n",
      "layer   3  Sparsity: 70.8471%\n",
      "total_backward_count 616770 real_backward_count 64988  10.537%\n",
      "fc layer 1 self.abs_max_out: 11997.0\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.371059/  1.657418, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1441%\n",
      "layer   2  Sparsity: 68.9078%\n",
      "layer   3  Sparsity: 70.8205%\n",
      "total_backward_count 626560 real_backward_count 65807  10.503%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.376347/  1.629210, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1330%\n",
      "layer   2  Sparsity: 68.6117%\n",
      "layer   3  Sparsity: 70.5277%\n",
      "total_backward_count 636350 real_backward_count 66663  10.476%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.368416/  1.648717, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1358%\n",
      "layer   2  Sparsity: 68.9452%\n",
      "layer   3  Sparsity: 71.8112%\n",
      "total_backward_count 646140 real_backward_count 67497  10.446%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.393908/  1.657118, val:  67.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1424%\n",
      "layer   2  Sparsity: 69.1315%\n",
      "layer   3  Sparsity: 72.2804%\n",
      "total_backward_count 655930 real_backward_count 68320  10.416%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.397029/  1.600641, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1483%\n",
      "layer   2  Sparsity: 68.8656%\n",
      "layer   3  Sparsity: 71.9488%\n",
      "total_backward_count 665720 real_backward_count 69112  10.382%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.381612/  1.584371, val:  81.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1296%\n",
      "layer   2  Sparsity: 68.9335%\n",
      "layer   3  Sparsity: 72.0733%\n",
      "total_backward_count 675510 real_backward_count 69919  10.351%\n",
      "lif layer 1 self.abs_max_v: 21422.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.374796/  1.604405, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1562%\n",
      "layer   2  Sparsity: 69.0206%\n",
      "layer   3  Sparsity: 71.5557%\n",
      "total_backward_count 685300 real_backward_count 70685  10.314%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.361853/  1.601285, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1430%\n",
      "layer   2  Sparsity: 68.8637%\n",
      "layer   3  Sparsity: 70.4305%\n",
      "total_backward_count 695090 real_backward_count 71454  10.280%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.365841/  1.661924, val:  60.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1387%\n",
      "layer   2  Sparsity: 69.2181%\n",
      "layer   3  Sparsity: 71.3510%\n",
      "total_backward_count 704880 real_backward_count 72205  10.244%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.382249/  1.615065, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1447%\n",
      "layer   2  Sparsity: 69.1032%\n",
      "layer   3  Sparsity: 72.3876%\n",
      "total_backward_count 714670 real_backward_count 73060  10.223%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.409455/  1.670526, val:  65.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1507%\n",
      "layer   2  Sparsity: 68.8584%\n",
      "layer   3  Sparsity: 73.0066%\n",
      "total_backward_count 724460 real_backward_count 73879  10.198%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.408679/  1.652399, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1254%\n",
      "layer   2  Sparsity: 69.1228%\n",
      "layer   3  Sparsity: 72.5821%\n",
      "total_backward_count 734250 real_backward_count 74696  10.173%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.373546/  1.595295, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1420%\n",
      "layer   2  Sparsity: 68.9881%\n",
      "layer   3  Sparsity: 70.3878%\n",
      "total_backward_count 744040 real_backward_count 75464  10.142%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.362807/  1.565110, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1370%\n",
      "layer   2  Sparsity: 69.1976%\n",
      "layer   3  Sparsity: 72.0124%\n",
      "total_backward_count 753830 real_backward_count 76248  10.115%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.376533/  1.636332, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1549%\n",
      "layer   2  Sparsity: 69.1903%\n",
      "layer   3  Sparsity: 72.4201%\n",
      "total_backward_count 763620 real_backward_count 77059  10.091%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.367904/  1.613176, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1351%\n",
      "layer   2  Sparsity: 68.8909%\n",
      "layer   3  Sparsity: 71.9176%\n",
      "total_backward_count 773410 real_backward_count 77858  10.067%\n",
      "fc layer 1 self.abs_max_out: 12007.0\n",
      "lif layer 1 self.abs_max_v: 21844.5\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.343989/  1.618427, val:  68.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1521%\n",
      "layer   2  Sparsity: 68.8388%\n",
      "layer   3  Sparsity: 70.3669%\n",
      "total_backward_count 783200 real_backward_count 78641  10.041%\n",
      "fc layer 1 self.abs_max_out: 12311.0\n",
      "lif layer 1 self.abs_max_v: 22060.5\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.350873/  1.588475, val:  69.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1510%\n",
      "layer   2  Sparsity: 69.1436%\n",
      "layer   3  Sparsity: 71.2564%\n",
      "total_backward_count 792990 real_backward_count 79377  10.010%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.359898/  1.616120, val:  67.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1502%\n",
      "layer   2  Sparsity: 69.2715%\n",
      "layer   3  Sparsity: 70.5588%\n",
      "total_backward_count 802780 real_backward_count 80123   9.981%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.355491/  1.597570, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1397%\n",
      "layer   2  Sparsity: 69.1409%\n",
      "layer   3  Sparsity: 71.0320%\n",
      "total_backward_count 812570 real_backward_count 80968   9.964%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.350811/  1.617318, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1499%\n",
      "layer   2  Sparsity: 68.9814%\n",
      "layer   3  Sparsity: 71.2889%\n",
      "total_backward_count 822360 real_backward_count 81695   9.934%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.346859/  1.584378, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1375%\n",
      "layer   2  Sparsity: 68.9623%\n",
      "layer   3  Sparsity: 71.7770%\n",
      "total_backward_count 832150 real_backward_count 82448   9.908%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.356707/  1.647012, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1608%\n",
      "layer   2  Sparsity: 69.3471%\n",
      "layer   3  Sparsity: 72.1488%\n",
      "total_backward_count 841940 real_backward_count 83251   9.888%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.354505/  1.591402, val:  78.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1555%\n",
      "layer   2  Sparsity: 69.1902%\n",
      "layer   3  Sparsity: 70.7328%\n",
      "total_backward_count 851730 real_backward_count 83959   9.857%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.355095/  1.600583, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1463%\n",
      "layer   2  Sparsity: 69.3780%\n",
      "layer   3  Sparsity: 72.4668%\n",
      "total_backward_count 861520 real_backward_count 84714   9.833%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.363200/  1.579576, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1460%\n",
      "layer   2  Sparsity: 68.7936%\n",
      "layer   3  Sparsity: 71.6852%\n",
      "total_backward_count 871310 real_backward_count 85496   9.812%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.362835/  1.580095, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1420%\n",
      "layer   2  Sparsity: 68.8137%\n",
      "layer   3  Sparsity: 71.3180%\n",
      "total_backward_count 881100 real_backward_count 86303   9.795%\n",
      "lif layer 1 self.abs_max_v: 22152.5\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.363480/  1.571927, val:  79.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1614%\n",
      "layer   2  Sparsity: 69.1086%\n",
      "layer   3  Sparsity: 71.2537%\n",
      "total_backward_count 890890 real_backward_count 87058   9.772%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.323570/  1.577426, val:  68.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1448%\n",
      "layer   2  Sparsity: 69.0329%\n",
      "layer   3  Sparsity: 70.4800%\n",
      "total_backward_count 900680 real_backward_count 87838   9.752%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.328574/  1.532285, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1457%\n",
      "layer   2  Sparsity: 68.7913%\n",
      "layer   3  Sparsity: 70.1564%\n",
      "total_backward_count 910470 real_backward_count 88625   9.734%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.291166/  1.588616, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1377%\n",
      "layer   2  Sparsity: 69.3069%\n",
      "layer   3  Sparsity: 70.3437%\n",
      "total_backward_count 920260 real_backward_count 89375   9.712%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.311223/  1.549348, val:  70.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1374%\n",
      "layer   2  Sparsity: 69.4842%\n",
      "layer   3  Sparsity: 70.4532%\n",
      "total_backward_count 930050 real_backward_count 90148   9.693%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.331116/  1.556164, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1389%\n",
      "layer   2  Sparsity: 69.2917%\n",
      "layer   3  Sparsity: 72.2804%\n",
      "total_backward_count 939840 real_backward_count 90921   9.674%\n",
      "lif layer 1 self.abs_max_v: 22168.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.349361/  1.596199, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1515%\n",
      "layer   2  Sparsity: 68.9236%\n",
      "layer   3  Sparsity: 72.1877%\n",
      "total_backward_count 949630 real_backward_count 91696   9.656%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.384658/  1.589103, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1301%\n",
      "layer   2  Sparsity: 68.9924%\n",
      "layer   3  Sparsity: 74.2678%\n",
      "total_backward_count 959420 real_backward_count 92423   9.633%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.371277/  1.628039, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1332%\n",
      "layer   2  Sparsity: 69.2769%\n",
      "layer   3  Sparsity: 73.8465%\n",
      "total_backward_count 969210 real_backward_count 93162   9.612%\n",
      "lif layer 1 self.abs_max_v: 22473.0\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.349114/  1.599151, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1287%\n",
      "layer   2  Sparsity: 69.2297%\n",
      "layer   3  Sparsity: 71.6762%\n",
      "total_backward_count 979000 real_backward_count 93901   9.592%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.335953/  1.589275, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1440%\n",
      "layer   2  Sparsity: 69.0560%\n",
      "layer   3  Sparsity: 70.2616%\n",
      "total_backward_count 988790 real_backward_count 94586   9.566%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.338618/  1.570678, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1571%\n",
      "layer   2  Sparsity: 69.0348%\n",
      "layer   3  Sparsity: 71.4238%\n",
      "total_backward_count 998580 real_backward_count 95327   9.546%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.327848/  1.591357, val:  77.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1470%\n",
      "layer   2  Sparsity: 69.0502%\n",
      "layer   3  Sparsity: 70.3640%\n",
      "total_backward_count 1008370 real_backward_count 96095   9.530%\n",
      "lif layer 2 self.abs_max_v: 10556.5\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.326083/  1.529471, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 94.1296%\n",
      "layer   2  Sparsity: 68.9275%\n",
      "layer   3  Sparsity: 70.1351%\n",
      "total_backward_count 1018160 real_backward_count 96806   9.508%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.305050/  1.582097, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1357%\n",
      "layer   2  Sparsity: 69.0153%\n",
      "layer   3  Sparsity: 70.9466%\n",
      "total_backward_count 1027950 real_backward_count 97548   9.490%\n",
      "fc layer 1 self.abs_max_out: 12557.0\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.326134/  1.540144, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1387%\n",
      "layer   2  Sparsity: 69.3229%\n",
      "layer   3  Sparsity: 70.8219%\n",
      "total_backward_count 1037740 real_backward_count 98287   9.471%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.296276/  1.567453, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1653%\n",
      "layer   2  Sparsity: 69.2162%\n",
      "layer   3  Sparsity: 70.9306%\n",
      "total_backward_count 1047530 real_backward_count 99027   9.453%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.295527/  1.557840, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1307%\n",
      "layer   2  Sparsity: 68.9528%\n",
      "layer   3  Sparsity: 71.3464%\n",
      "total_backward_count 1057320 real_backward_count 99787   9.438%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.297804/  1.524119, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1475%\n",
      "layer   2  Sparsity: 69.0477%\n",
      "layer   3  Sparsity: 70.1616%\n",
      "total_backward_count 1067110 real_backward_count 100484   9.416%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.290152/  1.504998, val:  84.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1474%\n",
      "layer   2  Sparsity: 68.9563%\n",
      "layer   3  Sparsity: 70.5679%\n",
      "total_backward_count 1076900 real_backward_count 101216   9.399%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.296654/  1.564759, val:  73.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1394%\n",
      "layer   2  Sparsity: 68.7911%\n",
      "layer   3  Sparsity: 71.2865%\n",
      "total_backward_count 1086690 real_backward_count 101982   9.385%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.304633/  1.534966, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1400%\n",
      "layer   2  Sparsity: 68.8532%\n",
      "layer   3  Sparsity: 69.8108%\n",
      "total_backward_count 1096480 real_backward_count 102660   9.363%\n",
      "lif layer 2 self.abs_max_v: 10575.0\n",
      "lif layer 2 self.abs_max_v: 10667.5\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.298968/  1.521625, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1280%\n",
      "layer   2  Sparsity: 68.7452%\n",
      "layer   3  Sparsity: 69.9479%\n",
      "total_backward_count 1106270 real_backward_count 103382   9.345%\n",
      "lif layer 2 self.abs_max_v: 10809.0\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.294922/  1.589646, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1382%\n",
      "layer   2  Sparsity: 68.6959%\n",
      "layer   3  Sparsity: 70.6194%\n",
      "total_backward_count 1116060 real_backward_count 104093   9.327%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.311586/  1.552965, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1368%\n",
      "layer   2  Sparsity: 69.0174%\n",
      "layer   3  Sparsity: 69.7772%\n",
      "total_backward_count 1125850 real_backward_count 104823   9.311%\n",
      "lif layer 2 self.abs_max_v: 10819.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.288462/  1.540737, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1422%\n",
      "layer   2  Sparsity: 69.1976%\n",
      "layer   3  Sparsity: 69.5266%\n",
      "total_backward_count 1135640 real_backward_count 105589   9.298%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.288954/  1.541734, val:  71.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1403%\n",
      "layer   2  Sparsity: 69.6219%\n",
      "layer   3  Sparsity: 70.2783%\n",
      "total_backward_count 1145430 real_backward_count 106296   9.280%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.289430/  1.523597, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1440%\n",
      "layer   2  Sparsity: 69.1161%\n",
      "layer   3  Sparsity: 70.2904%\n",
      "total_backward_count 1155220 real_backward_count 107041   9.266%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.303640/  1.581683, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1527%\n",
      "layer   2  Sparsity: 68.8158%\n",
      "layer   3  Sparsity: 70.7221%\n",
      "total_backward_count 1165010 real_backward_count 107770   9.251%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.290901/  1.524609, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1428%\n",
      "layer   2  Sparsity: 68.8879%\n",
      "layer   3  Sparsity: 69.6003%\n",
      "total_backward_count 1174800 real_backward_count 108505   9.236%\n",
      "lif layer 2 self.abs_max_v: 10863.5\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.270587/  1.545313, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1561%\n",
      "layer   2  Sparsity: 68.7894%\n",
      "layer   3  Sparsity: 70.9729%\n",
      "total_backward_count 1184590 real_backward_count 109246   9.222%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.267046/  1.553017, val:  67.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1484%\n",
      "layer   2  Sparsity: 68.8327%\n",
      "layer   3  Sparsity: 70.0401%\n",
      "total_backward_count 1194380 real_backward_count 110000   9.210%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.283560/  1.506599, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1584%\n",
      "layer   2  Sparsity: 68.8104%\n",
      "layer   3  Sparsity: 71.7910%\n",
      "total_backward_count 1204170 real_backward_count 110722   9.195%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.270172/  1.480121, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1594%\n",
      "layer   2  Sparsity: 68.9927%\n",
      "layer   3  Sparsity: 71.4039%\n",
      "total_backward_count 1213960 real_backward_count 111413   9.178%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.258288/  1.526965, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1659%\n",
      "layer   2  Sparsity: 69.1512%\n",
      "layer   3  Sparsity: 70.3578%\n",
      "total_backward_count 1223750 real_backward_count 112122   9.162%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.259776/  1.507979, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1557%\n",
      "layer   2  Sparsity: 69.1230%\n",
      "layer   3  Sparsity: 70.3574%\n",
      "total_backward_count 1233540 real_backward_count 112801   9.144%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.272096/  1.520723, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1500%\n",
      "layer   2  Sparsity: 68.9309%\n",
      "layer   3  Sparsity: 71.1422%\n",
      "total_backward_count 1243330 real_backward_count 113504   9.129%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.290691/  1.610018, val:  73.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1258%\n",
      "layer   2  Sparsity: 69.0110%\n",
      "layer   3  Sparsity: 71.1507%\n",
      "total_backward_count 1253120 real_backward_count 114191   9.113%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.293143/  1.538237, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1486%\n",
      "layer   2  Sparsity: 68.8055%\n",
      "layer   3  Sparsity: 69.9797%\n",
      "total_backward_count 1262910 real_backward_count 114861   9.095%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.277907/  1.515898, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1501%\n",
      "layer   2  Sparsity: 68.9256%\n",
      "layer   3  Sparsity: 69.6766%\n",
      "total_backward_count 1272700 real_backward_count 115605   9.083%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.279277/  1.562976, val:  69.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1564%\n",
      "layer   2  Sparsity: 68.8244%\n",
      "layer   3  Sparsity: 70.1113%\n",
      "total_backward_count 1282490 real_backward_count 116258   9.065%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.273442/  1.544989, val:  73.33%, val_best:  84.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1390%\n",
      "layer   2  Sparsity: 68.7060%\n",
      "layer   3  Sparsity: 69.3021%\n",
      "total_backward_count 1292280 real_backward_count 116993   9.053%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.276224/  1.514994, val:  80.42%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1520%\n",
      "layer   2  Sparsity: 68.9139%\n",
      "layer   3  Sparsity: 71.2683%\n",
      "total_backward_count 1302070 real_backward_count 117646   9.035%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.304202/  1.575072, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1533%\n",
      "layer   2  Sparsity: 68.5648%\n",
      "layer   3  Sparsity: 71.5638%\n",
      "total_backward_count 1311860 real_backward_count 118337   9.021%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.331815/  1.549553, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1417%\n",
      "layer   2  Sparsity: 68.7502%\n",
      "layer   3  Sparsity: 71.3739%\n",
      "total_backward_count 1321650 real_backward_count 119070   9.009%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.323206/  1.519259, val:  77.92%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1406%\n",
      "layer   2  Sparsity: 69.0768%\n",
      "layer   3  Sparsity: 71.3682%\n",
      "total_backward_count 1331440 real_backward_count 119808   8.998%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.284722/  1.509723, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1238%\n",
      "layer   2  Sparsity: 68.7714%\n",
      "layer   3  Sparsity: 70.7916%\n",
      "total_backward_count 1341230 real_backward_count 120560   8.989%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.268165/  1.472453, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1273%\n",
      "layer   2  Sparsity: 69.1957%\n",
      "layer   3  Sparsity: 70.8445%\n",
      "total_backward_count 1351020 real_backward_count 121325   8.980%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.261033/  1.536134, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1523%\n",
      "layer   2  Sparsity: 69.1826%\n",
      "layer   3  Sparsity: 70.8040%\n",
      "total_backward_count 1360810 real_backward_count 122050   8.969%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.260637/  1.509432, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1323%\n",
      "layer   2  Sparsity: 69.5564%\n",
      "layer   3  Sparsity: 70.1779%\n",
      "total_backward_count 1370600 real_backward_count 122739   8.955%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.265480/  1.523783, val:  76.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1458%\n",
      "layer   2  Sparsity: 69.3226%\n",
      "layer   3  Sparsity: 71.0304%\n",
      "total_backward_count 1380390 real_backward_count 123422   8.941%\n",
      "fc layer 1 self.abs_max_out: 12663.0\n",
      "lif layer 1 self.abs_max_v: 22910.5\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.276794/  1.547415, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1482%\n",
      "layer   2  Sparsity: 69.2514%\n",
      "layer   3  Sparsity: 71.2993%\n",
      "total_backward_count 1390180 real_backward_count 124132   8.929%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.286296/  1.506950, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1504%\n",
      "layer   2  Sparsity: 69.1094%\n",
      "layer   3  Sparsity: 71.5066%\n",
      "total_backward_count 1399970 real_backward_count 124813   8.915%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.245026/  1.484951, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1470%\n",
      "layer   2  Sparsity: 69.0043%\n",
      "layer   3  Sparsity: 70.3428%\n",
      "total_backward_count 1409760 real_backward_count 125543   8.905%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.249621/  1.513001, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1242%\n",
      "layer   2  Sparsity: 69.3364%\n",
      "layer   3  Sparsity: 71.1500%\n",
      "total_backward_count 1419550 real_backward_count 126258   8.894%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.250757/  1.519560, val:  70.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1474%\n",
      "layer   2  Sparsity: 69.1219%\n",
      "layer   3  Sparsity: 70.6312%\n",
      "total_backward_count 1429340 real_backward_count 126925   8.880%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.268517/  1.549595, val:  76.67%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1265%\n",
      "layer   2  Sparsity: 69.2114%\n",
      "layer   3  Sparsity: 69.8347%\n",
      "total_backward_count 1439130 real_backward_count 127617   8.868%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.286055/  1.532221, val:  72.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1268%\n",
      "layer   2  Sparsity: 68.9912%\n",
      "layer   3  Sparsity: 71.2564%\n",
      "total_backward_count 1448920 real_backward_count 128337   8.857%\n",
      "lif layer 2 self.abs_max_v: 11026.5\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.268905/  1.495227, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1601%\n",
      "layer   2  Sparsity: 69.0847%\n",
      "layer   3  Sparsity: 69.7971%\n",
      "total_backward_count 1458710 real_backward_count 129041   8.846%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.270286/  1.507798, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1439%\n",
      "layer   2  Sparsity: 69.0586%\n",
      "layer   3  Sparsity: 69.7904%\n",
      "total_backward_count 1468500 real_backward_count 129755   8.836%\n",
      "fc layer 1 self.abs_max_out: 12706.0\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.267215/  1.525090, val:  72.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1414%\n",
      "layer   2  Sparsity: 69.2747%\n",
      "layer   3  Sparsity: 70.4539%\n",
      "total_backward_count 1478290 real_backward_count 130426   8.823%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.277615/  1.533784, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 69.2672%\n",
      "layer   3  Sparsity: 71.3525%\n",
      "total_backward_count 1488080 real_backward_count 131064   8.808%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.275389/  1.521043, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1378%\n",
      "layer   2  Sparsity: 69.1349%\n",
      "layer   3  Sparsity: 70.7562%\n",
      "total_backward_count 1497870 real_backward_count 131752   8.796%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.302748/  1.539635, val:  84.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1294%\n",
      "layer   2  Sparsity: 69.0188%\n",
      "layer   3  Sparsity: 71.2800%\n",
      "total_backward_count 1507660 real_backward_count 132469   8.786%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.283815/  1.527336, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1363%\n",
      "layer   2  Sparsity: 69.5126%\n",
      "layer   3  Sparsity: 70.5269%\n",
      "total_backward_count 1517450 real_backward_count 133114   8.772%\n",
      "fc layer 1 self.abs_max_out: 12813.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.260001/  1.533260, val:  76.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1538%\n",
      "layer   2  Sparsity: 69.2234%\n",
      "layer   3  Sparsity: 69.5640%\n",
      "total_backward_count 1527240 real_backward_count 133781   8.760%\n",
      "lif layer 2 self.abs_max_v: 11059.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.267741/  1.492151, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1209%\n",
      "layer   2  Sparsity: 69.2525%\n",
      "layer   3  Sparsity: 70.1375%\n",
      "total_backward_count 1537030 real_backward_count 134427   8.746%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.259872/  1.480544, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1751%\n",
      "layer   2  Sparsity: 69.4283%\n",
      "layer   3  Sparsity: 69.7738%\n",
      "total_backward_count 1546820 real_backward_count 135081   8.733%\n",
      "lif layer 2 self.abs_max_v: 11063.0\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.256099/  1.499647, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1462%\n",
      "layer   2  Sparsity: 69.3691%\n",
      "layer   3  Sparsity: 70.0633%\n",
      "total_backward_count 1556610 real_backward_count 135748   8.721%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.273109/  1.504776, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1576%\n",
      "layer   2  Sparsity: 69.5214%\n",
      "layer   3  Sparsity: 72.1464%\n",
      "total_backward_count 1566400 real_backward_count 136392   8.707%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.291103/  1.504148, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1480%\n",
      "layer   2  Sparsity: 69.3407%\n",
      "layer   3  Sparsity: 72.2766%\n",
      "total_backward_count 1576190 real_backward_count 137050   8.695%\n",
      "lif layer 1 self.abs_max_v: 23031.0\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.249498/  1.496919, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1408%\n",
      "layer   2  Sparsity: 69.2901%\n",
      "layer   3  Sparsity: 71.4952%\n",
      "total_backward_count 1585980 real_backward_count 137701   8.682%\n",
      "lif layer 2 self.abs_max_v: 11142.0\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.270179/  1.527940, val:  68.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1291%\n",
      "layer   2  Sparsity: 69.3682%\n",
      "layer   3  Sparsity: 72.4998%\n",
      "total_backward_count 1595770 real_backward_count 138398   8.673%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.285420/  1.539785, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1363%\n",
      "layer   2  Sparsity: 69.3290%\n",
      "layer   3  Sparsity: 72.3041%\n",
      "total_backward_count 1605560 real_backward_count 139044   8.660%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.272552/  1.530278, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1664%\n",
      "layer   2  Sparsity: 69.0866%\n",
      "layer   3  Sparsity: 70.8823%\n",
      "total_backward_count 1615350 real_backward_count 139699   8.648%\n",
      "fc layer 1 self.abs_max_out: 13324.0\n",
      "fc layer 1 self.abs_max_out: 13513.0\n",
      "lif layer 1 self.abs_max_v: 24769.0\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.256877/  1.481029, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1471%\n",
      "layer   2  Sparsity: 69.0815%\n",
      "layer   3  Sparsity: 70.6129%\n",
      "total_backward_count 1625140 real_backward_count 140376   8.638%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.244723/  1.523150, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1381%\n",
      "layer   2  Sparsity: 69.1816%\n",
      "layer   3  Sparsity: 68.9943%\n",
      "total_backward_count 1634930 real_backward_count 140990   8.624%\n",
      "lif layer 2 self.abs_max_v: 11160.0\n",
      "lif layer 2 self.abs_max_v: 11256.5\n",
      "lif layer 2 self.abs_max_v: 11423.5\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.256379/  1.501014, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1349%\n",
      "layer   2  Sparsity: 69.0836%\n",
      "layer   3  Sparsity: 69.3281%\n",
      "total_backward_count 1644720 real_backward_count 141676   8.614%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.282144/  1.507451, val:  78.75%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.1424%\n",
      "layer   2  Sparsity: 68.8370%\n",
      "layer   3  Sparsity: 69.9606%\n",
      "total_backward_count 1654510 real_backward_count 142347   8.604%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.236014/  1.505035, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1577%\n",
      "layer   2  Sparsity: 69.0684%\n",
      "layer   3  Sparsity: 70.8943%\n",
      "total_backward_count 1664300 real_backward_count 142990   8.592%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.242720/  1.552825, val:  66.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1283%\n",
      "layer   2  Sparsity: 69.0630%\n",
      "layer   3  Sparsity: 70.6931%\n",
      "total_backward_count 1674090 real_backward_count 143675   8.582%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.255575/  1.566262, val:  69.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1280%\n",
      "layer   2  Sparsity: 69.0529%\n",
      "layer   3  Sparsity: 71.4731%\n",
      "total_backward_count 1683880 real_backward_count 144317   8.571%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.256259/  1.506414, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1463%\n",
      "layer   2  Sparsity: 69.1030%\n",
      "layer   3  Sparsity: 70.7216%\n",
      "total_backward_count 1693670 real_backward_count 144955   8.559%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.245718/  1.479802, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1499%\n",
      "layer   2  Sparsity: 69.0919%\n",
      "layer   3  Sparsity: 70.5596%\n",
      "total_backward_count 1703460 real_backward_count 145614   8.548%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.248499/  1.498606, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1359%\n",
      "layer   2  Sparsity: 68.8266%\n",
      "layer   3  Sparsity: 70.0959%\n",
      "total_backward_count 1713250 real_backward_count 146263   8.537%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.226898/  1.477661, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1480%\n",
      "layer   2  Sparsity: 68.8419%\n",
      "layer   3  Sparsity: 70.8233%\n",
      "total_backward_count 1723040 real_backward_count 146910   8.526%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.252511/  1.491223, val:  71.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1503%\n",
      "layer   2  Sparsity: 69.0244%\n",
      "layer   3  Sparsity: 70.2455%\n",
      "total_backward_count 1732830 real_backward_count 147545   8.515%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.234017/  1.496602, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1360%\n",
      "layer   2  Sparsity: 69.0539%\n",
      "layer   3  Sparsity: 70.3661%\n",
      "total_backward_count 1742620 real_backward_count 148226   8.506%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.222009/  1.494670, val:  75.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1382%\n",
      "layer   2  Sparsity: 69.1160%\n",
      "layer   3  Sparsity: 69.4307%\n",
      "total_backward_count 1752410 real_backward_count 148898   8.497%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.219661/  1.491963, val:  78.33%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1268%\n",
      "layer   2  Sparsity: 69.2900%\n",
      "layer   3  Sparsity: 71.0581%\n",
      "total_backward_count 1762200 real_backward_count 149589   8.489%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.225357/  1.439750, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1410%\n",
      "layer   2  Sparsity: 69.2390%\n",
      "layer   3  Sparsity: 69.9172%\n",
      "total_backward_count 1771990 real_backward_count 150228   8.478%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.213688/  1.485925, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1261%\n",
      "layer   2  Sparsity: 68.8113%\n",
      "layer   3  Sparsity: 69.1537%\n",
      "total_backward_count 1781780 real_backward_count 150898   8.469%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.230380/  1.526060, val:  75.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1428%\n",
      "layer   2  Sparsity: 68.8533%\n",
      "layer   3  Sparsity: 68.8961%\n",
      "total_backward_count 1791570 real_backward_count 151525   8.458%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.234045/  1.525438, val:  67.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1275%\n",
      "layer   2  Sparsity: 69.0521%\n",
      "layer   3  Sparsity: 69.5030%\n",
      "total_backward_count 1801360 real_backward_count 152200   8.449%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.241373/  1.495801, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1502%\n",
      "layer   2  Sparsity: 68.9349%\n",
      "layer   3  Sparsity: 71.4201%\n",
      "total_backward_count 1811150 real_backward_count 152812   8.437%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.220821/  1.494205, val:  73.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1523%\n",
      "layer   2  Sparsity: 68.9194%\n",
      "layer   3  Sparsity: 69.6840%\n",
      "total_backward_count 1820940 real_backward_count 153464   8.428%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.205189/  1.477109, val:  70.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1422%\n",
      "layer   2  Sparsity: 69.1188%\n",
      "layer   3  Sparsity: 69.3930%\n",
      "total_backward_count 1830730 real_backward_count 154087   8.417%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.197185/  1.493101, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1480%\n",
      "layer   2  Sparsity: 69.1454%\n",
      "layer   3  Sparsity: 69.8785%\n",
      "total_backward_count 1840520 real_backward_count 154691   8.405%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.212168/  1.463690, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1542%\n",
      "layer   2  Sparsity: 69.1663%\n",
      "layer   3  Sparsity: 70.9924%\n",
      "total_backward_count 1850310 real_backward_count 155331   8.395%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.217116/  1.436408, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1358%\n",
      "layer   2  Sparsity: 68.9114%\n",
      "layer   3  Sparsity: 70.8610%\n",
      "total_backward_count 1860100 real_backward_count 155977   8.385%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.207776/  1.493246, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1449%\n",
      "layer   2  Sparsity: 69.1240%\n",
      "layer   3  Sparsity: 70.7039%\n",
      "total_backward_count 1869890 real_backward_count 156600   8.375%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.243220/  1.488547, val:  71.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1516%\n",
      "layer   2  Sparsity: 68.9819%\n",
      "layer   3  Sparsity: 70.0590%\n",
      "total_backward_count 1879680 real_backward_count 157291   8.368%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.231472/  1.487041, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1623%\n",
      "layer   2  Sparsity: 68.8098%\n",
      "layer   3  Sparsity: 69.8697%\n",
      "total_backward_count 1889470 real_backward_count 157940   8.359%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.242913/  1.460754, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1461%\n",
      "layer   2  Sparsity: 69.1525%\n",
      "layer   3  Sparsity: 70.1299%\n",
      "total_backward_count 1899260 real_backward_count 158569   8.349%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.242617/  1.446627, val:  79.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1399%\n",
      "layer   2  Sparsity: 69.2369%\n",
      "layer   3  Sparsity: 71.9661%\n",
      "total_backward_count 1909050 real_backward_count 159176   8.338%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.236027/  1.515020, val:  67.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1496%\n",
      "layer   2  Sparsity: 68.8526%\n",
      "layer   3  Sparsity: 70.4390%\n",
      "total_backward_count 1918840 real_backward_count 159841   8.330%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.271037/  1.539001, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1495%\n",
      "layer   2  Sparsity: 68.9904%\n",
      "layer   3  Sparsity: 72.4922%\n",
      "total_backward_count 1928630 real_backward_count 160518   8.323%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.260946/  1.472879, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.1349%\n",
      "layer   2  Sparsity: 68.8219%\n",
      "layer   3  Sparsity: 70.6704%\n",
      "total_backward_count 1938420 real_backward_count 161155   8.314%\n",
      "fc layer 3 self.abs_max_out: 2790.0\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.219167/  1.493013, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.1401%\n",
      "layer   2  Sparsity: 68.9258%\n",
      "layer   3  Sparsity: 69.1322%\n",
      "total_backward_count 1948210 real_backward_count 161804   8.305%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.261733/  1.562006, val:  74.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.1328%\n",
      "layer   2  Sparsity: 68.9614%\n",
      "layer   3  Sparsity: 70.5030%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3671b5b5d5ea41f0abb5b90a9af572e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñà‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñà‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.26173</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.74167</td></tr><tr><td>val_loss</td><td>1.56201</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-86</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kpbosxhf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/kpbosxhf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_221951-kpbosxhf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: obpfafc7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_023908-obpfafc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obpfafc7' target=\"_blank\">zany-sweep-92</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obpfafc7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obpfafc7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251116_023917_192', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 553.0\n",
      "lif layer 1 self.abs_max_v: 553.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 256.0\n",
      "lif layer 2 self.abs_max_v: 256.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "lif layer 1 self.abs_max_v: 791.5\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 1090.0\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "lif layer 2 self.abs_max_v: 1062.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 1 self.abs_max_out: 833.0\n",
      "lif layer 1 self.abs_max_v: 1183.5\n",
      "lif layer 1 self.abs_max_v: 1405.0\n",
      "fc layer 2 self.abs_max_out: 1105.0\n",
      "lif layer 2 self.abs_max_v: 1557.5\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 1 self.abs_max_out: 885.0\n",
      "lif layer 1 self.abs_max_v: 1486.5\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 1 self.abs_max_out: 1178.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "lif layer 2 self.abs_max_v: 1705.5\n",
      "fc layer 1 self.abs_max_out: 1247.0\n",
      "lif layer 1 self.abs_max_v: 1607.5\n",
      "fc layer 2 self.abs_max_out: 1484.0\n",
      "lif layer 2 self.abs_max_v: 2201.5\n",
      "fc layer 1 self.abs_max_out: 1585.0\n",
      "lif layer 1 self.abs_max_v: 1716.0\n",
      "lif layer 1 self.abs_max_v: 1783.5\n",
      "lif layer 2 self.abs_max_v: 2214.0\n",
      "fc layer 1 self.abs_max_out: 1768.0\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 2 self.abs_max_out: 1524.0\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "fc layer 1 self.abs_max_out: 2055.0\n",
      "lif layer 1 self.abs_max_v: 2055.0\n",
      "lif layer 2 self.abs_max_v: 2378.5\n",
      "lif layer 1 self.abs_max_v: 2367.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "fc layer 3 self.abs_max_out: 667.0\n",
      "lif layer 1 self.abs_max_v: 2595.0\n",
      "lif layer 2 self.abs_max_v: 2439.0\n",
      "lif layer 1 self.abs_max_v: 2668.5\n",
      "lif layer 2 self.abs_max_v: 2568.5\n",
      "lif layer 2 self.abs_max_v: 2934.0\n",
      "fc layer 1 self.abs_max_out: 2105.0\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "fc layer 3 self.abs_max_out: 710.0\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "fc layer 2 self.abs_max_out: 1925.0\n",
      "fc layer 1 self.abs_max_out: 2418.0\n",
      "fc layer 1 self.abs_max_out: 2827.0\n",
      "lif layer 1 self.abs_max_v: 2904.0\n",
      "fc layer 2 self.abs_max_out: 2042.0\n",
      "lif layer 2 self.abs_max_v: 2991.5\n",
      "lif layer 2 self.abs_max_v: 3192.5\n",
      "fc layer 2 self.abs_max_out: 2198.0\n",
      "lif layer 2 self.abs_max_v: 3399.5\n",
      "lif layer 1 self.abs_max_v: 2935.0\n",
      "lif layer 1 self.abs_max_v: 2987.0\n",
      "lif layer 1 self.abs_max_v: 3187.0\n",
      "fc layer 1 self.abs_max_out: 3037.0\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "fc layer 2 self.abs_max_out: 2369.0\n",
      "fc layer 2 self.abs_max_out: 2515.0\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 2572.0\n",
      "lif layer 2 self.abs_max_v: 3673.5\n",
      "lif layer 2 self.abs_max_v: 3678.0\n",
      "lif layer 2 self.abs_max_v: 3705.0\n",
      "fc layer 1 self.abs_max_out: 3049.0\n",
      "fc layer 1 self.abs_max_out: 3229.0\n",
      "lif layer 1 self.abs_max_v: 3229.0\n",
      "fc layer 1 self.abs_max_out: 3432.0\n",
      "lif layer 1 self.abs_max_v: 3432.0\n",
      "lif layer 1 self.abs_max_v: 3542.0\n",
      "lif layer 2 self.abs_max_v: 3708.0\n",
      "lif layer 2 self.abs_max_v: 3977.0\n",
      "lif layer 2 self.abs_max_v: 3988.0\n",
      "lif layer 2 self.abs_max_v: 4182.5\n",
      "fc layer 1 self.abs_max_out: 3548.0\n",
      "lif layer 1 self.abs_max_v: 3548.0\n",
      "fc layer 2 self.abs_max_out: 2584.0\n",
      "fc layer 1 self.abs_max_out: 3565.0\n",
      "lif layer 1 self.abs_max_v: 3565.0\n",
      "fc layer 1 self.abs_max_out: 3600.0\n",
      "lif layer 1 self.abs_max_v: 3600.0\n",
      "fc layer 1 self.abs_max_out: 3739.0\n",
      "lif layer 1 self.abs_max_v: 3739.0\n",
      "fc layer 2 self.abs_max_out: 2608.0\n",
      "fc layer 3 self.abs_max_out: 1019.0\n",
      "fc layer 2 self.abs_max_out: 2611.0\n",
      "fc layer 1 self.abs_max_out: 3863.0\n",
      "lif layer 1 self.abs_max_v: 3863.0\n",
      "fc layer 1 self.abs_max_out: 3968.0\n",
      "lif layer 1 self.abs_max_v: 3968.0\n",
      "fc layer 2 self.abs_max_out: 2703.0\n",
      "fc layer 2 self.abs_max_out: 2864.0\n",
      "lif layer 1 self.abs_max_v: 3988.5\n",
      "lif layer 2 self.abs_max_v: 4185.5\n",
      "fc layer 1 self.abs_max_out: 4052.0\n",
      "lif layer 1 self.abs_max_v: 4052.0\n",
      "fc layer 1 self.abs_max_out: 4136.0\n",
      "lif layer 1 self.abs_max_v: 4136.0\n",
      "fc layer 1 self.abs_max_out: 4154.0\n",
      "lif layer 1 self.abs_max_v: 4154.0\n",
      "fc layer 1 self.abs_max_out: 4165.0\n",
      "lif layer 1 self.abs_max_v: 4165.0\n",
      "fc layer 1 self.abs_max_out: 4290.0\n",
      "lif layer 1 self.abs_max_v: 4290.0\n",
      "fc layer 2 self.abs_max_out: 2932.0\n",
      "fc layer 2 self.abs_max_out: 2964.0\n",
      "fc layer 2 self.abs_max_out: 3263.0\n",
      "fc layer 1 self.abs_max_out: 4772.0\n",
      "lif layer 1 self.abs_max_v: 4772.0\n",
      "lif layer 2 self.abs_max_v: 4204.5\n",
      "fc layer 1 self.abs_max_out: 4904.0\n",
      "lif layer 1 self.abs_max_v: 4904.0\n",
      "fc layer 1 self.abs_max_out: 5018.0\n",
      "lif layer 1 self.abs_max_v: 5018.0\n",
      "lif layer 2 self.abs_max_v: 4529.0\n",
      "lif layer 2 self.abs_max_v: 4548.5\n",
      "lif layer 2 self.abs_max_v: 4795.5\n",
      "fc layer 1 self.abs_max_out: 5250.0\n",
      "lif layer 1 self.abs_max_v: 5250.0\n",
      "lif layer 1 self.abs_max_v: 5461.0\n",
      "lif layer 1 self.abs_max_v: 5472.5\n",
      "lif layer 1 self.abs_max_v: 5636.5\n",
      "fc layer 1 self.abs_max_out: 5650.0\n",
      "lif layer 1 self.abs_max_v: 5650.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.053595/  2.086344, val:  44.17%, val_best:  44.17%, tr:  73.95%, tr_best:  73.95%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 78.8021%\n",
      "layer   3  Sparsity: 80.5385%\n",
      "total_backward_count 9790 real_backward_count 4500  45.965%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 5700.5\n",
      "lif layer 1 self.abs_max_v: 5724.5\n",
      "fc layer 1 self.abs_max_out: 5721.0\n",
      "fc layer 1 self.abs_max_out: 5929.0\n",
      "lif layer 1 self.abs_max_v: 5929.0\n",
      "lif layer 1 self.abs_max_v: 5932.5\n",
      "fc layer 2 self.abs_max_out: 3291.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.006940/  2.098444, val:  54.58%, val_best:  54.58%, tr:  90.40%, tr_best:  90.40%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.7314%\n",
      "layer   3  Sparsity: 78.0605%\n",
      "total_backward_count 19580 real_backward_count 7409  37.840%\n",
      "fc layer 2 self.abs_max_out: 3354.0\n",
      "lif layer 2 self.abs_max_v: 4852.0\n",
      "lif layer 1 self.abs_max_v: 6046.0\n",
      "fc layer 2 self.abs_max_out: 3381.0\n",
      "fc layer 1 self.abs_max_out: 6092.0\n",
      "lif layer 1 self.abs_max_v: 6092.0\n",
      "lif layer 1 self.abs_max_v: 6178.5\n",
      "fc layer 2 self.abs_max_out: 3453.0\n",
      "lif layer 1 self.abs_max_v: 6281.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.023335/  2.105379, val:  46.25%, val_best:  54.58%, tr:  95.40%, tr_best:  95.40%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.4613%\n",
      "layer   3  Sparsity: 77.6073%\n",
      "total_backward_count 29370 real_backward_count 9925  33.793%\n",
      "lif layer 2 self.abs_max_v: 4937.5\n",
      "lif layer 1 self.abs_max_v: 6307.5\n",
      "lif layer 1 self.abs_max_v: 6541.5\n",
      "lif layer 1 self.abs_max_v: 6716.0\n",
      "fc layer 1 self.abs_max_out: 6358.0\n",
      "fc layer 1 self.abs_max_out: 6562.0\n",
      "lif layer 1 self.abs_max_v: 6823.0\n",
      "fc layer 2 self.abs_max_out: 3660.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.036677/  2.114189, val:  48.75%, val_best:  54.58%, tr:  97.96%, tr_best:  97.96%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.0425%\n",
      "layer   3  Sparsity: 76.6348%\n",
      "total_backward_count 39160 real_backward_count 12092  30.878%\n",
      "lif layer 1 self.abs_max_v: 6876.0\n",
      "lif layer 1 self.abs_max_v: 7840.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.049845/  2.116889, val:  54.58%, val_best:  54.58%, tr:  98.47%, tr_best:  98.47%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.1767%\n",
      "layer   3  Sparsity: 76.7982%\n",
      "total_backward_count 48950 real_backward_count 14049  28.701%\n",
      "lif layer 2 self.abs_max_v: 5008.5\n",
      "fc layer 1 self.abs_max_out: 6644.0\n",
      "lif layer 1 self.abs_max_v: 8040.0\n",
      "lif layer 1 self.abs_max_v: 8068.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.057018/  2.126018, val:  62.92%, val_best:  62.92%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.1155%\n",
      "layer   3  Sparsity: 76.5282%\n",
      "total_backward_count 58740 real_backward_count 15843  26.971%\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.064603/  2.135312, val:  62.50%, val_best:  62.92%, tr:  99.18%, tr_best:  99.18%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.2260%\n",
      "layer   3  Sparsity: 76.2955%\n",
      "total_backward_count 68530 real_backward_count 17642  25.743%\n",
      "fc layer 1 self.abs_max_out: 6767.0\n",
      "lif layer 2 self.abs_max_v: 5155.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.065023/  2.124470, val:  54.17%, val_best:  62.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.2376%\n",
      "layer   3  Sparsity: 76.5906%\n",
      "total_backward_count 78320 real_backward_count 19319  24.667%\n",
      "fc layer 1 self.abs_max_out: 6858.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.070567/  2.125235, val:  57.08%, val_best:  62.92%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 77.0341%\n",
      "layer   3  Sparsity: 76.9673%\n",
      "total_backward_count 88110 real_backward_count 20963  23.792%\n",
      "fc layer 1 self.abs_max_out: 6962.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.069807/  2.142377, val:  55.00%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.7200%\n",
      "layer   3  Sparsity: 77.4598%\n",
      "total_backward_count 97900 real_backward_count 22478  22.960%\n",
      "fc layer 1 self.abs_max_out: 7108.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.075037/  2.131735, val:  61.25%, val_best:  62.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.5589%\n",
      "layer   3  Sparsity: 77.3566%\n",
      "total_backward_count 107690 real_backward_count 23996  22.282%\n",
      "lif layer 2 self.abs_max_v: 5223.5\n",
      "fc layer 1 self.abs_max_out: 7223.0\n",
      "lif layer 2 self.abs_max_v: 5233.5\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.069171/  2.123217, val:  58.33%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.5325%\n",
      "layer   3  Sparsity: 77.2813%\n",
      "total_backward_count 117480 real_backward_count 25469  21.679%\n",
      "fc layer 1 self.abs_max_out: 7260.0\n",
      "lif layer 1 self.abs_max_v: 8183.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.063010/  2.126294, val:  62.92%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.2127%\n",
      "layer   3  Sparsity: 77.4188%\n",
      "total_backward_count 127270 real_backward_count 26887  21.126%\n",
      "fc layer 2 self.abs_max_out: 3868.0\n",
      "fc layer 1 self.abs_max_out: 7315.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.061698/  2.131452, val:  55.00%, val_best:  62.92%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.3453%\n",
      "layer   3  Sparsity: 77.6826%\n",
      "total_backward_count 137060 real_backward_count 28306  20.652%\n",
      "fc layer 1 self.abs_max_out: 7384.0\n",
      "lif layer 2 self.abs_max_v: 5320.0\n",
      "fc layer 2 self.abs_max_out: 3899.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.055870/  2.115917, val:  59.17%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.5497%\n",
      "layer   3  Sparsity: 77.7733%\n",
      "total_backward_count 146850 real_backward_count 29638  20.182%\n",
      "lif layer 1 self.abs_max_v: 8483.5\n",
      "fc layer 1 self.abs_max_out: 7485.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.061738/  2.122472, val:  56.67%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.6802%\n",
      "layer   3  Sparsity: 78.2047%\n",
      "total_backward_count 156640 real_backward_count 30992  19.785%\n",
      "fc layer 2 self.abs_max_out: 4153.0\n",
      "fc layer 1 self.abs_max_out: 7529.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.061283/  2.112208, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.3906%\n",
      "layer   3  Sparsity: 78.5151%\n",
      "total_backward_count 166430 real_backward_count 32235  19.369%\n",
      "lif layer 1 self.abs_max_v: 8506.5\n",
      "fc layer 1 self.abs_max_out: 7617.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.061738/  2.116530, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0571%\n",
      "layer   3  Sparsity: 78.2639%\n",
      "total_backward_count 176220 real_backward_count 33466  18.991%\n",
      "fc layer 1 self.abs_max_out: 7643.0\n",
      "lif layer 1 self.abs_max_v: 8710.5\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.059509/  2.112083, val:  61.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.1089%\n",
      "layer   3  Sparsity: 78.6519%\n",
      "total_backward_count 186010 real_backward_count 34739  18.676%\n",
      "fc layer 1 self.abs_max_out: 7674.0\n",
      "lif layer 2 self.abs_max_v: 5546.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.059892/  2.122068, val:  55.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.2122%\n",
      "layer   3  Sparsity: 79.6437%\n",
      "total_backward_count 195800 real_backward_count 35939  18.355%\n",
      "lif layer 1 self.abs_max_v: 8813.0\n",
      "lif layer 2 self.abs_max_v: 5593.5\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.059580/  2.122247, val:  57.08%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.2783%\n",
      "layer   3  Sparsity: 79.7065%\n",
      "total_backward_count 205590 real_backward_count 37079  18.035%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.060616/  2.116569, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.2186%\n",
      "layer   3  Sparsity: 79.8430%\n",
      "total_backward_count 215380 real_backward_count 38261  17.764%\n",
      "lif layer 1 self.abs_max_v: 9188.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.062215/  2.107111, val:  73.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.2387%\n",
      "layer   3  Sparsity: 79.7453%\n",
      "total_backward_count 225170 real_backward_count 39395  17.496%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.053257/  2.110964, val:  68.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.2994%\n",
      "layer   3  Sparsity: 79.8143%\n",
      "total_backward_count 234960 real_backward_count 40461  17.220%\n",
      "fc layer 1 self.abs_max_out: 7698.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.061012/  2.115041, val:  68.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0746%\n",
      "layer   3  Sparsity: 79.7306%\n",
      "total_backward_count 244750 real_backward_count 41494  16.954%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.061778/  2.113752, val:  74.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.1309%\n",
      "layer   3  Sparsity: 79.9525%\n",
      "total_backward_count 254540 real_backward_count 42516  16.703%\n",
      "fc layer 1 self.abs_max_out: 7771.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.053077/  2.103261, val:  80.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.3087%\n",
      "layer   3  Sparsity: 79.7866%\n",
      "total_backward_count 264330 real_backward_count 43499  16.456%\n",
      "fc layer 1 self.abs_max_out: 7780.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.047861/  2.104235, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9981%\n",
      "layer   3  Sparsity: 79.5430%\n",
      "total_backward_count 274120 real_backward_count 44475  16.225%\n",
      "fc layer 1 self.abs_max_out: 7855.0\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.044306/  2.104325, val:  74.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9311%\n",
      "layer   3  Sparsity: 79.3269%\n",
      "total_backward_count 283910 real_backward_count 45400  15.991%\n",
      "fc layer 1 self.abs_max_out: 7883.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.038571/  2.095817, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0851%\n",
      "layer   3  Sparsity: 79.4439%\n",
      "total_backward_count 293700 real_backward_count 46323  15.772%\n",
      "fc layer 1 self.abs_max_out: 7932.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.040303/  2.101956, val:  80.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0582%\n",
      "layer   3  Sparsity: 79.7605%\n",
      "total_backward_count 303490 real_backward_count 47235  15.564%\n",
      "fc layer 2 self.abs_max_out: 4195.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.040693/  2.100309, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0985%\n",
      "layer   3  Sparsity: 80.0095%\n",
      "total_backward_count 313280 real_backward_count 48058  15.340%\n",
      "fc layer 1 self.abs_max_out: 7960.0\n",
      "lif layer 1 self.abs_max_v: 9705.5\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.038246/  2.095613, val:  72.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9902%\n",
      "layer   3  Sparsity: 79.8217%\n",
      "total_backward_count 323070 real_backward_count 48942  15.149%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.036406/  2.099586, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9067%\n",
      "layer   3  Sparsity: 80.0072%\n",
      "total_backward_count 332860 real_backward_count 49742  14.944%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.035426/  2.096349, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9929%\n",
      "layer   3  Sparsity: 79.8507%\n",
      "total_backward_count 342650 real_backward_count 50561  14.756%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.033971/  2.094221, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0338%\n",
      "layer   3  Sparsity: 80.3551%\n",
      "total_backward_count 352440 real_backward_count 51307  14.558%\n",
      "fc layer 1 self.abs_max_out: 7979.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.034292/  2.092041, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0530%\n",
      "layer   3  Sparsity: 80.8831%\n",
      "total_backward_count 362230 real_backward_count 52039  14.366%\n",
      "fc layer 1 self.abs_max_out: 8001.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.032832/  2.098266, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0740%\n",
      "layer   3  Sparsity: 81.1076%\n",
      "total_backward_count 372020 real_backward_count 52705  14.167%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.036136/  2.096656, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0097%\n",
      "layer   3  Sparsity: 80.6667%\n",
      "total_backward_count 381810 real_backward_count 53406  13.988%\n",
      "fc layer 1 self.abs_max_out: 8063.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.035696/  2.089945, val:  82.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0265%\n",
      "layer   3  Sparsity: 80.5987%\n",
      "total_backward_count 391600 real_backward_count 54083  13.811%\n",
      "lif layer 1 self.abs_max_v: 9870.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.032540/  2.092468, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9544%\n",
      "layer   3  Sparsity: 80.2375%\n",
      "total_backward_count 401390 real_backward_count 54772  13.646%\n",
      "fc layer 1 self.abs_max_out: 8067.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.032776/  2.091443, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9086%\n",
      "layer   3  Sparsity: 80.3759%\n",
      "total_backward_count 411180 real_backward_count 55435  13.482%\n",
      "fc layer 1 self.abs_max_out: 8132.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.031861/  2.091019, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9194%\n",
      "layer   3  Sparsity: 80.2328%\n",
      "total_backward_count 420970 real_backward_count 56092  13.324%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.029244/  2.089745, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9722%\n",
      "layer   3  Sparsity: 80.3373%\n",
      "total_backward_count 430760 real_backward_count 56699  13.163%\n",
      "lif layer 1 self.abs_max_v: 10081.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.025718/  2.086427, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0413%\n",
      "layer   3  Sparsity: 80.1281%\n",
      "total_backward_count 440550 real_backward_count 57337  13.015%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.019403/  2.080248, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8785%\n",
      "layer   3  Sparsity: 80.1745%\n",
      "total_backward_count 450340 real_backward_count 57945  12.867%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.013393/  2.078318, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8320%\n",
      "layer   3  Sparsity: 80.6738%\n",
      "total_backward_count 460130 real_backward_count 58527  12.720%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.012069/  2.073720, val:  72.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8371%\n",
      "layer   3  Sparsity: 80.5760%\n",
      "total_backward_count 469920 real_backward_count 59058  12.568%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.007642/  2.076578, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7653%\n",
      "layer   3  Sparsity: 80.5868%\n",
      "total_backward_count 479710 real_backward_count 59568  12.418%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.007977/  2.069139, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8370%\n",
      "layer   3  Sparsity: 80.9833%\n",
      "total_backward_count 489500 real_backward_count 60074  12.273%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.006484/  2.077251, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8575%\n",
      "layer   3  Sparsity: 80.8911%\n",
      "total_backward_count 499290 real_backward_count 60602  12.138%\n",
      "fc layer 1 self.abs_max_out: 8149.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.004849/  2.070448, val:  76.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8065%\n",
      "layer   3  Sparsity: 80.8959%\n",
      "total_backward_count 509080 real_backward_count 61129  12.008%\n",
      "fc layer 1 self.abs_max_out: 8189.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.004666/  2.074709, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9633%\n",
      "layer   3  Sparsity: 80.8023%\n",
      "total_backward_count 518870 real_backward_count 61620  11.876%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.006217/  2.070743, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8416%\n",
      "layer   3  Sparsity: 80.7325%\n",
      "total_backward_count 528660 real_backward_count 62126  11.752%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.001400/  2.072492, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6699%\n",
      "layer   3  Sparsity: 80.4722%\n",
      "total_backward_count 538450 real_backward_count 62608  11.627%\n",
      "fc layer 1 self.abs_max_out: 8223.0\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.999198/  2.062020, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5734%\n",
      "layer   3  Sparsity: 80.4678%\n",
      "total_backward_count 548240 real_backward_count 63060  11.502%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.999846/  2.067702, val:  80.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6671%\n",
      "layer   3  Sparsity: 80.3272%\n",
      "total_backward_count 558030 real_backward_count 63519  11.383%\n",
      "fc layer 1 self.abs_max_out: 8236.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.994838/  2.062177, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7451%\n",
      "layer   3  Sparsity: 80.4253%\n",
      "total_backward_count 567820 real_backward_count 63957  11.264%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.991516/  2.061707, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9472%\n",
      "layer   3  Sparsity: 80.8577%\n",
      "total_backward_count 577610 real_backward_count 64397  11.149%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.991098/  2.060545, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9951%\n",
      "layer   3  Sparsity: 81.0932%\n",
      "total_backward_count 587400 real_backward_count 64809  11.033%\n",
      "fc layer 1 self.abs_max_out: 8279.0\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.989106/  2.061678, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7876%\n",
      "layer   3  Sparsity: 81.1425%\n",
      "total_backward_count 597190 real_backward_count 65233  10.923%\n",
      "fc layer 1 self.abs_max_out: 8283.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.988245/  2.063798, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8618%\n",
      "layer   3  Sparsity: 80.9031%\n",
      "total_backward_count 606980 real_backward_count 65631  10.813%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.986660/  2.056917, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7077%\n",
      "layer   3  Sparsity: 80.5713%\n",
      "total_backward_count 616770 real_backward_count 66038  10.707%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.986072/  2.057764, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8038%\n",
      "layer   3  Sparsity: 80.4479%\n",
      "total_backward_count 626560 real_backward_count 66425  10.602%\n",
      "fc layer 1 self.abs_max_out: 8284.0\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.993419/  2.062252, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6009%\n",
      "layer   3  Sparsity: 80.6156%\n",
      "total_backward_count 636350 real_backward_count 66786  10.495%\n",
      "fc layer 1 self.abs_max_out: 8302.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.988701/  2.061744, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6752%\n",
      "layer   3  Sparsity: 81.0778%\n",
      "total_backward_count 646140 real_backward_count 67139  10.391%\n",
      "fc layer 1 self.abs_max_out: 8340.0\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.987650/  2.053759, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5966%\n",
      "layer   3  Sparsity: 81.0737%\n",
      "total_backward_count 655930 real_backward_count 67530  10.295%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.986718/  2.060195, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6255%\n",
      "layer   3  Sparsity: 81.3532%\n",
      "total_backward_count 665720 real_backward_count 67877  10.196%\n",
      "fc layer 1 self.abs_max_out: 8348.0\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.984043/  2.055072, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8842%\n",
      "layer   3  Sparsity: 81.0876%\n",
      "total_backward_count 675510 real_backward_count 68208  10.097%\n",
      "fc layer 1 self.abs_max_out: 8399.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.978066/  2.051356, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6822%\n",
      "layer   3  Sparsity: 80.8450%\n",
      "total_backward_count 685300 real_backward_count 68552  10.003%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.977910/  2.051482, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5409%\n",
      "layer   3  Sparsity: 80.7479%\n",
      "total_backward_count 695090 real_backward_count 68910   9.914%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.979927/  2.056141, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5708%\n",
      "layer   3  Sparsity: 80.9009%\n",
      "total_backward_count 704880 real_backward_count 69246   9.824%\n",
      "fc layer 1 self.abs_max_out: 8424.0\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.976366/  2.049583, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5642%\n",
      "layer   3  Sparsity: 80.3393%\n",
      "total_backward_count 714670 real_backward_count 69584   9.737%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.972916/  2.051143, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6092%\n",
      "layer   3  Sparsity: 81.0574%\n",
      "total_backward_count 724460 real_backward_count 69915   9.651%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.977790/  2.051354, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5936%\n",
      "layer   3  Sparsity: 81.4131%\n",
      "total_backward_count 734250 real_backward_count 70238   9.566%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.978101/  2.044236, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7121%\n",
      "layer   3  Sparsity: 81.0362%\n",
      "total_backward_count 744040 real_backward_count 70541   9.481%\n",
      "fc layer 1 self.abs_max_out: 8426.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.975237/  2.051980, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7023%\n",
      "layer   3  Sparsity: 80.8625%\n",
      "total_backward_count 753830 real_backward_count 70839   9.397%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.975209/  2.053480, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8686%\n",
      "layer   3  Sparsity: 81.1096%\n",
      "total_backward_count 763620 real_backward_count 71129   9.315%\n",
      "fc layer 1 self.abs_max_out: 8472.0\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.975792/  2.050227, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7752%\n",
      "layer   3  Sparsity: 80.9993%\n",
      "total_backward_count 773410 real_backward_count 71399   9.232%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.968513/  2.046788, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8678%\n",
      "layer   3  Sparsity: 80.6662%\n",
      "total_backward_count 783200 real_backward_count 71672   9.151%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.968945/  2.046290, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8869%\n",
      "layer   3  Sparsity: 81.0903%\n",
      "total_backward_count 792990 real_backward_count 71978   9.077%\n",
      "fc layer 1 self.abs_max_out: 8474.0\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.968086/  2.048028, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7692%\n",
      "layer   3  Sparsity: 81.1729%\n",
      "total_backward_count 802780 real_backward_count 72275   9.003%\n",
      "fc layer 1 self.abs_max_out: 8498.0\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.967825/  2.039201, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7019%\n",
      "layer   3  Sparsity: 81.0045%\n",
      "total_backward_count 812570 real_backward_count 72550   8.928%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.960494/  2.037154, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7824%\n",
      "layer   3  Sparsity: 81.2117%\n",
      "total_backward_count 822360 real_backward_count 72808   8.854%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.960512/  2.039208, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8604%\n",
      "layer   3  Sparsity: 81.1044%\n",
      "total_backward_count 832150 real_backward_count 73053   8.779%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.960153/  2.042141, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9689%\n",
      "layer   3  Sparsity: 81.2534%\n",
      "total_backward_count 841940 real_backward_count 73345   8.711%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.959805/  2.039035, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9303%\n",
      "layer   3  Sparsity: 81.2659%\n",
      "total_backward_count 851730 real_backward_count 73625   8.644%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.962644/  2.038202, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8599%\n",
      "layer   3  Sparsity: 81.2631%\n",
      "total_backward_count 861520 real_backward_count 73844   8.571%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.960678/  2.040885, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9045%\n",
      "layer   3  Sparsity: 81.2796%\n",
      "total_backward_count 871310 real_backward_count 74076   8.502%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.961704/  2.039826, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7892%\n",
      "layer   3  Sparsity: 81.2290%\n",
      "total_backward_count 881100 real_backward_count 74329   8.436%\n",
      "fc layer 1 self.abs_max_out: 8500.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.965260/  2.046708, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8382%\n",
      "layer   3  Sparsity: 81.6446%\n",
      "total_backward_count 890890 real_backward_count 74570   8.370%\n",
      "fc layer 1 self.abs_max_out: 8506.0\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.967609/  2.043134, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8764%\n",
      "layer   3  Sparsity: 81.6839%\n",
      "total_backward_count 900680 real_backward_count 74826   8.308%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.967749/  2.040285, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9336%\n",
      "layer   3  Sparsity: 81.8391%\n",
      "total_backward_count 910470 real_backward_count 75028   8.241%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.965090/  2.045313, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8824%\n",
      "layer   3  Sparsity: 81.8438%\n",
      "total_backward_count 920260 real_backward_count 75233   8.175%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.968810/  2.048005, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0022%\n",
      "layer   3  Sparsity: 81.9782%\n",
      "total_backward_count 930050 real_backward_count 75429   8.110%\n",
      "lif layer 1 self.abs_max_v: 10196.5\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.970933/  2.045035, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0510%\n",
      "layer   3  Sparsity: 82.1059%\n",
      "total_backward_count 939840 real_backward_count 75640   8.048%\n",
      "lif layer 1 self.abs_max_v: 10216.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.969015/  2.046898, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 76.0048%\n",
      "layer   3  Sparsity: 81.8536%\n",
      "total_backward_count 949630 real_backward_count 75853   7.988%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.968579/  2.046281, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7592%\n",
      "layer   3  Sparsity: 81.9479%\n",
      "total_backward_count 959420 real_backward_count 76069   7.929%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.966166/  2.044226, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8040%\n",
      "layer   3  Sparsity: 82.0073%\n",
      "total_backward_count 969210 real_backward_count 76280   7.870%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.965583/  2.045745, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9007%\n",
      "layer   3  Sparsity: 82.0853%\n",
      "total_backward_count 979000 real_backward_count 76473   7.811%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.967185/  2.045542, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9078%\n",
      "layer   3  Sparsity: 82.1080%\n",
      "total_backward_count 988790 real_backward_count 76652   7.752%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.966951/  2.042957, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9012%\n",
      "layer   3  Sparsity: 81.7043%\n",
      "total_backward_count 998580 real_backward_count 76847   7.696%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.965617/  2.047098, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6952%\n",
      "layer   3  Sparsity: 81.4731%\n",
      "total_backward_count 1008370 real_backward_count 77043   7.640%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.963850/  2.039249, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8582%\n",
      "layer   3  Sparsity: 81.6106%\n",
      "total_backward_count 1018160 real_backward_count 77232   7.585%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.958073/  2.039410, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8430%\n",
      "layer   3  Sparsity: 81.8251%\n",
      "total_backward_count 1027950 real_backward_count 77421   7.532%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.958646/  2.040966, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8536%\n",
      "layer   3  Sparsity: 82.2350%\n",
      "total_backward_count 1037740 real_backward_count 77606   7.478%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.961080/  2.041082, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8221%\n",
      "layer   3  Sparsity: 82.0893%\n",
      "total_backward_count 1047530 real_backward_count 77813   7.428%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.959132/  2.039913, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8027%\n",
      "layer   3  Sparsity: 81.9362%\n",
      "total_backward_count 1057320 real_backward_count 78012   7.378%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.959176/  2.038413, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6904%\n",
      "layer   3  Sparsity: 82.0335%\n",
      "total_backward_count 1067110 real_backward_count 78193   7.328%\n",
      "fc layer 1 self.abs_max_out: 8507.0\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.960587/  2.038938, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6870%\n",
      "layer   3  Sparsity: 81.9611%\n",
      "total_backward_count 1076900 real_backward_count 78383   7.279%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.959752/  2.039146, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7470%\n",
      "layer   3  Sparsity: 82.1239%\n",
      "total_backward_count 1086690 real_backward_count 78563   7.230%\n",
      "fc layer 1 self.abs_max_out: 8509.0\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.960192/  2.038395, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8027%\n",
      "layer   3  Sparsity: 82.0212%\n",
      "total_backward_count 1096480 real_backward_count 78729   7.180%\n",
      "fc layer 1 self.abs_max_out: 8520.0\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.957930/  2.034726, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8030%\n",
      "layer   3  Sparsity: 81.6516%\n",
      "total_backward_count 1106270 real_backward_count 78904   7.132%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.956537/  2.034626, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7263%\n",
      "layer   3  Sparsity: 81.5082%\n",
      "total_backward_count 1116060 real_backward_count 79081   7.086%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.955918/  2.036733, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7915%\n",
      "layer   3  Sparsity: 81.5331%\n",
      "total_backward_count 1125850 real_backward_count 79253   7.039%\n",
      "fc layer 1 self.abs_max_out: 8531.0\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.952249/  2.033605, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8047%\n",
      "layer   3  Sparsity: 81.7495%\n",
      "total_backward_count 1135640 real_backward_count 79420   6.993%\n",
      "fc layer 1 self.abs_max_out: 8533.0\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.952224/  2.037616, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6837%\n",
      "layer   3  Sparsity: 81.9543%\n",
      "total_backward_count 1145430 real_backward_count 79556   6.946%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.953953/  2.039579, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6950%\n",
      "layer   3  Sparsity: 81.7702%\n",
      "total_backward_count 1155220 real_backward_count 79745   6.903%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.950536/  2.035606, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6441%\n",
      "layer   3  Sparsity: 81.6128%\n",
      "total_backward_count 1165010 real_backward_count 79922   6.860%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.952225/  2.035191, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6855%\n",
      "layer   3  Sparsity: 81.6854%\n",
      "total_backward_count 1174800 real_backward_count 80101   6.818%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.951942/  2.041588, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6852%\n",
      "layer   3  Sparsity: 81.7169%\n",
      "total_backward_count 1184590 real_backward_count 80268   6.776%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.952093/  2.035797, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7636%\n",
      "layer   3  Sparsity: 81.6422%\n",
      "total_backward_count 1194380 real_backward_count 80423   6.733%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.948870/  2.029986, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7364%\n",
      "layer   3  Sparsity: 81.3292%\n",
      "total_backward_count 1204170 real_backward_count 80577   6.691%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.947720/  2.031374, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6492%\n",
      "layer   3  Sparsity: 81.2178%\n",
      "total_backward_count 1213960 real_backward_count 80748   6.652%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.945988/  2.029136, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6343%\n",
      "layer   3  Sparsity: 81.3684%\n",
      "total_backward_count 1223750 real_backward_count 80896   6.611%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.947073/  2.030132, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7370%\n",
      "layer   3  Sparsity: 81.7158%\n",
      "total_backward_count 1233540 real_backward_count 81055   6.571%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.944515/  2.032444, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7066%\n",
      "layer   3  Sparsity: 81.7165%\n",
      "total_backward_count 1243330 real_backward_count 81196   6.531%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.947097/  2.032660, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6718%\n",
      "layer   3  Sparsity: 81.6603%\n",
      "total_backward_count 1253120 real_backward_count 81337   6.491%\n",
      "fc layer 1 self.abs_max_out: 8571.0\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.948884/  2.032593, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6540%\n",
      "layer   3  Sparsity: 81.4164%\n",
      "total_backward_count 1262910 real_backward_count 81496   6.453%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.951767/  2.031203, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5908%\n",
      "layer   3  Sparsity: 81.7454%\n",
      "total_backward_count 1272700 real_backward_count 81631   6.414%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.950473/  2.033146, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7252%\n",
      "layer   3  Sparsity: 81.8826%\n",
      "total_backward_count 1282490 real_backward_count 81776   6.376%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.952524/  2.035027, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8191%\n",
      "layer   3  Sparsity: 81.9766%\n",
      "total_backward_count 1292280 real_backward_count 81923   6.339%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.950384/  2.034378, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8073%\n",
      "layer   3  Sparsity: 81.7698%\n",
      "total_backward_count 1302070 real_backward_count 82079   6.304%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.947979/  2.027458, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7852%\n",
      "layer   3  Sparsity: 81.9596%\n",
      "total_backward_count 1311860 real_backward_count 82226   6.268%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.944770/  2.028376, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7668%\n",
      "layer   3  Sparsity: 82.0039%\n",
      "total_backward_count 1321650 real_backward_count 82387   6.234%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.944150/  2.028389, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7518%\n",
      "layer   3  Sparsity: 81.9314%\n",
      "total_backward_count 1331440 real_backward_count 82528   6.198%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.941364/  2.025048, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7232%\n",
      "layer   3  Sparsity: 81.9833%\n",
      "total_backward_count 1341230 real_backward_count 82656   6.163%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.940991/  2.026193, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6683%\n",
      "layer   3  Sparsity: 81.8447%\n",
      "total_backward_count 1351020 real_backward_count 82778   6.127%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.940107/  2.024789, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7066%\n",
      "layer   3  Sparsity: 81.7112%\n",
      "total_backward_count 1360810 real_backward_count 82887   6.091%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.938252/  2.027624, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8007%\n",
      "layer   3  Sparsity: 81.7229%\n",
      "total_backward_count 1370600 real_backward_count 82990   6.055%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.936723/  2.024457, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7840%\n",
      "layer   3  Sparsity: 81.9049%\n",
      "total_backward_count 1380390 real_backward_count 83105   6.020%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.935974/  2.024595, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6112%\n",
      "layer   3  Sparsity: 81.7720%\n",
      "total_backward_count 1390180 real_backward_count 83213   5.986%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.938784/  2.021590, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7372%\n",
      "layer   3  Sparsity: 81.9897%\n",
      "total_backward_count 1399970 real_backward_count 83352   5.954%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.935332/  2.019405, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7290%\n",
      "layer   3  Sparsity: 82.0556%\n",
      "total_backward_count 1409760 real_backward_count 83463   5.920%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.935026/  2.022578, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7665%\n",
      "layer   3  Sparsity: 81.8779%\n",
      "total_backward_count 1419550 real_backward_count 83567   5.887%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.934212/  2.017201, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7984%\n",
      "layer   3  Sparsity: 81.7139%\n",
      "total_backward_count 1429340 real_backward_count 83681   5.855%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.932449/  2.019624, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8119%\n",
      "layer   3  Sparsity: 81.8275%\n",
      "total_backward_count 1439130 real_backward_count 83797   5.823%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.928384/  2.017346, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6989%\n",
      "layer   3  Sparsity: 81.7076%\n",
      "total_backward_count 1448920 real_backward_count 83933   5.793%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.929528/  2.017174, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8080%\n",
      "layer   3  Sparsity: 81.8918%\n",
      "total_backward_count 1458710 real_backward_count 84063   5.763%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.926458/  2.018066, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9441%\n",
      "layer   3  Sparsity: 81.9053%\n",
      "total_backward_count 1468500 real_backward_count 84178   5.732%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.929639/  2.019376, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9498%\n",
      "layer   3  Sparsity: 82.0829%\n",
      "total_backward_count 1478290 real_backward_count 84274   5.701%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.930209/  2.021797, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8610%\n",
      "layer   3  Sparsity: 82.1162%\n",
      "total_backward_count 1488080 real_backward_count 84394   5.671%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.928769/  2.020035, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9622%\n",
      "layer   3  Sparsity: 82.3652%\n",
      "total_backward_count 1497870 real_backward_count 84494   5.641%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.927607/  2.017108, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8982%\n",
      "layer   3  Sparsity: 82.2190%\n",
      "total_backward_count 1507660 real_backward_count 84591   5.611%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.926598/  2.017321, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8159%\n",
      "layer   3  Sparsity: 82.0867%\n",
      "total_backward_count 1517450 real_backward_count 84702   5.582%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.926618/  2.015072, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7207%\n",
      "layer   3  Sparsity: 82.0566%\n",
      "total_backward_count 1527240 real_backward_count 84810   5.553%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.927462/  2.016630, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8547%\n",
      "layer   3  Sparsity: 82.2940%\n",
      "total_backward_count 1537030 real_backward_count 84913   5.524%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.927166/  2.018467, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8294%\n",
      "layer   3  Sparsity: 82.3719%\n",
      "total_backward_count 1546820 real_backward_count 84999   5.495%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.925548/  2.016576, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9224%\n",
      "layer   3  Sparsity: 82.1710%\n",
      "total_backward_count 1556610 real_backward_count 85110   5.468%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.925421/  2.016086, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9506%\n",
      "layer   3  Sparsity: 82.0416%\n",
      "total_backward_count 1566400 real_backward_count 85205   5.440%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.925811/  2.009869, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7788%\n",
      "layer   3  Sparsity: 81.9691%\n",
      "total_backward_count 1576190 real_backward_count 85321   5.413%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.921823/  2.009798, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6130%\n",
      "layer   3  Sparsity: 81.7820%\n",
      "total_backward_count 1585980 real_backward_count 85442   5.387%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.922840/  2.012905, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6584%\n",
      "layer   3  Sparsity: 81.7782%\n",
      "total_backward_count 1595770 real_backward_count 85553   5.361%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.926771/  2.015160, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6391%\n",
      "layer   3  Sparsity: 82.0796%\n",
      "total_backward_count 1605560 real_backward_count 85667   5.336%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.924700/  2.013125, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5972%\n",
      "layer   3  Sparsity: 81.9619%\n",
      "total_backward_count 1615350 real_backward_count 85752   5.309%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.923299/  2.012896, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5883%\n",
      "layer   3  Sparsity: 81.9872%\n",
      "total_backward_count 1625140 real_backward_count 85850   5.283%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.922152/  2.015289, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6845%\n",
      "layer   3  Sparsity: 82.1302%\n",
      "total_backward_count 1634930 real_backward_count 85940   5.256%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.920141/  2.012940, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7100%\n",
      "layer   3  Sparsity: 82.3181%\n",
      "total_backward_count 1644720 real_backward_count 86031   5.231%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.922881/  2.014535, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5938%\n",
      "layer   3  Sparsity: 82.2279%\n",
      "total_backward_count 1654510 real_backward_count 86129   5.206%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.926635/  2.018235, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5551%\n",
      "layer   3  Sparsity: 82.1436%\n",
      "total_backward_count 1664300 real_backward_count 86253   5.183%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.923926/  2.015244, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.5773%\n",
      "layer   3  Sparsity: 81.9913%\n",
      "total_backward_count 1674090 real_backward_count 86369   5.159%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.923258/  2.016002, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6904%\n",
      "layer   3  Sparsity: 82.2042%\n",
      "total_backward_count 1683880 real_backward_count 86478   5.136%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.921539/  2.013514, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6041%\n",
      "layer   3  Sparsity: 82.1884%\n",
      "total_backward_count 1693670 real_backward_count 86574   5.112%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.922760/  2.017372, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6038%\n",
      "layer   3  Sparsity: 82.3706%\n",
      "total_backward_count 1703460 real_backward_count 86649   5.087%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.922755/  2.012089, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7333%\n",
      "layer   3  Sparsity: 82.2652%\n",
      "total_backward_count 1713250 real_backward_count 86741   5.063%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.923458/  2.018612, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7851%\n",
      "layer   3  Sparsity: 82.1461%\n",
      "total_backward_count 1723040 real_backward_count 86836   5.040%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.923118/  2.016368, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7819%\n",
      "layer   3  Sparsity: 82.3023%\n",
      "total_backward_count 1732830 real_backward_count 86944   5.017%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.919854/  2.011341, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9174%\n",
      "layer   3  Sparsity: 82.4476%\n",
      "total_backward_count 1742620 real_backward_count 87049   4.995%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.921225/  2.015934, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8923%\n",
      "layer   3  Sparsity: 82.5577%\n",
      "total_backward_count 1752410 real_backward_count 87129   4.972%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.920632/  2.014082, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9172%\n",
      "layer   3  Sparsity: 82.6382%\n",
      "total_backward_count 1762200 real_backward_count 87221   4.950%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.918517/  2.014455, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9357%\n",
      "layer   3  Sparsity: 82.6210%\n",
      "total_backward_count 1771990 real_backward_count 87320   4.928%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.922424/  2.014036, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7947%\n",
      "layer   3  Sparsity: 82.6511%\n",
      "total_backward_count 1781780 real_backward_count 87427   4.907%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.918332/  2.011213, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8958%\n",
      "layer   3  Sparsity: 82.5432%\n",
      "total_backward_count 1791570 real_backward_count 87503   4.884%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.915587/  2.011426, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8492%\n",
      "layer   3  Sparsity: 82.4343%\n",
      "total_backward_count 1801360 real_backward_count 87575   4.862%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.917897/  2.007591, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7975%\n",
      "layer   3  Sparsity: 82.3775%\n",
      "total_backward_count 1811150 real_backward_count 87649   4.839%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.915160/  2.008600, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7190%\n",
      "layer   3  Sparsity: 82.1797%\n",
      "total_backward_count 1820940 real_backward_count 87727   4.818%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.915665/  2.009439, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6619%\n",
      "layer   3  Sparsity: 82.4290%\n",
      "total_backward_count 1830730 real_backward_count 87807   4.796%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.917480/  2.008861, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6352%\n",
      "layer   3  Sparsity: 82.3565%\n",
      "total_backward_count 1840520 real_backward_count 87884   4.775%\n",
      "lif layer 1 self.abs_max_v: 10278.0\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.916087/  2.009885, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6550%\n",
      "layer   3  Sparsity: 82.3710%\n",
      "total_backward_count 1850310 real_backward_count 87963   4.754%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.915484/  2.005132, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7070%\n",
      "layer   3  Sparsity: 82.2227%\n",
      "total_backward_count 1860100 real_backward_count 88050   4.734%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.913127/  2.007088, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7669%\n",
      "layer   3  Sparsity: 82.1737%\n",
      "total_backward_count 1869890 real_backward_count 88146   4.714%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.915641/  2.012300, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7379%\n",
      "layer   3  Sparsity: 82.4277%\n",
      "total_backward_count 1879680 real_backward_count 88223   4.694%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.917797/  2.012276, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6966%\n",
      "layer   3  Sparsity: 82.4723%\n",
      "total_backward_count 1889470 real_backward_count 88298   4.673%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.919153/  2.013254, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.6639%\n",
      "layer   3  Sparsity: 82.5782%\n",
      "total_backward_count 1899260 real_backward_count 88386   4.654%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.918610/  2.015440, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.7855%\n",
      "layer   3  Sparsity: 82.6235%\n",
      "total_backward_count 1909050 real_backward_count 88460   4.634%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.915622/  2.011104, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8817%\n",
      "layer   3  Sparsity: 82.6003%\n",
      "total_backward_count 1918840 real_backward_count 88531   4.614%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.916374/  2.013207, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.9565%\n",
      "layer   3  Sparsity: 82.6324%\n",
      "total_backward_count 1928630 real_backward_count 88603   4.594%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.917453/  2.016082, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8398%\n",
      "layer   3  Sparsity: 82.9436%\n",
      "total_backward_count 1938420 real_backward_count 88679   4.575%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.920459/  2.017384, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8656%\n",
      "layer   3  Sparsity: 82.9744%\n",
      "total_backward_count 1948210 real_backward_count 88745   4.555%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.921235/  2.018285, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.2682%\n",
      "layer   2  Sparsity: 75.8352%\n",
      "layer   3  Sparsity: 82.9414%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285d384d90c249c6971603bd402d3dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.92123</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>2.01829</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-92</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obpfafc7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/obpfafc7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_023908-obpfafc7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0y80rrqx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_065802-0y80rrqx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0y80rrqx' target=\"_blank\">misty-sweep-98</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0y80rrqx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0y80rrqx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251116_065811_537', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 1, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 608.0\n",
      "lif layer 1 self.abs_max_v: 608.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 854.0\n",
      "lif layer 2 self.abs_max_v: 854.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "fc layer 1 self.abs_max_out: 669.0\n",
      "lif layer 1 self.abs_max_v: 850.5\n",
      "lif layer 2 self.abs_max_v: 1005.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 1 self.abs_max_out: 683.0\n",
      "lif layer 1 self.abs_max_v: 917.0\n",
      "fc layer 2 self.abs_max_out: 879.0\n",
      "lif layer 2 self.abs_max_v: 1341.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 1 self.abs_max_out: 768.0\n",
      "fc layer 2 self.abs_max_out: 981.0\n",
      "lif layer 2 self.abs_max_v: 1525.5\n",
      "fc layer 1 self.abs_max_out: 1397.0\n",
      "lif layer 1 self.abs_max_v: 1397.0\n",
      "lif layer 2 self.abs_max_v: 1681.0\n",
      "fc layer 1 self.abs_max_out: 1417.0\n",
      "lif layer 1 self.abs_max_v: 1417.0\n",
      "lif layer 2 self.abs_max_v: 1701.5\n",
      "fc layer 1 self.abs_max_out: 1432.0\n",
      "lif layer 1 self.abs_max_v: 1432.0\n",
      "fc layer 2 self.abs_max_out: 1157.0\n",
      "lif layer 2 self.abs_max_v: 1984.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "lif layer 1 self.abs_max_v: 1628.5\n",
      "lif layer 1 self.abs_max_v: 1949.5\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 1 self.abs_max_out: 1689.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 2 self.abs_max_out: 1285.0\n",
      "fc layer 1 self.abs_max_out: 2048.0\n",
      "lif layer 1 self.abs_max_v: 2048.0\n",
      "fc layer 1 self.abs_max_out: 2177.0\n",
      "lif layer 1 self.abs_max_v: 2177.0\n",
      "fc layer 1 self.abs_max_out: 2246.0\n",
      "lif layer 1 self.abs_max_v: 2804.0\n",
      "fc layer 1 self.abs_max_out: 2894.0\n",
      "lif layer 1 self.abs_max_v: 3781.0\n",
      "fc layer 2 self.abs_max_out: 1295.0\n",
      "lif layer 2 self.abs_max_v: 2144.5\n",
      "lif layer 2 self.abs_max_v: 2147.0\n",
      "fc layer 2 self.abs_max_out: 1356.0\n",
      "fc layer 1 self.abs_max_out: 2963.0\n",
      "fc layer 2 self.abs_max_out: 1390.0\n",
      "lif layer 1 self.abs_max_v: 3977.0\n",
      "lif layer 2 self.abs_max_v: 2182.0\n",
      "lif layer 1 self.abs_max_v: 4055.0\n",
      "lif layer 2 self.abs_max_v: 2261.0\n",
      "fc layer 2 self.abs_max_out: 1490.0\n",
      "fc layer 2 self.abs_max_out: 1574.0\n",
      "lif layer 2 self.abs_max_v: 2323.0\n",
      "fc layer 1 self.abs_max_out: 3212.0\n",
      "lif layer 1 self.abs_max_v: 4181.5\n",
      "lif layer 1 self.abs_max_v: 4262.0\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "fc layer 2 self.abs_max_out: 1668.0\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "fc layer 2 self.abs_max_out: 1723.0\n",
      "lif layer 2 self.abs_max_v: 2421.5\n",
      "lif layer 2 self.abs_max_v: 2432.5\n",
      "lif layer 2 self.abs_max_v: 2583.5\n",
      "lif layer 2 self.abs_max_v: 2627.5\n",
      "fc layer 2 self.abs_max_out: 1818.0\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "fc layer 1 self.abs_max_out: 3470.0\n",
      "lif layer 2 self.abs_max_v: 2629.0\n",
      "lif layer 2 self.abs_max_v: 2652.0\n",
      "lif layer 2 self.abs_max_v: 2658.0\n",
      "lif layer 2 self.abs_max_v: 2733.0\n",
      "fc layer 2 self.abs_max_out: 1829.0\n",
      "lif layer 1 self.abs_max_v: 4377.5\n",
      "lif layer 1 self.abs_max_v: 4818.0\n",
      "lif layer 2 self.abs_max_v: 2844.5\n",
      "lif layer 2 self.abs_max_v: 2857.5\n",
      "lif layer 2 self.abs_max_v: 2908.0\n",
      "fc layer 2 self.abs_max_out: 1837.0\n",
      "fc layer 2 self.abs_max_out: 1905.0\n",
      "fc layer 2 self.abs_max_out: 1920.0\n",
      "fc layer 2 self.abs_max_out: 1935.0\n",
      "fc layer 2 self.abs_max_out: 1949.0\n",
      "lif layer 2 self.abs_max_v: 3022.0\n",
      "lif layer 2 self.abs_max_v: 3033.0\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "lif layer 2 self.abs_max_v: 3036.0\n",
      "lif layer 2 self.abs_max_v: 3100.5\n",
      "lif layer 2 self.abs_max_v: 3101.0\n",
      "lif layer 2 self.abs_max_v: 3180.0\n",
      "lif layer 2 self.abs_max_v: 3295.0\n",
      "lif layer 1 self.abs_max_v: 4975.5\n",
      "lif layer 1 self.abs_max_v: 5206.5\n",
      "lif layer 1 self.abs_max_v: 5595.5\n",
      "lif layer 1 self.abs_max_v: 5850.5\n",
      "fc layer 2 self.abs_max_out: 2017.0\n",
      "fc layer 3 self.abs_max_out: 727.0\n",
      "lif layer 2 self.abs_max_v: 3337.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 3 self.abs_max_out: 829.0\n",
      "lif layer 2 self.abs_max_v: 3414.0\n",
      "lif layer 1 self.abs_max_v: 6048.0\n",
      "fc layer 3 self.abs_max_out: 879.0\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "fc layer 3 self.abs_max_out: 881.0\n",
      "fc layer 3 self.abs_max_out: 909.0\n",
      "lif layer 2 self.abs_max_v: 3496.0\n",
      "fc layer 1 self.abs_max_out: 3501.0\n",
      "fc layer 1 self.abs_max_out: 3535.0\n",
      "fc layer 1 self.abs_max_out: 3545.0\n",
      "lif layer 1 self.abs_max_v: 6323.5\n",
      "lif layer 1 self.abs_max_v: 6511.0\n",
      "lif layer 1 self.abs_max_v: 6682.5\n",
      "fc layer 1 self.abs_max_out: 3627.0\n",
      "fc layer 3 self.abs_max_out: 932.0\n",
      "lif layer 2 self.abs_max_v: 3603.5\n",
      "fc layer 1 self.abs_max_out: 3957.0\n",
      "fc layer 3 self.abs_max_out: 933.0\n",
      "fc layer 3 self.abs_max_out: 959.0\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "lif layer 1 self.abs_max_v: 6937.5\n",
      "lif layer 1 self.abs_max_v: 7080.0\n",
      "lif layer 2 self.abs_max_v: 3709.5\n",
      "fc layer 3 self.abs_max_out: 980.0\n",
      "fc layer 3 self.abs_max_out: 989.0\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "fc layer 2 self.abs_max_out: 2047.0\n",
      "lif layer 2 self.abs_max_v: 3798.5\n",
      "lif layer 2 self.abs_max_v: 3885.5\n",
      "lif layer 2 self.abs_max_v: 3896.0\n",
      "fc layer 1 self.abs_max_out: 4372.0\n",
      "lif layer 1 self.abs_max_v: 7556.0\n",
      "lif layer 1 self.abs_max_v: 7726.0\n",
      "fc layer 3 self.abs_max_out: 1013.0\n",
      "fc layer 1 self.abs_max_out: 4425.0\n",
      "lif layer 1 self.abs_max_v: 8178.0\n",
      "fc layer 1 self.abs_max_out: 4644.0\n",
      "lif layer 1 self.abs_max_v: 8367.0\n",
      "fc layer 3 self.abs_max_out: 1063.0\n",
      "fc layer 1 self.abs_max_out: 4837.0\n",
      "lif layer 1 self.abs_max_v: 9000.0\n",
      "fc layer 1 self.abs_max_out: 4971.0\n",
      "lif layer 1 self.abs_max_v: 9420.0\n",
      "lif layer 2 self.abs_max_v: 3915.0\n",
      "lif layer 2 self.abs_max_v: 3921.5\n",
      "fc layer 1 self.abs_max_out: 4992.0\n",
      "lif layer 2 self.abs_max_v: 3978.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "lif layer 2 self.abs_max_v: 3987.0\n",
      "lif layer 2 self.abs_max_v: 4127.5\n",
      "lif layer 2 self.abs_max_v: 4139.0\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "fc layer 3 self.abs_max_out: 1261.0\n",
      "fc layer 3 self.abs_max_out: 1302.0\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "fc layer 2 self.abs_max_out: 2242.0\n",
      "lif layer 2 self.abs_max_v: 4243.0\n",
      "lif layer 2 self.abs_max_v: 4298.0\n",
      "fc layer 1 self.abs_max_out: 5363.0\n",
      "lif layer 1 self.abs_max_v: 9615.0\n",
      "fc layer 2 self.abs_max_out: 2268.0\n",
      "lif layer 2 self.abs_max_v: 4350.0\n",
      "fc layer 2 self.abs_max_out: 2278.0\n",
      "lif layer 1 self.abs_max_v: 9788.5\n",
      "fc layer 1 self.abs_max_out: 5581.0\n",
      "lif layer 1 self.abs_max_v: 9919.0\n",
      "fc layer 1 self.abs_max_out: 5768.0\n",
      "lif layer 1 self.abs_max_v: 10727.5\n",
      "fc layer 1 self.abs_max_out: 5845.0\n",
      "lif layer 1 self.abs_max_v: 11209.0\n",
      "fc layer 1 self.abs_max_out: 5852.0\n",
      "fc layer 1 self.abs_max_out: 5859.0\n",
      "fc layer 1 self.abs_max_out: 5877.0\n",
      "fc layer 2 self.abs_max_out: 2294.0\n",
      "fc layer 2 self.abs_max_out: 2409.0\n",
      "lif layer 2 self.abs_max_v: 4401.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.508912/  1.872822, val:  29.17%, val_best:  29.17%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 66.6752%\n",
      "layer   3  Sparsity: 60.9992%\n",
      "total_backward_count 9790 real_backward_count 1223  12.492%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 4533.0\n",
      "lif layer 2 self.abs_max_v: 4646.5\n",
      "fc layer 1 self.abs_max_out: 6173.0\n",
      "fc layer 1 self.abs_max_out: 6179.0\n",
      "fc layer 3 self.abs_max_out: 1303.0\n",
      "fc layer 2 self.abs_max_out: 2413.0\n",
      "fc layer 1 self.abs_max_out: 6495.0\n",
      "fc layer 1 self.abs_max_out: 6523.0\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "fc layer 3 self.abs_max_out: 1351.0\n",
      "fc layer 3 self.abs_max_out: 1420.0\n",
      "fc layer 3 self.abs_max_out: 1441.0\n",
      "fc layer 2 self.abs_max_out: 2437.0\n",
      "fc layer 2 self.abs_max_out: 2459.0\n",
      "fc layer 2 self.abs_max_out: 2489.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.366224/  1.804310, val:  35.00%, val_best:  35.00%, tr:  99.28%, tr_best:  99.49%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.2196%\n",
      "layer   3  Sparsity: 67.1075%\n",
      "total_backward_count 19580 real_backward_count 2428  12.400%\n",
      "fc layer 1 self.abs_max_out: 6597.0\n",
      "fc layer 1 self.abs_max_out: 6682.0\n",
      "lif layer 1 self.abs_max_v: 11521.0\n",
      "fc layer 2 self.abs_max_out: 2606.0\n",
      "fc layer 2 self.abs_max_out: 2623.0\n",
      "fc layer 1 self.abs_max_out: 6838.0\n",
      "fc layer 1 self.abs_max_out: 6968.0\n",
      "lif layer 1 self.abs_max_v: 11663.0\n",
      "lif layer 1 self.abs_max_v: 12103.0\n",
      "fc layer 3 self.abs_max_out: 1513.0\n",
      "fc layer 3 self.abs_max_out: 1530.0\n",
      "fc layer 3 self.abs_max_out: 1540.0\n",
      "lif layer 1 self.abs_max_v: 12250.5\n",
      "lif layer 1 self.abs_max_v: 12369.5\n",
      "lif layer 2 self.abs_max_v: 4702.0\n",
      "lif layer 2 self.abs_max_v: 4830.5\n",
      "lif layer 2 self.abs_max_v: 4905.5\n",
      "fc layer 2 self.abs_max_out: 2647.0\n",
      "lif layer 2 self.abs_max_v: 4948.0\n",
      "fc layer 2 self.abs_max_out: 2744.0\n",
      "fc layer 3 self.abs_max_out: 1546.0\n",
      "fc layer 3 self.abs_max_out: 1548.0\n",
      "fc layer 3 self.abs_max_out: 1601.0\n",
      "fc layer 3 self.abs_max_out: 1613.0\n",
      "fc layer 3 self.abs_max_out: 1634.0\n",
      "fc layer 3 self.abs_max_out: 1639.0\n",
      "fc layer 3 self.abs_max_out: 1757.0\n",
      "fc layer 3 self.abs_max_out: 1776.0\n",
      "fc layer 3 self.abs_max_out: 1789.0\n",
      "lif layer 1 self.abs_max_v: 12677.5\n",
      "lif layer 1 self.abs_max_v: 12740.0\n",
      "fc layer 1 self.abs_max_out: 7317.0\n",
      "lif layer 1 self.abs_max_v: 13637.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.301540/  1.692600, val:  41.67%, val_best:  41.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.0335%\n",
      "layer   3  Sparsity: 67.8785%\n",
      "total_backward_count 29370 real_backward_count 3626  12.346%\n",
      "fc layer 1 self.abs_max_out: 7496.0\n",
      "lif layer 1 self.abs_max_v: 13993.5\n",
      "fc layer 1 self.abs_max_out: 7593.0\n",
      "fc layer 1 self.abs_max_out: 7839.0\n",
      "lif layer 1 self.abs_max_v: 14037.5\n",
      "lif layer 1 self.abs_max_v: 14279.0\n",
      "lif layer 1 self.abs_max_v: 14433.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.224646/  1.742201, val:  38.75%, val_best:  41.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 75.5612%\n",
      "layer   3  Sparsity: 67.0358%\n",
      "total_backward_count 39160 real_backward_count 4866  12.426%\n",
      "fc layer 2 self.abs_max_out: 2750.0\n",
      "fc layer 2 self.abs_max_out: 2754.0\n",
      "fc layer 2 self.abs_max_out: 2845.0\n",
      "fc layer 2 self.abs_max_out: 2861.0\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "lif layer 2 self.abs_max_v: 5085.5\n",
      "lif layer 2 self.abs_max_v: 5104.5\n",
      "lif layer 2 self.abs_max_v: 5205.5\n",
      "lif layer 2 self.abs_max_v: 5353.5\n",
      "lif layer 2 self.abs_max_v: 5412.0\n",
      "lif layer 2 self.abs_max_v: 5459.0\n",
      "fc layer 1 self.abs_max_out: 7903.0\n",
      "lif layer 1 self.abs_max_v: 14858.0\n",
      "fc layer 2 self.abs_max_out: 3046.0\n",
      "fc layer 2 self.abs_max_out: 3073.0\n",
      "lif layer 2 self.abs_max_v: 5459.5\n",
      "lif layer 2 self.abs_max_v: 5552.0\n",
      "lif layer 2 self.abs_max_v: 5586.0\n",
      "fc layer 2 self.abs_max_out: 3076.0\n",
      "lif layer 2 self.abs_max_v: 5726.5\n",
      "fc layer 2 self.abs_max_out: 3080.0\n",
      "lif layer 2 self.abs_max_v: 5890.5\n",
      "lif layer 2 self.abs_max_v: 5895.5\n",
      "fc layer 2 self.abs_max_out: 3331.0\n",
      "fc layer 1 self.abs_max_out: 8038.0\n",
      "lif layer 1 self.abs_max_v: 14870.0\n",
      "lif layer 1 self.abs_max_v: 15155.0\n",
      "fc layer 1 self.abs_max_out: 8051.0\n",
      "lif layer 2 self.abs_max_v: 5999.0\n",
      "lif layer 2 self.abs_max_v: 6047.5\n",
      "lif layer 2 self.abs_max_v: 6052.0\n",
      "fc layer 1 self.abs_max_out: 8418.0\n",
      "lif layer 1 self.abs_max_v: 15351.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.231849/  1.660110, val:  46.67%, val_best:  46.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.5071%\n",
      "layer   3  Sparsity: 68.3546%\n",
      "total_backward_count 48950 real_backward_count 6013  12.284%\n",
      "fc layer 1 self.abs_max_out: 8638.0\n",
      "fc layer 1 self.abs_max_out: 8900.0\n",
      "lif layer 1 self.abs_max_v: 15493.0\n",
      "lif layer 1 self.abs_max_v: 15684.5\n",
      "fc layer 2 self.abs_max_out: 3403.0\n",
      "lif layer 2 self.abs_max_v: 6061.0\n",
      "lif layer 2 self.abs_max_v: 6211.5\n",
      "lif layer 2 self.abs_max_v: 6397.0\n",
      "lif layer 2 self.abs_max_v: 6516.0\n",
      "lif layer 2 self.abs_max_v: 6526.0\n",
      "lif layer 2 self.abs_max_v: 6547.0\n",
      "fc layer 1 self.abs_max_out: 8959.0\n",
      "fc layer 1 self.abs_max_out: 9003.0\n",
      "fc layer 1 self.abs_max_out: 9369.0\n",
      "lif layer 1 self.abs_max_v: 16307.5\n",
      "fc layer 1 self.abs_max_out: 9370.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.179398/  1.675334, val:  40.42%, val_best:  46.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.2245%\n",
      "layer   3  Sparsity: 68.8178%\n",
      "total_backward_count 58740 real_backward_count 7213  12.280%\n",
      "fc layer 1 self.abs_max_out: 9519.0\n",
      "fc layer 3 self.abs_max_out: 1823.0\n",
      "lif layer 1 self.abs_max_v: 16453.0\n",
      "lif layer 1 self.abs_max_v: 16626.5\n",
      "fc layer 1 self.abs_max_out: 9607.0\n",
      "lif layer 1 self.abs_max_v: 17134.0\n",
      "lif layer 1 self.abs_max_v: 17902.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.212791/  1.705894, val:  44.58%, val_best:  46.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.8247%\n",
      "layer   3  Sparsity: 70.0566%\n",
      "total_backward_count 68530 real_backward_count 8417  12.282%\n",
      "fc layer 2 self.abs_max_out: 3509.0\n",
      "fc layer 1 self.abs_max_out: 9715.0\n",
      "fc layer 2 self.abs_max_out: 3520.0\n",
      "fc layer 1 self.abs_max_out: 9724.0\n",
      "lif layer 2 self.abs_max_v: 6560.0\n",
      "lif layer 2 self.abs_max_v: 6566.0\n",
      "fc layer 2 self.abs_max_out: 3601.0\n",
      "lif layer 2 self.abs_max_v: 6884.0\n",
      "fc layer 1 self.abs_max_out: 9743.0\n",
      "fc layer 2 self.abs_max_out: 3674.0\n",
      "fc layer 2 self.abs_max_out: 3748.0\n",
      "lif layer 2 self.abs_max_v: 6937.5\n",
      "lif layer 2 self.abs_max_v: 7179.0\n",
      "fc layer 2 self.abs_max_out: 3818.0\n",
      "lif layer 2 self.abs_max_v: 7407.5\n",
      "fc layer 2 self.abs_max_out: 3828.0\n",
      "fc layer 2 self.abs_max_out: 3860.0\n",
      "fc layer 2 self.abs_max_out: 3865.0\n",
      "fc layer 2 self.abs_max_out: 4014.0\n",
      "lif layer 2 self.abs_max_v: 7697.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.210659/  1.719064, val:  31.67%, val_best:  46.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.7485%\n",
      "layer   3  Sparsity: 70.8998%\n",
      "total_backward_count 78320 real_backward_count 9562  12.209%\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.191000/  1.564076, val:  52.08%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4821%\n",
      "layer   3  Sparsity: 71.8922%\n",
      "total_backward_count 88110 real_backward_count 10778  12.232%\n",
      "lif layer 1 self.abs_max_v: 18054.0\n",
      "fc layer 1 self.abs_max_out: 9946.0\n",
      "lif layer 1 self.abs_max_v: 18104.5\n",
      "lif layer 1 self.abs_max_v: 18269.5\n",
      "lif layer 1 self.abs_max_v: 18313.5\n",
      "fc layer 1 self.abs_max_out: 9955.0\n",
      "lif layer 1 self.abs_max_v: 19112.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.165877/  1.622375, val:  47.08%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3296%\n",
      "layer   3  Sparsity: 71.8245%\n",
      "total_backward_count 97900 real_backward_count 11973  12.230%\n",
      "fc layer 1 self.abs_max_out: 10062.0\n",
      "fc layer 3 self.abs_max_out: 1835.0\n",
      "fc layer 1 self.abs_max_out: 10258.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.120216/  1.665847, val:  43.75%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.4407%\n",
      "layer   3  Sparsity: 70.8187%\n",
      "total_backward_count 107690 real_backward_count 13172  12.231%\n",
      "fc layer 3 self.abs_max_out: 1879.0\n",
      "fc layer 2 self.abs_max_out: 4118.0\n",
      "lif layer 2 self.abs_max_v: 7831.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.129750/  1.593865, val:  40.83%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6793%\n",
      "layer   3  Sparsity: 70.2281%\n",
      "total_backward_count 117480 real_backward_count 14338  12.205%\n",
      "fc layer 2 self.abs_max_out: 4185.0\n",
      "lif layer 2 self.abs_max_v: 7883.0\n",
      "lif layer 2 self.abs_max_v: 7943.5\n",
      "lif layer 2 self.abs_max_v: 7972.0\n",
      "fc layer 2 self.abs_max_out: 4253.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.078394/  1.541935, val:  43.75%, val_best:  52.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5881%\n",
      "layer   3  Sparsity: 69.3318%\n",
      "total_backward_count 127270 real_backward_count 15445  12.136%\n",
      "fc layer 2 self.abs_max_out: 4397.0\n",
      "lif layer 2 self.abs_max_v: 8252.0\n",
      "lif layer 2 self.abs_max_v: 8455.0\n",
      "fc layer 3 self.abs_max_out: 1882.0\n",
      "fc layer 3 self.abs_max_out: 1940.0\n",
      "fc layer 3 self.abs_max_out: 1944.0\n",
      "fc layer 1 self.abs_max_out: 10283.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.041318/  1.538204, val:  47.08%, val_best:  52.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8412%\n",
      "layer   3  Sparsity: 70.5006%\n",
      "total_backward_count 137060 real_backward_count 16598  12.110%\n",
      "fc layer 1 self.abs_max_out: 10672.0\n",
      "fc layer 3 self.abs_max_out: 1995.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.050708/  1.583112, val:  38.75%, val_best:  52.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7786%\n",
      "layer   3  Sparsity: 70.4002%\n",
      "total_backward_count 146850 real_backward_count 17725  12.070%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.054839/  1.659921, val:  37.92%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3400%\n",
      "layer   3  Sparsity: 70.5758%\n",
      "total_backward_count 156640 real_backward_count 18872  12.048%\n",
      "lif layer 1 self.abs_max_v: 19138.5\n",
      "lif layer 1 self.abs_max_v: 19399.5\n",
      "fc layer 1 self.abs_max_out: 11487.0\n",
      "lif layer 1 self.abs_max_v: 19973.5\n",
      "lif layer 1 self.abs_max_v: 20155.0\n",
      "lif layer 1 self.abs_max_v: 20223.0\n",
      "fc layer 1 self.abs_max_out: 11786.0\n",
      "lif layer 1 self.abs_max_v: 20573.0\n",
      "fc layer 1 self.abs_max_out: 11796.0\n",
      "lif layer 1 self.abs_max_v: 21747.5\n",
      "lif layer 1 self.abs_max_v: 21833.5\n",
      "lif layer 1 self.abs_max_v: 21895.0\n",
      "fc layer 1 self.abs_max_out: 11842.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.052105/  1.526925, val:  56.67%, val_best:  56.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5274%\n",
      "layer   3  Sparsity: 70.2566%\n",
      "total_backward_count 166430 real_backward_count 19984  12.007%\n",
      "fc layer 1 self.abs_max_out: 12270.0\n",
      "lif layer 1 self.abs_max_v: 23035.5\n",
      "fc layer 1 self.abs_max_out: 12552.0\n",
      "fc layer 2 self.abs_max_out: 4557.0\n",
      "lif layer 2 self.abs_max_v: 8502.0\n",
      "fc layer 1 self.abs_max_out: 12591.0\n",
      "fc layer 3 self.abs_max_out: 2040.0\n",
      "fc layer 1 self.abs_max_out: 12902.0\n",
      "lif layer 1 self.abs_max_v: 23331.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.053439/  1.486400, val:  57.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8936%\n",
      "layer   3  Sparsity: 70.2329%\n",
      "total_backward_count 176220 real_backward_count 21097  11.972%\n",
      "fc layer 3 self.abs_max_out: 2042.0\n",
      "lif layer 1 self.abs_max_v: 23850.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.005298/  1.459157, val:  51.67%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3871%\n",
      "layer   3  Sparsity: 69.2904%\n",
      "total_backward_count 186010 real_backward_count 22231  11.952%\n",
      "fc layer 3 self.abs_max_out: 2229.0\n",
      "fc layer 1 self.abs_max_out: 13303.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.956981/  1.537042, val:  33.33%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.0812%\n",
      "layer   3  Sparsity: 69.3721%\n",
      "total_backward_count 195800 real_backward_count 23307  11.903%\n",
      "lif layer 1 self.abs_max_v: 24475.0\n",
      "fc layer 2 self.abs_max_out: 4704.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.956302/  1.570773, val:  42.92%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6836%\n",
      "layer   3  Sparsity: 68.8211%\n",
      "total_backward_count 205590 real_backward_count 24387  11.862%\n",
      "fc layer 1 self.abs_max_out: 13407.0\n",
      "lif layer 1 self.abs_max_v: 24940.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.991199/  1.507023, val:  50.83%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6252%\n",
      "layer   3  Sparsity: 69.2891%\n",
      "total_backward_count 215380 real_backward_count 25461  11.821%\n",
      "fc layer 1 self.abs_max_out: 13655.0\n",
      "lif layer 1 self.abs_max_v: 25398.5\n",
      "lif layer 2 self.abs_max_v: 8679.5\n",
      "lif layer 2 self.abs_max_v: 8823.5\n",
      "fc layer 1 self.abs_max_out: 13803.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.958609/  1.426365, val:  51.67%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.7763%\n",
      "layer   3  Sparsity: 68.1792%\n",
      "total_backward_count 225170 real_backward_count 26578  11.804%\n",
      "fc layer 1 self.abs_max_out: 13872.0\n",
      "lif layer 1 self.abs_max_v: 25746.5\n",
      "fc layer 2 self.abs_max_out: 4800.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.957876/  1.431837, val:  54.58%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1068%\n",
      "layer   3  Sparsity: 67.6373%\n",
      "total_backward_count 234960 real_backward_count 27631  11.760%\n",
      "fc layer 1 self.abs_max_out: 13988.0\n",
      "lif layer 1 self.abs_max_v: 25914.5\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.927471/  1.374274, val:  57.50%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.2928%\n",
      "layer   3  Sparsity: 68.6472%\n",
      "total_backward_count 244750 real_backward_count 28694  11.724%\n",
      "fc layer 2 self.abs_max_out: 4946.0\n",
      "lif layer 2 self.abs_max_v: 8979.0\n",
      "lif layer 2 self.abs_max_v: 9159.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.967328/  1.354718, val:  64.17%, val_best:  64.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7018%\n",
      "layer   3  Sparsity: 69.8023%\n",
      "total_backward_count 254540 real_backward_count 29842  11.724%\n",
      "fc layer 3 self.abs_max_out: 2232.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.917752/  1.363458, val:  52.92%, val_best:  64.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3650%\n",
      "layer   3  Sparsity: 69.6674%\n",
      "total_backward_count 264330 real_backward_count 30920  11.697%\n",
      "fc layer 3 self.abs_max_out: 2241.0\n",
      "fc layer 3 self.abs_max_out: 2278.0\n",
      "fc layer 3 self.abs_max_out: 2280.0\n",
      "fc layer 3 self.abs_max_out: 2333.0\n",
      "fc layer 3 self.abs_max_out: 2393.0\n",
      "fc layer 3 self.abs_max_out: 2399.0\n",
      "fc layer 3 self.abs_max_out: 2449.0\n",
      "fc layer 3 self.abs_max_out: 2483.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.917633/  1.446495, val:  51.67%, val_best:  64.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.4030%\n",
      "layer   3  Sparsity: 68.3081%\n",
      "total_backward_count 274120 real_backward_count 31979  11.666%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.894479/  1.467460, val:  50.42%, val_best:  64.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6259%\n",
      "layer   3  Sparsity: 69.1066%\n",
      "total_backward_count 283910 real_backward_count 33045  11.639%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.888201/  1.454243, val:  48.33%, val_best:  64.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5237%\n",
      "layer   3  Sparsity: 69.0287%\n",
      "total_backward_count 293700 real_backward_count 34104  11.612%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.881741/  1.377797, val:  59.58%, val_best:  64.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3761%\n",
      "layer   3  Sparsity: 69.2669%\n",
      "total_backward_count 303490 real_backward_count 35164  11.587%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.875678/  1.364370, val:  55.42%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.4122%\n",
      "layer   3  Sparsity: 68.8595%\n",
      "total_backward_count 313280 real_backward_count 36251  11.571%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.874781/  1.408195, val:  52.50%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8654%\n",
      "layer   3  Sparsity: 68.0930%\n",
      "total_backward_count 323070 real_backward_count 37249  11.530%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.883384/  1.373270, val:  60.83%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.0568%\n",
      "layer   3  Sparsity: 67.8071%\n",
      "total_backward_count 332860 real_backward_count 38289  11.503%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.872850/  1.416117, val:  57.50%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.5512%\n",
      "layer   3  Sparsity: 67.7677%\n",
      "total_backward_count 342650 real_backward_count 39347  11.483%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.853815/  1.340532, val:  60.00%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.4978%\n",
      "layer   3  Sparsity: 67.0156%\n",
      "total_backward_count 352440 real_backward_count 40343  11.447%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.848186/  1.350520, val:  55.00%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.0455%\n",
      "layer   3  Sparsity: 68.6809%\n",
      "total_backward_count 362230 real_backward_count 41338  11.412%\n",
      "fc layer 3 self.abs_max_out: 2562.0\n",
      "fc layer 3 self.abs_max_out: 2618.0\n",
      "fc layer 3 self.abs_max_out: 2653.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.826165/  1.342148, val:  57.08%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.8587%\n",
      "layer   3  Sparsity: 68.4159%\n",
      "total_backward_count 372020 real_backward_count 42289  11.367%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.847423/  1.399777, val:  52.50%, val_best:  64.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.8933%\n",
      "layer   3  Sparsity: 67.4959%\n",
      "total_backward_count 381810 real_backward_count 43334  11.350%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.781364/  1.374933, val:  58.75%, val_best:  64.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.9451%\n",
      "layer   3  Sparsity: 67.2916%\n",
      "total_backward_count 391600 real_backward_count 44355  11.327%\n",
      "fc layer 2 self.abs_max_out: 4947.0\n",
      "lif layer 2 self.abs_max_v: 9214.5\n",
      "lif layer 2 self.abs_max_v: 9342.5\n",
      "lif layer 2 self.abs_max_v: 9428.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.812287/  1.325954, val:  56.25%, val_best:  64.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.8314%\n",
      "layer   3  Sparsity: 66.8889%\n",
      "total_backward_count 401390 real_backward_count 45351  11.298%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.821486/  1.371128, val:  58.33%, val_best:  64.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.6095%\n",
      "layer   3  Sparsity: 68.4502%\n",
      "total_backward_count 411180 real_backward_count 46352  11.273%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.823223/  1.261166, val:  58.75%, val_best:  64.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.3489%\n",
      "layer   3  Sparsity: 68.2241%\n",
      "total_backward_count 420970 real_backward_count 47317  11.240%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.781389/  1.301792, val:  58.33%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.5231%\n",
      "layer   3  Sparsity: 66.5884%\n",
      "total_backward_count 430760 real_backward_count 48281  11.208%\n",
      "lif layer 2 self.abs_max_v: 9434.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.812577/  1.386921, val:  48.33%, val_best:  64.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.0817%\n",
      "layer   3  Sparsity: 68.4171%\n",
      "total_backward_count 440550 real_backward_count 49266  11.183%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.800074/  1.325789, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.1516%\n",
      "layer   3  Sparsity: 67.0747%\n",
      "total_backward_count 450340 real_backward_count 50253  11.159%\n",
      "fc layer 2 self.abs_max_out: 5196.0\n",
      "lif layer 2 self.abs_max_v: 9454.5\n",
      "lif layer 2 self.abs_max_v: 9602.5\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.787476/  1.275289, val:  56.67%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.1532%\n",
      "layer   3  Sparsity: 65.5073%\n",
      "total_backward_count 460130 real_backward_count 51225  11.133%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.752279/  1.349585, val:  55.42%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.0753%\n",
      "layer   3  Sparsity: 66.2831%\n",
      "total_backward_count 469920 real_backward_count 52252  11.119%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.798735/  1.362199, val:  59.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.0620%\n",
      "layer   3  Sparsity: 68.9431%\n",
      "total_backward_count 479710 real_backward_count 53242  11.099%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.814314/  1.341064, val:  58.33%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.9076%\n",
      "layer   3  Sparsity: 69.3680%\n",
      "total_backward_count 489500 real_backward_count 54277  11.088%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.833361/  1.325854, val:  59.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.4521%\n",
      "layer   3  Sparsity: 69.6089%\n",
      "total_backward_count 499290 real_backward_count 55270  11.070%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.796930/  1.279803, val:  55.42%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.2733%\n",
      "layer   3  Sparsity: 69.8857%\n",
      "total_backward_count 509080 real_backward_count 56244  11.048%\n",
      "fc layer 3 self.abs_max_out: 2675.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.810283/  1.371778, val:  56.67%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.3116%\n",
      "layer   3  Sparsity: 68.9071%\n",
      "total_backward_count 518870 real_backward_count 57254  11.034%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.816359/  1.353412, val:  57.92%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.3032%\n",
      "layer   3  Sparsity: 69.3706%\n",
      "total_backward_count 528660 real_backward_count 58263  11.021%\n",
      "fc layer 3 self.abs_max_out: 2683.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.795352/  1.396196, val:  52.50%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.7205%\n",
      "layer   3  Sparsity: 69.8717%\n",
      "total_backward_count 538450 real_backward_count 59274  11.008%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.808878/  1.287668, val:  58.33%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6866%\n",
      "layer   3  Sparsity: 69.8040%\n",
      "total_backward_count 548240 real_backward_count 60278  10.995%\n",
      "fc layer 3 self.abs_max_out: 2753.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.755876/  1.430601, val:  46.25%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8251%\n",
      "layer   3  Sparsity: 67.5132%\n",
      "total_backward_count 558030 real_backward_count 61220  10.971%\n",
      "fc layer 3 self.abs_max_out: 2825.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.715633/  1.345016, val:  50.83%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3651%\n",
      "layer   3  Sparsity: 66.6727%\n",
      "total_backward_count 567820 real_backward_count 62152  10.946%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.747076/  1.220267, val:  62.50%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7782%\n",
      "layer   3  Sparsity: 67.4790%\n",
      "total_backward_count 577610 real_backward_count 63040  10.914%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.735648/  1.293495, val:  60.00%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.7767%\n",
      "layer   3  Sparsity: 67.8933%\n",
      "total_backward_count 587400 real_backward_count 63994  10.894%\n",
      "fc layer 3 self.abs_max_out: 2875.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.773433/  1.287215, val:  57.92%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.3860%\n",
      "layer   3  Sparsity: 68.7906%\n",
      "total_backward_count 597190 real_backward_count 64920  10.871%\n",
      "lif layer 1 self.abs_max_v: 26001.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.736864/  1.228192, val:  58.33%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.1377%\n",
      "layer   3  Sparsity: 67.0553%\n",
      "total_backward_count 606980 real_backward_count 65894  10.856%\n",
      "lif layer 1 self.abs_max_v: 26329.5\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.739995/  1.264543, val:  57.08%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.7199%\n",
      "layer   3  Sparsity: 68.0285%\n",
      "total_backward_count 616770 real_backward_count 66849  10.839%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.731810/  1.337463, val:  54.17%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.6961%\n",
      "layer   3  Sparsity: 68.4360%\n",
      "total_backward_count 626560 real_backward_count 67711  10.807%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.740512/  1.291788, val:  61.67%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.6303%\n",
      "layer   3  Sparsity: 68.9897%\n",
      "total_backward_count 636350 real_backward_count 68638  10.786%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.754059/  1.303264, val:  56.67%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.1256%\n",
      "layer   3  Sparsity: 68.8986%\n",
      "total_backward_count 646140 real_backward_count 69521  10.759%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.752306/  1.240051, val:  60.00%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 69.6728%\n",
      "layer   3  Sparsity: 68.8411%\n",
      "total_backward_count 655930 real_backward_count 70464  10.743%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.737526/  1.244124, val:  60.00%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.5403%\n",
      "layer   3  Sparsity: 69.5926%\n",
      "total_backward_count 665720 real_backward_count 71369  10.721%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.696851/  1.200426, val:  61.67%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1749%\n",
      "layer   3  Sparsity: 67.3305%\n",
      "total_backward_count 675510 real_backward_count 72291  10.702%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.694747/  1.266799, val:  60.00%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.4421%\n",
      "layer   3  Sparsity: 67.5331%\n",
      "total_backward_count 685300 real_backward_count 73221  10.685%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.707610/  1.242436, val:  64.17%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6903%\n",
      "layer   3  Sparsity: 68.1204%\n",
      "total_backward_count 695090 real_backward_count 74149  10.668%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.723278/  1.346035, val:  57.08%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8713%\n",
      "layer   3  Sparsity: 69.8028%\n",
      "total_backward_count 704880 real_backward_count 75084  10.652%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.735322/  1.209396, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.4411%\n",
      "layer   3  Sparsity: 69.3046%\n",
      "total_backward_count 714670 real_backward_count 75997  10.634%\n",
      "fc layer 3 self.abs_max_out: 2896.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.714123/  1.325331, val:  61.25%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.2643%\n",
      "layer   3  Sparsity: 68.7913%\n",
      "total_backward_count 724460 real_backward_count 76884  10.613%\n",
      "lif layer 1 self.abs_max_v: 26513.5\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.715848/  1.258391, val:  59.58%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8192%\n",
      "layer   3  Sparsity: 69.8118%\n",
      "total_backward_count 734250 real_backward_count 77802  10.596%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.740783/  1.296393, val:  59.58%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1908%\n",
      "layer   3  Sparsity: 70.5698%\n",
      "total_backward_count 744040 real_backward_count 78685  10.575%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.775747/  1.288702, val:  65.83%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5218%\n",
      "layer   3  Sparsity: 70.8603%\n",
      "total_backward_count 753830 real_backward_count 79622  10.562%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.763144/  1.463551, val:  47.08%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1634%\n",
      "layer   3  Sparsity: 71.6598%\n",
      "total_backward_count 763620 real_backward_count 80528  10.546%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.793731/  1.330656, val:  58.75%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.4138%\n",
      "layer   3  Sparsity: 73.1498%\n",
      "total_backward_count 773410 real_backward_count 81413  10.526%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.790231/  1.261466, val:  63.75%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1236%\n",
      "layer   3  Sparsity: 73.9584%\n",
      "total_backward_count 783200 real_backward_count 82342  10.514%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.771934/  1.332974, val:  58.75%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.9651%\n",
      "layer   3  Sparsity: 73.1288%\n",
      "total_backward_count 792990 real_backward_count 83265  10.500%\n",
      "lif layer 1 self.abs_max_v: 26663.5\n",
      "fc layer 1 self.abs_max_out: 14069.0\n",
      "lif layer 1 self.abs_max_v: 27401.0\n",
      "fc layer 1 self.abs_max_out: 14175.0\n",
      "lif layer 1 self.abs_max_v: 27875.5\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.731843/  1.388653, val:  49.17%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.0688%\n",
      "layer   3  Sparsity: 72.7372%\n",
      "total_backward_count 802780 real_backward_count 84122  10.479%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.733152/  1.211089, val:  63.75%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3278%\n",
      "layer   3  Sparsity: 71.0156%\n",
      "total_backward_count 812570 real_backward_count 85026  10.464%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.706841/  1.379370, val:  52.50%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8763%\n",
      "layer   3  Sparsity: 70.0084%\n",
      "total_backward_count 822360 real_backward_count 85916  10.447%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.719244/  1.345699, val:  52.92%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8619%\n",
      "layer   3  Sparsity: 70.1386%\n",
      "total_backward_count 832150 real_backward_count 86814  10.432%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.727649/  1.261966, val:  61.67%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.0924%\n",
      "layer   3  Sparsity: 70.8057%\n",
      "total_backward_count 841940 real_backward_count 87714  10.418%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.714447/  1.239593, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7960%\n",
      "layer   3  Sparsity: 69.5650%\n",
      "total_backward_count 851730 real_backward_count 88576  10.400%\n",
      "lif layer 2 self.abs_max_v: 9692.5\n",
      "lif layer 2 self.abs_max_v: 9768.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.699225/  1.203333, val:  62.08%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.2415%\n",
      "layer   3  Sparsity: 70.3178%\n",
      "total_backward_count 861520 real_backward_count 89436  10.381%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.660465/  1.282526, val:  59.58%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.0896%\n",
      "layer   3  Sparsity: 70.5334%\n",
      "total_backward_count 871310 real_backward_count 90292  10.363%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.695780/  1.244127, val:  61.67%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.2307%\n",
      "layer   3  Sparsity: 71.6477%\n",
      "total_backward_count 881100 real_backward_count 91144  10.344%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.677698/  1.281164, val:  56.67%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7701%\n",
      "layer   3  Sparsity: 71.5278%\n",
      "total_backward_count 890890 real_backward_count 92010  10.328%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.677560/  1.217768, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.0166%\n",
      "layer   3  Sparsity: 70.5356%\n",
      "total_backward_count 900680 real_backward_count 92860  10.310%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.675677/  1.291322, val:  61.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1694%\n",
      "layer   3  Sparsity: 70.4761%\n",
      "total_backward_count 910470 real_backward_count 93685  10.290%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.688675/  1.231002, val:  60.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2676%\n",
      "layer   3  Sparsity: 70.4596%\n",
      "total_backward_count 920260 real_backward_count 94574  10.277%\n",
      "fc layer 1 self.abs_max_out: 14286.0\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.702084/  1.307057, val:  62.08%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8358%\n",
      "layer   3  Sparsity: 70.6073%\n",
      "total_backward_count 930050 real_backward_count 95415  10.259%\n",
      "fc layer 1 self.abs_max_out: 14631.0\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.673890/  1.269597, val:  60.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.5408%\n",
      "layer   3  Sparsity: 69.7868%\n",
      "total_backward_count 939840 real_backward_count 96229  10.239%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.687239/  1.356287, val:  51.67%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3936%\n",
      "layer   3  Sparsity: 69.9418%\n",
      "total_backward_count 949630 real_backward_count 97087  10.224%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.689577/  1.187637, val:  66.25%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.0420%\n",
      "layer   3  Sparsity: 69.2287%\n",
      "total_backward_count 959420 real_backward_count 97906  10.205%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.657016/  1.273899, val:  61.67%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2399%\n",
      "layer   3  Sparsity: 69.7164%\n",
      "total_backward_count 969210 real_backward_count 98739  10.188%\n",
      "fc layer 1 self.abs_max_out: 15333.0\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.667186/  1.226243, val:  64.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9870%\n",
      "layer   3  Sparsity: 69.8333%\n",
      "total_backward_count 979000 real_backward_count 99543  10.168%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.658455/  1.235873, val:  62.08%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6609%\n",
      "layer   3  Sparsity: 70.6093%\n",
      "total_backward_count 988790 real_backward_count 100358  10.150%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.664220/  1.190996, val:  65.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 74.1638%\n",
      "layer   3  Sparsity: 71.2621%\n",
      "total_backward_count 998580 real_backward_count 101256  10.140%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.690689/  1.213395, val:  63.75%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4681%\n",
      "layer   3  Sparsity: 72.4337%\n",
      "total_backward_count 1008370 real_backward_count 102095  10.125%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.705358/  1.293343, val:  57.08%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1151%\n",
      "layer   3  Sparsity: 72.1491%\n",
      "total_backward_count 1018160 real_backward_count 102928  10.109%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.718058/  1.261325, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.5767%\n",
      "layer   3  Sparsity: 71.1658%\n",
      "total_backward_count 1027950 real_backward_count 103759  10.094%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.692270/  1.248361, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.4714%\n",
      "layer   3  Sparsity: 70.4404%\n",
      "total_backward_count 1037740 real_backward_count 104606  10.080%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.719991/  1.403548, val:  52.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.3014%\n",
      "layer   3  Sparsity: 71.2774%\n",
      "total_backward_count 1047530 real_backward_count 105448  10.066%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.688755/  1.310948, val:  55.83%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.6304%\n",
      "layer   3  Sparsity: 69.6808%\n",
      "total_backward_count 1057320 real_backward_count 106251  10.049%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.683886/  1.244437, val:  60.42%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.6034%\n",
      "layer   3  Sparsity: 72.1131%\n",
      "total_backward_count 1067110 real_backward_count 107019  10.029%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.691937/  1.267422, val:  57.50%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1979%\n",
      "layer   3  Sparsity: 71.3916%\n",
      "total_backward_count 1076900 real_backward_count 107826  10.013%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.682079/  1.206570, val:  64.58%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.4755%\n",
      "layer   3  Sparsity: 71.7436%\n",
      "total_backward_count 1086690 real_backward_count 108637   9.997%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.698852/  1.259697, val:  62.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.6212%\n",
      "layer   3  Sparsity: 71.6973%\n",
      "total_backward_count 1096480 real_backward_count 109447   9.982%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.725379/  1.290854, val:  61.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.9807%\n",
      "layer   3  Sparsity: 72.9695%\n",
      "total_backward_count 1106270 real_backward_count 110281   9.969%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.716621/  1.292702, val:  59.58%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.4983%\n",
      "layer   3  Sparsity: 72.7337%\n",
      "total_backward_count 1116060 real_backward_count 111086   9.953%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.709565/  1.252312, val:  60.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1521%\n",
      "layer   3  Sparsity: 72.0520%\n",
      "total_backward_count 1125850 real_backward_count 111915   9.940%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.693187/  1.231839, val:  63.75%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7749%\n",
      "layer   3  Sparsity: 72.4232%\n",
      "total_backward_count 1135640 real_backward_count 112738   9.927%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.708031/  1.229912, val:  62.50%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2148%\n",
      "layer   3  Sparsity: 71.9977%\n",
      "total_backward_count 1145430 real_backward_count 113627   9.920%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.677271/  1.230549, val:  60.42%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.0603%\n",
      "layer   3  Sparsity: 71.0856%\n",
      "total_backward_count 1155220 real_backward_count 114421   9.905%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.650724/  1.272326, val:  55.42%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5326%\n",
      "layer   3  Sparsity: 70.6520%\n",
      "total_backward_count 1165010 real_backward_count 115220   9.890%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.643322/  1.211169, val:  64.17%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5630%\n",
      "layer   3  Sparsity: 69.6343%\n",
      "total_backward_count 1174800 real_backward_count 116020   9.876%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.640184/  1.234167, val:  60.42%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1925%\n",
      "layer   3  Sparsity: 70.4448%\n",
      "total_backward_count 1184590 real_backward_count 116786   9.859%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.630070/  1.241899, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3016%\n",
      "layer   3  Sparsity: 70.9767%\n",
      "total_backward_count 1194380 real_backward_count 117564   9.843%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.667901/  1.237546, val:  62.50%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3413%\n",
      "layer   3  Sparsity: 70.3648%\n",
      "total_backward_count 1204170 real_backward_count 118385   9.831%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.681968/  1.296971, val:  59.17%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.2817%\n",
      "layer   3  Sparsity: 71.6941%\n",
      "total_backward_count 1213960 real_backward_count 119220   9.821%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.682403/  1.227834, val:  62.50%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7239%\n",
      "layer   3  Sparsity: 70.6656%\n",
      "total_backward_count 1223750 real_backward_count 120036   9.809%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.649004/  1.222509, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.2845%\n",
      "layer   3  Sparsity: 70.1027%\n",
      "total_backward_count 1233540 real_backward_count 120883   9.800%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.674727/  1.266648, val:  62.92%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1846%\n",
      "layer   3  Sparsity: 72.2349%\n",
      "total_backward_count 1243330 real_backward_count 121718   9.790%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.646281/  1.269274, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1180%\n",
      "layer   3  Sparsity: 70.4830%\n",
      "total_backward_count 1253120 real_backward_count 122507   9.776%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.631363/  1.303964, val:  60.83%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7944%\n",
      "layer   3  Sparsity: 69.1732%\n",
      "total_backward_count 1262910 real_backward_count 123270   9.761%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.624004/  1.205188, val:  62.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3490%\n",
      "layer   3  Sparsity: 69.9698%\n",
      "total_backward_count 1272700 real_backward_count 124048   9.747%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.623690/  1.234517, val:  62.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5905%\n",
      "layer   3  Sparsity: 70.1176%\n",
      "total_backward_count 1282490 real_backward_count 124844   9.735%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.660570/  1.255761, val:  62.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1483%\n",
      "layer   3  Sparsity: 70.0193%\n",
      "total_backward_count 1292280 real_backward_count 125699   9.727%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.677324/  1.255195, val:  61.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3737%\n",
      "layer   3  Sparsity: 71.7819%\n",
      "total_backward_count 1302070 real_backward_count 126493   9.715%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.638585/  1.345859, val:  56.67%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8848%\n",
      "layer   3  Sparsity: 70.2836%\n",
      "total_backward_count 1311860 real_backward_count 127254   9.700%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.615810/  1.306142, val:  56.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.7687%\n",
      "layer   3  Sparsity: 69.7773%\n",
      "total_backward_count 1321650 real_backward_count 127998   9.685%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.657083/  1.292861, val:  57.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3378%\n",
      "layer   3  Sparsity: 69.9830%\n",
      "total_backward_count 1331440 real_backward_count 128775   9.672%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.648018/  1.276789, val:  58.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.2841%\n",
      "layer   3  Sparsity: 71.2952%\n",
      "total_backward_count 1341230 real_backward_count 129548   9.659%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.647387/  1.275540, val:  56.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5402%\n",
      "layer   3  Sparsity: 70.5961%\n",
      "total_backward_count 1351020 real_backward_count 130362   9.649%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.659153/  1.238890, val:  61.67%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.9449%\n",
      "layer   3  Sparsity: 72.0392%\n",
      "total_backward_count 1360810 real_backward_count 131220   9.643%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.642561/  1.387032, val:  51.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1517%\n",
      "layer   3  Sparsity: 70.5948%\n",
      "total_backward_count 1370600 real_backward_count 132032   9.633%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.651787/  1.245785, val:  58.75%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3331%\n",
      "layer   3  Sparsity: 69.6121%\n",
      "total_backward_count 1380390 real_backward_count 132846   9.624%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.634767/  1.230706, val:  65.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3617%\n",
      "layer   3  Sparsity: 70.2136%\n",
      "total_backward_count 1390180 real_backward_count 133657   9.614%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.642378/  1.197135, val:  62.08%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6833%\n",
      "layer   3  Sparsity: 70.2491%\n",
      "total_backward_count 1399970 real_backward_count 134461   9.605%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.614771/  1.240289, val:  54.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.8528%\n",
      "layer   3  Sparsity: 69.7932%\n",
      "total_backward_count 1409760 real_backward_count 135284   9.596%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.611316/  1.165057, val:  64.58%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3981%\n",
      "layer   3  Sparsity: 70.1025%\n",
      "total_backward_count 1419550 real_backward_count 136043   9.584%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.621228/  1.190653, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1103%\n",
      "layer   3  Sparsity: 69.6033%\n",
      "total_backward_count 1429340 real_backward_count 136829   9.573%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.613152/  1.198092, val:  61.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6689%\n",
      "layer   3  Sparsity: 71.1415%\n",
      "total_backward_count 1439130 real_backward_count 137644   9.564%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.599412/  1.345105, val:  55.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6912%\n",
      "layer   3  Sparsity: 70.6210%\n",
      "total_backward_count 1448920 real_backward_count 138341   9.548%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.614684/  1.233936, val:  59.58%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7947%\n",
      "layer   3  Sparsity: 69.9363%\n",
      "total_backward_count 1458710 real_backward_count 139130   9.538%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.616260/  1.300351, val:  55.83%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1145%\n",
      "layer   3  Sparsity: 70.2979%\n",
      "total_backward_count 1468500 real_backward_count 139967   9.531%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.590960/  1.257808, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.0237%\n",
      "layer   3  Sparsity: 69.7173%\n",
      "total_backward_count 1478290 real_backward_count 140705   9.518%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.613702/  1.205620, val:  59.58%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.2227%\n",
      "layer   3  Sparsity: 69.1692%\n",
      "total_backward_count 1488080 real_backward_count 141493   9.508%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.612671/  1.290867, val:  55.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.5095%\n",
      "layer   3  Sparsity: 70.7872%\n",
      "total_backward_count 1497870 real_backward_count 142209   9.494%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.613937/  1.157208, val:  66.67%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.2842%\n",
      "layer   3  Sparsity: 70.6421%\n",
      "total_backward_count 1507660 real_backward_count 142995   9.485%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.592496/  1.285928, val:  56.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.3277%\n",
      "layer   3  Sparsity: 70.2492%\n",
      "total_backward_count 1517450 real_backward_count 143728   9.472%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.631217/  1.209263, val:  62.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.1261%\n",
      "layer   3  Sparsity: 70.9143%\n",
      "total_backward_count 1527240 real_backward_count 144536   9.464%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.631091/  1.255077, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6467%\n",
      "layer   3  Sparsity: 73.0790%\n",
      "total_backward_count 1537030 real_backward_count 145307   9.454%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.650537/  1.225370, val:  65.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.9570%\n",
      "layer   3  Sparsity: 71.8344%\n",
      "total_backward_count 1546820 real_backward_count 146104   9.445%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.664359/  1.251114, val:  60.83%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.2389%\n",
      "layer   3  Sparsity: 72.1638%\n",
      "total_backward_count 1556610 real_backward_count 146920   9.438%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.654214/  1.266323, val:  60.83%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5896%\n",
      "layer   3  Sparsity: 72.1471%\n",
      "total_backward_count 1566400 real_backward_count 147658   9.427%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.632536/  1.245070, val:  58.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.8935%\n",
      "layer   3  Sparsity: 71.6641%\n",
      "total_backward_count 1576190 real_backward_count 148431   9.417%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.651784/  1.234092, val:  63.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6049%\n",
      "layer   3  Sparsity: 70.5269%\n",
      "total_backward_count 1585980 real_backward_count 149220   9.409%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.609677/  1.163687, val:  65.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5972%\n",
      "layer   3  Sparsity: 70.4351%\n",
      "total_backward_count 1595770 real_backward_count 149927   9.395%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.611851/  1.201145, val:  61.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7685%\n",
      "layer   3  Sparsity: 71.2605%\n",
      "total_backward_count 1605560 real_backward_count 150672   9.384%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.622539/  1.245681, val:  61.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6634%\n",
      "layer   3  Sparsity: 71.2553%\n",
      "total_backward_count 1615350 real_backward_count 151419   9.374%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.644444/  1.241570, val:  61.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.6793%\n",
      "layer   3  Sparsity: 71.9438%\n",
      "total_backward_count 1625140 real_backward_count 152215   9.366%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.649292/  1.236090, val:  60.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.9702%\n",
      "layer   3  Sparsity: 72.0696%\n",
      "total_backward_count 1634930 real_backward_count 153019   9.359%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.646628/  1.367222, val:  52.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3956%\n",
      "layer   3  Sparsity: 72.2043%\n",
      "total_backward_count 1644720 real_backward_count 153800   9.351%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.610022/  1.474303, val:  52.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.0128%\n",
      "layer   3  Sparsity: 69.9506%\n",
      "total_backward_count 1654510 real_backward_count 154585   9.343%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.629162/  1.231692, val:  62.50%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1974%\n",
      "layer   3  Sparsity: 69.6972%\n",
      "total_backward_count 1664300 real_backward_count 155327   9.333%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.638694/  1.299276, val:  55.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.9325%\n",
      "layer   3  Sparsity: 70.6119%\n",
      "total_backward_count 1674090 real_backward_count 156040   9.321%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.581143/  1.265705, val:  55.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.6614%\n",
      "layer   3  Sparsity: 68.7182%\n",
      "total_backward_count 1683880 real_backward_count 156774   9.310%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.606748/  1.221926, val:  60.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.4271%\n",
      "layer   3  Sparsity: 69.8676%\n",
      "total_backward_count 1693670 real_backward_count 157526   9.301%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.569218/  1.174532, val:  62.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.5519%\n",
      "layer   3  Sparsity: 69.4706%\n",
      "total_backward_count 1703460 real_backward_count 158268   9.291%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.577769/  1.167413, val:  62.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.1087%\n",
      "layer   3  Sparsity: 69.9508%\n",
      "total_backward_count 1713250 real_backward_count 159006   9.281%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.580045/  1.217925, val:  63.75%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.7640%\n",
      "layer   3  Sparsity: 69.5476%\n",
      "total_backward_count 1723040 real_backward_count 159793   9.274%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.573066/  1.160578, val:  63.75%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8437%\n",
      "layer   3  Sparsity: 69.4801%\n",
      "total_backward_count 1732830 real_backward_count 160548   9.265%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.559336/  1.135022, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.7473%\n",
      "layer   3  Sparsity: 70.1713%\n",
      "total_backward_count 1742620 real_backward_count 161244   9.253%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.556163/  1.342425, val:  54.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9972%\n",
      "layer   3  Sparsity: 70.1014%\n",
      "total_backward_count 1752410 real_backward_count 161995   9.244%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.556706/  1.186467, val:  60.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.5607%\n",
      "layer   3  Sparsity: 70.1210%\n",
      "total_backward_count 1762200 real_backward_count 162729   9.234%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.556860/  1.147071, val:  62.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1119%\n",
      "layer   3  Sparsity: 70.4178%\n",
      "total_backward_count 1771990 real_backward_count 163430   9.223%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.532426/  1.187764, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7745%\n",
      "layer   3  Sparsity: 68.3014%\n",
      "total_backward_count 1781780 real_backward_count 164096   9.210%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.556866/  1.213786, val:  60.00%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.4023%\n",
      "layer   3  Sparsity: 69.1628%\n",
      "total_backward_count 1791570 real_backward_count 164795   9.198%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.565887/  1.259328, val:  57.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3320%\n",
      "layer   3  Sparsity: 69.7298%\n",
      "total_backward_count 1801360 real_backward_count 165508   9.188%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.556712/  1.210796, val:  62.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3407%\n",
      "layer   3  Sparsity: 68.9338%\n",
      "total_backward_count 1811150 real_backward_count 166211   9.177%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.552080/  1.147946, val:  64.17%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7281%\n",
      "layer   3  Sparsity: 68.4764%\n",
      "total_backward_count 1820940 real_backward_count 166957   9.169%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.569768/  1.195172, val:  63.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 70.6706%\n",
      "layer   3  Sparsity: 70.3048%\n",
      "total_backward_count 1830730 real_backward_count 167718   9.161%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.600346/  1.242151, val:  56.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.3471%\n",
      "layer   3  Sparsity: 71.1400%\n",
      "total_backward_count 1840520 real_backward_count 168465   9.153%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.573500/  1.176400, val:  64.17%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.5926%\n",
      "layer   3  Sparsity: 70.1930%\n",
      "total_backward_count 1850310 real_backward_count 169183   9.143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc layer 3 self.abs_max_out: 2979.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.585612/  1.166876, val:  62.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7461%\n",
      "layer   3  Sparsity: 69.8847%\n",
      "total_backward_count 1860100 real_backward_count 169920   9.135%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.583759/  1.180507, val:  62.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.4282%\n",
      "layer   3  Sparsity: 69.3226%\n",
      "total_backward_count 1869890 real_backward_count 170678   9.128%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.565669/  1.259377, val:  56.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 71.7975%\n",
      "layer   3  Sparsity: 68.6461%\n",
      "total_backward_count 1879680 real_backward_count 171436   9.120%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.555352/  1.215552, val:  60.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.6788%\n",
      "layer   3  Sparsity: 68.9518%\n",
      "total_backward_count 1889470 real_backward_count 172152   9.111%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.553820/  1.313459, val:  52.08%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.8862%\n",
      "layer   3  Sparsity: 69.1220%\n",
      "total_backward_count 1899260 real_backward_count 172850   9.101%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.554653/  1.319497, val:  56.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.9780%\n",
      "layer   3  Sparsity: 68.9590%\n",
      "total_backward_count 1909050 real_backward_count 173581   9.093%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.562383/  1.246147, val:  57.08%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.3525%\n",
      "layer   3  Sparsity: 69.4180%\n",
      "total_backward_count 1918840 real_backward_count 174334   9.085%\n",
      "fc layer 3 self.abs_max_out: 2984.0\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.546393/  1.194461, val:  64.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.1258%\n",
      "layer   3  Sparsity: 68.6760%\n",
      "total_backward_count 1928630 real_backward_count 175033   9.076%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.569366/  1.316658, val:  53.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.2969%\n",
      "layer   3  Sparsity: 69.4255%\n",
      "total_backward_count 1938420 real_backward_count 175778   9.068%\n",
      "fc layer 3 self.abs_max_out: 3001.0\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.549721/  1.323797, val:  58.75%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 72.6877%\n",
      "layer   3  Sparsity: 69.7118%\n",
      "total_backward_count 1948210 real_backward_count 176489   9.059%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.577021/  1.168461, val:  62.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.6628%\n",
      "layer   2  Sparsity: 73.2427%\n",
      "layer   3  Sparsity: 70.1104%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b03c4314cd345c79cdbd74b70fa933b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÉ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.57702</td></tr><tr><td>val_acc_best</td><td>0.67917</td></tr><tr><td>val_acc_now</td><td>0.625</td></tr><tr><td>val_loss</td><td>1.16846</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-sweep-98</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0y80rrqx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/0y80rrqx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_065802-0y80rrqx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n9xtvc1b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_111628-n9xtvc1b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/n9xtvc1b' target=\"_blank\">tough-sweep-104</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/n9xtvc1b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/n9xtvc1b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251116_111638_499', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 173.0\n",
      "lif layer 1 self.abs_max_v: 173.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 243.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "fc layer 1 self.abs_max_out: 207.0\n",
      "lif layer 1 self.abs_max_v: 229.0\n",
      "fc layer 2 self.abs_max_out: 472.0\n",
      "lif layer 2 self.abs_max_v: 553.5\n",
      "lif layer 1 self.abs_max_v: 269.0\n",
      "lif layer 2 self.abs_max_v: 612.0\n",
      "fc layer 1 self.abs_max_out: 248.0\n",
      "lif layer 1 self.abs_max_v: 279.5\n",
      "fc layer 1 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 346.0\n",
      "lif layer 2 self.abs_max_v: 634.5\n",
      "fc layer 1 self.abs_max_out: 370.0\n",
      "lif layer 1 self.abs_max_v: 415.5\n",
      "fc layer 2 self.abs_max_out: 473.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 1 self.abs_max_out: 390.0\n",
      "lif layer 1 self.abs_max_v: 489.0\n",
      "fc layer 2 self.abs_max_out: 516.0\n",
      "lif layer 2 self.abs_max_v: 813.0\n",
      "fc layer 1 self.abs_max_out: 395.0\n",
      "lif layer 1 self.abs_max_v: 601.5\n",
      "lif layer 2 self.abs_max_v: 813.5\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 416.0\n",
      "fc layer 1 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "fc layer 1 self.abs_max_out: 522.0\n",
      "lif layer 1 self.abs_max_v: 624.0\n",
      "lif layer 1 self.abs_max_v: 649.5\n",
      "lif layer 1 self.abs_max_v: 672.0\n",
      "lif layer 1 self.abs_max_v: 749.0\n",
      "fc layer 1 self.abs_max_out: 567.0\n",
      "lif layer 1 self.abs_max_v: 847.5\n",
      "fc layer 2 self.abs_max_out: 603.0\n",
      "lif layer 2 self.abs_max_v: 958.0\n",
      "fc layer 1 self.abs_max_out: 600.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 1 self.abs_max_out: 738.0\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 921.5\n",
      "fc layer 2 self.abs_max_out: 640.0\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "fc layer 2 self.abs_max_out: 655.0\n",
      "fc layer 2 self.abs_max_out: 686.0\n",
      "lif layer 2 self.abs_max_v: 1031.0\n",
      "lif layer 2 self.abs_max_v: 1139.5\n",
      "fc layer 1 self.abs_max_out: 938.0\n",
      "lif layer 1 self.abs_max_v: 938.0\n",
      "lif layer 1 self.abs_max_v: 1102.5\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "fc layer 3 self.abs_max_out: 294.0\n",
      "lif layer 1 self.abs_max_v: 1111.5\n",
      "fc layer 2 self.abs_max_out: 726.0\n",
      "fc layer 1 self.abs_max_out: 988.0\n",
      "lif layer 1 self.abs_max_v: 1153.0\n",
      "lif layer 1 self.abs_max_v: 1274.5\n",
      "lif layer 2 self.abs_max_v: 1152.5\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 2 self.abs_max_out: 729.0\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 1388.5\n",
      "lif layer 1 self.abs_max_v: 1397.5\n",
      "lif layer 1 self.abs_max_v: 1536.0\n",
      "fc layer 2 self.abs_max_out: 835.0\n",
      "fc layer 1 self.abs_max_out: 1127.0\n",
      "fc layer 1 self.abs_max_out: 1215.0\n",
      "lif layer 1 self.abs_max_v: 1656.5\n",
      "fc layer 2 self.abs_max_out: 837.0\n",
      "lif layer 2 self.abs_max_v: 1158.5\n",
      "lif layer 1 self.abs_max_v: 1671.0\n",
      "fc layer 1 self.abs_max_out: 1234.0\n",
      "fc layer 2 self.abs_max_out: 920.0\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 1 self.abs_max_out: 1294.0\n",
      "fc layer 2 self.abs_max_out: 966.0\n",
      "fc layer 1 self.abs_max_out: 1407.0\n",
      "fc layer 2 self.abs_max_out: 994.0\n",
      "lif layer 2 self.abs_max_v: 1188.5\n",
      "fc layer 2 self.abs_max_out: 1012.0\n",
      "lif layer 2 self.abs_max_v: 1279.0\n",
      "fc layer 2 self.abs_max_out: 1018.0\n",
      "lif layer 1 self.abs_max_v: 1764.5\n",
      "lif layer 1 self.abs_max_v: 1945.5\n",
      "lif layer 2 self.abs_max_v: 1367.5\n",
      "fc layer 2 self.abs_max_out: 1038.0\n",
      "fc layer 1 self.abs_max_out: 1719.0\n",
      "lif layer 1 self.abs_max_v: 2020.0\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1414.0\n",
      "lif layer 2 self.abs_max_v: 1417.0\n",
      "lif layer 2 self.abs_max_v: 1456.5\n",
      "lif layer 2 self.abs_max_v: 1468.5\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "lif layer 2 self.abs_max_v: 1471.5\n",
      "lif layer 2 self.abs_max_v: 1506.0\n",
      "fc layer 2 self.abs_max_out: 1178.0\n",
      "lif layer 2 self.abs_max_v: 1531.5\n",
      "lif layer 2 self.abs_max_v: 1534.0\n",
      "lif layer 2 self.abs_max_v: 1634.0\n",
      "lif layer 2 self.abs_max_v: 1662.5\n",
      "lif layer 2 self.abs_max_v: 1705.5\n",
      "lif layer 2 self.abs_max_v: 1742.0\n",
      "lif layer 2 self.abs_max_v: 1822.0\n",
      "lif layer 2 self.abs_max_v: 1873.0\n",
      "lif layer 2 self.abs_max_v: 1920.5\n",
      "lif layer 2 self.abs_max_v: 1941.5\n",
      "lif layer 2 self.abs_max_v: 1964.0\n",
      "lif layer 2 self.abs_max_v: 1974.5\n",
      "lif layer 1 self.abs_max_v: 2134.0\n",
      "lif layer 1 self.abs_max_v: 2198.0\n",
      "lif layer 2 self.abs_max_v: 1975.5\n",
      "lif layer 2 self.abs_max_v: 1988.0\n",
      "lif layer 2 self.abs_max_v: 2014.5\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "lif layer 1 self.abs_max_v: 2438.0\n",
      "lif layer 1 self.abs_max_v: 2760.0\n",
      "fc layer 1 self.abs_max_out: 1804.0\n",
      "lif layer 2 self.abs_max_v: 2036.5\n",
      "lif layer 2 self.abs_max_v: 2092.5\n",
      "lif layer 2 self.abs_max_v: 2121.5\n",
      "lif layer 2 self.abs_max_v: 2147.0\n",
      "lif layer 2 self.abs_max_v: 2150.5\n",
      "fc layer 2 self.abs_max_out: 1305.0\n",
      "fc layer 2 self.abs_max_out: 1310.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "lif layer 1 self.abs_max_v: 3047.0\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "fc layer 1 self.abs_max_out: 1821.0\n",
      "fc layer 3 self.abs_max_out: 529.0\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 558.0\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 1873.0\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "fc layer 1 self.abs_max_out: 1905.0\n",
      "lif layer 1 self.abs_max_v: 3342.5\n",
      "fc layer 1 self.abs_max_out: 1982.0\n",
      "lif layer 1 self.abs_max_v: 3356.5\n",
      "fc layer 1 self.abs_max_out: 1983.0\n",
      "lif layer 1 self.abs_max_v: 3378.0\n",
      "lif layer 2 self.abs_max_v: 2169.5\n",
      "lif layer 2 self.abs_max_v: 2199.0\n",
      "lif layer 2 self.abs_max_v: 2275.0\n",
      "lif layer 2 self.abs_max_v: 2377.0\n",
      "lif layer 1 self.abs_max_v: 3438.5\n",
      "fc layer 1 self.abs_max_out: 2020.0\n",
      "lif layer 1 self.abs_max_v: 3737.5\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "fc layer 1 self.abs_max_out: 2083.0\n",
      "fc layer 1 self.abs_max_out: 2213.0\n",
      "lif layer 1 self.abs_max_v: 4052.5\n",
      "fc layer 2 self.abs_max_out: 1429.0\n",
      "fc layer 1 self.abs_max_out: 2363.0\n",
      "fc layer 1 self.abs_max_out: 2448.0\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "fc layer 1 self.abs_max_out: 2572.0\n",
      "lif layer 1 self.abs_max_v: 4519.0\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "fc layer 2 self.abs_max_out: 1525.0\n",
      "lif layer 1 self.abs_max_v: 4653.0\n",
      "fc layer 1 self.abs_max_out: 2822.0\n",
      "lif layer 1 self.abs_max_v: 5148.5\n",
      "lif layer 2 self.abs_max_v: 2412.5\n",
      "fc layer 1 self.abs_max_out: 3080.0\n",
      "lif layer 1 self.abs_max_v: 5151.5\n",
      "fc layer 3 self.abs_max_out: 748.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "lif layer 2 self.abs_max_v: 2479.0\n",
      "fc layer 2 self.abs_max_out: 1558.0\n",
      "lif layer 1 self.abs_max_v: 5157.5\n",
      "lif layer 1 self.abs_max_v: 5311.0\n",
      "fc layer 2 self.abs_max_out: 1560.0\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1607.0\n",
      "fc layer 2 self.abs_max_out: 1628.0\n",
      "lif layer 2 self.abs_max_v: 2520.0\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "lif layer 2 self.abs_max_v: 2529.5\n",
      "lif layer 1 self.abs_max_v: 5392.5\n",
      "fc layer 2 self.abs_max_out: 1747.0\n",
      "fc layer 1 self.abs_max_out: 3118.0\n",
      "fc layer 1 self.abs_max_out: 3230.0\n",
      "lif layer 1 self.abs_max_v: 5422.5\n",
      "fc layer 1 self.abs_max_out: 3428.0\n",
      "lif layer 1 self.abs_max_v: 6139.5\n",
      "fc layer 1 self.abs_max_out: 3574.0\n",
      "lif layer 1 self.abs_max_v: 6445.0\n",
      "lif layer 1 self.abs_max_v: 6729.5\n",
      "fc layer 1 self.abs_max_out: 3587.0\n",
      "lif layer 1 self.abs_max_v: 6747.0\n",
      "lif layer 2 self.abs_max_v: 2629.5\n",
      "lif layer 2 self.abs_max_v: 2673.5\n",
      "lif layer 2 self.abs_max_v: 2730.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.420360/  1.816958, val:  37.92%, val_best:  37.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.6648%\n",
      "layer   3  Sparsity: 64.5648%\n",
      "total_backward_count 9790 real_backward_count 1381  14.106%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1855.0\n",
      "lif layer 2 self.abs_max_v: 2796.5\n",
      "lif layer 2 self.abs_max_v: 2937.5\n",
      "lif layer 2 self.abs_max_v: 2948.0\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "fc layer 2 self.abs_max_out: 1907.0\n",
      "lif layer 2 self.abs_max_v: 3050.0\n",
      "lif layer 2 self.abs_max_v: 3167.5\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 2 self.abs_max_out: 1957.0\n",
      "lif layer 2 self.abs_max_v: 3310.0\n",
      "lif layer 2 self.abs_max_v: 3358.0\n",
      "fc layer 2 self.abs_max_out: 1987.0\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "fc layer 2 self.abs_max_out: 2138.0\n",
      "fc layer 2 self.abs_max_out: 2161.0\n",
      "fc layer 1 self.abs_max_out: 3656.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "fc layer 2 self.abs_max_out: 2236.0\n",
      "lif layer 1 self.abs_max_v: 6802.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.246985/  1.781030, val:  39.58%, val_best:  39.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4502%\n",
      "layer   3  Sparsity: 66.1237%\n",
      "total_backward_count 19580 real_backward_count 2699  13.784%\n",
      "fc layer 1 self.abs_max_out: 3705.0\n",
      "fc layer 2 self.abs_max_out: 2260.0\n",
      "fc layer 2 self.abs_max_out: 2352.0\n",
      "lif layer 2 self.abs_max_v: 3370.0\n",
      "fc layer 1 self.abs_max_out: 3820.0\n",
      "fc layer 3 self.abs_max_out: 860.0\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "fc layer 3 self.abs_max_out: 898.0\n",
      "lif layer 2 self.abs_max_v: 3379.0\n",
      "lif layer 2 self.abs_max_v: 3733.5\n",
      "lif layer 2 self.abs_max_v: 3852.5\n",
      "fc layer 1 self.abs_max_out: 4212.0\n",
      "lif layer 1 self.abs_max_v: 6884.0\n",
      "lif layer 1 self.abs_max_v: 7193.0\n",
      "fc layer 1 self.abs_max_out: 4248.0\n",
      "lif layer 1 self.abs_max_v: 7656.5\n",
      "lif layer 1 self.abs_max_v: 7772.5\n",
      "lif layer 1 self.abs_max_v: 7947.5\n",
      "fc layer 1 self.abs_max_out: 4335.0\n",
      "lif layer 1 self.abs_max_v: 8264.0\n",
      "fc layer 1 self.abs_max_out: 4404.0\n",
      "lif layer 1 self.abs_max_v: 8536.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.207401/  1.677876, val:  42.08%, val_best:  42.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.6678%\n",
      "layer   3  Sparsity: 66.7278%\n",
      "total_backward_count 29370 real_backward_count 4017  13.677%\n",
      "fc layer 1 self.abs_max_out: 4454.0\n",
      "lif layer 2 self.abs_max_v: 4102.0\n",
      "fc layer 2 self.abs_max_out: 2392.0\n",
      "fc layer 1 self.abs_max_out: 4587.0\n",
      "fc layer 1 self.abs_max_out: 4664.0\n",
      "lif layer 1 self.abs_max_v: 8659.5\n",
      "lif layer 1 self.abs_max_v: 8890.0\n",
      "fc layer 1 self.abs_max_out: 4764.0\n",
      "lif layer 1 self.abs_max_v: 9209.0\n",
      "fc layer 1 self.abs_max_out: 4850.0\n",
      "lif layer 1 self.abs_max_v: 9454.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.188003/  1.687987, val:  43.75%, val_best:  43.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.4555%\n",
      "layer   3  Sparsity: 67.1318%\n",
      "total_backward_count 39160 real_backward_count 5300  13.534%\n",
      "lif layer 2 self.abs_max_v: 4145.0\n",
      "lif layer 2 self.abs_max_v: 4158.0\n",
      "lif layer 2 self.abs_max_v: 4170.0\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "fc layer 3 self.abs_max_out: 904.0\n",
      "fc layer 3 self.abs_max_out: 923.0\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.119594/  1.561195, val:  46.25%, val_best:  46.25%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.8240%\n",
      "layer   3  Sparsity: 68.7685%\n",
      "total_backward_count 48950 real_backward_count 6547  13.375%\n",
      "fc layer 3 self.abs_max_out: 979.0\n",
      "fc layer 2 self.abs_max_out: 2414.0\n",
      "fc layer 2 self.abs_max_out: 2496.0\n",
      "lif layer 2 self.abs_max_v: 4231.5\n",
      "lif layer 2 self.abs_max_v: 4360.0\n",
      "lif layer 2 self.abs_max_v: 4567.0\n",
      "fc layer 2 self.abs_max_out: 2523.0\n",
      "fc layer 1 self.abs_max_out: 5195.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.038170/  1.549292, val:  52.08%, val_best:  52.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0074%\n",
      "layer   3  Sparsity: 67.8048%\n",
      "total_backward_count 58740 real_backward_count 7694  13.098%\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.018876/  1.536491, val:  52.50%, val_best:  52.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1190%\n",
      "layer   3  Sparsity: 68.0459%\n",
      "total_backward_count 68530 real_backward_count 8829  12.883%\n",
      "lif layer 2 self.abs_max_v: 4575.0\n",
      "lif layer 2 self.abs_max_v: 4723.0\n",
      "fc layer 3 self.abs_max_out: 987.0\n",
      "fc layer 3 self.abs_max_out: 1045.0\n",
      "fc layer 1 self.abs_max_out: 5375.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  0.981492/  1.564465, val:  41.67%, val_best:  52.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.6192%\n",
      "layer   3  Sparsity: 68.4911%\n",
      "total_backward_count 78320 real_backward_count 9950  12.704%\n",
      "fc layer 2 self.abs_max_out: 2563.0\n",
      "fc layer 2 self.abs_max_out: 2580.0\n",
      "lif layer 2 self.abs_max_v: 4729.5\n",
      "fc layer 2 self.abs_max_out: 2860.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  0.949766/  1.351549, val:  60.00%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.6977%\n",
      "layer   3  Sparsity: 68.3451%\n",
      "total_backward_count 88110 real_backward_count 11110  12.609%\n",
      "fc layer 3 self.abs_max_out: 1048.0\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "lif layer 2 self.abs_max_v: 4736.0\n",
      "lif layer 2 self.abs_max_v: 4862.0\n",
      "lif layer 2 self.abs_max_v: 4919.0\n",
      "fc layer 1 self.abs_max_out: 5394.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  0.894336/  1.454902, val:  48.75%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.2901%\n",
      "layer   3  Sparsity: 68.2005%\n",
      "total_backward_count 97900 real_backward_count 12129  12.389%\n",
      "fc layer 3 self.abs_max_out: 1072.0\n",
      "fc layer 2 self.abs_max_out: 2950.0\n",
      "fc layer 1 self.abs_max_out: 5989.0\n",
      "lif layer 1 self.abs_max_v: 9735.0\n",
      "lif layer 1 self.abs_max_v: 10166.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  0.862294/  1.391578, val:  55.42%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.0591%\n",
      "layer   3  Sparsity: 68.4307%\n",
      "total_backward_count 107690 real_backward_count 13114  12.178%\n",
      "fc layer 3 self.abs_max_out: 1073.0\n",
      "fc layer 3 self.abs_max_out: 1115.0\n",
      "lif layer 2 self.abs_max_v: 5021.0\n",
      "lif layer 2 self.abs_max_v: 5146.0\n",
      "fc layer 3 self.abs_max_out: 1129.0\n",
      "fc layer 3 self.abs_max_out: 1130.0\n",
      "fc layer 3 self.abs_max_out: 1138.0\n",
      "fc layer 3 self.abs_max_out: 1195.0\n",
      "fc layer 1 self.abs_max_out: 6015.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  0.832702/  1.270896, val:  61.67%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.7579%\n",
      "layer   3  Sparsity: 68.0240%\n",
      "total_backward_count 117480 real_backward_count 14085  11.989%\n",
      "fc layer 1 self.abs_max_out: 6305.0\n",
      "lif layer 1 self.abs_max_v: 10242.0\n",
      "lif layer 1 self.abs_max_v: 10853.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  0.771248/  1.298550, val:  62.50%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 76.1975%\n",
      "layer   3  Sparsity: 68.1437%\n",
      "total_backward_count 127270 real_backward_count 15009  11.793%\n",
      "lif layer 2 self.abs_max_v: 5156.0\n",
      "fc layer 3 self.abs_max_out: 1212.0\n",
      "fc layer 3 self.abs_max_out: 1218.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  0.739169/  1.224510, val:  62.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.7913%\n",
      "layer   3  Sparsity: 67.7015%\n",
      "total_backward_count 137060 real_backward_count 15877  11.584%\n",
      "fc layer 3 self.abs_max_out: 1292.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  0.735824/  1.292999, val:  59.58%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.9558%\n",
      "layer   3  Sparsity: 68.3694%\n",
      "total_backward_count 146850 real_backward_count 16700  11.372%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  0.726237/  1.265551, val:  66.67%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.5112%\n",
      "layer   3  Sparsity: 68.5686%\n",
      "total_backward_count 156640 real_backward_count 17568  11.216%\n",
      "fc layer 3 self.abs_max_out: 1332.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.684582/  1.091401, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.5307%\n",
      "layer   3  Sparsity: 68.4989%\n",
      "total_backward_count 166430 real_backward_count 18348  11.024%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.639517/  1.109655, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.4880%\n",
      "layer   3  Sparsity: 68.8654%\n",
      "total_backward_count 176220 real_backward_count 19089  10.832%\n",
      "fc layer 3 self.abs_max_out: 1359.0\n",
      "fc layer 2 self.abs_max_out: 3229.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  0.629795/  1.226547, val:  67.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.3991%\n",
      "layer   3  Sparsity: 68.5981%\n",
      "total_backward_count 186010 real_backward_count 19803  10.646%\n",
      "fc layer 1 self.abs_max_out: 6327.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.600778/  1.179902, val:  58.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.5135%\n",
      "layer   3  Sparsity: 68.4226%\n",
      "total_backward_count 195800 real_backward_count 20499  10.469%\n",
      "fc layer 3 self.abs_max_out: 1371.0\n",
      "fc layer 1 self.abs_max_out: 6341.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.551970/  1.058449, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.4427%\n",
      "layer   3  Sparsity: 68.9103%\n",
      "total_backward_count 205590 real_backward_count 21153  10.289%\n",
      "lif layer 2 self.abs_max_v: 5230.5\n",
      "lif layer 2 self.abs_max_v: 5291.5\n",
      "fc layer 1 self.abs_max_out: 6570.0\n",
      "lif layer 1 self.abs_max_v: 11394.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.559453/  1.116576, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.2090%\n",
      "layer   3  Sparsity: 68.9808%\n",
      "total_backward_count 215380 real_backward_count 21754  10.100%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.555974/  1.085510, val:  76.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8053%\n",
      "layer   3  Sparsity: 68.6750%\n",
      "total_backward_count 225170 real_backward_count 22339   9.921%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.524735/  1.026974, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0296%\n",
      "layer   3  Sparsity: 69.2902%\n",
      "total_backward_count 234960 real_backward_count 22882   9.739%\n",
      "fc layer 3 self.abs_max_out: 1384.0\n",
      "fc layer 3 self.abs_max_out: 1388.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.511858/  1.025085, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0858%\n",
      "layer   3  Sparsity: 69.7200%\n",
      "total_backward_count 244750 real_backward_count 23372   9.549%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.503539/  0.976080, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9533%\n",
      "layer   3  Sparsity: 69.6182%\n",
      "total_backward_count 254540 real_backward_count 23891   9.386%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.478527/  0.999735, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8935%\n",
      "layer   3  Sparsity: 69.6230%\n",
      "total_backward_count 264330 real_backward_count 24326   9.203%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.462960/  1.008807, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0200%\n",
      "layer   3  Sparsity: 69.9830%\n",
      "total_backward_count 274120 real_backward_count 24769   9.036%\n",
      "fc layer 3 self.abs_max_out: 1439.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.445913/  0.925831, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.2227%\n",
      "layer   3  Sparsity: 69.7882%\n",
      "total_backward_count 283910 real_backward_count 25197   8.875%\n",
      "fc layer 3 self.abs_max_out: 1463.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.435732/  0.983867, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.2068%\n",
      "layer   3  Sparsity: 70.6344%\n",
      "total_backward_count 293700 real_backward_count 25568   8.705%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.433400/  0.960379, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9214%\n",
      "layer   3  Sparsity: 70.4950%\n",
      "total_backward_count 303490 real_backward_count 25924   8.542%\n",
      "fc layer 3 self.abs_max_out: 1465.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.413991/  0.941420, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8233%\n",
      "layer   3  Sparsity: 70.2056%\n",
      "total_backward_count 313280 real_backward_count 26254   8.380%\n",
      "fc layer 3 self.abs_max_out: 1489.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.405659/  0.948640, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8797%\n",
      "layer   3  Sparsity: 70.3039%\n",
      "total_backward_count 323070 real_backward_count 26550   8.218%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.413526/  0.978557, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0676%\n",
      "layer   3  Sparsity: 70.5092%\n",
      "total_backward_count 332860 real_backward_count 26848   8.066%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.402068/  0.982737, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.4849%\n",
      "layer   3  Sparsity: 70.4703%\n",
      "total_backward_count 342650 real_backward_count 27122   7.915%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.406057/  0.952486, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0964%\n",
      "layer   3  Sparsity: 69.7975%\n",
      "total_backward_count 352440 real_backward_count 27431   7.783%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.378647/  0.955636, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1463%\n",
      "layer   3  Sparsity: 70.0707%\n",
      "total_backward_count 362230 real_backward_count 27710   7.650%\n",
      "fc layer 3 self.abs_max_out: 1531.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.379711/  1.033261, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.3566%\n",
      "layer   3  Sparsity: 69.8556%\n",
      "total_backward_count 372020 real_backward_count 27965   7.517%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.365654/  0.934520, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.3397%\n",
      "layer   3  Sparsity: 70.8950%\n",
      "total_backward_count 381810 real_backward_count 28192   7.384%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.339226/  0.908476, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.8574%\n",
      "layer   3  Sparsity: 70.8570%\n",
      "total_backward_count 391600 real_backward_count 28355   7.241%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.343126/  0.891697, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.4577%\n",
      "layer   3  Sparsity: 70.8130%\n",
      "total_backward_count 401390 real_backward_count 28584   7.121%\n",
      "lif layer 2 self.abs_max_v: 5342.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.343947/  0.878284, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1515%\n",
      "layer   3  Sparsity: 70.8028%\n",
      "total_backward_count 411180 real_backward_count 28810   7.007%\n",
      "fc layer 3 self.abs_max_out: 1535.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.329894/  0.891261, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.3993%\n",
      "layer   3  Sparsity: 70.6116%\n",
      "total_backward_count 420970 real_backward_count 28994   6.887%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.328665/  0.925692, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.2175%\n",
      "layer   3  Sparsity: 71.2638%\n",
      "total_backward_count 430760 real_backward_count 29182   6.775%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.340239/  0.898902, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0762%\n",
      "layer   3  Sparsity: 71.1590%\n",
      "total_backward_count 440550 real_backward_count 29365   6.666%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.338934/  0.927422, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8767%\n",
      "layer   3  Sparsity: 70.9478%\n",
      "total_backward_count 450340 real_backward_count 29521   6.555%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.322421/  0.937506, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9752%\n",
      "layer   3  Sparsity: 70.8967%\n",
      "total_backward_count 460130 real_backward_count 29687   6.452%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.311623/  0.914699, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.5914%\n",
      "layer   3  Sparsity: 71.8214%\n",
      "total_backward_count 469920 real_backward_count 29833   6.349%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.294549/  0.891850, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7421%\n",
      "layer   3  Sparsity: 71.8779%\n",
      "total_backward_count 479710 real_backward_count 29964   6.246%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.291760/  0.889213, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0265%\n",
      "layer   3  Sparsity: 71.7973%\n",
      "total_backward_count 489500 real_backward_count 30080   6.145%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.303809/  0.875899, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7496%\n",
      "layer   3  Sparsity: 71.5290%\n",
      "total_backward_count 499290 real_backward_count 30222   6.053%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.291089/  0.889492, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6832%\n",
      "layer   3  Sparsity: 71.5439%\n",
      "total_backward_count 509080 real_backward_count 30336   5.959%\n",
      "fc layer 3 self.abs_max_out: 1551.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.291005/  0.875426, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7717%\n",
      "layer   3  Sparsity: 71.1186%\n",
      "total_backward_count 518870 real_backward_count 30476   5.874%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.285258/  0.898174, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9035%\n",
      "layer   3  Sparsity: 71.4428%\n",
      "total_backward_count 528660 real_backward_count 30578   5.784%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.283158/  0.867426, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9684%\n",
      "layer   3  Sparsity: 71.1935%\n",
      "total_backward_count 538450 real_backward_count 30697   5.701%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.282465/  0.898934, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7923%\n",
      "layer   3  Sparsity: 71.4945%\n",
      "total_backward_count 548240 real_backward_count 30801   5.618%\n",
      "fc layer 2 self.abs_max_out: 3241.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.285513/  0.889904, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8839%\n",
      "layer   3  Sparsity: 71.7885%\n",
      "total_backward_count 558030 real_backward_count 30912   5.539%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.282512/  0.840820, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7218%\n",
      "layer   3  Sparsity: 71.8736%\n",
      "total_backward_count 567820 real_backward_count 31015   5.462%\n",
      "lif layer 2 self.abs_max_v: 5352.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.267899/  0.875289, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8086%\n",
      "layer   3  Sparsity: 72.4127%\n",
      "total_backward_count 577610 real_backward_count 31102   5.385%\n",
      "lif layer 2 self.abs_max_v: 5365.5\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.259753/  0.884750, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7422%\n",
      "layer   3  Sparsity: 72.7036%\n",
      "total_backward_count 587400 real_backward_count 31183   5.309%\n",
      "lif layer 2 self.abs_max_v: 5422.0\n",
      "fc layer 2 self.abs_max_out: 3247.0\n",
      "lif layer 2 self.abs_max_v: 5740.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.257978/  0.890469, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6498%\n",
      "layer   3  Sparsity: 72.5327%\n",
      "total_backward_count 597190 real_backward_count 31250   5.233%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.264010/  0.946951, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9276%\n",
      "layer   3  Sparsity: 72.1957%\n",
      "total_backward_count 606980 real_backward_count 31344   5.164%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.248813/  0.894383, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9361%\n",
      "layer   3  Sparsity: 72.3360%\n",
      "total_backward_count 616770 real_backward_count 31422   5.095%\n",
      "fc layer 3 self.abs_max_out: 1554.0\n",
      "fc layer 3 self.abs_max_out: 1589.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.262136/  0.870431, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7662%\n",
      "layer   3  Sparsity: 72.5566%\n",
      "total_backward_count 626560 real_backward_count 31511   5.029%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.257197/  0.875087, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6405%\n",
      "layer   3  Sparsity: 72.7323%\n",
      "total_backward_count 636350 real_backward_count 31570   4.961%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.255745/  0.871342, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7795%\n",
      "layer   3  Sparsity: 72.4994%\n",
      "total_backward_count 646140 real_backward_count 31619   4.894%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.253982/  0.899382, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8763%\n",
      "layer   3  Sparsity: 72.9256%\n",
      "total_backward_count 655930 real_backward_count 31692   4.832%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.250446/  0.880439, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9797%\n",
      "layer   3  Sparsity: 72.6514%\n",
      "total_backward_count 665720 real_backward_count 31747   4.769%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.248178/  0.873205, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9519%\n",
      "layer   3  Sparsity: 72.5367%\n",
      "total_backward_count 675510 real_backward_count 31814   4.710%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.249069/  0.907297, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7055%\n",
      "layer   3  Sparsity: 72.2788%\n",
      "total_backward_count 685300 real_backward_count 31880   4.652%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.261578/  0.896290, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7977%\n",
      "layer   3  Sparsity: 72.4846%\n",
      "total_backward_count 695090 real_backward_count 31937   4.595%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.257689/  0.875859, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8151%\n",
      "layer   3  Sparsity: 72.5027%\n",
      "total_backward_count 704880 real_backward_count 31985   4.538%\n",
      "fc layer 3 self.abs_max_out: 1605.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.250292/  0.882296, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6744%\n",
      "layer   3  Sparsity: 72.5110%\n",
      "total_backward_count 714670 real_backward_count 32067   4.487%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.251514/  0.878424, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7040%\n",
      "layer   3  Sparsity: 72.4599%\n",
      "total_backward_count 724460 real_backward_count 32118   4.433%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.247231/  0.890550, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7411%\n",
      "layer   3  Sparsity: 72.7284%\n",
      "total_backward_count 734250 real_backward_count 32174   4.382%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.239097/  0.871419, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7795%\n",
      "layer   3  Sparsity: 72.9452%\n",
      "total_backward_count 744040 real_backward_count 32211   4.329%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.246399/  0.914855, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7642%\n",
      "layer   3  Sparsity: 72.4906%\n",
      "total_backward_count 753830 real_backward_count 32261   4.280%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.232570/  0.900435, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7420%\n",
      "layer   3  Sparsity: 72.5000%\n",
      "total_backward_count 763620 real_backward_count 32317   4.232%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.227898/  0.863442, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7032%\n",
      "layer   3  Sparsity: 72.2918%\n",
      "total_backward_count 773410 real_backward_count 32345   4.182%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.224778/  0.882185, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8566%\n",
      "layer   3  Sparsity: 72.6196%\n",
      "total_backward_count 783200 real_backward_count 32382   4.135%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.222007/  0.872519, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8420%\n",
      "layer   3  Sparsity: 72.4374%\n",
      "total_backward_count 792990 real_backward_count 32418   4.088%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.222972/  0.874607, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8386%\n",
      "layer   3  Sparsity: 72.3799%\n",
      "total_backward_count 802780 real_backward_count 32451   4.042%\n",
      "fc layer 2 self.abs_max_out: 3316.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.226700/  0.874577, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6195%\n",
      "layer   3  Sparsity: 72.5763%\n",
      "total_backward_count 812570 real_backward_count 32512   4.001%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.220417/  0.888154, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7947%\n",
      "layer   3  Sparsity: 73.3375%\n",
      "total_backward_count 822360 real_backward_count 32557   3.959%\n",
      "fc layer 2 self.abs_max_out: 3322.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.222721/  0.859815, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.4511%\n",
      "layer   3  Sparsity: 73.6106%\n",
      "total_backward_count 832150 real_backward_count 32600   3.918%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.225363/  0.899034, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.3842%\n",
      "layer   3  Sparsity: 73.3116%\n",
      "total_backward_count 841940 real_backward_count 32648   3.878%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.229416/  0.872103, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.1800%\n",
      "layer   3  Sparsity: 73.0725%\n",
      "total_backward_count 851730 real_backward_count 32692   3.838%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.225373/  0.862169, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.3775%\n",
      "layer   3  Sparsity: 73.0219%\n",
      "total_backward_count 861520 real_backward_count 32756   3.802%\n",
      "fc layer 3 self.abs_max_out: 1624.0\n",
      "lif layer 2 self.abs_max_v: 5753.5\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.218779/  0.866340, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.5243%\n",
      "layer   3  Sparsity: 73.2859%\n",
      "total_backward_count 871310 real_backward_count 32789   3.763%\n",
      "fc layer 2 self.abs_max_out: 3350.0\n",
      "lif layer 2 self.abs_max_v: 6070.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.214580/  0.865763, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6787%\n",
      "layer   3  Sparsity: 73.2417%\n",
      "total_backward_count 881100 real_backward_count 32827   3.726%\n",
      "fc layer 2 self.abs_max_out: 3410.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.213099/  0.851925, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6999%\n",
      "layer   3  Sparsity: 73.1259%\n",
      "total_backward_count 890890 real_backward_count 32865   3.689%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.212890/  0.859812, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6376%\n",
      "layer   3  Sparsity: 73.1323%\n",
      "total_backward_count 900680 real_backward_count 32889   3.652%\n",
      "fc layer 2 self.abs_max_out: 3457.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.211312/  0.846990, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7778%\n",
      "layer   3  Sparsity: 73.2565%\n",
      "total_backward_count 910470 real_backward_count 32913   3.615%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.207697/  0.872863, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0766%\n",
      "layer   3  Sparsity: 73.1595%\n",
      "total_backward_count 920260 real_backward_count 32933   3.579%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.210157/  0.885687, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1961%\n",
      "layer   3  Sparsity: 73.0247%\n",
      "total_backward_count 930050 real_backward_count 32959   3.544%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.210729/  0.851395, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1757%\n",
      "layer   3  Sparsity: 72.4425%\n",
      "total_backward_count 939840 real_backward_count 32992   3.510%\n",
      "fc layer 3 self.abs_max_out: 1675.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.209226/  0.853376, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9441%\n",
      "layer   3  Sparsity: 72.4820%\n",
      "total_backward_count 949630 real_backward_count 33028   3.478%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.211205/  0.847667, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0365%\n",
      "layer   3  Sparsity: 72.4842%\n",
      "total_backward_count 959420 real_backward_count 33067   3.447%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.203982/  0.852286, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1060%\n",
      "layer   3  Sparsity: 72.5669%\n",
      "total_backward_count 969210 real_backward_count 33099   3.415%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.203387/  0.877283, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0773%\n",
      "layer   3  Sparsity: 72.9984%\n",
      "total_backward_count 979000 real_backward_count 33127   3.384%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.208166/  0.830108, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1808%\n",
      "layer   3  Sparsity: 72.9477%\n",
      "total_backward_count 988790 real_backward_count 33170   3.355%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.202834/  0.847759, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.3466%\n",
      "layer   3  Sparsity: 72.6898%\n",
      "total_backward_count 998580 real_backward_count 33200   3.325%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.192657/  0.829812, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.2101%\n",
      "layer   3  Sparsity: 72.8440%\n",
      "total_backward_count 1008370 real_backward_count 33226   3.295%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.185982/  0.823089, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1631%\n",
      "layer   3  Sparsity: 72.8329%\n",
      "total_backward_count 1018160 real_backward_count 33233   3.264%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.189572/  0.852342, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1273%\n",
      "layer   3  Sparsity: 72.8886%\n",
      "total_backward_count 1027950 real_backward_count 33250   3.235%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.193491/  0.866790, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1864%\n",
      "layer   3  Sparsity: 73.3051%\n",
      "total_backward_count 1037740 real_backward_count 33280   3.207%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.194428/  0.859627, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1670%\n",
      "layer   3  Sparsity: 73.5266%\n",
      "total_backward_count 1047530 real_backward_count 33296   3.179%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.194955/  0.863600, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9326%\n",
      "layer   3  Sparsity: 73.4214%\n",
      "total_backward_count 1057320 real_backward_count 33319   3.151%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.197406/  0.880667, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9504%\n",
      "layer   3  Sparsity: 73.0410%\n",
      "total_backward_count 1067110 real_backward_count 33350   3.125%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.198657/  0.851702, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9213%\n",
      "layer   3  Sparsity: 72.8447%\n",
      "total_backward_count 1076900 real_backward_count 33372   3.099%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.192479/  0.844595, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0406%\n",
      "layer   3  Sparsity: 72.9319%\n",
      "total_backward_count 1086690 real_backward_count 33399   3.073%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.185589/  0.842982, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.1172%\n",
      "layer   3  Sparsity: 72.7770%\n",
      "total_backward_count 1096480 real_backward_count 33405   3.047%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.192104/  0.842390, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9505%\n",
      "layer   3  Sparsity: 72.7751%\n",
      "total_backward_count 1106270 real_backward_count 33412   3.020%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.192486/  0.858130, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8759%\n",
      "layer   3  Sparsity: 72.8114%\n",
      "total_backward_count 1116060 real_backward_count 33423   2.995%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.192886/  0.857041, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9107%\n",
      "layer   3  Sparsity: 72.6280%\n",
      "total_backward_count 1125850 real_backward_count 33439   2.970%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.183794/  0.848978, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0393%\n",
      "layer   3  Sparsity: 72.9195%\n",
      "total_backward_count 1135640 real_backward_count 33454   2.946%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.193819/  0.851103, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8245%\n",
      "layer   3  Sparsity: 72.8087%\n",
      "total_backward_count 1145430 real_backward_count 33481   2.923%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.189502/  0.847925, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7197%\n",
      "layer   3  Sparsity: 72.9151%\n",
      "total_backward_count 1155220 real_backward_count 33494   2.899%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.191695/  0.859359, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8919%\n",
      "layer   3  Sparsity: 72.9795%\n",
      "total_backward_count 1165010 real_backward_count 33515   2.877%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.190727/  0.874794, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9301%\n",
      "layer   3  Sparsity: 73.0696%\n",
      "total_backward_count 1174800 real_backward_count 33526   2.854%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.183722/  0.855465, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8296%\n",
      "layer   3  Sparsity: 72.8029%\n",
      "total_backward_count 1184590 real_backward_count 33539   2.831%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.180999/  0.858831, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8607%\n",
      "layer   3  Sparsity: 73.0948%\n",
      "total_backward_count 1194380 real_backward_count 33560   2.810%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.176214/  0.846011, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8945%\n",
      "layer   3  Sparsity: 73.2411%\n",
      "total_backward_count 1204170 real_backward_count 33574   2.788%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.177592/  0.859122, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9374%\n",
      "layer   3  Sparsity: 73.2877%\n",
      "total_backward_count 1213960 real_backward_count 33582   2.766%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.181679/  0.858444, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9137%\n",
      "layer   3  Sparsity: 73.0447%\n",
      "total_backward_count 1223750 real_backward_count 33599   2.746%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.182103/  0.850099, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9855%\n",
      "layer   3  Sparsity: 73.0356%\n",
      "total_backward_count 1233540 real_backward_count 33613   2.725%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.184230/  0.862282, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8989%\n",
      "layer   3  Sparsity: 72.8954%\n",
      "total_backward_count 1243330 real_backward_count 33633   2.705%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.182317/  0.876524, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8967%\n",
      "layer   3  Sparsity: 73.1524%\n",
      "total_backward_count 1253120 real_backward_count 33649   2.685%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.181800/  0.881840, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8849%\n",
      "layer   3  Sparsity: 73.5286%\n",
      "total_backward_count 1262910 real_backward_count 33666   2.666%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.180323/  0.850992, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9908%\n",
      "layer   3  Sparsity: 73.5728%\n",
      "total_backward_count 1272700 real_backward_count 33675   2.646%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.178064/  0.858150, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0339%\n",
      "layer   3  Sparsity: 73.5951%\n",
      "total_backward_count 1282490 real_backward_count 33689   2.627%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.176633/  0.857317, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0366%\n",
      "layer   3  Sparsity: 73.5521%\n",
      "total_backward_count 1292280 real_backward_count 33694   2.607%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.181031/  0.870898, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9947%\n",
      "layer   3  Sparsity: 73.5013%\n",
      "total_backward_count 1302070 real_backward_count 33709   2.589%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.186865/  0.857579, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8960%\n",
      "layer   3  Sparsity: 73.3739%\n",
      "total_backward_count 1311860 real_backward_count 33741   2.572%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.187683/  0.850715, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9680%\n",
      "layer   3  Sparsity: 72.9477%\n",
      "total_backward_count 1321650 real_backward_count 33769   2.555%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.183251/  0.840369, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0073%\n",
      "layer   3  Sparsity: 72.7139%\n",
      "total_backward_count 1331440 real_backward_count 33800   2.539%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.184445/  0.836177, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9367%\n",
      "layer   3  Sparsity: 72.9006%\n",
      "total_backward_count 1341230 real_backward_count 33818   2.521%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.183221/  0.841819, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8121%\n",
      "layer   3  Sparsity: 73.3088%\n",
      "total_backward_count 1351020 real_backward_count 33837   2.505%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.180809/  0.884870, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7569%\n",
      "layer   3  Sparsity: 73.2483%\n",
      "total_backward_count 1360810 real_backward_count 33856   2.488%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.177786/  0.827493, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7130%\n",
      "layer   3  Sparsity: 73.3304%\n",
      "total_backward_count 1370600 real_backward_count 33874   2.471%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.178724/  0.860557, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8144%\n",
      "layer   3  Sparsity: 73.3264%\n",
      "total_backward_count 1380390 real_backward_count 33895   2.455%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.182350/  0.843444, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8063%\n",
      "layer   3  Sparsity: 73.2829%\n",
      "total_backward_count 1390180 real_backward_count 33904   2.439%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.175130/  0.868265, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8126%\n",
      "layer   3  Sparsity: 73.2012%\n",
      "total_backward_count 1399970 real_backward_count 33908   2.422%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.174493/  0.860835, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8507%\n",
      "layer   3  Sparsity: 73.1883%\n",
      "total_backward_count 1409760 real_backward_count 33911   2.405%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.182850/  0.891828, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7868%\n",
      "layer   3  Sparsity: 73.0677%\n",
      "total_backward_count 1419550 real_backward_count 33939   2.391%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.185780/  0.850779, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6663%\n",
      "layer   3  Sparsity: 73.1594%\n",
      "total_backward_count 1429340 real_backward_count 33958   2.376%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.181161/  0.811563, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6469%\n",
      "layer   3  Sparsity: 73.4395%\n",
      "total_backward_count 1439130 real_backward_count 33976   2.361%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.179519/  0.830957, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7003%\n",
      "layer   3  Sparsity: 73.3061%\n",
      "total_backward_count 1448920 real_backward_count 33998   2.346%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.179750/  0.807374, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7047%\n",
      "layer   3  Sparsity: 73.4018%\n",
      "total_backward_count 1458710 real_backward_count 34034   2.333%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.169953/  0.812530, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8131%\n",
      "layer   3  Sparsity: 73.5644%\n",
      "total_backward_count 1468500 real_backward_count 34050   2.319%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.171208/  0.819304, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8074%\n",
      "layer   3  Sparsity: 73.4661%\n",
      "total_backward_count 1478290 real_backward_count 34055   2.304%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.175847/  0.833277, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7129%\n",
      "layer   3  Sparsity: 73.5800%\n",
      "total_backward_count 1488080 real_backward_count 34068   2.289%\n",
      "fc layer 3 self.abs_max_out: 1701.0\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.176648/  0.810107, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7177%\n",
      "layer   3  Sparsity: 73.7647%\n",
      "total_backward_count 1497870 real_backward_count 34077   2.275%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.174636/  0.812010, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.8038%\n",
      "layer   3  Sparsity: 73.7593%\n",
      "total_backward_count 1507660 real_backward_count 34091   2.261%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.176859/  0.848346, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7405%\n",
      "layer   3  Sparsity: 73.8153%\n",
      "total_backward_count 1517450 real_backward_count 34111   2.248%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.176537/  0.845727, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.6546%\n",
      "layer   3  Sparsity: 73.7826%\n",
      "total_backward_count 1527240 real_backward_count 34132   2.235%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.182533/  0.830535, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.7395%\n",
      "layer   3  Sparsity: 73.5537%\n",
      "total_backward_count 1537030 real_backward_count 34159   2.222%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.174418/  0.826632, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9078%\n",
      "layer   3  Sparsity: 73.4221%\n",
      "total_backward_count 1546820 real_backward_count 34174   2.209%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.169088/  0.867067, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9578%\n",
      "layer   3  Sparsity: 73.5767%\n",
      "total_backward_count 1556610 real_backward_count 34184   2.196%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.168855/  0.877269, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9293%\n",
      "layer   3  Sparsity: 73.6768%\n",
      "total_backward_count 1566400 real_backward_count 34195   2.183%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.178124/  0.847477, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9391%\n",
      "layer   3  Sparsity: 73.5320%\n",
      "total_backward_count 1576190 real_backward_count 34204   2.170%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.173558/  0.868389, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 74.9424%\n",
      "layer   3  Sparsity: 73.6182%\n",
      "total_backward_count 1585980 real_backward_count 34209   2.157%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.172697/  0.849756, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0594%\n",
      "layer   3  Sparsity: 73.8307%\n",
      "total_backward_count 1595770 real_backward_count 34220   2.144%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.167826/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0685%\n",
      "layer   3  Sparsity: 73.9845%\n",
      "total_backward_count 1605560 real_backward_count 34224   2.132%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1615350 real_backward_count 34224   2.119%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1625140 real_backward_count 34224   2.106%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1634930 real_backward_count 34224   2.093%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1644720 real_backward_count 34224   2.081%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1654510 real_backward_count 34224   2.069%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1664300 real_backward_count 34224   2.056%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1674090 real_backward_count 34224   2.044%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1683880 real_backward_count 34224   2.032%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1693670 real_backward_count 34224   2.021%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1703460 real_backward_count 34224   2.009%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1713250 real_backward_count 34224   1.998%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1723040 real_backward_count 34224   1.986%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1732830 real_backward_count 34224   1.975%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1742620 real_backward_count 34224   1.964%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1752410 real_backward_count 34224   1.953%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1762200 real_backward_count 34224   1.942%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1771990 real_backward_count 34224   1.931%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1781780 real_backward_count 34224   1.921%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1791570 real_backward_count 34224   1.910%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1801360 real_backward_count 34224   1.900%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1811150 real_backward_count 34224   1.890%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1820940 real_backward_count 34224   1.879%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1830730 real_backward_count 34224   1.869%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1840520 real_backward_count 34224   1.859%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1850310 real_backward_count 34224   1.850%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1860100 real_backward_count 34224   1.840%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1869890 real_backward_count 34224   1.830%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1879680 real_backward_count 34224   1.821%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1889470 real_backward_count 34224   1.811%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1899260 real_backward_count 34224   1.802%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1909050 real_backward_count 34224   1.793%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1918840 real_backward_count 34224   1.784%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1928630 real_backward_count 34224   1.775%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1938420 real_backward_count 34224   1.766%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n",
      "total_backward_count 1948210 real_backward_count 34224   1.757%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.167632/  0.863145, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 73.9648%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c23b141c5e44a5840aca4e99b854c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.16763</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>0.86314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-104</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/n9xtvc1b' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/n9xtvc1b</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_111628-n9xtvc1b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6db7nz16 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_153357-6db7nz16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6db7nz16' target=\"_blank\">giddy-sweep-110</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6db7nz16' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6db7nz16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251116_153406_336', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 681.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 414.0\n",
      "lif layer 2 self.abs_max_v: 414.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "lif layer 1 self.abs_max_v: 1131.5\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "lif layer 2 self.abs_max_v: 1293.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 1181.0\n",
      "fc layer 1 self.abs_max_out: 989.0\n",
      "lif layer 1 self.abs_max_v: 1242.5\n",
      "fc layer 2 self.abs_max_out: 1170.0\n",
      "lif layer 2 self.abs_max_v: 1399.5\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "fc layer 1 self.abs_max_out: 1323.0\n",
      "lif layer 1 self.abs_max_v: 1567.5\n",
      "fc layer 2 self.abs_max_out: 1215.0\n",
      "lif layer 2 self.abs_max_v: 1875.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 1 self.abs_max_out: 1494.0\n",
      "fc layer 1 self.abs_max_out: 1904.0\n",
      "lif layer 1 self.abs_max_v: 1904.0\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "fc layer 2 self.abs_max_out: 1509.0\n",
      "lif layer 2 self.abs_max_v: 2170.5\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 1 self.abs_max_out: 1929.0\n",
      "lif layer 1 self.abs_max_v: 1929.0\n",
      "fc layer 1 self.abs_max_out: 2089.0\n",
      "lif layer 1 self.abs_max_v: 2089.0\n",
      "fc layer 2 self.abs_max_out: 1559.0\n",
      "fc layer 1 self.abs_max_out: 2355.0\n",
      "lif layer 1 self.abs_max_v: 2355.0\n",
      "fc layer 2 self.abs_max_out: 1703.0\n",
      "lif layer 2 self.abs_max_v: 2492.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 1 self.abs_max_out: 2607.0\n",
      "lif layer 1 self.abs_max_v: 2607.0\n",
      "fc layer 1 self.abs_max_out: 3090.0\n",
      "lif layer 1 self.abs_max_v: 3090.0\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "lif layer 2 self.abs_max_v: 2835.0\n",
      "fc layer 3 self.abs_max_out: 651.0\n",
      "fc layer 3 self.abs_max_out: 740.0\n",
      "fc layer 2 self.abs_max_out: 1776.0\n",
      "fc layer 1 self.abs_max_out: 3216.0\n",
      "lif layer 1 self.abs_max_v: 3236.0\n",
      "lif layer 1 self.abs_max_v: 3397.5\n",
      "lif layer 2 self.abs_max_v: 2890.5\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "fc layer 1 self.abs_max_out: 3505.0\n",
      "lif layer 1 self.abs_max_v: 3505.0\n",
      "fc layer 2 self.abs_max_out: 1915.0\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "lif layer 2 self.abs_max_v: 2988.5\n",
      "lif layer 2 self.abs_max_v: 3090.5\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "lif layer 2 self.abs_max_v: 3240.5\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "lif layer 2 self.abs_max_v: 3530.0\n",
      "lif layer 2 self.abs_max_v: 3555.0\n",
      "fc layer 1 self.abs_max_out: 4179.0\n",
      "lif layer 1 self.abs_max_v: 4179.0\n",
      "fc layer 2 self.abs_max_out: 2048.0\n",
      "fc layer 2 self.abs_max_out: 2397.0\n",
      "fc layer 2 self.abs_max_out: 2796.0\n",
      "lif layer 2 self.abs_max_v: 4178.5\n",
      "lif layer 1 self.abs_max_v: 4211.5\n",
      "fc layer 1 self.abs_max_out: 4183.0\n",
      "lif layer 1 self.abs_max_v: 4232.0\n",
      "fc layer 1 self.abs_max_out: 4529.0\n",
      "lif layer 1 self.abs_max_v: 4529.0\n",
      "fc layer 1 self.abs_max_out: 5303.0\n",
      "lif layer 1 self.abs_max_v: 5303.0\n",
      "fc layer 3 self.abs_max_out: 906.0\n",
      "lif layer 2 self.abs_max_v: 4387.0\n",
      "fc layer 2 self.abs_max_out: 2861.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "lif layer 1 self.abs_max_v: 6097.5\n",
      "lif layer 1 self.abs_max_v: 6144.0\n",
      "fc layer 2 self.abs_max_out: 2863.0\n",
      "fc layer 2 self.abs_max_out: 2939.0\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "fc layer 1 self.abs_max_out: 5315.0\n",
      "lif layer 2 self.abs_max_v: 4390.5\n",
      "lif layer 2 self.abs_max_v: 4625.5\n",
      "fc layer 2 self.abs_max_out: 3491.0\n",
      "lif layer 1 self.abs_max_v: 6172.5\n",
      "lif layer 1 self.abs_max_v: 6459.5\n",
      "lif layer 1 self.abs_max_v: 6655.0\n",
      "fc layer 1 self.abs_max_out: 5745.0\n",
      "fc layer 1 self.abs_max_out: 5832.0\n",
      "lif layer 1 self.abs_max_v: 7231.0\n",
      "lif layer 1 self.abs_max_v: 7697.5\n",
      "lif layer 1 self.abs_max_v: 7765.0\n",
      "lif layer 1 self.abs_max_v: 8020.0\n",
      "fc layer 1 self.abs_max_out: 6048.0\n",
      "fc layer 2 self.abs_max_out: 3586.0\n",
      "fc layer 2 self.abs_max_out: 3778.0\n",
      "lif layer 2 self.abs_max_v: 4681.0\n",
      "lif layer 2 self.abs_max_v: 4862.5\n",
      "lif layer 2 self.abs_max_v: 4923.5\n",
      "lif layer 2 self.abs_max_v: 5071.0\n",
      "lif layer 1 self.abs_max_v: 8301.5\n",
      "fc layer 3 self.abs_max_out: 1007.0\n",
      "fc layer 1 self.abs_max_out: 6245.0\n",
      "fc layer 1 self.abs_max_out: 6816.0\n",
      "lif layer 2 self.abs_max_v: 5154.0\n",
      "lif layer 2 self.abs_max_v: 5293.5\n",
      "lif layer 1 self.abs_max_v: 8526.0\n",
      "lif layer 1 self.abs_max_v: 9394.0\n",
      "lif layer 1 self.abs_max_v: 9645.0\n",
      "lif layer 1 self.abs_max_v: 9772.0\n",
      "lif layer 1 self.abs_max_v: 9893.0\n",
      "lif layer 1 self.abs_max_v: 9958.0\n",
      "lif layer 1 self.abs_max_v: 9966.0\n",
      "lif layer 2 self.abs_max_v: 5333.5\n",
      "fc layer 1 self.abs_max_out: 7006.0\n",
      "lif layer 1 self.abs_max_v: 10685.0\n",
      "lif layer 1 self.abs_max_v: 10796.0\n",
      "lif layer 1 self.abs_max_v: 11731.5\n",
      "lif layer 2 self.abs_max_v: 5750.5\n",
      "lif layer 2 self.abs_max_v: 5949.5\n",
      "lif layer 2 self.abs_max_v: 6130.0\n",
      "lif layer 2 self.abs_max_v: 6252.0\n",
      "fc layer 1 self.abs_max_out: 7471.0\n",
      "fc layer 3 self.abs_max_out: 1048.0\n",
      "fc layer 3 self.abs_max_out: 1098.0\n",
      "fc layer 1 self.abs_max_out: 7584.0\n",
      "lif layer 1 self.abs_max_v: 12543.5\n",
      "lif layer 1 self.abs_max_v: 12555.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.923288/  2.049716, val:  37.08%, val_best:  37.08%, tr:  96.02%, tr_best:  96.02%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0013%\n",
      "layer   3  Sparsity: 81.8487%\n",
      "total_backward_count 9790 real_backward_count 2371  24.219%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1174.0\n",
      "fc layer 1 self.abs_max_out: 7710.0\n",
      "lif layer 1 self.abs_max_v: 12659.5\n",
      "lif layer 1 self.abs_max_v: 13153.5\n",
      "lif layer 1 self.abs_max_v: 13501.5\n",
      "fc layer 1 self.abs_max_out: 7760.0\n",
      "fc layer 1 self.abs_max_out: 8242.0\n",
      "fc layer 1 self.abs_max_out: 9064.0\n",
      "lif layer 1 self.abs_max_v: 14680.0\n",
      "lif layer 1 self.abs_max_v: 14967.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.852169/  2.035539, val:  40.00%, val_best:  40.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3975%\n",
      "layer   3  Sparsity: 80.0412%\n",
      "total_backward_count 19580 real_backward_count 4043  20.649%\n",
      "fc layer 2 self.abs_max_out: 3875.0\n",
      "fc layer 3 self.abs_max_out: 1188.0\n",
      "fc layer 3 self.abs_max_out: 1212.0\n",
      "fc layer 3 self.abs_max_out: 1249.0\n",
      "fc layer 2 self.abs_max_out: 4011.0\n",
      "fc layer 2 self.abs_max_out: 4094.0\n",
      "fc layer 2 self.abs_max_out: 4393.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.831706/  1.987420, val:  50.42%, val_best:  50.42%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4062%\n",
      "layer   3  Sparsity: 78.7779%\n",
      "total_backward_count 29370 real_backward_count 5627  19.159%\n",
      "fc layer 3 self.abs_max_out: 1271.0\n",
      "fc layer 1 self.abs_max_out: 9343.0\n",
      "lif layer 1 self.abs_max_v: 15375.5\n",
      "fc layer 1 self.abs_max_out: 9463.0\n",
      "fc layer 1 self.abs_max_out: 10416.0\n",
      "lif layer 1 self.abs_max_v: 17119.5\n",
      "lif layer 1 self.abs_max_v: 17630.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.793158/  1.982154, val:  41.25%, val_best:  50.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8583%\n",
      "layer   3  Sparsity: 77.8463%\n",
      "total_backward_count 39160 real_backward_count 7112  18.161%\n",
      "fc layer 3 self.abs_max_out: 1296.0\n",
      "fc layer 1 self.abs_max_out: 10727.0\n",
      "lif layer 1 self.abs_max_v: 18087.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.805910/  1.994776, val:  42.50%, val_best:  50.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7047%\n",
      "layer   3  Sparsity: 79.0696%\n",
      "total_backward_count 48950 real_backward_count 8532  17.430%\n",
      "fc layer 3 self.abs_max_out: 1323.0\n",
      "fc layer 3 self.abs_max_out: 1396.0\n",
      "fc layer 3 self.abs_max_out: 1398.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.788709/  1.981735, val:  44.58%, val_best:  50.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7074%\n",
      "layer   3  Sparsity: 79.6080%\n",
      "total_backward_count 58740 real_backward_count 9982  16.994%\n",
      "fc layer 1 self.abs_max_out: 10909.0\n",
      "lif layer 1 self.abs_max_v: 18252.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.808775/  1.991502, val:  46.67%, val_best:  50.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8566%\n",
      "layer   3  Sparsity: 80.9541%\n",
      "total_backward_count 68530 real_backward_count 11453  16.712%\n",
      "fc layer 1 self.abs_max_out: 11495.0\n",
      "lif layer 1 self.abs_max_v: 19070.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.795930/  1.956600, val:  50.83%, val_best:  50.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2252%\n",
      "layer   3  Sparsity: 81.0568%\n",
      "total_backward_count 78320 real_backward_count 12898  16.468%\n",
      "fc layer 1 self.abs_max_out: 11642.0\n",
      "lif layer 1 self.abs_max_v: 19298.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.796957/  1.936350, val:  54.17%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.8881%\n",
      "layer   3  Sparsity: 81.0042%\n",
      "total_backward_count 88110 real_backward_count 14342  16.277%\n",
      "fc layer 3 self.abs_max_out: 1426.0\n",
      "fc layer 3 self.abs_max_out: 1440.0\n",
      "fc layer 3 self.abs_max_out: 1490.0\n",
      "lif layer 1 self.abs_max_v: 19428.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.779149/  1.950934, val:  50.00%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.5664%\n",
      "layer   3  Sparsity: 81.1582%\n",
      "total_backward_count 97900 real_backward_count 15690  16.027%\n",
      "lif layer 2 self.abs_max_v: 6487.0\n",
      "lif layer 2 self.abs_max_v: 6502.5\n",
      "lif layer 2 self.abs_max_v: 6649.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.771175/  1.967394, val:  39.58%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.7662%\n",
      "layer   3  Sparsity: 81.3757%\n",
      "total_backward_count 107690 real_backward_count 17059  15.841%\n",
      "fc layer 1 self.abs_max_out: 11699.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.784311/  1.964574, val:  45.83%, val_best:  54.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.6420%\n",
      "layer   3  Sparsity: 81.6361%\n",
      "total_backward_count 117480 real_backward_count 18406  15.667%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.780549/  1.971131, val:  50.00%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0589%\n",
      "layer   3  Sparsity: 82.1437%\n",
      "total_backward_count 127270 real_backward_count 19718  15.493%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.791320/  1.996016, val:  36.25%, val_best:  54.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.8948%\n",
      "layer   3  Sparsity: 82.1987%\n",
      "total_backward_count 137060 real_backward_count 20990  15.314%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.784419/  1.949215, val:  50.00%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.5806%\n",
      "layer   3  Sparsity: 82.0860%\n",
      "total_backward_count 146850 real_backward_count 22275  15.169%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.784484/  1.932152, val:  50.00%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.3464%\n",
      "layer   3  Sparsity: 81.7590%\n",
      "total_backward_count 156640 real_backward_count 23559  15.040%\n",
      "fc layer 1 self.abs_max_out: 12043.0\n",
      "lif layer 1 self.abs_max_v: 19721.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.763082/  1.925393, val:  65.00%, val_best:  65.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.3756%\n",
      "layer   3  Sparsity: 82.0654%\n",
      "total_backward_count 166430 real_backward_count 24842  14.926%\n",
      "lif layer 2 self.abs_max_v: 6659.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.767534/  1.945633, val:  58.33%, val_best:  65.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1937%\n",
      "layer   3  Sparsity: 82.2459%\n",
      "total_backward_count 176220 real_backward_count 26142  14.835%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.747640/  1.893628, val:  52.92%, val_best:  65.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3018%\n",
      "layer   3  Sparsity: 81.1319%\n",
      "total_backward_count 186010 real_backward_count 27396  14.728%\n",
      "fc layer 3 self.abs_max_out: 1574.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.743194/  1.947967, val:  48.33%, val_best:  65.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1854%\n",
      "layer   3  Sparsity: 82.4664%\n",
      "total_backward_count 195800 real_backward_count 28560  14.586%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.734288/  1.932345, val:  51.25%, val_best:  65.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.8696%\n",
      "layer   3  Sparsity: 82.4622%\n",
      "total_backward_count 205590 real_backward_count 29734  14.463%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.746991/  1.920831, val:  52.92%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.6059%\n",
      "layer   3  Sparsity: 81.9646%\n",
      "total_backward_count 215380 real_backward_count 30931  14.361%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.753295/  1.902840, val:  54.17%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1023%\n",
      "layer   3  Sparsity: 82.9103%\n",
      "total_backward_count 225170 real_backward_count 32156  14.281%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.734649/  1.878720, val:  64.17%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3091%\n",
      "layer   3  Sparsity: 82.6116%\n",
      "total_backward_count 234960 real_backward_count 33286  14.167%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.731520/  1.872000, val:  78.75%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3731%\n",
      "layer   3  Sparsity: 82.9333%\n",
      "total_backward_count 244750 real_backward_count 34420  14.063%\n",
      "lif layer 2 self.abs_max_v: 6834.5\n",
      "lif layer 2 self.abs_max_v: 6836.0\n",
      "lif layer 2 self.abs_max_v: 6853.5\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.752549/  1.926393, val:  67.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2510%\n",
      "layer   3  Sparsity: 83.1952%\n",
      "total_backward_count 254540 real_backward_count 35639  14.001%\n",
      "lif layer 2 self.abs_max_v: 7083.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.741596/  1.873650, val:  70.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7542%\n",
      "layer   3  Sparsity: 82.8549%\n",
      "total_backward_count 264330 real_backward_count 36703  13.885%\n",
      "lif layer 2 self.abs_max_v: 7183.5\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.725091/  1.913762, val:  66.25%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.6725%\n",
      "layer   3  Sparsity: 82.6452%\n",
      "total_backward_count 274120 real_backward_count 37796  13.788%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.724304/  1.879256, val:  60.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3768%\n",
      "layer   3  Sparsity: 81.9268%\n",
      "total_backward_count 283910 real_backward_count 38874  13.692%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.710585/  1.902723, val:  53.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8834%\n",
      "layer   3  Sparsity: 82.0696%\n",
      "total_backward_count 293700 real_backward_count 39913  13.590%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.711941/  1.863526, val:  63.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9470%\n",
      "layer   3  Sparsity: 82.6084%\n",
      "total_backward_count 303490 real_backward_count 40984  13.504%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.706585/  1.906576, val:  65.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0617%\n",
      "layer   3  Sparsity: 83.1452%\n",
      "total_backward_count 313280 real_backward_count 42054  13.424%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.711742/  1.877189, val:  60.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4761%\n",
      "layer   3  Sparsity: 82.2044%\n",
      "total_backward_count 323070 real_backward_count 43021  13.316%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.702850/  1.851437, val:  68.33%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.6113%\n",
      "layer   3  Sparsity: 81.7676%\n",
      "total_backward_count 332860 real_backward_count 44111  13.252%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.680948/  1.865596, val:  55.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7947%\n",
      "layer   3  Sparsity: 82.5847%\n",
      "total_backward_count 342650 real_backward_count 45077  13.155%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.705629/  1.870495, val:  70.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1774%\n",
      "layer   3  Sparsity: 82.7749%\n",
      "total_backward_count 352440 real_backward_count 46100  13.080%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.671427/  1.832229, val:  67.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0057%\n",
      "layer   3  Sparsity: 82.4193%\n",
      "total_backward_count 362230 real_backward_count 47023  12.982%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.667507/  1.881363, val:  62.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9895%\n",
      "layer   3  Sparsity: 82.3019%\n",
      "total_backward_count 372020 real_backward_count 47887  12.872%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.695098/  1.889052, val:  61.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8324%\n",
      "layer   3  Sparsity: 82.8027%\n",
      "total_backward_count 381810 real_backward_count 48821  12.787%\n",
      "fc layer 2 self.abs_max_out: 4459.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.679497/  1.865220, val:  65.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5701%\n",
      "layer   3  Sparsity: 82.1348%\n",
      "total_backward_count 391600 real_backward_count 49732  12.700%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.682925/  1.890345, val:  62.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3988%\n",
      "layer   3  Sparsity: 82.4067%\n",
      "total_backward_count 401390 real_backward_count 50660  12.621%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.691584/  1.887132, val:  69.17%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7580%\n",
      "layer   3  Sparsity: 83.0325%\n",
      "total_backward_count 411180 real_backward_count 51562  12.540%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.685044/  1.878938, val:  62.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8080%\n",
      "layer   3  Sparsity: 82.9298%\n",
      "total_backward_count 420970 real_backward_count 52411  12.450%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.682181/  1.877310, val:  66.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5756%\n",
      "layer   3  Sparsity: 81.9956%\n",
      "total_backward_count 430760 real_backward_count 53289  12.371%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.670310/  1.852567, val:  65.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4870%\n",
      "layer   3  Sparsity: 82.5253%\n",
      "total_backward_count 440550 real_backward_count 54184  12.299%\n",
      "fc layer 3 self.abs_max_out: 1607.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.669254/  1.861152, val:  64.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5123%\n",
      "layer   3  Sparsity: 83.0746%\n",
      "total_backward_count 450340 real_backward_count 55085  12.232%\n",
      "fc layer 3 self.abs_max_out: 1613.0\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.677130/  1.872913, val:  60.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3171%\n",
      "layer   3  Sparsity: 82.6713%\n",
      "total_backward_count 460130 real_backward_count 55956  12.161%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.680719/  1.871867, val:  65.00%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7731%\n",
      "layer   3  Sparsity: 83.3763%\n",
      "total_backward_count 469920 real_backward_count 56815  12.090%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.679478/  1.856084, val:  74.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8508%\n",
      "layer   3  Sparsity: 83.4887%\n",
      "total_backward_count 479710 real_backward_count 57627  12.013%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.665668/  1.809596, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7373%\n",
      "layer   3  Sparsity: 82.5715%\n",
      "total_backward_count 489500 real_backward_count 58447  11.940%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.672665/  1.855436, val:  64.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5815%\n",
      "layer   3  Sparsity: 82.7513%\n",
      "total_backward_count 499290 real_backward_count 59261  11.869%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.675234/  1.833543, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4884%\n",
      "layer   3  Sparsity: 82.6753%\n",
      "total_backward_count 509080 real_backward_count 60032  11.792%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.676027/  1.839278, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4615%\n",
      "layer   3  Sparsity: 83.0635%\n",
      "total_backward_count 518870 real_backward_count 60837  11.725%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.675228/  1.826791, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2515%\n",
      "layer   3  Sparsity: 82.9023%\n",
      "total_backward_count 528660 real_backward_count 61617  11.655%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.673998/  1.835613, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1892%\n",
      "layer   3  Sparsity: 83.0599%\n",
      "total_backward_count 538450 real_backward_count 62443  11.597%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.673020/  1.833697, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.6554%\n",
      "layer   3  Sparsity: 82.8698%\n",
      "total_backward_count 548240 real_backward_count 63222  11.532%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.671786/  1.855114, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1526%\n",
      "layer   3  Sparsity: 83.0703%\n",
      "total_backward_count 558030 real_backward_count 63978  11.465%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.648250/  1.843319, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1906%\n",
      "layer   3  Sparsity: 82.2989%\n",
      "total_backward_count 567820 real_backward_count 64702  11.395%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.623603/  1.804477, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.8346%\n",
      "layer   3  Sparsity: 81.7338%\n",
      "total_backward_count 577610 real_backward_count 65408  11.324%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.623559/  1.839460, val:  64.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.7479%\n",
      "layer   3  Sparsity: 82.2540%\n",
      "total_backward_count 587400 real_backward_count 66091  11.251%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.610396/  1.836430, val:  67.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.7803%\n",
      "layer   3  Sparsity: 82.0112%\n",
      "total_backward_count 597190 real_backward_count 66799  11.186%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.601013/  1.823941, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0166%\n",
      "layer   3  Sparsity: 81.6241%\n",
      "total_backward_count 606980 real_backward_count 67516  11.123%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.625816/  1.826255, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.7731%\n",
      "layer   3  Sparsity: 81.3237%\n",
      "total_backward_count 616770 real_backward_count 68299  11.074%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.614550/  1.821127, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.9825%\n",
      "layer   3  Sparsity: 81.8359%\n",
      "total_backward_count 626560 real_backward_count 68989  11.011%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.621084/  1.823614, val:  63.75%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0309%\n",
      "layer   3  Sparsity: 82.1777%\n",
      "total_backward_count 636350 real_backward_count 69759  10.962%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.591622/  1.823870, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2113%\n",
      "layer   3  Sparsity: 82.0576%\n",
      "total_backward_count 646140 real_backward_count 70441  10.902%\n",
      "fc layer 3 self.abs_max_out: 1647.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.596003/  1.792352, val:  75.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2879%\n",
      "layer   3  Sparsity: 82.2096%\n",
      "total_backward_count 655930 real_backward_count 71173  10.851%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.578503/  1.771591, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3976%\n",
      "layer   3  Sparsity: 81.5270%\n",
      "total_backward_count 665720 real_backward_count 71813  10.787%\n",
      "fc layer 3 self.abs_max_out: 1654.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.571450/  1.746225, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1735%\n",
      "layer   3  Sparsity: 81.5929%\n",
      "total_backward_count 675510 real_backward_count 72434  10.723%\n",
      "fc layer 3 self.abs_max_out: 1674.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.546874/  1.774427, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1037%\n",
      "layer   3  Sparsity: 81.1795%\n",
      "total_backward_count 685300 real_backward_count 73080  10.664%\n",
      "fc layer 3 self.abs_max_out: 1745.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.570698/  1.791316, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.7867%\n",
      "layer   3  Sparsity: 82.4472%\n",
      "total_backward_count 695090 real_backward_count 73740  10.609%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.579726/  1.817614, val:  67.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.6396%\n",
      "layer   3  Sparsity: 82.3324%\n",
      "total_backward_count 704880 real_backward_count 74359  10.549%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.595871/  1.790365, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.7534%\n",
      "layer   3  Sparsity: 82.0219%\n",
      "total_backward_count 714670 real_backward_count 74988  10.493%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.576045/  1.773589, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.6554%\n",
      "layer   3  Sparsity: 82.3962%\n",
      "total_backward_count 724460 real_backward_count 75553  10.429%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.583549/  1.793212, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.8970%\n",
      "layer   3  Sparsity: 82.7294%\n",
      "total_backward_count 734250 real_backward_count 76143  10.370%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.585852/  1.760650, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.9182%\n",
      "layer   3  Sparsity: 82.4913%\n",
      "total_backward_count 744040 real_backward_count 76739  10.314%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.564854/  1.790610, val:  62.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0939%\n",
      "layer   3  Sparsity: 82.4326%\n",
      "total_backward_count 753830 real_backward_count 77297  10.254%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.572002/  1.765801, val:  72.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0230%\n",
      "layer   3  Sparsity: 82.2166%\n",
      "total_backward_count 763620 real_backward_count 77903  10.202%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.590381/  1.812808, val:  67.50%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1314%\n",
      "layer   3  Sparsity: 82.9759%\n",
      "total_backward_count 773410 real_backward_count 78517  10.152%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.584086/  1.797575, val:  80.83%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0998%\n",
      "layer   3  Sparsity: 82.8194%\n",
      "total_backward_count 783200 real_backward_count 79106  10.100%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.590958/  1.782679, val:  68.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0613%\n",
      "layer   3  Sparsity: 82.9617%\n",
      "total_backward_count 792990 real_backward_count 79740  10.056%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.584806/  1.808093, val:  68.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2396%\n",
      "layer   3  Sparsity: 82.6255%\n",
      "total_backward_count 802780 real_backward_count 80347  10.009%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.579611/  1.805620, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4801%\n",
      "layer   3  Sparsity: 82.5189%\n",
      "total_backward_count 812570 real_backward_count 80944   9.961%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.587379/  1.808281, val:  59.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3566%\n",
      "layer   3  Sparsity: 82.2677%\n",
      "total_backward_count 822360 real_backward_count 81581   9.920%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.573597/  1.771942, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3257%\n",
      "layer   3  Sparsity: 81.9305%\n",
      "total_backward_count 832150 real_backward_count 82156   9.873%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.592673/  1.818483, val:  73.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4556%\n",
      "layer   3  Sparsity: 82.6722%\n",
      "total_backward_count 841940 real_backward_count 82746   9.828%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.566287/  1.785029, val:  69.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2393%\n",
      "layer   3  Sparsity: 82.6646%\n",
      "total_backward_count 851730 real_backward_count 83302   9.780%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.585369/  1.772306, val:  74.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4292%\n",
      "layer   3  Sparsity: 82.7468%\n",
      "total_backward_count 861520 real_backward_count 83901   9.739%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.578313/  1.787124, val:  75.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3721%\n",
      "layer   3  Sparsity: 82.5525%\n",
      "total_backward_count 871310 real_backward_count 84472   9.695%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.555748/  1.764703, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2920%\n",
      "layer   3  Sparsity: 82.2396%\n",
      "total_backward_count 881100 real_backward_count 85010   9.648%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.563412/  1.793389, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3582%\n",
      "layer   3  Sparsity: 82.3880%\n",
      "total_backward_count 890890 real_backward_count 85571   9.605%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.573338/  1.750920, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3251%\n",
      "layer   3  Sparsity: 82.2512%\n",
      "total_backward_count 900680 real_backward_count 86076   9.557%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.548335/  1.761075, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.9956%\n",
      "layer   3  Sparsity: 82.0040%\n",
      "total_backward_count 910470 real_backward_count 86615   9.513%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.538071/  1.764177, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.9267%\n",
      "layer   3  Sparsity: 82.0578%\n",
      "total_backward_count 920260 real_backward_count 87099   9.465%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.561473/  1.772869, val:  71.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0008%\n",
      "layer   3  Sparsity: 82.4423%\n",
      "total_backward_count 930050 real_backward_count 87622   9.421%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.538691/  1.762061, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0162%\n",
      "layer   3  Sparsity: 81.6477%\n",
      "total_backward_count 939840 real_backward_count 88112   9.375%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.512650/  1.738840, val:  75.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5087%\n",
      "layer   3  Sparsity: 80.7384%\n",
      "total_backward_count 949630 real_backward_count 88569   9.327%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.509502/  1.744732, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1541%\n",
      "layer   3  Sparsity: 81.2155%\n",
      "total_backward_count 959420 real_backward_count 89078   9.285%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.522493/  1.757339, val:  77.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2818%\n",
      "layer   3  Sparsity: 82.0498%\n",
      "total_backward_count 969210 real_backward_count 89575   9.242%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.510124/  1.737787, val:  79.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2580%\n",
      "layer   3  Sparsity: 81.6851%\n",
      "total_backward_count 979000 real_backward_count 90025   9.196%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.533363/  1.771795, val:  70.00%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0348%\n",
      "layer   3  Sparsity: 82.0543%\n",
      "total_backward_count 988790 real_backward_count 90552   9.158%\n",
      "fc layer 2 self.abs_max_out: 4461.0\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.532608/  1.737495, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2861%\n",
      "layer   3  Sparsity: 81.5656%\n",
      "total_backward_count 998580 real_backward_count 91060   9.119%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.542029/  1.762677, val:  70.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4872%\n",
      "layer   3  Sparsity: 81.4297%\n",
      "total_backward_count 1008370 real_backward_count 91528   9.077%\n",
      "fc layer 3 self.abs_max_out: 1750.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.528426/  1.751596, val:  69.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.6289%\n",
      "layer   3  Sparsity: 81.9089%\n",
      "total_backward_count 1018160 real_backward_count 91988   9.035%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.538124/  1.754652, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4352%\n",
      "layer   3  Sparsity: 81.8953%\n",
      "total_backward_count 1027950 real_backward_count 92462   8.995%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.521578/  1.749765, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5947%\n",
      "layer   3  Sparsity: 81.3262%\n",
      "total_backward_count 1037740 real_backward_count 92905   8.953%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.505118/  1.755821, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5057%\n",
      "layer   3  Sparsity: 81.6439%\n",
      "total_backward_count 1047530 real_backward_count 93363   8.913%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.488611/  1.739735, val:  72.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3007%\n",
      "layer   3  Sparsity: 81.0378%\n",
      "total_backward_count 1057320 real_backward_count 93871   8.878%\n",
      "fc layer 3 self.abs_max_out: 1776.0\n",
      "fc layer 3 self.abs_max_out: 1913.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.496810/  1.757807, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4197%\n",
      "layer   3  Sparsity: 81.6712%\n",
      "total_backward_count 1067110 real_backward_count 94330   8.840%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.505775/  1.761314, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5087%\n",
      "layer   3  Sparsity: 81.4108%\n",
      "total_backward_count 1076900 real_backward_count 94803   8.803%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.493160/  1.697777, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4317%\n",
      "layer   3  Sparsity: 81.1271%\n",
      "total_backward_count 1086690 real_backward_count 95251   8.765%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.477478/  1.723778, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5094%\n",
      "layer   3  Sparsity: 81.5067%\n",
      "total_backward_count 1096480 real_backward_count 95694   8.727%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.504646/  1.732591, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1943%\n",
      "layer   3  Sparsity: 81.9630%\n",
      "total_backward_count 1106270 real_backward_count 96145   8.691%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.505633/  1.739970, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2169%\n",
      "layer   3  Sparsity: 81.8246%\n",
      "total_backward_count 1116060 real_backward_count 96624   8.658%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.482466/  1.711560, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2192%\n",
      "layer   3  Sparsity: 81.4861%\n",
      "total_backward_count 1125850 real_backward_count 97039   8.619%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.478420/  1.763997, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4249%\n",
      "layer   3  Sparsity: 81.3571%\n",
      "total_backward_count 1135640 real_backward_count 97454   8.581%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.503518/  1.724554, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2111%\n",
      "layer   3  Sparsity: 81.8898%\n",
      "total_backward_count 1145430 real_backward_count 97882   8.545%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.489404/  1.735642, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5121%\n",
      "layer   3  Sparsity: 81.7244%\n",
      "total_backward_count 1155220 real_backward_count 98285   8.508%\n",
      "fc layer 2 self.abs_max_out: 4517.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.502791/  1.711896, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2232%\n",
      "layer   3  Sparsity: 81.7075%\n",
      "total_backward_count 1165010 real_backward_count 98705   8.472%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.476849/  1.730814, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0554%\n",
      "layer   3  Sparsity: 81.4881%\n",
      "total_backward_count 1174800 real_backward_count 99113   8.437%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.483210/  1.723527, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3560%\n",
      "layer   3  Sparsity: 82.3010%\n",
      "total_backward_count 1184590 real_backward_count 99503   8.400%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.494243/  1.732734, val:  70.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4555%\n",
      "layer   3  Sparsity: 82.0499%\n",
      "total_backward_count 1194380 real_backward_count 99941   8.368%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.506059/  1.733129, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1223%\n",
      "layer   3  Sparsity: 83.1595%\n",
      "total_backward_count 1204170 real_backward_count 100339   8.333%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.516100/  1.746623, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1589%\n",
      "layer   3  Sparsity: 83.2766%\n",
      "total_backward_count 1213960 real_backward_count 100756   8.300%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.518188/  1.725269, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 76.8134%\n",
      "layer   3  Sparsity: 82.9985%\n",
      "total_backward_count 1223750 real_backward_count 101152   8.266%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.516897/  1.722654, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1627%\n",
      "layer   3  Sparsity: 82.2849%\n",
      "total_backward_count 1233540 real_backward_count 101599   8.236%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.496060/  1.726949, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5041%\n",
      "layer   3  Sparsity: 81.8931%\n",
      "total_backward_count 1243330 real_backward_count 101998   8.204%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.493691/  1.703299, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3959%\n",
      "layer   3  Sparsity: 81.6548%\n",
      "total_backward_count 1253120 real_backward_count 102394   8.171%\n",
      "fc layer 3 self.abs_max_out: 1943.0\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.500424/  1.725054, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.2057%\n",
      "layer   3  Sparsity: 81.2240%\n",
      "total_backward_count 1262910 real_backward_count 102797   8.140%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.494144/  1.704983, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1703%\n",
      "layer   3  Sparsity: 81.6399%\n",
      "total_backward_count 1272700 real_backward_count 103192   8.108%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.488399/  1.732819, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5262%\n",
      "layer   3  Sparsity: 81.6725%\n",
      "total_backward_count 1282490 real_backward_count 103578   8.076%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.488582/  1.723149, val:  74.58%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1978%\n",
      "layer   3  Sparsity: 81.4800%\n",
      "total_backward_count 1292280 real_backward_count 103999   8.048%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.478634/  1.731998, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0711%\n",
      "layer   3  Sparsity: 81.4968%\n",
      "total_backward_count 1302070 real_backward_count 104414   8.019%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.476921/  1.722788, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.4693%\n",
      "layer   3  Sparsity: 81.6831%\n",
      "total_backward_count 1311860 real_backward_count 104780   7.987%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.465144/  1.697763, val:  78.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1463%\n",
      "layer   3  Sparsity: 81.0733%\n",
      "total_backward_count 1321650 real_backward_count 105183   7.958%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.466331/  1.704468, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.0986%\n",
      "layer   3  Sparsity: 81.5392%\n",
      "total_backward_count 1331440 real_backward_count 105568   7.929%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.480914/  1.723574, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.1866%\n",
      "layer   3  Sparsity: 82.3133%\n",
      "total_backward_count 1341230 real_backward_count 105921   7.897%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.466703/  1.722022, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3527%\n",
      "layer   3  Sparsity: 81.7091%\n",
      "total_backward_count 1351020 real_backward_count 106310   7.869%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.470161/  1.727581, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.3286%\n",
      "layer   3  Sparsity: 81.4755%\n",
      "total_backward_count 1360810 real_backward_count 106659   7.838%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.466455/  1.697401, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.7412%\n",
      "layer   3  Sparsity: 81.7386%\n",
      "total_backward_count 1370600 real_backward_count 107004   7.807%\n",
      "fc layer 3 self.abs_max_out: 1978.0\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.472629/  1.720506, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5192%\n",
      "layer   3  Sparsity: 82.0927%\n",
      "total_backward_count 1380390 real_backward_count 107383   7.779%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.501631/  1.719409, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.5755%\n",
      "layer   3  Sparsity: 82.1591%\n",
      "total_backward_count 1390180 real_backward_count 107716   7.748%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.481320/  1.746617, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.6958%\n",
      "layer   3  Sparsity: 81.5751%\n",
      "total_backward_count 1399970 real_backward_count 108083   7.720%\n",
      "fc layer 3 self.abs_max_out: 2018.0\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.496121/  1.728325, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.6405%\n",
      "layer   3  Sparsity: 82.2557%\n",
      "total_backward_count 1409760 real_backward_count 108428   7.691%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.526671/  1.756050, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.8605%\n",
      "layer   3  Sparsity: 82.4352%\n",
      "total_backward_count 1419550 real_backward_count 108845   7.668%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.515081/  1.759350, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9928%\n",
      "layer   3  Sparsity: 81.9576%\n",
      "total_backward_count 1429340 real_backward_count 109193   7.639%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.522324/  1.741479, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9801%\n",
      "layer   3  Sparsity: 81.9240%\n",
      "total_backward_count 1439130 real_backward_count 109570   7.614%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.503852/  1.736135, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9831%\n",
      "layer   3  Sparsity: 81.9856%\n",
      "total_backward_count 1448920 real_backward_count 109948   7.588%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.495865/  1.738841, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9360%\n",
      "layer   3  Sparsity: 82.1906%\n",
      "total_backward_count 1458710 real_backward_count 110287   7.561%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.496021/  1.723379, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0894%\n",
      "layer   3  Sparsity: 82.5336%\n",
      "total_backward_count 1468500 real_backward_count 110632   7.534%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.504152/  1.774953, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9573%\n",
      "layer   3  Sparsity: 83.1798%\n",
      "total_backward_count 1478290 real_backward_count 111004   7.509%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.538567/  1.766600, val:  70.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0735%\n",
      "layer   3  Sparsity: 83.4286%\n",
      "total_backward_count 1488080 real_backward_count 111334   7.482%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.507973/  1.766910, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.2235%\n",
      "layer   3  Sparsity: 82.8844%\n",
      "total_backward_count 1497870 real_backward_count 111715   7.458%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.516137/  1.736843, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.2785%\n",
      "layer   3  Sparsity: 82.5983%\n",
      "total_backward_count 1507660 real_backward_count 112057   7.433%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.506049/  1.710514, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0199%\n",
      "layer   3  Sparsity: 82.2409%\n",
      "total_backward_count 1517450 real_backward_count 112408   7.408%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.499044/  1.724045, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1749%\n",
      "layer   3  Sparsity: 82.4816%\n",
      "total_backward_count 1527240 real_backward_count 112736   7.382%\n",
      "lif layer 2 self.abs_max_v: 7224.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.513237/  1.765991, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1561%\n",
      "layer   3  Sparsity: 82.7816%\n",
      "total_backward_count 1537030 real_backward_count 113068   7.356%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.523379/  1.747784, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0849%\n",
      "layer   3  Sparsity: 82.9523%\n",
      "total_backward_count 1546820 real_backward_count 113424   7.333%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.526473/  1.735733, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1360%\n",
      "layer   3  Sparsity: 82.7838%\n",
      "total_backward_count 1556610 real_backward_count 113795   7.310%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.529349/  1.749485, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9605%\n",
      "layer   3  Sparsity: 83.5746%\n",
      "total_backward_count 1566400 real_backward_count 114121   7.286%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.517936/  1.719463, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9768%\n",
      "layer   3  Sparsity: 82.5690%\n",
      "total_backward_count 1576190 real_backward_count 114448   7.261%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.517346/  1.792261, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0895%\n",
      "layer   3  Sparsity: 83.0509%\n",
      "total_backward_count 1585980 real_backward_count 114853   7.242%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.527946/  1.757506, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1645%\n",
      "layer   3  Sparsity: 83.3939%\n",
      "total_backward_count 1595770 real_backward_count 115205   7.219%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.502644/  1.747431, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 77.9203%\n",
      "layer   3  Sparsity: 82.5171%\n",
      "total_backward_count 1605560 real_backward_count 115568   7.198%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.504314/  1.732855, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0443%\n",
      "layer   3  Sparsity: 82.4089%\n",
      "total_backward_count 1615350 real_backward_count 115902   7.175%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.507230/  1.745873, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0013%\n",
      "layer   3  Sparsity: 82.8650%\n",
      "total_backward_count 1625140 real_backward_count 116230   7.152%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.504076/  1.731689, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1151%\n",
      "layer   3  Sparsity: 82.8228%\n",
      "total_backward_count 1634930 real_backward_count 116534   7.128%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.500117/  1.723232, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3011%\n",
      "layer   3  Sparsity: 82.5278%\n",
      "total_backward_count 1644720 real_backward_count 116880   7.106%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.510011/  1.740409, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.0343%\n",
      "layer   3  Sparsity: 82.9055%\n",
      "total_backward_count 1654510 real_backward_count 117190   7.083%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.496980/  1.717809, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3868%\n",
      "layer   3  Sparsity: 82.8975%\n",
      "total_backward_count 1664300 real_backward_count 117509   7.061%\n",
      "fc layer 3 self.abs_max_out: 2040.0\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.488802/  1.712009, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4635%\n",
      "layer   3  Sparsity: 82.8900%\n",
      "total_backward_count 1674090 real_backward_count 117793   7.036%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.483635/  1.715927, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4304%\n",
      "layer   3  Sparsity: 83.0285%\n",
      "total_backward_count 1683880 real_backward_count 118163   7.017%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.477412/  1.728781, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.7158%\n",
      "layer   3  Sparsity: 82.8736%\n",
      "total_backward_count 1693670 real_backward_count 118456   6.994%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.484139/  1.719321, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.6479%\n",
      "layer   3  Sparsity: 82.1322%\n",
      "total_backward_count 1703460 real_backward_count 118753   6.971%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.480496/  1.711251, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.6219%\n",
      "layer   3  Sparsity: 82.5687%\n",
      "total_backward_count 1713250 real_backward_count 119044   6.948%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.483307/  1.705624, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.6641%\n",
      "layer   3  Sparsity: 82.1655%\n",
      "total_backward_count 1723040 real_backward_count 119382   6.929%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.501543/  1.733078, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3216%\n",
      "layer   3  Sparsity: 82.8921%\n",
      "total_backward_count 1732830 real_backward_count 119679   6.907%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.490811/  1.722963, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4058%\n",
      "layer   3  Sparsity: 82.6926%\n",
      "total_backward_count 1742620 real_backward_count 119960   6.884%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.487721/  1.709075, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4904%\n",
      "layer   3  Sparsity: 82.4572%\n",
      "total_backward_count 1752410 real_backward_count 120278   6.864%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.477459/  1.695804, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4517%\n",
      "layer   3  Sparsity: 81.8489%\n",
      "total_backward_count 1762200 real_backward_count 120572   6.842%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.476928/  1.707834, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3635%\n",
      "layer   3  Sparsity: 82.1030%\n",
      "total_backward_count 1771990 real_backward_count 120860   6.821%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.469961/  1.699528, val:  77.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4828%\n",
      "layer   3  Sparsity: 81.7006%\n",
      "total_backward_count 1781780 real_backward_count 121200   6.802%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.466106/  1.721460, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3720%\n",
      "layer   3  Sparsity: 82.2598%\n",
      "total_backward_count 1791570 real_backward_count 121504   6.782%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.474768/  1.723864, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.2824%\n",
      "layer   3  Sparsity: 82.5664%\n",
      "total_backward_count 1801360 real_backward_count 121824   6.763%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.483453/  1.725479, val:  75.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.5544%\n",
      "layer   3  Sparsity: 82.7728%\n",
      "total_backward_count 1811150 real_backward_count 122097   6.741%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.455250/  1.677249, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.7053%\n",
      "layer   3  Sparsity: 82.0081%\n",
      "total_backward_count 1820940 real_backward_count 122381   6.721%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.435845/  1.678177, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.6576%\n",
      "layer   3  Sparsity: 82.0277%\n",
      "total_backward_count 1830730 real_backward_count 122664   6.700%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.441277/  1.707080, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4307%\n",
      "layer   3  Sparsity: 82.3776%\n",
      "total_backward_count 1840520 real_backward_count 122958   6.681%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.434917/  1.667672, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.5854%\n",
      "layer   3  Sparsity: 82.2388%\n",
      "total_backward_count 1850310 real_backward_count 123233   6.660%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.448248/  1.685647, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.2071%\n",
      "layer   3  Sparsity: 82.1515%\n",
      "total_backward_count 1860100 real_backward_count 123482   6.638%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.460392/  1.693070, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.3363%\n",
      "layer   3  Sparsity: 82.3268%\n",
      "total_backward_count 1869890 real_backward_count 123774   6.619%\n",
      "fc layer 3 self.abs_max_out: 2068.0\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.431662/  1.665982, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.5486%\n",
      "layer   3  Sparsity: 81.6276%\n",
      "total_backward_count 1879680 real_backward_count 124077   6.601%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.438496/  1.679659, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4912%\n",
      "layer   3  Sparsity: 82.2116%\n",
      "total_backward_count 1889470 real_backward_count 124337   6.581%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.439040/  1.695557, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.1872%\n",
      "layer   3  Sparsity: 81.9560%\n",
      "total_backward_count 1899260 real_backward_count 124590   6.560%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.436016/  1.685873, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.2089%\n",
      "layer   3  Sparsity: 82.3322%\n",
      "total_backward_count 1909050 real_backward_count 124845   6.540%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.430872/  1.668008, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4699%\n",
      "layer   3  Sparsity: 82.4925%\n",
      "total_backward_count 1918840 real_backward_count 125122   6.521%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.442472/  1.712662, val:  79.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.4090%\n",
      "layer   3  Sparsity: 82.2318%\n",
      "total_backward_count 1928630 real_backward_count 125434   6.504%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.455985/  1.700482, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.5928%\n",
      "layer   3  Sparsity: 82.5212%\n",
      "total_backward_count 1938420 real_backward_count 125728   6.486%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.433640/  1.692946, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.5888%\n",
      "layer   3  Sparsity: 82.4161%\n",
      "total_backward_count 1948210 real_backward_count 125976   6.466%\n",
      "fc layer 3 self.abs_max_out: 2073.0\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.435161/  1.693973, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 89.4536%\n",
      "layer   2  Sparsity: 78.5774%\n",
      "layer   3  Sparsity: 82.7753%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c3d2b9695d40ba9a58a7e9b44691ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.43516</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>1.69397</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-sweep-110</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6db7nz16' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6db7nz16</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_153357-6db7nz16/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z3svooms with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_195140-z3svooms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3svooms' target=\"_blank\">lyric-sweep-116</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3svooms' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3svooms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251116_195149_239', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 236.0\n",
      "lif layer 1 self.abs_max_v: 236.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 345.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 1 self.abs_max_out: 302.0\n",
      "lif layer 1 self.abs_max_v: 319.5\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 517.0\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "lif layer 1 self.abs_max_v: 365.0\n",
      "fc layer 2 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 698.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "lif layer 1 self.abs_max_v: 389.5\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "lif layer 1 self.abs_max_v: 582.0\n",
      "fc layer 2 self.abs_max_out: 485.0\n",
      "lif layer 2 self.abs_max_v: 774.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 1 self.abs_max_out: 648.0\n",
      "lif layer 1 self.abs_max_v: 648.0\n",
      "fc layer 1 self.abs_max_out: 754.0\n",
      "lif layer 1 self.abs_max_v: 754.0\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 785.5\n",
      "lif layer 2 self.abs_max_v: 813.0\n",
      "fc layer 1 self.abs_max_out: 777.0\n",
      "lif layer 1 self.abs_max_v: 777.0\n",
      "lif layer 2 self.abs_max_v: 852.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 1 self.abs_max_out: 785.0\n",
      "lif layer 1 self.abs_max_v: 879.0\n",
      "lif layer 1 self.abs_max_v: 905.5\n",
      "lif layer 1 self.abs_max_v: 983.0\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 1 self.abs_max_out: 817.0\n",
      "lif layer 1 self.abs_max_v: 1017.5\n",
      "fc layer 1 self.abs_max_out: 820.0\n",
      "fc layer 1 self.abs_max_out: 1007.0\n",
      "lif layer 1 self.abs_max_v: 1079.0\n",
      "fc layer 2 self.abs_max_out: 622.0\n",
      "lif layer 2 self.abs_max_v: 877.5\n",
      "lif layer 2 self.abs_max_v: 884.0\n",
      "lif layer 2 self.abs_max_v: 928.0\n",
      "lif layer 1 self.abs_max_v: 1085.0\n",
      "fc layer 3 self.abs_max_out: 277.0\n",
      "fc layer 2 self.abs_max_out: 665.0\n",
      "fc layer 1 self.abs_max_out: 1138.0\n",
      "lif layer 1 self.abs_max_v: 1138.0\n",
      "fc layer 1 self.abs_max_out: 1439.0\n",
      "lif layer 1 self.abs_max_v: 1439.0\n",
      "lif layer 2 self.abs_max_v: 971.0\n",
      "fc layer 2 self.abs_max_out: 689.0\n",
      "lif layer 2 self.abs_max_v: 1022.0\n",
      "fc layer 1 self.abs_max_out: 1451.0\n",
      "lif layer 1 self.abs_max_v: 1451.0\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "fc layer 2 self.abs_max_out: 824.0\n",
      "fc layer 3 self.abs_max_out: 295.0\n",
      "lif layer 2 self.abs_max_v: 1041.5\n",
      "lif layer 2 self.abs_max_v: 1081.0\n",
      "fc layer 1 self.abs_max_out: 1482.0\n",
      "lif layer 1 self.abs_max_v: 1613.0\n",
      "fc layer 1 self.abs_max_out: 1620.0\n",
      "lif layer 1 self.abs_max_v: 1993.5\n",
      "fc layer 1 self.abs_max_out: 1766.0\n",
      "lif layer 1 self.abs_max_v: 2213.0\n",
      "lif layer 1 self.abs_max_v: 2245.5\n",
      "lif layer 2 self.abs_max_v: 1084.0\n",
      "lif layer 2 self.abs_max_v: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1117.0\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "lif layer 2 self.abs_max_v: 1203.5\n",
      "lif layer 2 self.abs_max_v: 1232.5\n",
      "lif layer 2 self.abs_max_v: 1370.5\n",
      "lif layer 2 self.abs_max_v: 1376.5\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 1391.0\n",
      "lif layer 1 self.abs_max_v: 2395.0\n",
      "lif layer 2 self.abs_max_v: 1439.0\n",
      "lif layer 1 self.abs_max_v: 2533.5\n",
      "fc layer 1 self.abs_max_out: 1791.0\n",
      "fc layer 2 self.abs_max_out: 924.0\n",
      "fc layer 1 self.abs_max_out: 1872.0\n",
      "lif layer 2 self.abs_max_v: 1468.0\n",
      "lif layer 2 self.abs_max_v: 1485.5\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "lif layer 2 self.abs_max_v: 1530.0\n",
      "lif layer 2 self.abs_max_v: 1620.0\n",
      "fc layer 1 self.abs_max_out: 2064.0\n",
      "lif layer 1 self.abs_max_v: 2689.0\n",
      "lif layer 1 self.abs_max_v: 2757.5\n",
      "lif layer 1 self.abs_max_v: 2862.5\n",
      "lif layer 1 self.abs_max_v: 2863.5\n",
      "fc layer 1 self.abs_max_out: 2197.0\n",
      "lif layer 1 self.abs_max_v: 3015.0\n",
      "lif layer 1 self.abs_max_v: 3165.5\n",
      "lif layer 1 self.abs_max_v: 3289.0\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "lif layer 2 self.abs_max_v: 1696.0\n",
      "lif layer 2 self.abs_max_v: 1752.0\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "lif layer 2 self.abs_max_v: 1766.5\n",
      "fc layer 2 self.abs_max_out: 1035.0\n",
      "lif layer 2 self.abs_max_v: 1796.5\n",
      "lif layer 2 self.abs_max_v: 1798.5\n",
      "lif layer 2 self.abs_max_v: 1824.5\n",
      "lif layer 2 self.abs_max_v: 1831.5\n",
      "lif layer 2 self.abs_max_v: 1833.5\n",
      "lif layer 2 self.abs_max_v: 1923.0\n",
      "lif layer 2 self.abs_max_v: 1966.5\n",
      "fc layer 2 self.abs_max_out: 1068.0\n",
      "fc layer 2 self.abs_max_out: 1147.0\n",
      "lif layer 2 self.abs_max_v: 2119.0\n",
      "fc layer 2 self.abs_max_out: 1149.0\n",
      "fc layer 2 self.abs_max_out: 1205.0\n",
      "fc layer 2 self.abs_max_out: 1216.0\n",
      "fc layer 2 self.abs_max_out: 1226.0\n",
      "fc layer 2 self.abs_max_out: 1269.0\n",
      "lif layer 2 self.abs_max_v: 2138.0\n",
      "lif layer 2 self.abs_max_v: 2174.0\n",
      "lif layer 2 self.abs_max_v: 2268.0\n",
      "fc layer 2 self.abs_max_out: 1290.0\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 3 self.abs_max_out: 350.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "lif layer 1 self.abs_max_v: 3581.0\n",
      "lif layer 1 self.abs_max_v: 3590.5\n",
      "lif layer 1 self.abs_max_v: 3642.0\n",
      "fc layer 1 self.abs_max_out: 2342.0\n",
      "lif layer 1 self.abs_max_v: 3910.0\n",
      "lif layer 1 self.abs_max_v: 4113.0\n",
      "lif layer 1 self.abs_max_v: 4182.0\n",
      "lif layer 2 self.abs_max_v: 2277.5\n",
      "lif layer 2 self.abs_max_v: 2280.5\n",
      "fc layer 2 self.abs_max_out: 1397.0\n",
      "lif layer 2 self.abs_max_v: 2354.5\n",
      "lif layer 2 self.abs_max_v: 2379.5\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "fc layer 2 self.abs_max_out: 1439.0\n",
      "fc layer 2 self.abs_max_out: 1503.0\n",
      "lif layer 2 self.abs_max_v: 2414.5\n",
      "lif layer 2 self.abs_max_v: 2449.5\n",
      "lif layer 2 self.abs_max_v: 2487.0\n",
      "lif layer 2 self.abs_max_v: 2518.0\n",
      "lif layer 2 self.abs_max_v: 2523.0\n",
      "lif layer 2 self.abs_max_v: 2545.5\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1524.0\n",
      "fc layer 2 self.abs_max_out: 1530.0\n",
      "fc layer 2 self.abs_max_out: 1551.0\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "fc layer 1 self.abs_max_out: 2495.0\n",
      "fc layer 1 self.abs_max_out: 2660.0\n",
      "lif layer 1 self.abs_max_v: 4427.5\n",
      "fc layer 2 self.abs_max_out: 1574.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "lif layer 1 self.abs_max_v: 4517.5\n",
      "lif layer 2 self.abs_max_v: 2577.5\n",
      "lif layer 2 self.abs_max_v: 2595.0\n",
      "fc layer 3 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 2629.5\n",
      "lif layer 1 self.abs_max_v: 4675.5\n",
      "lif layer 2 self.abs_max_v: 2703.5\n",
      "lif layer 2 self.abs_max_v: 2788.0\n",
      "fc layer 1 self.abs_max_out: 2786.0\n",
      "lif layer 1 self.abs_max_v: 4915.5\n",
      "fc layer 1 self.abs_max_out: 3064.0\n",
      "lif layer 1 self.abs_max_v: 5467.5\n",
      "lif layer 1 self.abs_max_v: 5539.5\n",
      "lif layer 1 self.abs_max_v: 5718.0\n",
      "lif layer 2 self.abs_max_v: 2788.5\n",
      "lif layer 2 self.abs_max_v: 2806.5\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "fc layer 3 self.abs_max_out: 436.0\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 2 self.abs_max_out: 1580.0\n",
      "fc layer 2 self.abs_max_out: 1623.0\n",
      "fc layer 2 self.abs_max_out: 1649.0\n",
      "lif layer 1 self.abs_max_v: 5738.0\n",
      "lif layer 1 self.abs_max_v: 5782.0\n",
      "fc layer 1 self.abs_max_out: 3243.0\n",
      "lif layer 1 self.abs_max_v: 6134.0\n",
      "fc layer 1 self.abs_max_out: 3271.0\n",
      "fc layer 1 self.abs_max_out: 3380.0\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "fc layer 2 self.abs_max_out: 1702.0\n",
      "lif layer 2 self.abs_max_v: 2836.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.674878/  1.945974, val:  30.00%, val_best:  30.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3599%\n",
      "layer   2  Sparsity: 66.2307%\n",
      "layer   3  Sparsity: 59.1252%\n",
      "total_backward_count 9790 real_backward_count 1556  15.894%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 3721.0\n",
      "fc layer 1 self.abs_max_out: 3792.0\n",
      "lif layer 1 self.abs_max_v: 6428.0\n",
      "lif layer 1 self.abs_max_v: 6477.5\n",
      "lif layer 1 self.abs_max_v: 6573.5\n",
      "fc layer 2 self.abs_max_out: 1730.0\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "fc layer 2 self.abs_max_out: 1966.0\n",
      "fc layer 2 self.abs_max_out: 1992.0\n",
      "lif layer 2 self.abs_max_v: 2860.0\n",
      "lif layer 2 self.abs_max_v: 2861.5\n",
      "lif layer 1 self.abs_max_v: 6609.0\n",
      "lif layer 1 self.abs_max_v: 6893.0\n",
      "lif layer 1 self.abs_max_v: 7051.5\n",
      "fc layer 2 self.abs_max_out: 2020.0\n",
      "lif layer 2 self.abs_max_v: 2862.0\n",
      "lif layer 2 self.abs_max_v: 2891.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.605951/  1.886759, val:  45.00%, val_best:  45.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3392%\n",
      "layer   2  Sparsity: 74.1399%\n",
      "layer   3  Sparsity: 63.2391%\n",
      "total_backward_count 19580 real_backward_count 2942  15.026%\n",
      "lif layer 2 self.abs_max_v: 2908.5\n",
      "lif layer 2 self.abs_max_v: 2920.5\n",
      "lif layer 2 self.abs_max_v: 2945.0\n",
      "lif layer 2 self.abs_max_v: 2969.5\n",
      "fc layer 1 self.abs_max_out: 4041.0\n",
      "lif layer 2 self.abs_max_v: 2998.5\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 524.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 2 self.abs_max_out: 2039.0\n",
      "lif layer 2 self.abs_max_v: 3002.0\n",
      "lif layer 2 self.abs_max_v: 3054.5\n",
      "fc layer 1 self.abs_max_out: 4156.0\n",
      "lif layer 1 self.abs_max_v: 7239.5\n",
      "lif layer 1 self.abs_max_v: 7287.0\n",
      "fc layer 1 self.abs_max_out: 4255.0\n",
      "fc layer 2 self.abs_max_out: 2056.0\n",
      "lif layer 2 self.abs_max_v: 3110.5\n",
      "lif layer 1 self.abs_max_v: 7353.0\n",
      "lif layer 1 self.abs_max_v: 7838.5\n",
      "lif layer 2 self.abs_max_v: 3121.5\n",
      "lif layer 2 self.abs_max_v: 3226.5\n",
      "lif layer 2 self.abs_max_v: 3307.0\n",
      "lif layer 2 self.abs_max_v: 3384.0\n",
      "fc layer 1 self.abs_max_out: 4394.0\n",
      "lif layer 1 self.abs_max_v: 7984.0\n",
      "lif layer 1 self.abs_max_v: 8196.0\n",
      "fc layer 1 self.abs_max_out: 4449.0\n",
      "fc layer 1 self.abs_max_out: 4684.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.576704/  1.902101, val:  33.75%, val_best:  45.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 75.2684%\n",
      "layer   3  Sparsity: 63.7874%\n",
      "total_backward_count 29370 real_backward_count 4328  14.736%\n",
      "fc layer 2 self.abs_max_out: 2077.0\n",
      "fc layer 2 self.abs_max_out: 2111.0\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 2 self.abs_max_out: 2213.0\n",
      "fc layer 2 self.abs_max_out: 2285.0\n",
      "fc layer 1 self.abs_max_out: 4692.0\n",
      "lif layer 1 self.abs_max_v: 8326.5\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.572802/  1.834520, val:  50.83%, val_best:  50.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3457%\n",
      "layer   2  Sparsity: 76.3902%\n",
      "layer   3  Sparsity: 64.2427%\n",
      "total_backward_count 39160 real_backward_count 5576  14.239%\n",
      "fc layer 1 self.abs_max_out: 4693.0\n",
      "fc layer 1 self.abs_max_out: 4887.0\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "lif layer 2 self.abs_max_v: 3413.5\n",
      "lif layer 1 self.abs_max_v: 8506.0\n",
      "lif layer 1 self.abs_max_v: 8596.5\n",
      "fc layer 1 self.abs_max_out: 4929.0\n",
      "lif layer 1 self.abs_max_v: 8877.0\n",
      "lif layer 2 self.abs_max_v: 3428.0\n",
      "lif layer 2 self.abs_max_v: 3571.0\n",
      "fc layer 1 self.abs_max_out: 5003.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.533222/  1.842791, val:  45.83%, val_best:  50.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 76.1349%\n",
      "layer   3  Sparsity: 64.5715%\n",
      "total_backward_count 48950 real_backward_count 6782  13.855%\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 3 self.abs_max_out: 576.0\n",
      "fc layer 2 self.abs_max_out: 2304.0\n",
      "lif layer 2 self.abs_max_v: 3719.5\n",
      "fc layer 1 self.abs_max_out: 5018.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.523431/  1.836148, val:  48.75%, val_best:  50.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 75.6536%\n",
      "layer   3  Sparsity: 65.5889%\n",
      "total_backward_count 58740 real_backward_count 7998  13.616%\n",
      "fc layer 1 self.abs_max_out: 5037.0\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "lif layer 2 self.abs_max_v: 3730.0\n",
      "fc layer 1 self.abs_max_out: 5120.0\n",
      "fc layer 1 self.abs_max_out: 5133.0\n",
      "lif layer 1 self.abs_max_v: 9486.0\n",
      "fc layer 2 self.abs_max_out: 2357.0\n",
      "fc layer 1 self.abs_max_out: 5162.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.539922/  1.806849, val:  47.50%, val_best:  50.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3125%\n",
      "layer   2  Sparsity: 76.9211%\n",
      "layer   3  Sparsity: 67.5484%\n",
      "total_backward_count 68530 real_backward_count 9197  13.420%\n",
      "fc layer 2 self.abs_max_out: 2371.0\n",
      "lif layer 2 self.abs_max_v: 3940.5\n",
      "lif layer 2 self.abs_max_v: 3943.5\n",
      "lif layer 2 self.abs_max_v: 3976.0\n",
      "lif layer 2 self.abs_max_v: 4085.0\n",
      "lif layer 2 self.abs_max_v: 4205.5\n",
      "fc layer 2 self.abs_max_out: 2431.0\n",
      "fc layer 1 self.abs_max_out: 5499.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.522849/  1.786946, val:  46.67%, val_best:  50.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3242%\n",
      "layer   2  Sparsity: 77.4796%\n",
      "layer   3  Sparsity: 67.2442%\n",
      "total_backward_count 78320 real_backward_count 10337  13.198%\n",
      "lif layer 1 self.abs_max_v: 9685.5\n",
      "fc layer 1 self.abs_max_out: 5500.0\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "fc layer 1 self.abs_max_out: 5811.0\n",
      "lif layer 1 self.abs_max_v: 10029.0\n",
      "lif layer 1 self.abs_max_v: 10304.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.520934/  1.774474, val:  53.33%, val_best:  53.33%, tr:  99.69%, tr_best:  99.80%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 78.0238%\n",
      "layer   3  Sparsity: 68.4320%\n",
      "total_backward_count 88110 real_backward_count 11487  13.037%\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.531585/  1.828790, val:  49.17%, val_best:  53.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 77.6030%\n",
      "layer   3  Sparsity: 69.2500%\n",
      "total_backward_count 97900 real_backward_count 12639  12.910%\n",
      "fc layer 2 self.abs_max_out: 2446.0\n",
      "lif layer 2 self.abs_max_v: 4513.0\n",
      "lif layer 2 self.abs_max_v: 4673.5\n",
      "fc layer 2 self.abs_max_out: 2516.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.524971/  1.808966, val:  48.75%, val_best:  53.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3585%\n",
      "layer   2  Sparsity: 76.8590%\n",
      "layer   3  Sparsity: 70.5919%\n",
      "total_backward_count 107690 real_backward_count 13698  12.720%\n",
      "lif layer 1 self.abs_max_v: 10365.0\n",
      "fc layer 2 self.abs_max_out: 2602.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.546026/  1.765559, val:  58.75%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 76.0152%\n",
      "layer   3  Sparsity: 69.7831%\n",
      "total_backward_count 117480 real_backward_count 14784  12.584%\n",
      "fc layer 2 self.abs_max_out: 2618.0\n",
      "lif layer 2 self.abs_max_v: 4865.0\n",
      "lif layer 2 self.abs_max_v: 4975.5\n",
      "fc layer 2 self.abs_max_out: 2690.0\n",
      "fc layer 1 self.abs_max_out: 5860.0\n",
      "lif layer 1 self.abs_max_v: 10721.5\n",
      "fc layer 1 self.abs_max_out: 6257.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.504474/  1.751569, val:  49.58%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3343%\n",
      "layer   2  Sparsity: 76.1105%\n",
      "layer   3  Sparsity: 70.7022%\n",
      "total_backward_count 127270 real_backward_count 15816  12.427%\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.491800/  1.789877, val:  47.50%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3433%\n",
      "layer   2  Sparsity: 75.8568%\n",
      "layer   3  Sparsity: 70.2713%\n",
      "total_backward_count 137060 real_backward_count 16857  12.299%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.460688/  1.697072, val:  55.00%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3454%\n",
      "layer   2  Sparsity: 75.6011%\n",
      "layer   3  Sparsity: 69.7947%\n",
      "total_backward_count 146850 real_backward_count 17901  12.190%\n",
      "lif layer 2 self.abs_max_v: 5036.5\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.467450/  1.716218, val:  60.00%, val_best:  60.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3256%\n",
      "layer   2  Sparsity: 76.4000%\n",
      "layer   3  Sparsity: 70.5086%\n",
      "total_backward_count 156640 real_backward_count 18930  12.085%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.469935/  1.720757, val:  59.17%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2795%\n",
      "layer   2  Sparsity: 75.9508%\n",
      "layer   3  Sparsity: 70.9562%\n",
      "total_backward_count 166430 real_backward_count 19964  11.995%\n",
      "lif layer 1 self.abs_max_v: 10937.5\n",
      "fc layer 1 self.abs_max_out: 6284.0\n",
      "fc layer 1 self.abs_max_out: 6602.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.449903/  1.683800, val:  57.92%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 74.9884%\n",
      "layer   3  Sparsity: 70.1948%\n",
      "total_backward_count 176220 real_backward_count 20980  11.906%\n",
      "lif layer 1 self.abs_max_v: 11401.0\n",
      "lif layer 1 self.abs_max_v: 11565.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.435558/  1.639256, val:  70.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3354%\n",
      "layer   2  Sparsity: 75.3893%\n",
      "layer   3  Sparsity: 69.3004%\n",
      "total_backward_count 186010 real_backward_count 21959  11.805%\n",
      "fc layer 1 self.abs_max_out: 6714.0\n",
      "lif layer 1 self.abs_max_v: 12054.0\n",
      "fc layer 2 self.abs_max_out: 2754.0\n",
      "lif layer 2 self.abs_max_v: 5141.5\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.402239/  1.731782, val:  49.58%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3349%\n",
      "layer   2  Sparsity: 75.9498%\n",
      "layer   3  Sparsity: 70.3180%\n",
      "total_backward_count 195800 real_backward_count 22941  11.717%\n",
      "fc layer 1 self.abs_max_out: 6803.0\n",
      "lif layer 1 self.abs_max_v: 12103.0\n",
      "fc layer 1 self.abs_max_out: 7051.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.400941/  1.684672, val:  58.75%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3081%\n",
      "layer   2  Sparsity: 75.2529%\n",
      "layer   3  Sparsity: 70.8575%\n",
      "total_backward_count 205590 real_backward_count 23860  11.606%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.431763/  1.708324, val:  59.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3738%\n",
      "layer   2  Sparsity: 74.9864%\n",
      "layer   3  Sparsity: 71.6705%\n",
      "total_backward_count 215380 real_backward_count 24860  11.542%\n",
      "fc layer 2 self.abs_max_out: 2766.0\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.421258/  1.653915, val:  54.58%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3379%\n",
      "layer   2  Sparsity: 75.2063%\n",
      "layer   3  Sparsity: 70.6637%\n",
      "total_backward_count 225170 real_backward_count 25820  11.467%\n",
      "lif layer 1 self.abs_max_v: 12267.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.378869/  1.621680, val:  64.58%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 75.5167%\n",
      "layer   3  Sparsity: 70.7073%\n",
      "total_backward_count 234960 real_backward_count 26709  11.367%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.388390/  1.583397, val:  72.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3679%\n",
      "layer   2  Sparsity: 74.2986%\n",
      "layer   3  Sparsity: 71.3235%\n",
      "total_backward_count 244750 real_backward_count 27589  11.272%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.373060/  1.609468, val:  70.00%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3476%\n",
      "layer   2  Sparsity: 73.9217%\n",
      "layer   3  Sparsity: 71.0672%\n",
      "total_backward_count 254540 real_backward_count 28511  11.201%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.338690/  1.613497, val:  64.58%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3467%\n",
      "layer   2  Sparsity: 74.2147%\n",
      "layer   3  Sparsity: 70.5833%\n",
      "total_backward_count 264330 real_backward_count 29339  11.099%\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "fc layer 3 self.abs_max_out: 685.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.336463/  1.617368, val:  72.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3408%\n",
      "layer   2  Sparsity: 74.3417%\n",
      "layer   3  Sparsity: 71.1480%\n",
      "total_backward_count 274120 real_backward_count 30199  11.017%\n",
      "lif layer 1 self.abs_max_v: 12715.5\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.332130/  1.591015, val:  69.58%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3173%\n",
      "layer   2  Sparsity: 74.3811%\n",
      "layer   3  Sparsity: 70.8301%\n",
      "total_backward_count 283910 real_backward_count 31015  10.924%\n",
      "fc layer 2 self.abs_max_out: 2768.0\n",
      "fc layer 1 self.abs_max_out: 7061.0\n",
      "fc layer 1 self.abs_max_out: 7320.0\n",
      "lif layer 1 self.abs_max_v: 13287.0\n",
      "fc layer 2 self.abs_max_out: 2832.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.308240/  1.577235, val:  54.17%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3423%\n",
      "layer   2  Sparsity: 74.2026%\n",
      "layer   3  Sparsity: 70.2692%\n",
      "total_backward_count 293700 real_backward_count 31821  10.835%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.299522/  1.553650, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 73.8129%\n",
      "layer   3  Sparsity: 70.8506%\n",
      "total_backward_count 303490 real_backward_count 32650  10.758%\n",
      "fc layer 3 self.abs_max_out: 744.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.287250/  1.585992, val:  57.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3777%\n",
      "layer   2  Sparsity: 73.7776%\n",
      "layer   3  Sparsity: 71.1322%\n",
      "total_backward_count 313280 real_backward_count 33468  10.683%\n",
      "fc layer 2 self.abs_max_out: 2894.0\n",
      "lif layer 2 self.abs_max_v: 5388.0\n",
      "fc layer 1 self.abs_max_out: 7377.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.255360/  1.520536, val:  66.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 86.3057%\n",
      "layer   2  Sparsity: 74.5285%\n",
      "layer   3  Sparsity: 71.4033%\n",
      "total_backward_count 323070 real_backward_count 34222  10.593%\n",
      "fc layer 1 self.abs_max_out: 7920.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.230792/  1.524279, val:  67.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3417%\n",
      "layer   2  Sparsity: 74.1016%\n",
      "layer   3  Sparsity: 70.6776%\n",
      "total_backward_count 332860 real_backward_count 34957  10.502%\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "fc layer 2 self.abs_max_out: 2925.0\n",
      "fc layer 2 self.abs_max_out: 3009.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.260259/  1.524815, val:  65.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3143%\n",
      "layer   2  Sparsity: 73.8108%\n",
      "layer   3  Sparsity: 70.1471%\n",
      "total_backward_count 342650 real_backward_count 35711  10.422%\n",
      "fc layer 2 self.abs_max_out: 3044.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.243090/  1.560968, val:  58.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3625%\n",
      "layer   2  Sparsity: 73.6695%\n",
      "layer   3  Sparsity: 70.5004%\n",
      "total_backward_count 352440 real_backward_count 36422  10.334%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.227833/  1.511282, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3480%\n",
      "layer   2  Sparsity: 74.1295%\n",
      "layer   3  Sparsity: 70.8026%\n",
      "total_backward_count 362230 real_backward_count 37132  10.251%\n",
      "lif layer 1 self.abs_max_v: 13344.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.225451/  1.561804, val:  60.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 74.0268%\n",
      "layer   3  Sparsity: 71.5387%\n",
      "total_backward_count 372020 real_backward_count 37790  10.158%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.215936/  1.478956, val:  67.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3158%\n",
      "layer   2  Sparsity: 73.9316%\n",
      "layer   3  Sparsity: 70.8160%\n",
      "total_backward_count 381810 real_backward_count 38479  10.078%\n",
      "lif layer 1 self.abs_max_v: 13405.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.189982/  1.477468, val:  68.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3330%\n",
      "layer   2  Sparsity: 74.0004%\n",
      "layer   3  Sparsity: 70.9221%\n",
      "total_backward_count 391600 real_backward_count 39120   9.990%\n",
      "lif layer 2 self.abs_max_v: 5418.5\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.186862/  1.458233, val:  70.42%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3621%\n",
      "layer   2  Sparsity: 73.4150%\n",
      "layer   3  Sparsity: 70.0130%\n",
      "total_backward_count 401390 real_backward_count 39811   9.918%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.172865/  1.452011, val:  68.75%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3357%\n",
      "layer   2  Sparsity: 73.5043%\n",
      "layer   3  Sparsity: 70.4716%\n",
      "total_backward_count 411180 real_backward_count 40449   9.837%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.166910/  1.430219, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 73.7301%\n",
      "layer   3  Sparsity: 70.7037%\n",
      "total_backward_count 420970 real_backward_count 41056   9.753%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.160336/  1.431854, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3415%\n",
      "layer   2  Sparsity: 73.8845%\n",
      "layer   3  Sparsity: 70.9705%\n",
      "total_backward_count 430760 real_backward_count 41665   9.672%\n",
      "fc layer 2 self.abs_max_out: 3048.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.128919/  1.437535, val:  77.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3327%\n",
      "layer   2  Sparsity: 73.5194%\n",
      "layer   3  Sparsity: 70.9256%\n",
      "total_backward_count 440550 real_backward_count 42272   9.595%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.137472/  1.444209, val:  66.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 73.1981%\n",
      "layer   3  Sparsity: 70.7933%\n",
      "total_backward_count 450340 real_backward_count 42860   9.517%\n",
      "lif layer 2 self.abs_max_v: 5622.5\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.123371/  1.412965, val:  66.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3312%\n",
      "layer   2  Sparsity: 72.9961%\n",
      "layer   3  Sparsity: 70.1346%\n",
      "total_backward_count 460130 real_backward_count 43462   9.446%\n",
      "fc layer 1 self.abs_max_out: 7986.0\n",
      "lif layer 1 self.abs_max_v: 14239.5\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.105581/  1.426936, val:  68.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 73.2325%\n",
      "layer   3  Sparsity: 70.3499%\n",
      "total_backward_count 469920 real_backward_count 44038   9.371%\n",
      "fc layer 2 self.abs_max_out: 3061.0\n",
      "fc layer 2 self.abs_max_out: 3124.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.092281/  1.417283, val:  69.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3594%\n",
      "layer   2  Sparsity: 73.1548%\n",
      "layer   3  Sparsity: 70.5148%\n",
      "total_backward_count 479710 real_backward_count 44546   9.286%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.089958/  1.381759, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3317%\n",
      "layer   2  Sparsity: 72.9863%\n",
      "layer   3  Sparsity: 70.7444%\n",
      "total_backward_count 489500 real_backward_count 45080   9.209%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.059914/  1.380304, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 72.8327%\n",
      "layer   3  Sparsity: 70.9438%\n",
      "total_backward_count 499290 real_backward_count 45572   9.127%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.050748/  1.381313, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3353%\n",
      "layer   2  Sparsity: 72.1918%\n",
      "layer   3  Sparsity: 70.4272%\n",
      "total_backward_count 509080 real_backward_count 46106   9.057%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.060950/  1.393637, val:  65.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 72.5750%\n",
      "layer   3  Sparsity: 70.6825%\n",
      "total_backward_count 518870 real_backward_count 46670   8.995%\n",
      "fc layer 1 self.abs_max_out: 8043.0\n",
      "lif layer 1 self.abs_max_v: 14503.5\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.053481/  1.345799, val:  74.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3163%\n",
      "layer   2  Sparsity: 72.6463%\n",
      "layer   3  Sparsity: 70.8795%\n",
      "total_backward_count 528660 real_backward_count 47174   8.923%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.039458/  1.380732, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3340%\n",
      "layer   2  Sparsity: 73.1170%\n",
      "layer   3  Sparsity: 70.7845%\n",
      "total_backward_count 538450 real_backward_count 47645   8.849%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.024347/  1.365390, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.3274%\n",
      "layer   2  Sparsity: 73.2519%\n",
      "layer   3  Sparsity: 70.6482%\n",
      "total_backward_count 548240 real_backward_count 48126   8.778%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.033407/  1.361413, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3386%\n",
      "layer   2  Sparsity: 72.9103%\n",
      "layer   3  Sparsity: 70.3653%\n",
      "total_backward_count 558030 real_backward_count 48599   8.709%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.019759/  1.319239, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3535%\n",
      "layer   2  Sparsity: 72.9616%\n",
      "layer   3  Sparsity: 70.1297%\n",
      "total_backward_count 567820 real_backward_count 48997   8.629%\n",
      "fc layer 1 self.abs_max_out: 8065.0\n",
      "lif layer 1 self.abs_max_v: 14531.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.993738/  1.327033, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3322%\n",
      "layer   2  Sparsity: 73.1065%\n",
      "layer   3  Sparsity: 70.1016%\n",
      "total_backward_count 577610 real_backward_count 49456   8.562%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.995810/  1.339803, val:  70.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 72.3561%\n",
      "layer   3  Sparsity: 69.8828%\n",
      "total_backward_count 587400 real_backward_count 49878   8.491%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.986241/  1.322172, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3436%\n",
      "layer   2  Sparsity: 72.5360%\n",
      "layer   3  Sparsity: 69.7842%\n",
      "total_backward_count 597190 real_backward_count 50319   8.426%\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "fc layer 1 self.abs_max_out: 8262.0\n",
      "lif layer 1 self.abs_max_v: 14746.5\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.947628/  1.326756, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3487%\n",
      "layer   2  Sparsity: 73.0385%\n",
      "layer   3  Sparsity: 70.1283%\n",
      "total_backward_count 606980 real_backward_count 50731   8.358%\n",
      "fc layer 3 self.abs_max_out: 804.0\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.948930/  1.320586, val:  70.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2906%\n",
      "layer   2  Sparsity: 72.7440%\n",
      "layer   3  Sparsity: 69.5705%\n",
      "total_backward_count 616770 real_backward_count 51157   8.294%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.932415/  1.297715, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 72.2510%\n",
      "layer   3  Sparsity: 69.6495%\n",
      "total_backward_count 626560 real_backward_count 51540   8.226%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  0.929174/  1.294062, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3683%\n",
      "layer   2  Sparsity: 72.3773%\n",
      "layer   3  Sparsity: 69.3477%\n",
      "total_backward_count 636350 real_backward_count 51919   8.159%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.919680/  1.284278, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3233%\n",
      "layer   2  Sparsity: 72.6427%\n",
      "layer   3  Sparsity: 68.4232%\n",
      "total_backward_count 646140 real_backward_count 52337   8.100%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.913339/  1.259739, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3300%\n",
      "layer   2  Sparsity: 72.7902%\n",
      "layer   3  Sparsity: 69.0858%\n",
      "total_backward_count 655930 real_backward_count 52739   8.040%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.907449/  1.276608, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3041%\n",
      "layer   2  Sparsity: 72.8232%\n",
      "layer   3  Sparsity: 69.2498%\n",
      "total_backward_count 665720 real_backward_count 53122   7.980%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.890658/  1.237888, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3483%\n",
      "layer   2  Sparsity: 72.6233%\n",
      "layer   3  Sparsity: 69.3121%\n",
      "total_backward_count 675510 real_backward_count 53482   7.917%\n",
      "fc layer 1 self.abs_max_out: 8272.0\n",
      "lif layer 1 self.abs_max_v: 14981.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.876944/  1.263309, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3217%\n",
      "layer   2  Sparsity: 72.6868%\n",
      "layer   3  Sparsity: 69.7409%\n",
      "total_backward_count 685300 real_backward_count 53824   7.854%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.879179/  1.249737, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 72.7588%\n",
      "layer   3  Sparsity: 69.8360%\n",
      "total_backward_count 695090 real_backward_count 54166   7.793%\n",
      "fc layer 1 self.abs_max_out: 8311.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.879042/  1.267862, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3388%\n",
      "layer   2  Sparsity: 72.7070%\n",
      "layer   3  Sparsity: 69.3489%\n",
      "total_backward_count 704880 real_backward_count 54515   7.734%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.882040/  1.238804, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3409%\n",
      "layer   2  Sparsity: 72.5045%\n",
      "layer   3  Sparsity: 69.5073%\n",
      "total_backward_count 714670 real_backward_count 54857   7.676%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.871051/  1.246726, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3058%\n",
      "layer   2  Sparsity: 72.6623%\n",
      "layer   3  Sparsity: 69.4588%\n",
      "total_backward_count 724460 real_backward_count 55167   7.615%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.888470/  1.216585, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3068%\n",
      "layer   2  Sparsity: 72.2963%\n",
      "layer   3  Sparsity: 69.4037%\n",
      "total_backward_count 734250 real_backward_count 55473   7.555%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.869672/  1.254068, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2863%\n",
      "layer   2  Sparsity: 72.4269%\n",
      "layer   3  Sparsity: 69.1847%\n",
      "total_backward_count 744040 real_backward_count 55802   7.500%\n",
      "fc layer 3 self.abs_max_out: 825.0\n",
      "fc layer 3 self.abs_max_out: 832.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.860082/  1.242959, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 72.9424%\n",
      "layer   3  Sparsity: 70.0111%\n",
      "total_backward_count 753830 real_backward_count 56082   7.440%\n",
      "fc layer 2 self.abs_max_out: 3167.0\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.864358/  1.269010, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3268%\n",
      "layer   2  Sparsity: 73.0823%\n",
      "layer   3  Sparsity: 70.0476%\n",
      "total_backward_count 763620 real_backward_count 56364   7.381%\n",
      "fc layer 1 self.abs_max_out: 8391.0\n",
      "lif layer 1 self.abs_max_v: 15205.0\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.873265/  1.256262, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3537%\n",
      "layer   2  Sparsity: 72.8131%\n",
      "layer   3  Sparsity: 69.4714%\n",
      "total_backward_count 773410 real_backward_count 56659   7.326%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.848926/  1.245549, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3138%\n",
      "layer   2  Sparsity: 72.8098%\n",
      "layer   3  Sparsity: 69.6015%\n",
      "total_backward_count 783200 real_backward_count 56908   7.266%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.849636/  1.255384, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 73.0461%\n",
      "layer   3  Sparsity: 69.6583%\n",
      "total_backward_count 792990 real_backward_count 57168   7.209%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.840509/  1.280918, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 72.9527%\n",
      "layer   3  Sparsity: 69.3539%\n",
      "total_backward_count 802780 real_backward_count 57413   7.152%\n",
      "fc layer 1 self.abs_max_out: 8710.0\n",
      "lif layer 1 self.abs_max_v: 15300.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.840378/  1.214811, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3336%\n",
      "layer   2  Sparsity: 73.0747%\n",
      "layer   3  Sparsity: 68.9441%\n",
      "total_backward_count 812570 real_backward_count 57638   7.093%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.830297/  1.267000, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3339%\n",
      "layer   2  Sparsity: 72.8900%\n",
      "layer   3  Sparsity: 69.0947%\n",
      "total_backward_count 822360 real_backward_count 57893   7.040%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.832988/  1.219892, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3401%\n",
      "layer   2  Sparsity: 72.7401%\n",
      "layer   3  Sparsity: 68.9732%\n",
      "total_backward_count 832150 real_backward_count 58126   6.985%\n",
      "lif layer 2 self.abs_max_v: 5673.5\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.835216/  1.222122, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3605%\n",
      "layer   2  Sparsity: 72.6521%\n",
      "layer   3  Sparsity: 68.6768%\n",
      "total_backward_count 841940 real_backward_count 58384   6.934%\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.823784/  1.221825, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3079%\n",
      "layer   2  Sparsity: 72.9050%\n",
      "layer   3  Sparsity: 68.5402%\n",
      "total_backward_count 851730 real_backward_count 58619   6.882%\n",
      "fc layer 3 self.abs_max_out: 849.0\n",
      "fc layer 3 self.abs_max_out: 865.0\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.818621/  1.210606, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3506%\n",
      "layer   2  Sparsity: 72.7125%\n",
      "layer   3  Sparsity: 68.4960%\n",
      "total_backward_count 861520 real_backward_count 58850   6.831%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.816302/  1.239720, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3556%\n",
      "layer   2  Sparsity: 72.5749%\n",
      "layer   3  Sparsity: 68.1789%\n",
      "total_backward_count 871310 real_backward_count 59086   6.781%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.817463/  1.190111, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3377%\n",
      "layer   2  Sparsity: 72.5263%\n",
      "layer   3  Sparsity: 68.3385%\n",
      "total_backward_count 881100 real_backward_count 59316   6.732%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.806070/  1.215488, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 72.5484%\n",
      "layer   3  Sparsity: 68.5913%\n",
      "total_backward_count 890890 real_backward_count 59566   6.686%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.795947/  1.185633, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2931%\n",
      "layer   2  Sparsity: 72.4007%\n",
      "layer   3  Sparsity: 68.5036%\n",
      "total_backward_count 900680 real_backward_count 59808   6.640%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.784621/  1.196092, val:  80.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3298%\n",
      "layer   2  Sparsity: 72.4941%\n",
      "layer   3  Sparsity: 68.6636%\n",
      "total_backward_count 910470 real_backward_count 60023   6.593%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.772258/  1.172652, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 72.3794%\n",
      "layer   3  Sparsity: 69.0456%\n",
      "total_backward_count 920260 real_backward_count 60204   6.542%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.771744/  1.177582, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3206%\n",
      "layer   2  Sparsity: 72.6402%\n",
      "layer   3  Sparsity: 68.7924%\n",
      "total_backward_count 930050 real_backward_count 60415   6.496%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.762182/  1.180669, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2899%\n",
      "layer   2  Sparsity: 72.7194%\n",
      "layer   3  Sparsity: 68.7484%\n",
      "total_backward_count 939840 real_backward_count 60601   6.448%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.757993/  1.193866, val:  72.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3063%\n",
      "layer   2  Sparsity: 72.3429%\n",
      "layer   3  Sparsity: 68.5830%\n",
      "total_backward_count 949630 real_backward_count 60799   6.402%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.754333/  1.135128, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 72.4493%\n",
      "layer   3  Sparsity: 68.6360%\n",
      "total_backward_count 959420 real_backward_count 60993   6.357%\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.752627/  1.165328, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2930%\n",
      "layer   2  Sparsity: 72.3196%\n",
      "layer   3  Sparsity: 68.1856%\n",
      "total_backward_count 969210 real_backward_count 61201   6.315%\n",
      "fc layer 1 self.abs_max_out: 9155.0\n",
      "lif layer 1 self.abs_max_v: 16085.5\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.736723/  1.153329, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 72.5301%\n",
      "layer   3  Sparsity: 68.2288%\n",
      "total_backward_count 979000 real_backward_count 61387   6.270%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.724400/  1.157976, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3598%\n",
      "layer   2  Sparsity: 72.7894%\n",
      "layer   3  Sparsity: 69.0641%\n",
      "total_backward_count 988790 real_backward_count 61529   6.223%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.730467/  1.137461, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   2  Sparsity: 72.7086%\n",
      "layer   3  Sparsity: 69.1305%\n",
      "total_backward_count 998580 real_backward_count 61702   6.179%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.731431/  1.139951, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 72.5588%\n",
      "layer   3  Sparsity: 68.8977%\n",
      "total_backward_count 1008370 real_backward_count 61887   6.137%\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.726253/  1.156294, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3183%\n",
      "layer   2  Sparsity: 72.7433%\n",
      "layer   3  Sparsity: 68.5692%\n",
      "total_backward_count 1018160 real_backward_count 62048   6.094%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.724629/  1.155812, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3369%\n",
      "layer   2  Sparsity: 73.0121%\n",
      "layer   3  Sparsity: 69.1370%\n",
      "total_backward_count 1027950 real_backward_count 62223   6.053%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.708182/  1.183082, val:  74.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3501%\n",
      "layer   2  Sparsity: 73.0089%\n",
      "layer   3  Sparsity: 68.9913%\n",
      "total_backward_count 1037740 real_backward_count 62388   6.012%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.718107/  1.161351, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3182%\n",
      "layer   2  Sparsity: 72.7395%\n",
      "layer   3  Sparsity: 69.3863%\n",
      "total_backward_count 1047530 real_backward_count 62546   5.971%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.709956/  1.137650, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3324%\n",
      "layer   2  Sparsity: 72.6627%\n",
      "layer   3  Sparsity: 69.2344%\n",
      "total_backward_count 1057320 real_backward_count 62711   5.931%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.695393/  1.148410, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3035%\n",
      "layer   2  Sparsity: 72.8496%\n",
      "layer   3  Sparsity: 68.9478%\n",
      "total_backward_count 1067110 real_backward_count 62860   5.891%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.699199/  1.123054, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3076%\n",
      "layer   2  Sparsity: 72.5475%\n",
      "layer   3  Sparsity: 68.9486%\n",
      "total_backward_count 1076900 real_backward_count 62989   5.849%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.688506/  1.128477, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3637%\n",
      "layer   2  Sparsity: 72.5378%\n",
      "layer   3  Sparsity: 68.9001%\n",
      "total_backward_count 1086690 real_backward_count 63124   5.809%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.690472/  1.128276, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3031%\n",
      "layer   2  Sparsity: 72.5829%\n",
      "layer   3  Sparsity: 68.9726%\n",
      "total_backward_count 1096480 real_backward_count 63270   5.770%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.684154/  1.125284, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3565%\n",
      "layer   2  Sparsity: 72.5449%\n",
      "layer   3  Sparsity: 69.2194%\n",
      "total_backward_count 1106270 real_backward_count 63421   5.733%\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.676511/  1.144513, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3365%\n",
      "layer   2  Sparsity: 72.7161%\n",
      "layer   3  Sparsity: 69.4410%\n",
      "total_backward_count 1116060 real_backward_count 63546   5.694%\n",
      "fc layer 3 self.abs_max_out: 931.0\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.685553/  1.128982, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3261%\n",
      "layer   2  Sparsity: 72.5398%\n",
      "layer   3  Sparsity: 69.0883%\n",
      "total_backward_count 1125850 real_backward_count 63684   5.657%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.679867/  1.105236, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3047%\n",
      "layer   2  Sparsity: 72.7922%\n",
      "layer   3  Sparsity: 68.9693%\n",
      "total_backward_count 1135640 real_backward_count 63825   5.620%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.675645/  1.110554, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3151%\n",
      "layer   2  Sparsity: 72.5251%\n",
      "layer   3  Sparsity: 68.6750%\n",
      "total_backward_count 1145430 real_backward_count 63934   5.582%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.668778/  1.097671, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 72.7005%\n",
      "layer   3  Sparsity: 68.9226%\n",
      "total_backward_count 1155220 real_backward_count 64062   5.545%\n",
      "fc layer 3 self.abs_max_out: 960.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.666807/  1.131882, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3311%\n",
      "layer   2  Sparsity: 72.6750%\n",
      "layer   3  Sparsity: 69.4050%\n",
      "total_backward_count 1165010 real_backward_count 64198   5.511%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.672431/  1.120101, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3779%\n",
      "layer   2  Sparsity: 72.5014%\n",
      "layer   3  Sparsity: 69.3305%\n",
      "total_backward_count 1174800 real_backward_count 64345   5.477%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.666141/  1.095056, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3366%\n",
      "layer   2  Sparsity: 72.8774%\n",
      "layer   3  Sparsity: 69.0821%\n",
      "total_backward_count 1184590 real_backward_count 64487   5.444%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.660367/  1.096034, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3407%\n",
      "layer   2  Sparsity: 73.0942%\n",
      "layer   3  Sparsity: 68.9349%\n",
      "total_backward_count 1194380 real_backward_count 64582   5.407%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.653077/  1.100384, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3551%\n",
      "layer   2  Sparsity: 73.2437%\n",
      "layer   3  Sparsity: 69.2983%\n",
      "total_backward_count 1204170 real_backward_count 64715   5.374%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.649094/  1.098800, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3231%\n",
      "layer   2  Sparsity: 73.1086%\n",
      "layer   3  Sparsity: 69.7842%\n",
      "total_backward_count 1213960 real_backward_count 64818   5.339%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.658078/  1.117455, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3334%\n",
      "layer   2  Sparsity: 72.5590%\n",
      "layer   3  Sparsity: 69.4016%\n",
      "total_backward_count 1223750 real_backward_count 64940   5.307%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.652618/  1.123143, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3626%\n",
      "layer   2  Sparsity: 72.5173%\n",
      "layer   3  Sparsity: 69.4180%\n",
      "total_backward_count 1233540 real_backward_count 65059   5.274%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.651644/  1.130460, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3858%\n",
      "layer   2  Sparsity: 72.5453%\n",
      "layer   3  Sparsity: 69.7454%\n",
      "total_backward_count 1243330 real_backward_count 65154   5.240%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.652111/  1.107942, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3263%\n",
      "layer   2  Sparsity: 72.7582%\n",
      "layer   3  Sparsity: 69.5584%\n",
      "total_backward_count 1253120 real_backward_count 65252   5.207%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.654600/  1.132489, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3586%\n",
      "layer   2  Sparsity: 72.6725%\n",
      "layer   3  Sparsity: 69.1402%\n",
      "total_backward_count 1262910 real_backward_count 65342   5.174%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.642723/  1.108900, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3090%\n",
      "layer   2  Sparsity: 72.7852%\n",
      "layer   3  Sparsity: 68.9874%\n",
      "total_backward_count 1272700 real_backward_count 65455   5.143%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.634246/  1.104351, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3631%\n",
      "layer   2  Sparsity: 72.5216%\n",
      "layer   3  Sparsity: 69.2630%\n",
      "total_backward_count 1282490 real_backward_count 65537   5.110%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.632483/  1.094180, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3278%\n",
      "layer   2  Sparsity: 72.6453%\n",
      "layer   3  Sparsity: 69.6454%\n",
      "total_backward_count 1292280 real_backward_count 65614   5.077%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.627127/  1.107271, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3214%\n",
      "layer   2  Sparsity: 72.4519%\n",
      "layer   3  Sparsity: 69.4236%\n",
      "total_backward_count 1302070 real_backward_count 65699   5.046%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.629995/  1.107619, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3374%\n",
      "layer   2  Sparsity: 72.4285%\n",
      "layer   3  Sparsity: 69.3014%\n",
      "total_backward_count 1311860 real_backward_count 65799   5.016%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.637110/  1.119545, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 72.5219%\n",
      "layer   3  Sparsity: 69.3274%\n",
      "total_backward_count 1321650 real_backward_count 65899   4.986%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.638837/  1.106230, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 72.4054%\n",
      "layer   3  Sparsity: 69.6626%\n",
      "total_backward_count 1331440 real_backward_count 66007   4.958%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.619071/  1.094247, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 86.3424%\n",
      "layer   2  Sparsity: 72.4043%\n",
      "layer   3  Sparsity: 70.0565%\n",
      "total_backward_count 1341230 real_backward_count 66070   4.926%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.620688/  1.088515, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3496%\n",
      "layer   2  Sparsity: 72.6823%\n",
      "layer   3  Sparsity: 69.9190%\n",
      "total_backward_count 1351020 real_backward_count 66163   4.897%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.609297/  1.092390, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3279%\n",
      "layer   2  Sparsity: 72.7402%\n",
      "layer   3  Sparsity: 70.1441%\n",
      "total_backward_count 1360810 real_backward_count 66238   4.868%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.604812/  1.080186, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 72.7950%\n",
      "layer   3  Sparsity: 70.4958%\n",
      "total_backward_count 1370600 real_backward_count 66312   4.838%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.604012/  1.078828, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3190%\n",
      "layer   2  Sparsity: 73.0079%\n",
      "layer   3  Sparsity: 70.1878%\n",
      "total_backward_count 1380390 real_backward_count 66389   4.809%\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.617238/  1.083965, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3559%\n",
      "layer   2  Sparsity: 73.1732%\n",
      "layer   3  Sparsity: 69.9455%\n",
      "total_backward_count 1390180 real_backward_count 66462   4.781%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.606647/  1.078637, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3515%\n",
      "layer   2  Sparsity: 73.1170%\n",
      "layer   3  Sparsity: 70.0924%\n",
      "total_backward_count 1399970 real_backward_count 66524   4.752%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.613125/  1.079312, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2864%\n",
      "layer   2  Sparsity: 72.9217%\n",
      "layer   3  Sparsity: 70.0430%\n",
      "total_backward_count 1409760 real_backward_count 66573   4.722%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.612923/  1.089503, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3405%\n",
      "layer   2  Sparsity: 72.8959%\n",
      "layer   3  Sparsity: 69.9472%\n",
      "total_backward_count 1419550 real_backward_count 66645   4.695%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.610295/  1.088611, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3323%\n",
      "layer   2  Sparsity: 73.1393%\n",
      "layer   3  Sparsity: 69.5762%\n",
      "total_backward_count 1429340 real_backward_count 66718   4.668%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.611167/  1.091690, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3318%\n",
      "layer   2  Sparsity: 72.8267%\n",
      "layer   3  Sparsity: 69.7162%\n",
      "total_backward_count 1439130 real_backward_count 66802   4.642%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.608001/  1.072042, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3289%\n",
      "layer   2  Sparsity: 73.0307%\n",
      "layer   3  Sparsity: 69.4309%\n",
      "total_backward_count 1448920 real_backward_count 66891   4.617%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.607255/  1.120217, val:  73.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3204%\n",
      "layer   2  Sparsity: 72.8745%\n",
      "layer   3  Sparsity: 69.4395%\n",
      "total_backward_count 1458710 real_backward_count 66989   4.592%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.600757/  1.069341, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3307%\n",
      "layer   2  Sparsity: 72.9242%\n",
      "layer   3  Sparsity: 69.5479%\n",
      "total_backward_count 1468500 real_backward_count 67059   4.566%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.590359/  1.061787, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 72.7578%\n",
      "layer   3  Sparsity: 69.6027%\n",
      "total_backward_count 1478290 real_backward_count 67137   4.542%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.588302/  1.087125, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3448%\n",
      "layer   2  Sparsity: 72.6078%\n",
      "layer   3  Sparsity: 69.9983%\n",
      "total_backward_count 1488080 real_backward_count 67211   4.517%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.583432/  1.074208, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3282%\n",
      "layer   2  Sparsity: 72.8474%\n",
      "layer   3  Sparsity: 69.6164%\n",
      "total_backward_count 1497870 real_backward_count 67278   4.492%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.588221/  1.077829, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 72.7256%\n",
      "layer   3  Sparsity: 69.5610%\n",
      "total_backward_count 1507660 real_backward_count 67347   4.467%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.589504/  1.079443, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3642%\n",
      "layer   2  Sparsity: 72.9132%\n",
      "layer   3  Sparsity: 69.5890%\n",
      "total_backward_count 1517450 real_backward_count 67424   4.443%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.597299/  1.077834, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2999%\n",
      "layer   2  Sparsity: 72.6039%\n",
      "layer   3  Sparsity: 69.3933%\n",
      "total_backward_count 1527240 real_backward_count 67496   4.419%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.589958/  1.074999, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3447%\n",
      "layer   2  Sparsity: 72.8199%\n",
      "layer   3  Sparsity: 69.4555%\n",
      "total_backward_count 1537030 real_backward_count 67560   4.395%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.585585/  1.059828, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3001%\n",
      "layer   2  Sparsity: 72.8300%\n",
      "layer   3  Sparsity: 69.4696%\n",
      "total_backward_count 1546820 real_backward_count 67615   4.371%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.589759/  1.073167, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3402%\n",
      "layer   2  Sparsity: 72.8263%\n",
      "layer   3  Sparsity: 69.0278%\n",
      "total_backward_count 1556610 real_backward_count 67675   4.348%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.586292/  1.048447, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3253%\n",
      "layer   2  Sparsity: 72.7107%\n",
      "layer   3  Sparsity: 69.1908%\n",
      "total_backward_count 1566400 real_backward_count 67737   4.324%\n",
      "fc layer 3 self.abs_max_out: 979.0\n",
      "fc layer 3 self.abs_max_out: 1006.0\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.581098/  1.072406, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3443%\n",
      "layer   2  Sparsity: 72.6791%\n",
      "layer   3  Sparsity: 69.2678%\n",
      "total_backward_count 1576190 real_backward_count 67797   4.301%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.586976/  1.045317, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3023%\n",
      "layer   2  Sparsity: 72.7276%\n",
      "layer   3  Sparsity: 68.7742%\n",
      "total_backward_count 1585980 real_backward_count 67865   4.279%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.576403/  1.041060, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3355%\n",
      "layer   2  Sparsity: 72.6840%\n",
      "layer   3  Sparsity: 68.9666%\n",
      "total_backward_count 1595770 real_backward_count 67928   4.257%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.579478/  1.057450, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 72.9162%\n",
      "layer   3  Sparsity: 69.3242%\n",
      "total_backward_count 1605560 real_backward_count 68000   4.235%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.574440/  1.053779, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2903%\n",
      "layer   2  Sparsity: 72.8835%\n",
      "layer   3  Sparsity: 69.6049%\n",
      "total_backward_count 1615350 real_backward_count 68053   4.213%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.572293/  1.064752, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3084%\n",
      "layer   2  Sparsity: 72.7500%\n",
      "layer   3  Sparsity: 69.6634%\n",
      "total_backward_count 1625140 real_backward_count 68111   4.191%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.574553/  1.081007, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3134%\n",
      "layer   2  Sparsity: 72.4242%\n",
      "layer   3  Sparsity: 69.5824%\n",
      "total_backward_count 1634930 real_backward_count 68161   4.169%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.577928/  1.067406, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 72.4053%\n",
      "layer   3  Sparsity: 69.3906%\n",
      "total_backward_count 1644720 real_backward_count 68224   4.148%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.568389/  1.088419, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3690%\n",
      "layer   2  Sparsity: 72.5179%\n",
      "layer   3  Sparsity: 69.6521%\n",
      "total_backward_count 1654510 real_backward_count 68268   4.126%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.573196/  1.081491, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3504%\n",
      "layer   2  Sparsity: 72.4906%\n",
      "layer   3  Sparsity: 69.7201%\n",
      "total_backward_count 1664300 real_backward_count 68322   4.105%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.566740/  1.072684, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3495%\n",
      "layer   2  Sparsity: 72.7672%\n",
      "layer   3  Sparsity: 69.8929%\n",
      "total_backward_count 1674090 real_backward_count 68376   4.084%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.566980/  1.050720, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3696%\n",
      "layer   2  Sparsity: 72.9287%\n",
      "layer   3  Sparsity: 69.6473%\n",
      "total_backward_count 1683880 real_backward_count 68437   4.064%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.572697/  1.043322, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3602%\n",
      "layer   2  Sparsity: 72.7407%\n",
      "layer   3  Sparsity: 69.6190%\n",
      "total_backward_count 1693670 real_backward_count 68487   4.044%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.571089/  1.046322, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3412%\n",
      "layer   2  Sparsity: 72.6966%\n",
      "layer   3  Sparsity: 69.5403%\n",
      "total_backward_count 1703460 real_backward_count 68539   4.024%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.564776/  1.049525, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 72.8804%\n",
      "layer   3  Sparsity: 69.9854%\n",
      "total_backward_count 1713250 real_backward_count 68583   4.003%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.567404/  1.077074, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.2967%\n",
      "layer   2  Sparsity: 72.5733%\n",
      "layer   3  Sparsity: 69.8771%\n",
      "total_backward_count 1723040 real_backward_count 68632   3.983%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.572482/  1.048835, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 72.3578%\n",
      "layer   3  Sparsity: 69.8970%\n",
      "total_backward_count 1732830 real_backward_count 68698   3.964%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.573106/  1.079355, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3123%\n",
      "layer   2  Sparsity: 72.4623%\n",
      "layer   3  Sparsity: 69.4845%\n",
      "total_backward_count 1742620 real_backward_count 68774   3.947%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.578274/  1.048970, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3472%\n",
      "layer   2  Sparsity: 72.6041%\n",
      "layer   3  Sparsity: 69.2786%\n",
      "total_backward_count 1752410 real_backward_count 68823   3.927%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.577769/  1.057977, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3422%\n",
      "layer   2  Sparsity: 72.4938%\n",
      "layer   3  Sparsity: 69.3180%\n",
      "total_backward_count 1762200 real_backward_count 68867   3.908%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.579210/  1.055783, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3571%\n",
      "layer   2  Sparsity: 72.2585%\n",
      "layer   3  Sparsity: 69.3781%\n",
      "total_backward_count 1771990 real_backward_count 68909   3.889%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.576645/  1.060229, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3210%\n",
      "layer   2  Sparsity: 72.2721%\n",
      "layer   3  Sparsity: 69.4285%\n",
      "total_backward_count 1781780 real_backward_count 68967   3.871%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.574436/  1.054121, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3381%\n",
      "layer   2  Sparsity: 72.2620%\n",
      "layer   3  Sparsity: 69.5566%\n",
      "total_backward_count 1791570 real_backward_count 69005   3.852%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.572497/  1.057703, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3218%\n",
      "layer   2  Sparsity: 72.3449%\n",
      "layer   3  Sparsity: 69.6739%\n",
      "total_backward_count 1801360 real_backward_count 69046   3.833%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.569834/  1.056291, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3481%\n",
      "layer   2  Sparsity: 72.4039%\n",
      "layer   3  Sparsity: 69.8975%\n",
      "total_backward_count 1811150 real_backward_count 69082   3.814%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.564982/  1.059541, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 72.4604%\n",
      "layer   3  Sparsity: 69.6005%\n",
      "total_backward_count 1820940 real_backward_count 69121   3.796%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.562666/  1.059992, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3375%\n",
      "layer   2  Sparsity: 72.6249%\n",
      "layer   3  Sparsity: 69.2475%\n",
      "total_backward_count 1830730 real_backward_count 69159   3.778%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.564810/  1.061449, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3426%\n",
      "layer   2  Sparsity: 72.5690%\n",
      "layer   3  Sparsity: 69.4050%\n",
      "total_backward_count 1840520 real_backward_count 69193   3.759%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.562628/  1.041822, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 72.3778%\n",
      "layer   3  Sparsity: 69.8721%\n",
      "total_backward_count 1850310 real_backward_count 69244   3.742%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.563232/  1.051280, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 72.5126%\n",
      "layer   3  Sparsity: 70.0702%\n",
      "total_backward_count 1860100 real_backward_count 69294   3.725%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.558497/  1.057393, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 72.2919%\n",
      "layer   3  Sparsity: 69.9948%\n",
      "total_backward_count 1869890 real_backward_count 69319   3.707%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.556350/  1.092325, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3118%\n",
      "layer   2  Sparsity: 72.3170%\n",
      "layer   3  Sparsity: 69.9820%\n",
      "total_backward_count 1879680 real_backward_count 69355   3.690%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.552683/  1.066208, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 72.6166%\n",
      "layer   3  Sparsity: 70.2457%\n",
      "total_backward_count 1889470 real_backward_count 69406   3.673%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.553290/  1.051269, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3283%\n",
      "layer   2  Sparsity: 72.6539%\n",
      "layer   3  Sparsity: 70.2785%\n",
      "total_backward_count 1899260 real_backward_count 69455   3.657%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.544339/  1.063777, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3299%\n",
      "layer   2  Sparsity: 72.7828%\n",
      "layer   3  Sparsity: 70.2540%\n",
      "total_backward_count 1909050 real_backward_count 69479   3.639%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.544648/  1.041925, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3164%\n",
      "layer   2  Sparsity: 72.7438%\n",
      "layer   3  Sparsity: 69.9114%\n",
      "total_backward_count 1918840 real_backward_count 69516   3.623%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.545953/  1.053449, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3302%\n",
      "layer   2  Sparsity: 72.5719%\n",
      "layer   3  Sparsity: 70.2385%\n",
      "total_backward_count 1928630 real_backward_count 69552   3.606%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.539669/  1.056617, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3580%\n",
      "layer   2  Sparsity: 72.6706%\n",
      "layer   3  Sparsity: 70.3906%\n",
      "total_backward_count 1938420 real_backward_count 69593   3.590%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.542945/  1.021773, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3238%\n",
      "layer   2  Sparsity: 72.5402%\n",
      "layer   3  Sparsity: 69.8947%\n",
      "total_backward_count 1948210 real_backward_count 69633   3.574%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.554690/  1.057409, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3284%\n",
      "layer   2  Sparsity: 72.3668%\n",
      "layer   3  Sparsity: 69.6430%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0874597898a4e749ff3403b2d276a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.55469</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.80417</td></tr><tr><td>val_loss</td><td>1.05741</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-sweep-116</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3svooms' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3svooms</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_195140-z3svooms/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rgs8muo9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_000940-rgs8muo9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rgs8muo9' target=\"_blank\">swept-sweep-121</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rgs8muo9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rgs8muo9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_000949_181', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1385.0\n",
      "lif layer 1 self.abs_max_v: 1385.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "lif layer 2 self.abs_max_v: 1182.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 1 self.abs_max_out: 1704.0\n",
      "lif layer 1 self.abs_max_v: 2217.5\n",
      "lif layer 2 self.abs_max_v: 1351.5\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "fc layer 2 self.abs_max_out: 1233.0\n",
      "lif layer 2 self.abs_max_v: 1856.0\n",
      "fc layer 2 self.abs_max_out: 1543.0\n",
      "lif layer 2 self.abs_max_v: 2388.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 1 self.abs_max_out: 2454.0\n",
      "lif layer 1 self.abs_max_v: 2454.0\n",
      "fc layer 2 self.abs_max_out: 1611.0\n",
      "lif layer 2 self.abs_max_v: 2805.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 1 self.abs_max_out: 2530.0\n",
      "lif layer 1 self.abs_max_v: 2530.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 1 self.abs_max_out: 3431.0\n",
      "lif layer 1 self.abs_max_v: 3431.0\n",
      "fc layer 2 self.abs_max_out: 1777.0\n",
      "lif layer 2 self.abs_max_v: 2963.0\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "fc layer 3 self.abs_max_out: 815.0\n",
      "fc layer 2 self.abs_max_out: 1780.0\n",
      "fc layer 2 self.abs_max_out: 1883.0\n",
      "fc layer 3 self.abs_max_out: 1111.0\n",
      "fc layer 1 self.abs_max_out: 4180.0\n",
      "lif layer 1 self.abs_max_v: 4180.0\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "lif layer 2 self.abs_max_v: 3058.0\n",
      "fc layer 2 self.abs_max_out: 2601.0\n",
      "lif layer 2 self.abs_max_v: 3256.0\n",
      "fc layer 1 self.abs_max_out: 4210.0\n",
      "lif layer 1 self.abs_max_v: 4210.0\n",
      "fc layer 1 self.abs_max_out: 4220.0\n",
      "lif layer 1 self.abs_max_v: 4597.0\n",
      "lif layer 2 self.abs_max_v: 3417.0\n",
      "fc layer 1 self.abs_max_out: 5432.0\n",
      "lif layer 1 self.abs_max_v: 6138.0\n",
      "lif layer 1 self.abs_max_v: 6452.5\n",
      "lif layer 1 self.abs_max_v: 6537.0\n",
      "lif layer 1 self.abs_max_v: 7028.5\n",
      "lif layer 2 self.abs_max_v: 3621.0\n",
      "fc layer 1 self.abs_max_out: 5634.0\n",
      "lif layer 1 self.abs_max_v: 7415.5\n",
      "lif layer 2 self.abs_max_v: 3834.0\n",
      "lif layer 1 self.abs_max_v: 8422.0\n",
      "lif layer 2 self.abs_max_v: 3901.0\n",
      "fc layer 1 self.abs_max_out: 6077.0\n",
      "lif layer 1 self.abs_max_v: 9561.0\n",
      "lif layer 1 self.abs_max_v: 10031.5\n",
      "fc layer 2 self.abs_max_out: 2626.0\n",
      "lif layer 2 self.abs_max_v: 4068.0\n",
      "fc layer 2 self.abs_max_out: 2688.0\n",
      "fc layer 2 self.abs_max_out: 2822.0\n",
      "fc layer 2 self.abs_max_out: 3120.0\n",
      "lif layer 2 self.abs_max_v: 4093.0\n",
      "lif layer 2 self.abs_max_v: 4216.5\n",
      "lif layer 2 self.abs_max_v: 4255.0\n",
      "lif layer 2 self.abs_max_v: 4384.0\n",
      "lif layer 2 self.abs_max_v: 4417.5\n",
      "fc layer 2 self.abs_max_out: 3526.0\n",
      "lif layer 1 self.abs_max_v: 10676.0\n",
      "lif layer 1 self.abs_max_v: 10829.5\n",
      "lif layer 2 self.abs_max_v: 4497.5\n",
      "lif layer 2 self.abs_max_v: 4500.0\n",
      "lif layer 2 self.abs_max_v: 4501.5\n",
      "lif layer 2 self.abs_max_v: 4529.0\n",
      "lif layer 2 self.abs_max_v: 4812.5\n",
      "fc layer 2 self.abs_max_out: 3568.0\n",
      "fc layer 2 self.abs_max_out: 3595.0\n",
      "fc layer 1 self.abs_max_out: 6247.0\n",
      "fc layer 3 self.abs_max_out: 1115.0\n",
      "fc layer 3 self.abs_max_out: 1266.0\n",
      "fc layer 1 self.abs_max_out: 6419.0\n",
      "lif layer 1 self.abs_max_v: 10985.0\n",
      "lif layer 1 self.abs_max_v: 11014.5\n",
      "fc layer 2 self.abs_max_out: 3873.0\n",
      "lif layer 1 self.abs_max_v: 11342.5\n",
      "lif layer 1 self.abs_max_v: 11769.5\n",
      "fc layer 1 self.abs_max_out: 6582.0\n",
      "fc layer 1 self.abs_max_out: 6699.0\n",
      "fc layer 1 self.abs_max_out: 7713.0\n",
      "lif layer 2 self.abs_max_v: 4884.5\n",
      "lif layer 2 self.abs_max_v: 5167.5\n",
      "fc layer 2 self.abs_max_out: 3959.0\n",
      "fc layer 2 self.abs_max_out: 3983.0\n",
      "fc layer 2 self.abs_max_out: 4220.0\n",
      "fc layer 3 self.abs_max_out: 1284.0\n",
      "fc layer 3 self.abs_max_out: 1359.0\n",
      "fc layer 3 self.abs_max_out: 1374.0\n",
      "fc layer 3 self.abs_max_out: 1526.0\n",
      "lif layer 2 self.abs_max_v: 5208.5\n",
      "lif layer 2 self.abs_max_v: 5342.0\n",
      "lif layer 2 self.abs_max_v: 5390.0\n",
      "lif layer 2 self.abs_max_v: 5474.0\n",
      "lif layer 2 self.abs_max_v: 5587.5\n",
      "lif layer 2 self.abs_max_v: 5789.5\n",
      "lif layer 2 self.abs_max_v: 5830.0\n",
      "lif layer 1 self.abs_max_v: 12132.5\n",
      "lif layer 2 self.abs_max_v: 5860.5\n",
      "lif layer 2 self.abs_max_v: 5916.5\n",
      "lif layer 2 self.abs_max_v: 5937.5\n",
      "lif layer 2 self.abs_max_v: 6075.0\n",
      "lif layer 1 self.abs_max_v: 12198.0\n",
      "lif layer 1 self.abs_max_v: 13069.5\n",
      "lif layer 1 self.abs_max_v: 13505.5\n",
      "lif layer 1 self.abs_max_v: 13929.0\n",
      "fc layer 1 self.abs_max_out: 8342.0\n",
      "fc layer 1 self.abs_max_out: 8646.0\n",
      "lif layer 2 self.abs_max_v: 6683.5\n",
      "lif layer 2 self.abs_max_v: 6908.0\n",
      "lif layer 2 self.abs_max_v: 7132.5\n",
      "fc layer 1 self.abs_max_out: 9098.0\n",
      "fc layer 3 self.abs_max_out: 1589.0\n",
      "lif layer 1 self.abs_max_v: 13991.5\n",
      "lif layer 1 self.abs_max_v: 14212.0\n",
      "fc layer 1 self.abs_max_out: 9216.0\n",
      "lif layer 1 self.abs_max_v: 14893.5\n",
      "lif layer 1 self.abs_max_v: 16217.0\n",
      "fc layer 1 self.abs_max_out: 9220.0\n",
      "fc layer 1 self.abs_max_out: 9290.0\n",
      "fc layer 1 self.abs_max_out: 9615.0\n",
      "fc layer 1 self.abs_max_out: 10177.0\n",
      "fc layer 1 self.abs_max_out: 10338.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.846540/  2.008063, val:  27.92%, val_best:  27.92%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0431%\n",
      "layer   2  Sparsity: 70.6364%\n",
      "layer   3  Sparsity: 74.1566%\n",
      "total_backward_count 9790 real_backward_count 1917  19.581%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 17313.5\n",
      "fc layer 3 self.abs_max_out: 1595.0\n",
      "lif layer 1 self.abs_max_v: 17486.0\n",
      "lif layer 1 self.abs_max_v: 17732.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.770792/  2.020288, val:  30.83%, val_best:  30.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9920%\n",
      "layer   2  Sparsity: 73.0982%\n",
      "layer   3  Sparsity: 74.9347%\n",
      "total_backward_count 19580 real_backward_count 3504  17.896%\n",
      "lif layer 1 self.abs_max_v: 18058.0\n",
      "fc layer 1 self.abs_max_out: 10361.0\n",
      "lif layer 1 self.abs_max_v: 18347.5\n",
      "lif layer 1 self.abs_max_v: 19111.0\n",
      "fc layer 1 self.abs_max_out: 11062.0\n",
      "lif layer 1 self.abs_max_v: 20617.5\n",
      "fc layer 1 self.abs_max_out: 11605.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.787269/  1.979232, val:  47.08%, val_best:  47.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9805%\n",
      "layer   2  Sparsity: 75.2495%\n",
      "layer   3  Sparsity: 76.1825%\n",
      "total_backward_count 29370 real_backward_count 5055  17.211%\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.784382/  1.994619, val:  48.33%, val_best:  48.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0118%\n",
      "layer   2  Sparsity: 76.1944%\n",
      "layer   3  Sparsity: 76.8571%\n",
      "total_backward_count 39160 real_backward_count 6518  16.645%\n",
      "fc layer 3 self.abs_max_out: 1624.0\n",
      "lif layer 1 self.abs_max_v: 20682.5\n",
      "fc layer 1 self.abs_max_out: 11710.0\n",
      "lif layer 1 self.abs_max_v: 21095.5\n",
      "lif layer 1 self.abs_max_v: 21289.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.780501/  1.964342, val:  42.50%, val_best:  48.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9938%\n",
      "layer   2  Sparsity: 76.3938%\n",
      "layer   3  Sparsity: 77.3259%\n",
      "total_backward_count 48950 real_backward_count 7967  16.276%\n",
      "fc layer 1 self.abs_max_out: 12204.0\n",
      "lif layer 1 self.abs_max_v: 22512.0\n",
      "fc layer 1 self.abs_max_out: 12295.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.757720/  2.000430, val:  47.92%, val_best:  48.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9826%\n",
      "layer   2  Sparsity: 76.4746%\n",
      "layer   3  Sparsity: 77.5462%\n",
      "total_backward_count 58740 real_backward_count 9421  16.038%\n",
      "fc layer 3 self.abs_max_out: 1682.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.802164/  2.024240, val:  38.33%, val_best:  48.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9659%\n",
      "layer   2  Sparsity: 75.8832%\n",
      "layer   3  Sparsity: 79.1253%\n",
      "total_backward_count 68530 real_backward_count 10866  15.856%\n",
      "fc layer 1 self.abs_max_out: 12430.0\n",
      "fc layer 1 self.abs_max_out: 12888.0\n",
      "lif layer 1 self.abs_max_v: 22837.0\n",
      "lif layer 1 self.abs_max_v: 23346.5\n",
      "lif layer 1 self.abs_max_v: 24005.0\n",
      "lif layer 1 self.abs_max_v: 24768.5\n",
      "fc layer 1 self.abs_max_out: 13834.0\n",
      "lif layer 1 self.abs_max_v: 25394.0\n",
      "lif layer 1 self.abs_max_v: 25450.0\n",
      "fc layer 3 self.abs_max_out: 1698.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.802094/  1.980893, val:  52.92%, val_best:  52.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9604%\n",
      "layer   2  Sparsity: 75.5852%\n",
      "layer   3  Sparsity: 79.7927%\n",
      "total_backward_count 78320 real_backward_count 12242  15.631%\n",
      "fc layer 1 self.abs_max_out: 13850.0\n",
      "lif layer 1 self.abs_max_v: 25476.5\n",
      "fc layer 2 self.abs_max_out: 4325.0\n",
      "fc layer 2 self.abs_max_out: 4463.0\n",
      "fc layer 2 self.abs_max_out: 4608.0\n",
      "lif layer 2 self.abs_max_v: 7164.0\n",
      "fc layer 2 self.abs_max_out: 4632.0\n",
      "lif layer 2 self.abs_max_v: 7216.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.794235/  1.972824, val:  47.92%, val_best:  52.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9974%\n",
      "layer   2  Sparsity: 74.1702%\n",
      "layer   3  Sparsity: 79.0047%\n",
      "total_backward_count 88110 real_backward_count 13674  15.519%\n",
      "lif layer 2 self.abs_max_v: 7405.5\n",
      "lif layer 2 self.abs_max_v: 7467.0\n",
      "lif layer 2 self.abs_max_v: 7493.0\n",
      "lif layer 2 self.abs_max_v: 7664.5\n",
      "lif layer 2 self.abs_max_v: 7829.5\n",
      "lif layer 2 self.abs_max_v: 7902.5\n",
      "lif layer 2 self.abs_max_v: 7914.5\n",
      "lif layer 2 self.abs_max_v: 7918.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.825592/  2.048008, val:  41.25%, val_best:  52.92%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9717%\n",
      "layer   2  Sparsity: 73.8841%\n",
      "layer   3  Sparsity: 80.0281%\n",
      "total_backward_count 97900 real_backward_count 15086  15.410%\n",
      "lif layer 2 self.abs_max_v: 7931.5\n",
      "fc layer 2 self.abs_max_out: 4736.0\n",
      "lif layer 2 self.abs_max_v: 8294.5\n",
      "lif layer 2 self.abs_max_v: 8457.5\n",
      "fc layer 2 self.abs_max_out: 4901.0\n",
      "fc layer 1 self.abs_max_out: 14116.0\n",
      "lif layer 2 self.abs_max_v: 8588.0\n",
      "lif layer 2 self.abs_max_v: 8905.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.828834/  2.007162, val:  43.75%, val_best:  52.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0082%\n",
      "layer   2  Sparsity: 73.1503%\n",
      "layer   3  Sparsity: 80.7753%\n",
      "total_backward_count 107690 real_backward_count 16558  15.376%\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.850636/  1.978540, val:  36.67%, val_best:  52.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0161%\n",
      "layer   2  Sparsity: 73.6443%\n",
      "layer   3  Sparsity: 81.1709%\n",
      "total_backward_count 117480 real_backward_count 17970  15.296%\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.854288/  2.073255, val:  34.17%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9957%\n",
      "layer   2  Sparsity: 74.2584%\n",
      "layer   3  Sparsity: 83.0192%\n",
      "total_backward_count 127270 real_backward_count 19408  15.249%\n",
      "lif layer 2 self.abs_max_v: 8908.0\n",
      "lif layer 2 self.abs_max_v: 8958.0\n",
      "lif layer 2 self.abs_max_v: 9085.5\n",
      "lif layer 1 self.abs_max_v: 25537.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.864533/  2.047306, val:  35.00%, val_best:  52.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9945%\n",
      "layer   2  Sparsity: 74.4927%\n",
      "layer   3  Sparsity: 82.2690%\n",
      "total_backward_count 137060 real_backward_count 20823  15.193%\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.828599/  2.037667, val:  38.75%, val_best:  52.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0263%\n",
      "layer   2  Sparsity: 74.5203%\n",
      "layer   3  Sparsity: 80.3241%\n",
      "total_backward_count 146850 real_backward_count 22185  15.107%\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.825531/  2.042005, val:  37.50%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 81.9765%\n",
      "layer   2  Sparsity: 75.5869%\n",
      "layer   3  Sparsity: 79.9894%\n",
      "total_backward_count 156640 real_backward_count 23545  15.031%\n",
      "fc layer 1 self.abs_max_out: 14229.0\n",
      "fc layer 2 self.abs_max_out: 4990.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.854017/  1.986323, val:  52.08%, val_best:  52.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9240%\n",
      "layer   2  Sparsity: 75.8261%\n",
      "layer   3  Sparsity: 81.7825%\n",
      "total_backward_count 166430 real_backward_count 24911  14.968%\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.825754/  1.999544, val:  45.42%, val_best:  52.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9600%\n",
      "layer   2  Sparsity: 75.3484%\n",
      "layer   3  Sparsity: 79.5493%\n",
      "total_backward_count 176220 real_backward_count 26239  14.890%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.837101/  1.975722, val:  51.67%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9833%\n",
      "layer   2  Sparsity: 74.6512%\n",
      "layer   3  Sparsity: 80.4846%\n",
      "total_backward_count 186010 real_backward_count 27704  14.894%\n",
      "fc layer 2 self.abs_max_out: 5118.0\n",
      "lif layer 1 self.abs_max_v: 25723.5\n",
      "lif layer 1 self.abs_max_v: 26455.0\n",
      "lif layer 1 self.abs_max_v: 26900.5\n",
      "fc layer 1 self.abs_max_out: 16575.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.841782/  2.037833, val:  38.33%, val_best:  52.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 81.9980%\n",
      "layer   2  Sparsity: 74.9422%\n",
      "layer   3  Sparsity: 80.8290%\n",
      "total_backward_count 195800 real_backward_count 29033  14.828%\n",
      "lif layer 1 self.abs_max_v: 27153.5\n",
      "fc layer 1 self.abs_max_out: 16956.0\n",
      "fc layer 2 self.abs_max_out: 5228.0\n",
      "fc layer 2 self.abs_max_out: 5245.0\n",
      "lif layer 2 self.abs_max_v: 9660.0\n",
      "fc layer 1 self.abs_max_out: 18302.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.841512/  2.015290, val:  46.25%, val_best:  52.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9542%\n",
      "layer   2  Sparsity: 75.6569%\n",
      "layer   3  Sparsity: 81.0098%\n",
      "total_backward_count 205590 real_backward_count 30426  14.799%\n",
      "fc layer 1 self.abs_max_out: 18638.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.853214/  2.019102, val:  36.25%, val_best:  52.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0265%\n",
      "layer   2  Sparsity: 75.5681%\n",
      "layer   3  Sparsity: 81.0002%\n",
      "total_backward_count 215380 real_backward_count 31831  14.779%\n",
      "fc layer 1 self.abs_max_out: 18901.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.833062/  1.994966, val:  48.75%, val_best:  52.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9802%\n",
      "layer   2  Sparsity: 75.1903%\n",
      "layer   3  Sparsity: 80.0142%\n",
      "total_backward_count 225170 real_backward_count 33219  14.753%\n",
      "fc layer 1 self.abs_max_out: 18956.0\n",
      "fc layer 1 self.abs_max_out: 18994.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.856278/  1.994138, val:  56.67%, val_best:  56.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9660%\n",
      "layer   2  Sparsity: 74.9831%\n",
      "layer   3  Sparsity: 81.5667%\n",
      "total_backward_count 234960 real_backward_count 34572  14.714%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.853672/  1.979132, val:  53.75%, val_best:  56.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0323%\n",
      "layer   2  Sparsity: 75.2151%\n",
      "layer   3  Sparsity: 81.2991%\n",
      "total_backward_count 244750 real_backward_count 35925  14.678%\n",
      "fc layer 1 self.abs_max_out: 19277.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.844320/  2.003841, val:  57.92%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0132%\n",
      "layer   2  Sparsity: 74.8865%\n",
      "layer   3  Sparsity: 81.7136%\n",
      "total_backward_count 254540 real_backward_count 37395  14.691%\n",
      "lif layer 1 self.abs_max_v: 27562.5\n",
      "lif layer 1 self.abs_max_v: 28475.5\n",
      "lif layer 1 self.abs_max_v: 29294.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.880766/  1.996673, val:  48.33%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0198%\n",
      "layer   2  Sparsity: 75.4934%\n",
      "layer   3  Sparsity: 82.5117%\n",
      "total_backward_count 264330 real_backward_count 38835  14.692%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.822294/  1.980062, val:  50.42%, val_best:  57.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0298%\n",
      "layer   2  Sparsity: 74.9504%\n",
      "layer   3  Sparsity: 80.5760%\n",
      "total_backward_count 274120 real_backward_count 40199  14.665%\n",
      "fc layer 1 self.abs_max_out: 19372.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.799590/  2.021720, val:  46.67%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9687%\n",
      "layer   2  Sparsity: 74.0956%\n",
      "layer   3  Sparsity: 80.0343%\n",
      "total_backward_count 283910 real_backward_count 41541  14.632%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.808512/  1.976343, val:  48.33%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0079%\n",
      "layer   2  Sparsity: 74.9042%\n",
      "layer   3  Sparsity: 80.0846%\n",
      "total_backward_count 293700 real_backward_count 42863  14.594%\n",
      "fc layer 1 self.abs_max_out: 19542.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.813162/  1.978451, val:  53.75%, val_best:  57.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9906%\n",
      "layer   2  Sparsity: 75.3043%\n",
      "layer   3  Sparsity: 80.2174%\n",
      "total_backward_count 303490 real_backward_count 44214  14.569%\n",
      "fc layer 1 self.abs_max_out: 19628.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.785751/  1.977891, val:  48.75%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0374%\n",
      "layer   2  Sparsity: 76.7424%\n",
      "layer   3  Sparsity: 78.7088%\n",
      "total_backward_count 313280 real_backward_count 45572  14.547%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.811574/  2.007167, val:  52.08%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9423%\n",
      "layer   2  Sparsity: 76.1406%\n",
      "layer   3  Sparsity: 80.3259%\n",
      "total_backward_count 323070 real_backward_count 46984  14.543%\n",
      "lif layer 1 self.abs_max_v: 30798.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.828064/  2.000532, val:  45.00%, val_best:  57.92%, tr:  99.08%, tr_best:  99.90%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0069%\n",
      "layer   2  Sparsity: 76.4507%\n",
      "layer   3  Sparsity: 80.7317%\n",
      "total_backward_count 332860 real_backward_count 48394  14.539%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.818842/  1.987870, val:  46.67%, val_best:  57.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9601%\n",
      "layer   2  Sparsity: 77.1308%\n",
      "layer   3  Sparsity: 80.4630%\n",
      "total_backward_count 342650 real_backward_count 49764  14.523%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.811179/  2.008808, val:  46.25%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0308%\n",
      "layer   2  Sparsity: 76.0604%\n",
      "layer   3  Sparsity: 79.4624%\n",
      "total_backward_count 352440 real_backward_count 51047  14.484%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.777191/  1.921886, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0219%\n",
      "layer   2  Sparsity: 75.5778%\n",
      "layer   3  Sparsity: 78.5097%\n",
      "total_backward_count 362230 real_backward_count 52340  14.449%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.761632/  1.964182, val:  53.33%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9920%\n",
      "layer   2  Sparsity: 75.6435%\n",
      "layer   3  Sparsity: 79.2643%\n",
      "total_backward_count 372020 real_backward_count 53619  14.413%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.785297/  1.977637, val:  53.33%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9475%\n",
      "layer   2  Sparsity: 76.5967%\n",
      "layer   3  Sparsity: 79.9664%\n",
      "total_backward_count 381810 real_backward_count 55008  14.407%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.764191/  1.954868, val:  45.00%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9835%\n",
      "layer   2  Sparsity: 75.4235%\n",
      "layer   3  Sparsity: 79.5928%\n",
      "total_backward_count 391600 real_backward_count 56329  14.384%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.748832/  1.923748, val:  53.75%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 82.0160%\n",
      "layer   2  Sparsity: 75.8134%\n",
      "layer   3  Sparsity: 77.8980%\n",
      "total_backward_count 401390 real_backward_count 57610  14.353%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.756155/  1.957310, val:  55.00%, val_best:  61.67%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9975%\n",
      "layer   2  Sparsity: 75.4057%\n",
      "layer   3  Sparsity: 78.7751%\n",
      "total_backward_count 411180 real_backward_count 58929  14.332%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.762896/  1.955527, val:  55.00%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9822%\n",
      "layer   2  Sparsity: 76.1720%\n",
      "layer   3  Sparsity: 79.8308%\n",
      "total_backward_count 420970 real_backward_count 60258  14.314%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.735180/  1.930856, val:  51.25%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9922%\n",
      "layer   2  Sparsity: 77.3329%\n",
      "layer   3  Sparsity: 78.1372%\n",
      "total_backward_count 430760 real_backward_count 61561  14.291%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.723116/  1.949869, val:  51.67%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9817%\n",
      "layer   2  Sparsity: 76.5890%\n",
      "layer   3  Sparsity: 77.9465%\n",
      "total_backward_count 440550 real_backward_count 62883  14.274%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.732172/  1.909690, val:  57.50%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9689%\n",
      "layer   2  Sparsity: 77.1083%\n",
      "layer   3  Sparsity: 78.2277%\n",
      "total_backward_count 450340 real_backward_count 64185  14.253%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.742774/  1.943624, val:  42.08%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9912%\n",
      "layer   2  Sparsity: 76.7112%\n",
      "layer   3  Sparsity: 78.5305%\n",
      "total_backward_count 460130 real_backward_count 65573  14.251%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.732974/  1.972868, val:  39.58%, val_best:  61.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0038%\n",
      "layer   2  Sparsity: 76.2818%\n",
      "layer   3  Sparsity: 78.2952%\n",
      "total_backward_count 469920 real_backward_count 66862  14.228%\n",
      "lif layer 2 self.abs_max_v: 9694.0\n",
      "lif layer 2 self.abs_max_v: 9700.5\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.775050/  1.969128, val:  48.33%, val_best:  61.67%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0123%\n",
      "layer   2  Sparsity: 76.2463%\n",
      "layer   3  Sparsity: 78.7459%\n",
      "total_backward_count 479710 real_backward_count 68192  14.215%\n",
      "lif layer 1 self.abs_max_v: 31384.5\n",
      "lif layer 1 self.abs_max_v: 32180.5\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.760706/  1.935287, val:  58.75%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9657%\n",
      "layer   2  Sparsity: 75.2216%\n",
      "layer   3  Sparsity: 78.2715%\n",
      "total_backward_count 489500 real_backward_count 69488  14.196%\n",
      "lif layer 2 self.abs_max_v: 9743.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.787425/  1.988254, val:  47.50%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9809%\n",
      "layer   2  Sparsity: 74.3030%\n",
      "layer   3  Sparsity: 80.0971%\n",
      "total_backward_count 499290 real_backward_count 70834  14.187%\n",
      "fc layer 2 self.abs_max_out: 5273.0\n",
      "fc layer 2 self.abs_max_out: 5285.0\n",
      "fc layer 2 self.abs_max_out: 5287.0\n",
      "lif layer 2 self.abs_max_v: 10058.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.784760/  1.964396, val:  50.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9683%\n",
      "layer   2  Sparsity: 73.3495%\n",
      "layer   3  Sparsity: 80.2810%\n",
      "total_backward_count 509080 real_backward_count 72160  14.175%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.762643/  1.951955, val:  61.25%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9936%\n",
      "layer   2  Sparsity: 73.2788%\n",
      "layer   3  Sparsity: 79.1927%\n",
      "total_backward_count 518870 real_backward_count 73539  14.173%\n",
      "lif layer 1 self.abs_max_v: 33243.5\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.751788/  1.931779, val:  52.08%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9699%\n",
      "layer   2  Sparsity: 74.1797%\n",
      "layer   3  Sparsity: 78.4887%\n",
      "total_backward_count 528660 real_backward_count 74902  14.168%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.747679/  1.931294, val:  57.08%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9966%\n",
      "layer   2  Sparsity: 74.6680%\n",
      "layer   3  Sparsity: 78.3057%\n",
      "total_backward_count 538450 real_backward_count 76182  14.148%\n",
      "fc layer 2 self.abs_max_out: 5329.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.706562/  1.906226, val:  51.25%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9703%\n",
      "layer   2  Sparsity: 75.1573%\n",
      "layer   3  Sparsity: 77.3658%\n",
      "total_backward_count 548240 real_backward_count 77470  14.131%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.675840/  1.920137, val:  45.42%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9792%\n",
      "layer   2  Sparsity: 76.1778%\n",
      "layer   3  Sparsity: 76.1223%\n",
      "total_backward_count 558030 real_backward_count 78738  14.110%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.676710/  1.895291, val:  57.08%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0001%\n",
      "layer   2  Sparsity: 75.5724%\n",
      "layer   3  Sparsity: 75.7337%\n",
      "total_backward_count 567820 real_backward_count 79935  14.078%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.698642/  1.919282, val:  49.17%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9723%\n",
      "layer   2  Sparsity: 73.4196%\n",
      "layer   3  Sparsity: 77.4941%\n",
      "total_backward_count 577610 real_backward_count 81176  14.054%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.705980/  1.887239, val:  52.50%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0081%\n",
      "layer   2  Sparsity: 73.0981%\n",
      "layer   3  Sparsity: 77.3163%\n",
      "total_backward_count 587400 real_backward_count 82465  14.039%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.688362/  1.886048, val:  56.67%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0095%\n",
      "layer   2  Sparsity: 72.8599%\n",
      "layer   3  Sparsity: 76.0742%\n",
      "total_backward_count 597190 real_backward_count 83738  14.022%\n",
      "lif layer 1 self.abs_max_v: 33341.5\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.687639/  1.873358, val:  48.75%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0001%\n",
      "layer   2  Sparsity: 73.7282%\n",
      "layer   3  Sparsity: 76.3499%\n",
      "total_backward_count 606980 real_backward_count 85056  14.013%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.666790/  1.891539, val:  49.17%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9356%\n",
      "layer   2  Sparsity: 73.8021%\n",
      "layer   3  Sparsity: 75.7648%\n",
      "total_backward_count 616770 real_backward_count 86356  14.001%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.684855/  1.876739, val:  50.83%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9830%\n",
      "layer   2  Sparsity: 74.0955%\n",
      "layer   3  Sparsity: 77.0739%\n",
      "total_backward_count 626560 real_backward_count 87615  13.983%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.656145/  1.912113, val:  47.92%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0197%\n",
      "layer   2  Sparsity: 73.5592%\n",
      "layer   3  Sparsity: 75.4567%\n",
      "total_backward_count 636350 real_backward_count 88897  13.970%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.614911/  1.883393, val:  46.67%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9625%\n",
      "layer   2  Sparsity: 73.1630%\n",
      "layer   3  Sparsity: 75.1628%\n",
      "total_backward_count 646140 real_backward_count 90113  13.946%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.639620/  1.898153, val:  48.33%, val_best:  61.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9849%\n",
      "layer   2  Sparsity: 73.5215%\n",
      "layer   3  Sparsity: 76.1802%\n",
      "total_backward_count 655930 real_backward_count 91379  13.931%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.640554/  1.854424, val:  54.17%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9436%\n",
      "layer   2  Sparsity: 73.2885%\n",
      "layer   3  Sparsity: 75.5961%\n",
      "total_backward_count 665720 real_backward_count 92636  13.915%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.646399/  1.856785, val:  60.42%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0070%\n",
      "layer   2  Sparsity: 72.8260%\n",
      "layer   3  Sparsity: 74.9371%\n",
      "total_backward_count 675510 real_backward_count 93854  13.894%\n",
      "fc layer 3 self.abs_max_out: 1718.0\n",
      "fc layer 3 self.abs_max_out: 1769.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.590331/  1.872479, val:  42.92%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9707%\n",
      "layer   2  Sparsity: 72.5914%\n",
      "layer   3  Sparsity: 74.1374%\n",
      "total_backward_count 685300 real_backward_count 95090  13.876%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.592644/  1.850009, val:  57.50%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9792%\n",
      "layer   2  Sparsity: 71.5604%\n",
      "layer   3  Sparsity: 72.6765%\n",
      "total_backward_count 695090 real_backward_count 96332  13.859%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.588433/  1.830099, val:  51.67%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0042%\n",
      "layer   2  Sparsity: 72.6259%\n",
      "layer   3  Sparsity: 73.9837%\n",
      "total_backward_count 704880 real_backward_count 97593  13.845%\n",
      "fc layer 3 self.abs_max_out: 1774.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.589828/  1.797664, val:  59.58%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0078%\n",
      "layer   2  Sparsity: 71.5218%\n",
      "layer   3  Sparsity: 72.3573%\n",
      "total_backward_count 714670 real_backward_count 98780  13.822%\n",
      "fc layer 3 self.abs_max_out: 1880.0\n",
      "fc layer 3 self.abs_max_out: 1999.0\n",
      "fc layer 3 self.abs_max_out: 2053.0\n",
      "fc layer 3 self.abs_max_out: 2064.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.582234/  1.820992, val:  55.83%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9512%\n",
      "layer   2  Sparsity: 70.6335%\n",
      "layer   3  Sparsity: 73.3124%\n",
      "total_backward_count 724460 real_backward_count 99960  13.798%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.603374/  1.848830, val:  56.25%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9530%\n",
      "layer   2  Sparsity: 71.4516%\n",
      "layer   3  Sparsity: 75.5431%\n",
      "total_backward_count 734250 real_backward_count 101233  13.787%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.620096/  1.821207, val:  54.17%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9268%\n",
      "layer   2  Sparsity: 71.9278%\n",
      "layer   3  Sparsity: 75.5220%\n",
      "total_backward_count 744040 real_backward_count 102431  13.767%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.622986/  1.846905, val:  56.67%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0122%\n",
      "layer   2  Sparsity: 72.5906%\n",
      "layer   3  Sparsity: 75.8188%\n",
      "total_backward_count 753830 real_backward_count 103669  13.752%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.634852/  1.896721, val:  44.17%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9646%\n",
      "layer   2  Sparsity: 73.0743%\n",
      "layer   3  Sparsity: 75.1840%\n",
      "total_backward_count 763620 real_backward_count 104925  13.740%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.632929/  1.875901, val:  43.33%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9930%\n",
      "layer   2  Sparsity: 72.6418%\n",
      "layer   3  Sparsity: 75.4278%\n",
      "total_backward_count 773410 real_backward_count 106155  13.726%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.609859/  1.846150, val:  55.00%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9872%\n",
      "layer   2  Sparsity: 72.5312%\n",
      "layer   3  Sparsity: 75.2089%\n",
      "total_backward_count 783200 real_backward_count 107383  13.711%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.610137/  1.817062, val:  52.92%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9938%\n",
      "layer   2  Sparsity: 72.9630%\n",
      "layer   3  Sparsity: 75.9644%\n",
      "total_backward_count 792990 real_backward_count 108566  13.691%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.621742/  1.857434, val:  44.17%, val_best:  61.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9898%\n",
      "layer   2  Sparsity: 73.9848%\n",
      "layer   3  Sparsity: 75.9721%\n",
      "total_backward_count 802780 real_backward_count 109783  13.675%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.590381/  1.852631, val:  50.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0026%\n",
      "layer   2  Sparsity: 74.5381%\n",
      "layer   3  Sparsity: 74.9418%\n",
      "total_backward_count 812570 real_backward_count 111068  13.669%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.622822/  1.884604, val:  49.17%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9873%\n",
      "layer   2  Sparsity: 73.9406%\n",
      "layer   3  Sparsity: 75.3631%\n",
      "total_backward_count 822360 real_backward_count 112344  13.661%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.630232/  1.882061, val:  46.67%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0043%\n",
      "layer   2  Sparsity: 74.0238%\n",
      "layer   3  Sparsity: 75.5101%\n",
      "total_backward_count 832150 real_backward_count 113591  13.650%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.625917/  1.884951, val:  40.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0153%\n",
      "layer   2  Sparsity: 73.4610%\n",
      "layer   3  Sparsity: 75.0012%\n",
      "total_backward_count 841940 real_backward_count 114863  13.643%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.620853/  1.827458, val:  52.08%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9514%\n",
      "layer   2  Sparsity: 73.3222%\n",
      "layer   3  Sparsity: 74.7608%\n",
      "total_backward_count 851730 real_backward_count 116118  13.633%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.623062/  1.899507, val:  47.50%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0081%\n",
      "layer   2  Sparsity: 73.8395%\n",
      "layer   3  Sparsity: 75.3676%\n",
      "total_backward_count 861520 real_backward_count 117449  13.633%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.651567/  1.877616, val:  57.50%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9940%\n",
      "layer   2  Sparsity: 74.2138%\n",
      "layer   3  Sparsity: 76.2504%\n",
      "total_backward_count 871310 real_backward_count 118737  13.627%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.657443/  1.874409, val:  53.33%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9958%\n",
      "layer   2  Sparsity: 73.5511%\n",
      "layer   3  Sparsity: 75.1563%\n",
      "total_backward_count 881100 real_backward_count 120101  13.631%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.638742/  1.918310, val:  47.92%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9983%\n",
      "layer   2  Sparsity: 73.2701%\n",
      "layer   3  Sparsity: 76.4707%\n",
      "total_backward_count 890890 real_backward_count 121454  13.633%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.648157/  1.820237, val:  62.50%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9329%\n",
      "layer   2  Sparsity: 74.1637%\n",
      "layer   3  Sparsity: 75.4853%\n",
      "total_backward_count 900680 real_backward_count 122736  13.627%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.601956/  1.848369, val:  52.50%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9894%\n",
      "layer   2  Sparsity: 74.7338%\n",
      "layer   3  Sparsity: 74.0323%\n",
      "total_backward_count 910470 real_backward_count 123994  13.619%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.593082/  1.877938, val:  43.75%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9972%\n",
      "layer   2  Sparsity: 75.7429%\n",
      "layer   3  Sparsity: 73.7228%\n",
      "total_backward_count 920260 real_backward_count 125268  13.612%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.649918/  1.881137, val:  51.67%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9585%\n",
      "layer   2  Sparsity: 75.2191%\n",
      "layer   3  Sparsity: 74.5015%\n",
      "total_backward_count 930050 real_backward_count 126521  13.604%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.589186/  1.816173, val:  60.42%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9296%\n",
      "layer   2  Sparsity: 74.8680%\n",
      "layer   3  Sparsity: 71.7972%\n",
      "total_backward_count 939840 real_backward_count 127721  13.590%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.589815/  1.835754, val:  52.92%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9533%\n",
      "layer   2  Sparsity: 75.3554%\n",
      "layer   3  Sparsity: 73.6397%\n",
      "total_backward_count 949630 real_backward_count 128922  13.576%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.594512/  1.842341, val:  53.33%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9777%\n",
      "layer   2  Sparsity: 74.7248%\n",
      "layer   3  Sparsity: 74.0427%\n",
      "total_backward_count 959420 real_backward_count 130153  13.566%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.583226/  1.874804, val:  44.17%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9350%\n",
      "layer   2  Sparsity: 74.9681%\n",
      "layer   3  Sparsity: 73.7309%\n",
      "total_backward_count 969210 real_backward_count 131379  13.555%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.571707/  1.808136, val:  61.25%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9759%\n",
      "layer   2  Sparsity: 75.8441%\n",
      "layer   3  Sparsity: 73.7817%\n",
      "total_backward_count 979000 real_backward_count 132610  13.545%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.591724/  1.837984, val:  47.50%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0045%\n",
      "layer   2  Sparsity: 76.1361%\n",
      "layer   3  Sparsity: 73.8764%\n",
      "total_backward_count 988790 real_backward_count 133827  13.534%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.600953/  1.808240, val:  60.00%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9771%\n",
      "layer   2  Sparsity: 75.3847%\n",
      "layer   3  Sparsity: 75.0115%\n",
      "total_backward_count 998580 real_backward_count 135082  13.527%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.593225/  1.857707, val:  44.17%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9790%\n",
      "layer   2  Sparsity: 74.3073%\n",
      "layer   3  Sparsity: 74.8432%\n",
      "total_backward_count 1008370 real_backward_count 136288  13.516%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.597742/  1.841986, val:  47.92%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9708%\n",
      "layer   2  Sparsity: 74.8669%\n",
      "layer   3  Sparsity: 74.2244%\n",
      "total_backward_count 1018160 real_backward_count 137553  13.510%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.563393/  1.805139, val:  56.25%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9663%\n",
      "layer   2  Sparsity: 74.5822%\n",
      "layer   3  Sparsity: 73.0093%\n",
      "total_backward_count 1027950 real_backward_count 138768  13.499%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.560336/  1.803570, val:  59.58%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0047%\n",
      "layer   2  Sparsity: 74.9854%\n",
      "layer   3  Sparsity: 72.5811%\n",
      "total_backward_count 1037740 real_backward_count 140033  13.494%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.570650/  1.824577, val:  46.25%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9707%\n",
      "layer   2  Sparsity: 75.8713%\n",
      "layer   3  Sparsity: 73.7290%\n",
      "total_backward_count 1047530 real_backward_count 141264  13.485%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.598944/  1.809435, val:  55.42%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9957%\n",
      "layer   2  Sparsity: 76.4946%\n",
      "layer   3  Sparsity: 74.4591%\n",
      "total_backward_count 1057320 real_backward_count 142601  13.487%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.595947/  1.820564, val:  55.83%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9536%\n",
      "layer   2  Sparsity: 76.7879%\n",
      "layer   3  Sparsity: 75.2148%\n",
      "total_backward_count 1067110 real_backward_count 143830  13.478%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.575492/  1.837859, val:  51.25%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9495%\n",
      "layer   2  Sparsity: 78.2886%\n",
      "layer   3  Sparsity: 73.5796%\n",
      "total_backward_count 1076900 real_backward_count 145050  13.469%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.615562/  1.815404, val:  66.25%, val_best:  66.25%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0273%\n",
      "layer   2  Sparsity: 78.7442%\n",
      "layer   3  Sparsity: 75.4269%\n",
      "total_backward_count 1086690 real_backward_count 146314  13.464%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.621557/  1.857221, val:  51.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9361%\n",
      "layer   2  Sparsity: 77.5598%\n",
      "layer   3  Sparsity: 75.8822%\n",
      "total_backward_count 1096480 real_backward_count 147595  13.461%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.645069/  1.840462, val:  52.92%, val_best:  66.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0116%\n",
      "layer   2  Sparsity: 75.8851%\n",
      "layer   3  Sparsity: 75.7368%\n",
      "total_backward_count 1106270 real_backward_count 148878  13.458%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.623083/  1.865575, val:  49.17%, val_best:  66.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9991%\n",
      "layer   2  Sparsity: 75.2345%\n",
      "layer   3  Sparsity: 75.1780%\n",
      "total_backward_count 1116060 real_backward_count 150103  13.449%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.622775/  1.833431, val:  50.00%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9907%\n",
      "layer   2  Sparsity: 75.1591%\n",
      "layer   3  Sparsity: 74.4388%\n",
      "total_backward_count 1125850 real_backward_count 151386  13.446%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.601746/  1.878618, val:  42.92%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9437%\n",
      "layer   2  Sparsity: 76.1786%\n",
      "layer   3  Sparsity: 74.0581%\n",
      "total_backward_count 1135640 real_backward_count 152600  13.437%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.621798/  1.830026, val:  57.92%, val_best:  66.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9690%\n",
      "layer   2  Sparsity: 76.2443%\n",
      "layer   3  Sparsity: 74.1795%\n",
      "total_backward_count 1145430 real_backward_count 153860  13.433%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.603233/  1.815434, val:  50.83%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9821%\n",
      "layer   2  Sparsity: 76.2069%\n",
      "layer   3  Sparsity: 73.1265%\n",
      "total_backward_count 1155220 real_backward_count 155092  13.425%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.547079/  1.840820, val:  49.58%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9760%\n",
      "layer   2  Sparsity: 76.4818%\n",
      "layer   3  Sparsity: 71.8855%\n",
      "total_backward_count 1165010 real_backward_count 156304  13.417%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.526009/  1.818272, val:  47.50%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0267%\n",
      "layer   2  Sparsity: 76.4699%\n",
      "layer   3  Sparsity: 71.8641%\n",
      "total_backward_count 1174800 real_backward_count 157548  13.411%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.550478/  1.829642, val:  49.58%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0030%\n",
      "layer   2  Sparsity: 76.2221%\n",
      "layer   3  Sparsity: 73.0375%\n",
      "total_backward_count 1184590 real_backward_count 158755  13.402%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.530057/  1.795595, val:  47.92%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9935%\n",
      "layer   2  Sparsity: 76.2129%\n",
      "layer   3  Sparsity: 71.1857%\n",
      "total_backward_count 1194380 real_backward_count 159928  13.390%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.520883/  1.777272, val:  47.92%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0010%\n",
      "layer   2  Sparsity: 76.5911%\n",
      "layer   3  Sparsity: 70.2192%\n",
      "total_backward_count 1204170 real_backward_count 161124  13.381%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.527219/  1.790525, val:  55.42%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9869%\n",
      "layer   2  Sparsity: 77.0949%\n",
      "layer   3  Sparsity: 70.4726%\n",
      "total_backward_count 1213960 real_backward_count 162323  13.371%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.562761/  1.818039, val:  57.92%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9803%\n",
      "layer   2  Sparsity: 76.8295%\n",
      "layer   3  Sparsity: 72.7457%\n",
      "total_backward_count 1223750 real_backward_count 163534  13.363%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.546112/  1.827031, val:  41.25%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0293%\n",
      "layer   2  Sparsity: 76.2878%\n",
      "layer   3  Sparsity: 71.7007%\n",
      "total_backward_count 1233540 real_backward_count 164750  13.356%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.575517/  1.865800, val:  50.42%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0495%\n",
      "layer   2  Sparsity: 76.3485%\n",
      "layer   3  Sparsity: 72.4119%\n",
      "total_backward_count 1243330 real_backward_count 165977  13.349%\n",
      "fc layer 3 self.abs_max_out: 2166.0\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.552539/  1.803065, val:  53.75%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9705%\n",
      "layer   2  Sparsity: 77.4486%\n",
      "layer   3  Sparsity: 70.4916%\n",
      "total_backward_count 1253120 real_backward_count 167128  13.337%\n",
      "fc layer 3 self.abs_max_out: 2186.0\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.515348/  1.791076, val:  50.00%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0162%\n",
      "layer   2  Sparsity: 76.2066%\n",
      "layer   3  Sparsity: 71.2153%\n",
      "total_backward_count 1262910 real_backward_count 168249  13.322%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.496823/  1.778011, val:  57.50%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9466%\n",
      "layer   2  Sparsity: 76.9741%\n",
      "layer   3  Sparsity: 70.8946%\n",
      "total_backward_count 1272700 real_backward_count 169385  13.309%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.518551/  1.818086, val:  46.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0302%\n",
      "layer   2  Sparsity: 77.1853%\n",
      "layer   3  Sparsity: 72.3451%\n",
      "total_backward_count 1282490 real_backward_count 170549  13.298%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.521773/  1.762056, val:  57.08%, val_best:  66.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9664%\n",
      "layer   2  Sparsity: 77.5584%\n",
      "layer   3  Sparsity: 70.9680%\n",
      "total_backward_count 1292280 real_backward_count 171781  13.293%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.518425/  1.785789, val:  56.67%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9658%\n",
      "layer   2  Sparsity: 77.6845%\n",
      "layer   3  Sparsity: 72.0692%\n",
      "total_backward_count 1302070 real_backward_count 172963  13.284%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.547283/  1.822212, val:  55.42%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9952%\n",
      "layer   2  Sparsity: 77.4071%\n",
      "layer   3  Sparsity: 72.4137%\n",
      "total_backward_count 1311860 real_backward_count 174187  13.278%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.554633/  1.824447, val:  46.67%, val_best:  66.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0277%\n",
      "layer   2  Sparsity: 77.4653%\n",
      "layer   3  Sparsity: 71.7824%\n",
      "total_backward_count 1321650 real_backward_count 175395  13.271%\n",
      "fc layer 1 self.abs_max_out: 20357.0\n",
      "lif layer 1 self.abs_max_v: 36311.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.533230/  1.725127, val:  63.75%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9776%\n",
      "layer   2  Sparsity: 76.8133%\n",
      "layer   3  Sparsity: 70.6196%\n",
      "total_backward_count 1331440 real_backward_count 176570  13.262%\n",
      "fc layer 1 self.abs_max_out: 20752.0\n",
      "lif layer 1 self.abs_max_v: 37027.5\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.521156/  1.804269, val:  53.33%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9820%\n",
      "layer   2  Sparsity: 77.6184%\n",
      "layer   3  Sparsity: 72.1647%\n",
      "total_backward_count 1341230 real_backward_count 177736  13.252%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.522375/  1.797539, val:  49.58%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0084%\n",
      "layer   2  Sparsity: 76.4817%\n",
      "layer   3  Sparsity: 70.4740%\n",
      "total_backward_count 1351020 real_backward_count 178887  13.241%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.513283/  1.777245, val:  60.42%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9701%\n",
      "layer   2  Sparsity: 76.6859%\n",
      "layer   3  Sparsity: 70.4804%\n",
      "total_backward_count 1360810 real_backward_count 180029  13.230%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.541469/  1.767245, val:  60.00%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9881%\n",
      "layer   2  Sparsity: 77.2435%\n",
      "layer   3  Sparsity: 71.1938%\n",
      "total_backward_count 1370600 real_backward_count 181229  13.223%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.536035/  1.832883, val:  42.50%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9747%\n",
      "layer   2  Sparsity: 77.5786%\n",
      "layer   3  Sparsity: 72.4624%\n",
      "total_backward_count 1380390 real_backward_count 182417  13.215%\n",
      "fc layer 3 self.abs_max_out: 2190.0\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.529855/  1.791202, val:  54.17%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 82.0198%\n",
      "layer   2  Sparsity: 76.5584%\n",
      "layer   3  Sparsity: 73.0639%\n",
      "total_backward_count 1390180 real_backward_count 183586  13.206%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.531277/  1.799846, val:  57.08%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0146%\n",
      "layer   2  Sparsity: 77.1622%\n",
      "layer   3  Sparsity: 72.1409%\n",
      "total_backward_count 1399970 real_backward_count 184788  13.199%\n",
      "fc layer 3 self.abs_max_out: 2233.0\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.510072/  1.779963, val:  55.00%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 81.9267%\n",
      "layer   2  Sparsity: 76.9329%\n",
      "layer   3  Sparsity: 70.8477%\n",
      "total_backward_count 1409760 real_backward_count 186062  13.198%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.511643/  1.747894, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9861%\n",
      "layer   2  Sparsity: 76.3880%\n",
      "layer   3  Sparsity: 71.6575%\n",
      "total_backward_count 1419550 real_backward_count 187246  13.191%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.521661/  1.809260, val:  47.92%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9829%\n",
      "layer   2  Sparsity: 76.0599%\n",
      "layer   3  Sparsity: 70.9262%\n",
      "total_backward_count 1429340 real_backward_count 188443  13.184%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.549146/  1.834322, val:  43.75%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9811%\n",
      "layer   2  Sparsity: 75.2313%\n",
      "layer   3  Sparsity: 73.3339%\n",
      "total_backward_count 1439130 real_backward_count 189714  13.183%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.535339/  1.802373, val:  50.83%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9758%\n",
      "layer   2  Sparsity: 76.1705%\n",
      "layer   3  Sparsity: 73.7395%\n",
      "total_backward_count 1448920 real_backward_count 190904  13.176%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.573056/  1.773825, val:  63.75%, val_best:  66.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9684%\n",
      "layer   2  Sparsity: 76.9405%\n",
      "layer   3  Sparsity: 73.5286%\n",
      "total_backward_count 1458710 real_backward_count 192187  13.175%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.575683/  1.819209, val:  51.25%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9757%\n",
      "layer   2  Sparsity: 77.5285%\n",
      "layer   3  Sparsity: 73.7834%\n",
      "total_backward_count 1468500 real_backward_count 193475  13.175%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.560889/  1.799494, val:  54.58%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9700%\n",
      "layer   2  Sparsity: 76.7559%\n",
      "layer   3  Sparsity: 73.7620%\n",
      "total_backward_count 1478290 real_backward_count 194682  13.169%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.554332/  1.787035, val:  53.33%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0018%\n",
      "layer   2  Sparsity: 75.7437%\n",
      "layer   3  Sparsity: 70.6689%\n",
      "total_backward_count 1488080 real_backward_count 195853  13.161%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.572699/  1.802014, val:  50.83%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 81.9818%\n",
      "layer   2  Sparsity: 76.2074%\n",
      "layer   3  Sparsity: 73.1322%\n",
      "total_backward_count 1497870 real_backward_count 197015  13.153%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.595119/  1.809807, val:  62.08%, val_best:  66.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9828%\n",
      "layer   2  Sparsity: 75.9955%\n",
      "layer   3  Sparsity: 74.2886%\n",
      "total_backward_count 1507660 real_backward_count 198262  13.150%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.584411/  1.817064, val:  57.92%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0192%\n",
      "layer   2  Sparsity: 76.1216%\n",
      "layer   3  Sparsity: 73.2435%\n",
      "total_backward_count 1517450 real_backward_count 199516  13.148%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.579175/  1.781683, val:  57.92%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9523%\n",
      "layer   2  Sparsity: 76.7208%\n",
      "layer   3  Sparsity: 72.1357%\n",
      "total_backward_count 1527240 real_backward_count 200704  13.142%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.508475/  1.819317, val:  52.08%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9989%\n",
      "layer   2  Sparsity: 77.2818%\n",
      "layer   3  Sparsity: 70.8591%\n",
      "total_backward_count 1537030 real_backward_count 201851  13.133%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.520780/  1.741951, val:  57.50%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 81.9322%\n",
      "layer   2  Sparsity: 77.5229%\n",
      "layer   3  Sparsity: 70.5887%\n",
      "total_backward_count 1546820 real_backward_count 203008  13.124%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.517062/  1.774291, val:  54.17%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0113%\n",
      "layer   2  Sparsity: 76.4870%\n",
      "layer   3  Sparsity: 71.0486%\n",
      "total_backward_count 1556610 real_backward_count 204269  13.123%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.528732/  1.781978, val:  50.42%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9873%\n",
      "layer   2  Sparsity: 76.4235%\n",
      "layer   3  Sparsity: 70.6046%\n",
      "total_backward_count 1566400 real_backward_count 205422  13.114%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.499624/  1.748239, val:  58.33%, val_best:  66.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 82.0142%\n",
      "layer   2  Sparsity: 76.4832%\n",
      "layer   3  Sparsity: 70.9721%\n",
      "total_backward_count 1576190 real_backward_count 206592  13.107%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.565704/  1.787531, val:  62.50%, val_best:  66.25%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 81.9403%\n",
      "layer   2  Sparsity: 75.7040%\n",
      "layer   3  Sparsity: 74.0122%\n",
      "total_backward_count 1585980 real_backward_count 207831  13.104%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.535171/  1.797374, val:  54.17%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9934%\n",
      "layer   2  Sparsity: 76.2983%\n",
      "layer   3  Sparsity: 71.9425%\n",
      "total_backward_count 1595770 real_backward_count 209075  13.102%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.502617/  1.798529, val:  55.83%, val_best:  66.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9878%\n",
      "layer   2  Sparsity: 76.9327%\n",
      "layer   3  Sparsity: 71.2406%\n",
      "total_backward_count 1605560 real_backward_count 210244  13.095%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.514125/  1.788739, val:  48.33%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9371%\n",
      "layer   2  Sparsity: 77.1942%\n",
      "layer   3  Sparsity: 70.6881%\n",
      "total_backward_count 1615350 real_backward_count 211476  13.092%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.508653/  1.775119, val:  56.25%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9477%\n",
      "layer   2  Sparsity: 77.0960%\n",
      "layer   3  Sparsity: 71.0963%\n",
      "total_backward_count 1625140 real_backward_count 212696  13.088%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.498822/  1.767807, val:  53.75%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9616%\n",
      "layer   2  Sparsity: 76.3953%\n",
      "layer   3  Sparsity: 70.8123%\n",
      "total_backward_count 1634930 real_backward_count 213850  13.080%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.522244/  1.782418, val:  45.42%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9900%\n",
      "layer   2  Sparsity: 76.1214%\n",
      "layer   3  Sparsity: 72.8967%\n",
      "total_backward_count 1644720 real_backward_count 215029  13.074%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.556296/  1.836076, val:  56.67%, val_best:  66.25%, tr:  99.18%, tr_best:  99.90%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0237%\n",
      "layer   2  Sparsity: 76.6266%\n",
      "layer   3  Sparsity: 75.7191%\n",
      "total_backward_count 1654510 real_backward_count 216285  13.072%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.548859/  1.778567, val:  57.08%, val_best:  66.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9952%\n",
      "layer   2  Sparsity: 75.4575%\n",
      "layer   3  Sparsity: 73.1080%\n",
      "total_backward_count 1664300 real_backward_count 217447  13.065%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.546665/  1.789504, val:  49.17%, val_best:  66.25%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9985%\n",
      "layer   2  Sparsity: 75.3457%\n",
      "layer   3  Sparsity: 73.1077%\n",
      "total_backward_count 1674090 real_backward_count 218623  13.059%\n",
      "fc layer 3 self.abs_max_out: 2260.0\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.544646/  1.773239, val:  55.00%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0266%\n",
      "layer   2  Sparsity: 75.6971%\n",
      "layer   3  Sparsity: 73.1844%\n",
      "total_backward_count 1683880 real_backward_count 219805  13.053%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.556872/  1.772969, val:  59.58%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0081%\n",
      "layer   2  Sparsity: 75.6155%\n",
      "layer   3  Sparsity: 73.9347%\n",
      "total_backward_count 1693670 real_backward_count 220998  13.048%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.536591/  1.813346, val:  56.67%, val_best:  66.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9822%\n",
      "layer   2  Sparsity: 75.0852%\n",
      "layer   3  Sparsity: 72.9632%\n",
      "total_backward_count 1703460 real_backward_count 222122  13.039%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.539637/  1.749027, val:  60.00%, val_best:  66.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0072%\n",
      "layer   2  Sparsity: 76.2404%\n",
      "layer   3  Sparsity: 71.7687%\n",
      "total_backward_count 1713250 real_backward_count 223280  13.033%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.560146/  1.799938, val:  60.00%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9346%\n",
      "layer   2  Sparsity: 75.2599%\n",
      "layer   3  Sparsity: 73.3034%\n",
      "total_backward_count 1723040 real_backward_count 224522  13.031%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.571876/  1.771442, val:  50.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9931%\n",
      "layer   2  Sparsity: 75.7416%\n",
      "layer   3  Sparsity: 73.8582%\n",
      "total_backward_count 1732830 real_backward_count 225722  13.026%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.557935/  1.792264, val:  55.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9355%\n",
      "layer   2  Sparsity: 75.3596%\n",
      "layer   3  Sparsity: 73.1749%\n",
      "total_backward_count 1742620 real_backward_count 226861  13.018%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.560265/  1.841341, val:  50.83%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9969%\n",
      "layer   2  Sparsity: 76.3562%\n",
      "layer   3  Sparsity: 74.1384%\n",
      "total_backward_count 1752410 real_backward_count 228067  13.014%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.572981/  1.853999, val:  52.08%, val_best:  66.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9870%\n",
      "layer   2  Sparsity: 75.6742%\n",
      "layer   3  Sparsity: 73.5569%\n",
      "total_backward_count 1762200 real_backward_count 229312  13.013%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.573459/  1.852972, val:  50.83%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0214%\n",
      "layer   2  Sparsity: 76.0160%\n",
      "layer   3  Sparsity: 73.1418%\n",
      "total_backward_count 1771990 real_backward_count 230505  13.008%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.554987/  1.829057, val:  62.50%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9715%\n",
      "layer   2  Sparsity: 76.5843%\n",
      "layer   3  Sparsity: 72.8450%\n",
      "total_backward_count 1781780 real_backward_count 231608  12.999%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.524899/  1.791811, val:  54.58%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0071%\n",
      "layer   2  Sparsity: 75.9069%\n",
      "layer   3  Sparsity: 71.6881%\n",
      "total_backward_count 1791570 real_backward_count 232822  12.995%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.545487/  1.818020, val:  47.50%, val_best:  66.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9676%\n",
      "layer   2  Sparsity: 76.2767%\n",
      "layer   3  Sparsity: 72.6155%\n",
      "total_backward_count 1801360 real_backward_count 234036  12.992%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.550774/  1.808428, val:  54.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9820%\n",
      "layer   2  Sparsity: 76.6746%\n",
      "layer   3  Sparsity: 73.2171%\n",
      "total_backward_count 1811150 real_backward_count 235177  12.985%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.578152/  1.847063, val:  56.67%, val_best:  66.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9661%\n",
      "layer   2  Sparsity: 77.3965%\n",
      "layer   3  Sparsity: 75.4981%\n",
      "total_backward_count 1820940 real_backward_count 236379  12.981%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.598299/  1.836397, val:  56.25%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0015%\n",
      "layer   2  Sparsity: 77.6676%\n",
      "layer   3  Sparsity: 75.4123%\n",
      "total_backward_count 1830730 real_backward_count 237576  12.977%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.618490/  1.897301, val:  46.25%, val_best:  66.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9873%\n",
      "layer   2  Sparsity: 76.2992%\n",
      "layer   3  Sparsity: 76.4197%\n",
      "total_backward_count 1840520 real_backward_count 238851  12.977%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.590588/  1.826719, val:  58.75%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0042%\n",
      "layer   2  Sparsity: 75.5417%\n",
      "layer   3  Sparsity: 74.9771%\n",
      "total_backward_count 1850310 real_backward_count 240090  12.976%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.562910/  1.776743, val:  55.00%, val_best:  66.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9743%\n",
      "layer   2  Sparsity: 75.1336%\n",
      "layer   3  Sparsity: 72.9295%\n",
      "total_backward_count 1860100 real_backward_count 241282  12.971%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.545703/  1.747345, val:  60.42%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9977%\n",
      "layer   2  Sparsity: 75.5473%\n",
      "layer   3  Sparsity: 72.7382%\n",
      "total_backward_count 1869890 real_backward_count 242492  12.968%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.546188/  1.828066, val:  47.92%, val_best:  66.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9520%\n",
      "layer   2  Sparsity: 76.5800%\n",
      "layer   3  Sparsity: 73.4569%\n",
      "total_backward_count 1879680 real_backward_count 243698  12.965%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.517240/  1.781410, val:  58.33%, val_best:  66.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 82.0010%\n",
      "layer   2  Sparsity: 76.3069%\n",
      "layer   3  Sparsity: 71.9932%\n",
      "total_backward_count 1889470 real_backward_count 244879  12.960%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.541095/  1.860927, val:  46.25%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9745%\n",
      "layer   2  Sparsity: 74.5900%\n",
      "layer   3  Sparsity: 72.9231%\n",
      "total_backward_count 1899260 real_backward_count 246097  12.958%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.549166/  1.822981, val:  44.58%, val_best:  66.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9950%\n",
      "layer   2  Sparsity: 74.5314%\n",
      "layer   3  Sparsity: 72.8784%\n",
      "total_backward_count 1909050 real_backward_count 247261  12.952%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.579761/  1.855780, val:  50.83%, val_best:  66.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9617%\n",
      "layer   2  Sparsity: 74.1890%\n",
      "layer   3  Sparsity: 74.2428%\n",
      "total_backward_count 1918840 real_backward_count 248460  12.948%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.557395/  1.783073, val:  62.92%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 81.9834%\n",
      "layer   2  Sparsity: 74.6871%\n",
      "layer   3  Sparsity: 73.0257%\n",
      "total_backward_count 1928630 real_backward_count 249640  12.944%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.585160/  1.795703, val:  60.42%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 82.0219%\n",
      "layer   2  Sparsity: 75.3796%\n",
      "layer   3  Sparsity: 75.1358%\n",
      "total_backward_count 1938420 real_backward_count 250883  12.943%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.574642/  1.864129, val:  56.67%, val_best:  66.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9746%\n",
      "layer   2  Sparsity: 76.2185%\n",
      "layer   3  Sparsity: 74.6520%\n",
      "total_backward_count 1948210 real_backward_count 252115  12.941%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.588003/  1.789562, val:  60.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 81.9722%\n",
      "layer   2  Sparsity: 76.2719%\n",
      "layer   3  Sparsity: 74.0667%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a539549963c44250871f68b2284b0c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÖ‚ñá‚ñà‚ñÑ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÖ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99694</td></tr><tr><td>tr_epoch_loss</td><td>1.588</td></tr><tr><td>val_acc_best</td><td>0.6625</td></tr><tr><td>val_acc_now</td><td>0.6</td></tr><tr><td>val_loss</td><td>1.78956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-sweep-121</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rgs8muo9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rgs8muo9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_000940-rgs8muo9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sypcffri with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_042701-sypcffri</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sypcffri' target=\"_blank\">expert-sweep-127</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sypcffri' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sypcffri</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_042710_200', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 221.0\n",
      "lif layer 1 self.abs_max_v: 221.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 348.0\n",
      "fc layer 2 self.abs_max_out: 92.0\n",
      "lif layer 2 self.abs_max_v: 92.0\n",
      "fc layer 1 self.abs_max_out: 281.0\n",
      "lif layer 1 self.abs_max_v: 449.0\n",
      "fc layer 2 self.abs_max_out: 95.0\n",
      "lif layer 2 self.abs_max_v: 104.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 131.5\n",
      "fc layer 1 self.abs_max_out: 395.0\n",
      "lif layer 2 self.abs_max_v: 149.5\n",
      "fc layer 1 self.abs_max_out: 564.0\n",
      "lif layer 1 self.abs_max_v: 564.0\n",
      "fc layer 2 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 235.0\n",
      "fc layer 1 self.abs_max_out: 703.0\n",
      "lif layer 1 self.abs_max_v: 703.0\n",
      "fc layer 1 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 847.0\n",
      "fc layer 2 self.abs_max_out: 198.0\n",
      "fc layer 2 self.abs_max_out: 205.0\n",
      "lif layer 2 self.abs_max_v: 259.5\n",
      "fc layer 3 self.abs_max_out: 14.0\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "lif layer 2 self.abs_max_v: 359.5\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 415.0\n",
      "fc layer 1 self.abs_max_out: 970.0\n",
      "lif layer 1 self.abs_max_v: 970.0\n",
      "fc layer 1 self.abs_max_out: 978.0\n",
      "lif layer 1 self.abs_max_v: 978.0\n",
      "fc layer 2 self.abs_max_out: 299.0\n",
      "lif layer 2 self.abs_max_v: 420.5\n",
      "fc layer 1 self.abs_max_out: 1082.0\n",
      "lif layer 1 self.abs_max_v: 1082.0\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 439.5\n",
      "lif layer 2 self.abs_max_v: 454.0\n",
      "fc layer 2 self.abs_max_out: 363.0\n",
      "lif layer 2 self.abs_max_v: 461.5\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "lif layer 2 self.abs_max_v: 494.0\n",
      "fc layer 2 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 500.0\n",
      "fc layer 1 self.abs_max_out: 1238.0\n",
      "lif layer 1 self.abs_max_v: 1238.0\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 515.5\n",
      "fc layer 2 self.abs_max_out: 436.0\n",
      "fc layer 2 self.abs_max_out: 463.0\n",
      "lif layer 2 self.abs_max_v: 532.5\n",
      "lif layer 2 self.abs_max_v: 539.5\n",
      "fc layer 2 self.abs_max_out: 506.0\n",
      "lif layer 2 self.abs_max_v: 670.0\n",
      "fc layer 2 self.abs_max_out: 527.0\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "fc layer 1 self.abs_max_out: 1240.0\n",
      "lif layer 1 self.abs_max_v: 1240.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "lif layer 1 self.abs_max_v: 1258.0\n",
      "fc layer 2 self.abs_max_out: 568.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 1 self.abs_max_out: 1280.0\n",
      "lif layer 1 self.abs_max_v: 1280.0\n",
      "fc layer 2 self.abs_max_out: 574.0\n",
      "fc layer 1 self.abs_max_out: 1297.0\n",
      "lif layer 1 self.abs_max_v: 1297.0\n",
      "lif layer 2 self.abs_max_v: 672.5\n",
      "lif layer 2 self.abs_max_v: 692.5\n",
      "lif layer 2 self.abs_max_v: 712.5\n",
      "fc layer 1 self.abs_max_out: 1298.0\n",
      "lif layer 1 self.abs_max_v: 1298.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 3 self.abs_max_out: 117.0\n",
      "lif layer 2 self.abs_max_v: 730.0\n",
      "lif layer 2 self.abs_max_v: 766.5\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "fc layer 2 self.abs_max_out: 599.0\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "fc layer 1 self.abs_max_out: 1436.0\n",
      "lif layer 1 self.abs_max_v: 1436.0\n",
      "fc layer 2 self.abs_max_out: 666.0\n",
      "fc layer 1 self.abs_max_out: 1492.0\n",
      "lif layer 1 self.abs_max_v: 1492.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "lif layer 1 self.abs_max_v: 1627.0\n",
      "lif layer 2 self.abs_max_v: 787.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 2 self.abs_max_out: 713.0\n",
      "fc layer 1 self.abs_max_out: 1755.0\n",
      "lif layer 1 self.abs_max_v: 1755.0\n",
      "fc layer 2 self.abs_max_out: 735.0\n",
      "fc layer 2 self.abs_max_out: 768.0\n",
      "lif layer 1 self.abs_max_v: 1908.0\n",
      "fc layer 2 self.abs_max_out: 773.0\n",
      "fc layer 1 self.abs_max_out: 1759.0\n",
      "lif layer 1 self.abs_max_v: 1999.0\n",
      "lif layer 1 self.abs_max_v: 2016.5\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 2 self.abs_max_out: 821.0\n",
      "lif layer 2 self.abs_max_v: 821.0\n",
      "lif layer 1 self.abs_max_v: 2090.5\n",
      "lif layer 1 self.abs_max_v: 2355.0\n",
      "lif layer 1 self.abs_max_v: 2426.5\n",
      "fc layer 1 self.abs_max_out: 1851.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "fc layer 1 self.abs_max_out: 1933.0\n",
      "lif layer 1 self.abs_max_v: 2444.0\n",
      "lif layer 1 self.abs_max_v: 2464.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "lif layer 1 self.abs_max_v: 2548.0\n",
      "lif layer 1 self.abs_max_v: 2564.0\n",
      "lif layer 1 self.abs_max_v: 2725.0\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "lif layer 2 self.abs_max_v: 827.0\n",
      "fc layer 2 self.abs_max_out: 832.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "fc layer 2 self.abs_max_out: 865.0\n",
      "lif layer 2 self.abs_max_v: 865.0\n",
      "fc layer 2 self.abs_max_out: 904.0\n",
      "lif layer 2 self.abs_max_v: 904.0\n",
      "fc layer 1 self.abs_max_out: 1978.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "lif layer 2 self.abs_max_v: 931.0\n",
      "fc layer 1 self.abs_max_out: 1993.0\n",
      "fc layer 1 self.abs_max_out: 2128.0\n",
      "fc layer 1 self.abs_max_out: 2187.0\n",
      "lif layer 1 self.abs_max_v: 2754.0\n",
      "lif layer 1 self.abs_max_v: 2892.0\n",
      "fc layer 1 self.abs_max_out: 2200.0\n",
      "lif layer 1 self.abs_max_v: 3110.5\n",
      "lif layer 1 self.abs_max_v: 3252.5\n",
      "lif layer 1 self.abs_max_v: 3392.0\n",
      "fc layer 1 self.abs_max_out: 2280.0\n",
      "lif layer 1 self.abs_max_v: 3463.0\n",
      "lif layer 1 self.abs_max_v: 3585.5\n",
      "fc layer 1 self.abs_max_out: 2311.0\n",
      "fc layer 1 self.abs_max_out: 2399.0\n",
      "lif layer 1 self.abs_max_v: 3693.5\n",
      "lif layer 1 self.abs_max_v: 3779.0\n",
      "fc layer 1 self.abs_max_out: 2412.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.123585/  2.165352, val:  37.08%, val_best:  37.08%, tr:  88.97%, tr_best:  88.97%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6255%\n",
      "layer   2  Sparsity: 89.3674%\n",
      "layer   3  Sparsity: 92.0756%\n",
      "total_backward_count 9790 real_backward_count 3259  33.289%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 3834.0\n",
      "fc layer 3 self.abs_max_out: 168.0\n",
      "fc layer 1 self.abs_max_out: 2440.0\n",
      "fc layer 1 self.abs_max_out: 2461.0\n",
      "lif layer 1 self.abs_max_v: 3875.0\n",
      "fc layer 1 self.abs_max_out: 2468.0\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "lif layer 2 self.abs_max_v: 958.0\n",
      "lif layer 1 self.abs_max_v: 4020.5\n",
      "lif layer 1 self.abs_max_v: 4083.5\n",
      "lif layer 1 self.abs_max_v: 4201.0\n",
      "lif layer 1 self.abs_max_v: 4489.5\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "lif layer 2 self.abs_max_v: 961.0\n",
      "fc layer 1 self.abs_max_out: 2505.0\n",
      "lif layer 2 self.abs_max_v: 976.0\n",
      "fc layer 1 self.abs_max_out: 2569.0\n",
      "fc layer 1 self.abs_max_out: 2573.0\n",
      "fc layer 1 self.abs_max_out: 2837.0\n",
      "lif layer 2 self.abs_max_v: 1001.0\n",
      "lif layer 2 self.abs_max_v: 1026.5\n",
      "lif layer 2 self.abs_max_v: 1057.5\n",
      "fc layer 2 self.abs_max_out: 965.0\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 1059.0\n",
      "lif layer 2 self.abs_max_v: 1059.0\n",
      "fc layer 2 self.abs_max_out: 1082.0\n",
      "lif layer 2 self.abs_max_v: 1082.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "lif layer 2 self.abs_max_v: 1137.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.091747/  2.165953, val:  40.00%, val_best:  40.00%, tr:  97.85%, tr_best:  97.85%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6165%\n",
      "layer   2  Sparsity: 88.9435%\n",
      "layer   3  Sparsity: 90.6343%\n",
      "total_backward_count 19580 real_backward_count 5430  27.732%\n",
      "fc layer 1 self.abs_max_out: 2863.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  2.097532/  2.164506, val:  44.58%, val_best:  44.58%, tr:  98.26%, tr_best:  98.26%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6484%\n",
      "layer   2  Sparsity: 88.4180%\n",
      "layer   3  Sparsity: 90.7686%\n",
      "total_backward_count 29370 real_backward_count 7476  25.455%\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "lif layer 1 self.abs_max_v: 4507.0\n",
      "lif layer 2 self.abs_max_v: 1146.5\n",
      "lif layer 1 self.abs_max_v: 4576.0\n",
      "lif layer 1 self.abs_max_v: 4665.0\n",
      "fc layer 1 self.abs_max_out: 2980.0\n",
      "fc layer 1 self.abs_max_out: 3083.0\n",
      "lif layer 1 self.abs_max_v: 4669.0\n",
      "lif layer 1 self.abs_max_v: 4756.5\n",
      "lif layer 1 self.abs_max_v: 4793.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  2.091412/  2.183434, val:  33.75%, val_best:  44.58%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5950%\n",
      "layer   2  Sparsity: 88.4171%\n",
      "layer   3  Sparsity: 90.6829%\n",
      "total_backward_count 39160 real_backward_count 9466  24.173%\n",
      "lif layer 2 self.abs_max_v: 1177.5\n",
      "lif layer 2 self.abs_max_v: 1231.0\n",
      "fc layer 1 self.abs_max_out: 3235.0\n",
      "lif layer 1 self.abs_max_v: 4862.5\n",
      "lif layer 1 self.abs_max_v: 5124.5\n",
      "fc layer 2 self.abs_max_out: 1163.0\n",
      "lif layer 2 self.abs_max_v: 1255.0\n",
      "lif layer 2 self.abs_max_v: 1330.5\n",
      "fc layer 2 self.abs_max_out: 1184.0\n",
      "fc layer 1 self.abs_max_out: 3244.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  2.099693/  2.177056, val:  36.25%, val_best:  44.58%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5729%\n",
      "layer   2  Sparsity: 88.4121%\n",
      "layer   3  Sparsity: 90.6984%\n",
      "total_backward_count 48950 real_backward_count 11328  23.142%\n",
      "lif layer 1 self.abs_max_v: 5133.5\n",
      "lif layer 1 self.abs_max_v: 5284.0\n",
      "lif layer 1 self.abs_max_v: 5589.0\n",
      "lif layer 2 self.abs_max_v: 1353.5\n",
      "fc layer 2 self.abs_max_out: 1192.0\n",
      "fc layer 1 self.abs_max_out: 3424.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  2.097774/  2.174791, val:  42.50%, val_best:  44.58%, tr:  98.67%, tr_best:  99.28%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6141%\n",
      "layer   2  Sparsity: 87.8486%\n",
      "layer   3  Sparsity: 90.6625%\n",
      "total_backward_count 58740 real_backward_count 13077  22.263%\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "lif layer 1 self.abs_max_v: 5655.0\n",
      "fc layer 1 self.abs_max_out: 3469.0\n",
      "lif layer 1 self.abs_max_v: 5718.5\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "fc layer 2 self.abs_max_out: 1307.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "lif layer 1 self.abs_max_v: 5813.5\n",
      "lif layer 1 self.abs_max_v: 5816.0\n",
      "lif layer 1 self.abs_max_v: 5942.0\n",
      "fc layer 2 self.abs_max_out: 1369.0\n",
      "lif layer 2 self.abs_max_v: 1369.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  2.091269/  2.152726, val:  40.83%, val_best:  44.58%, tr:  98.98%, tr_best:  99.28%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6089%\n",
      "layer   2  Sparsity: 87.0998%\n",
      "layer   3  Sparsity: 90.6825%\n",
      "total_backward_count 68530 real_backward_count 14901  21.744%\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "lif layer 2 self.abs_max_v: 1430.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 2 self.abs_max_v: 1439.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  2.070065/  2.144676, val:  41.67%, val_best:  44.58%, tr:  99.28%, tr_best:  99.28%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6043%\n",
      "layer   2  Sparsity: 86.9947%\n",
      "layer   3  Sparsity: 90.3597%\n",
      "total_backward_count 78320 real_backward_count 16621  21.222%\n",
      "lif layer 2 self.abs_max_v: 1488.5\n",
      "lif layer 2 self.abs_max_v: 1502.5\n",
      "lif layer 2 self.abs_max_v: 1514.0\n",
      "lif layer 2 self.abs_max_v: 1531.5\n",
      "fc layer 2 self.abs_max_out: 1439.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  2.087712/  2.150131, val:  42.08%, val_best:  44.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6286%\n",
      "layer   2  Sparsity: 87.1378%\n",
      "layer   3  Sparsity: 90.8345%\n",
      "total_backward_count 88110 real_backward_count 18396  20.878%\n",
      "lif layer 2 self.abs_max_v: 1553.0\n",
      "lif layer 2 self.abs_max_v: 1560.5\n",
      "lif layer 2 self.abs_max_v: 1572.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  2.070651/  2.150189, val:  40.83%, val_best:  44.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6198%\n",
      "layer   2  Sparsity: 86.8477%\n",
      "layer   3  Sparsity: 90.1181%\n",
      "total_backward_count 97900 real_backward_count 20036  20.466%\n",
      "lif layer 2 self.abs_max_v: 1588.0\n",
      "lif layer 2 self.abs_max_v: 1619.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  2.065933/  2.145196, val:  47.08%, val_best:  47.08%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5928%\n",
      "layer   2  Sparsity: 86.7275%\n",
      "layer   3  Sparsity: 90.0433%\n",
      "total_backward_count 107690 real_backward_count 21697  20.148%\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "lif layer 1 self.abs_max_v: 6045.5\n",
      "lif layer 1 self.abs_max_v: 6208.5\n",
      "fc layer 1 self.abs_max_out: 3529.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  2.060979/  2.135268, val:  49.17%, val_best:  49.17%, tr:  99.28%, tr_best:  99.59%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5633%\n",
      "layer   2  Sparsity: 87.0384%\n",
      "layer   3  Sparsity: 89.3230%\n",
      "total_backward_count 117480 real_backward_count 23392  19.911%\n",
      "fc layer 1 self.abs_max_out: 3885.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  2.039393/  2.133172, val:  44.58%, val_best:  49.17%, tr:  98.98%, tr_best:  99.59%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5880%\n",
      "layer   2  Sparsity: 87.6946%\n",
      "layer   3  Sparsity: 89.3191%\n",
      "total_backward_count 127270 real_backward_count 25016  19.656%\n",
      "lif layer 1 self.abs_max_v: 6322.5\n",
      "lif layer 1 self.abs_max_v: 6366.0\n",
      "lif layer 1 self.abs_max_v: 6531.5\n",
      "fc layer 2 self.abs_max_out: 1478.0\n",
      "lif layer 1 self.abs_max_v: 6636.0\n",
      "lif layer 1 self.abs_max_v: 6650.0\n",
      "lif layer 1 self.abs_max_v: 6724.5\n",
      "lif layer 2 self.abs_max_v: 1622.0\n",
      "lif layer 2 self.abs_max_v: 1648.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  2.045141/  2.146885, val:  37.08%, val_best:  49.17%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5499%\n",
      "layer   2  Sparsity: 88.0310%\n",
      "layer   3  Sparsity: 88.6589%\n",
      "total_backward_count 137060 real_backward_count 26609  19.414%\n",
      "lif layer 2 self.abs_max_v: 1703.0\n",
      "lif layer 2 self.abs_max_v: 1703.5\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "lif layer 2 self.abs_max_v: 1775.5\n",
      "lif layer 2 self.abs_max_v: 1777.0\n",
      "fc layer 2 self.abs_max_out: 1510.0\n",
      "lif layer 2 self.abs_max_v: 1779.5\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  2.051396/  2.131523, val:  41.67%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5524%\n",
      "layer   2  Sparsity: 88.2210%\n",
      "layer   3  Sparsity: 88.7817%\n",
      "total_backward_count 146850 real_backward_count 28114  19.145%\n",
      "lif layer 2 self.abs_max_v: 1825.0\n",
      "fc layer 2 self.abs_max_out: 1521.0\n",
      "fc layer 2 self.abs_max_out: 1549.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  2.046952/  2.122085, val:  57.92%, val_best:  57.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5926%\n",
      "layer   2  Sparsity: 88.3898%\n",
      "layer   3  Sparsity: 89.2391%\n",
      "total_backward_count 156640 real_backward_count 29675  18.945%\n",
      "lif layer 1 self.abs_max_v: 6728.0\n",
      "fc layer 1 self.abs_max_out: 3934.0\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  2.045613/  2.130364, val:  53.75%, val_best:  57.92%, tr:  98.98%, tr_best:  99.80%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5971%\n",
      "layer   2  Sparsity: 88.3908%\n",
      "layer   3  Sparsity: 89.0489%\n",
      "total_backward_count 166430 real_backward_count 31200  18.747%\n",
      "lif layer 1 self.abs_max_v: 6736.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  2.031455/  2.120592, val:  48.75%, val_best:  57.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6346%\n",
      "layer   2  Sparsity: 88.2412%\n",
      "layer   3  Sparsity: 88.7587%\n",
      "total_backward_count 176220 real_backward_count 32722  18.569%\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 3 self.abs_max_out: 203.0\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "fc layer 3 self.abs_max_out: 219.0\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "lif layer 1 self.abs_max_v: 6845.5\n",
      "lif layer 1 self.abs_max_v: 6983.0\n",
      "lif layer 1 self.abs_max_v: 7130.5\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  2.026421/  2.107033, val:  47.08%, val_best:  57.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5893%\n",
      "layer   2  Sparsity: 87.9854%\n",
      "layer   3  Sparsity: 88.5255%\n",
      "total_backward_count 186010 real_backward_count 34346  18.465%\n",
      "fc layer 1 self.abs_max_out: 3982.0\n",
      "lif layer 1 self.abs_max_v: 7152.0\n",
      "lif layer 1 self.abs_max_v: 7288.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  2.016433/  2.122308, val:  38.33%, val_best:  57.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5713%\n",
      "layer   2  Sparsity: 87.6299%\n",
      "layer   3  Sparsity: 88.2351%\n",
      "total_backward_count 195800 real_backward_count 35836  18.302%\n",
      "fc layer 1 self.abs_max_out: 3993.0\n",
      "fc layer 1 self.abs_max_out: 4145.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  2.007495/  2.115178, val:  43.33%, val_best:  57.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6098%\n",
      "layer   2  Sparsity: 87.9071%\n",
      "layer   3  Sparsity: 88.2185%\n",
      "total_backward_count 205590 real_backward_count 37322  18.154%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  2.011915/  2.118036, val:  53.75%, val_best:  57.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5995%\n",
      "layer   2  Sparsity: 87.8455%\n",
      "layer   3  Sparsity: 88.2879%\n",
      "total_backward_count 215380 real_backward_count 38908  18.065%\n",
      "fc layer 1 self.abs_max_out: 4354.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  2.008277/  2.079648, val:  57.08%, val_best:  57.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5918%\n",
      "layer   2  Sparsity: 87.8114%\n",
      "layer   3  Sparsity: 87.8084%\n",
      "total_backward_count 225170 real_backward_count 40439  17.959%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.996597/  2.089660, val:  58.75%, val_best:  58.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5867%\n",
      "layer   2  Sparsity: 87.3556%\n",
      "layer   3  Sparsity: 87.9837%\n",
      "total_backward_count 234960 real_backward_count 41924  17.843%\n",
      "fc layer 1 self.abs_max_out: 4671.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.996474/  2.085601, val:  49.58%, val_best:  58.75%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6064%\n",
      "layer   2  Sparsity: 87.2693%\n",
      "layer   3  Sparsity: 88.0207%\n",
      "total_backward_count 244750 real_backward_count 43406  17.735%\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.983304/  2.084006, val:  53.33%, val_best:  58.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5786%\n",
      "layer   2  Sparsity: 87.1545%\n",
      "layer   3  Sparsity: 87.4334%\n",
      "total_backward_count 254540 real_backward_count 44980  17.671%\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.975149/  2.054436, val:  54.58%, val_best:  58.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6241%\n",
      "layer   2  Sparsity: 87.0840%\n",
      "layer   3  Sparsity: 86.6603%\n",
      "total_backward_count 264330 real_backward_count 46449  17.572%\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.958437/  2.069353, val:  58.33%, val_best:  58.75%, tr:  99.08%, tr_best:  99.80%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5833%\n",
      "layer   2  Sparsity: 86.4240%\n",
      "layer   3  Sparsity: 86.9648%\n",
      "total_backward_count 274120 real_backward_count 47889  17.470%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.970829/  2.079618, val:  54.17%, val_best:  58.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6163%\n",
      "layer   2  Sparsity: 86.1022%\n",
      "layer   3  Sparsity: 87.4272%\n",
      "total_backward_count 283910 real_backward_count 49350  17.382%\n",
      "fc layer 1 self.abs_max_out: 4884.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.970503/  2.066309, val:  52.08%, val_best:  58.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5954%\n",
      "layer   2  Sparsity: 85.5540%\n",
      "layer   3  Sparsity: 87.7226%\n",
      "total_backward_count 293700 real_backward_count 50849  17.313%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.959656/  2.041926, val:  59.17%, val_best:  59.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5569%\n",
      "layer   2  Sparsity: 85.2803%\n",
      "layer   3  Sparsity: 87.1174%\n",
      "total_backward_count 303490 real_backward_count 52315  17.238%\n",
      "lif layer 2 self.abs_max_v: 1850.0\n",
      "lif layer 2 self.abs_max_v: 1865.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.954204/  2.070231, val:  51.67%, val_best:  59.17%, tr:  99.28%, tr_best:  99.80%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5630%\n",
      "layer   2  Sparsity: 85.2505%\n",
      "layer   3  Sparsity: 86.7429%\n",
      "total_backward_count 313280 real_backward_count 53703  17.142%\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "lif layer 1 self.abs_max_v: 7574.5\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.954322/  2.077913, val:  52.50%, val_best:  59.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5835%\n",
      "layer   2  Sparsity: 85.3903%\n",
      "layer   3  Sparsity: 86.7218%\n",
      "total_backward_count 323070 real_backward_count 55083  17.050%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.958420/  2.060785, val:  56.25%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6123%\n",
      "layer   2  Sparsity: 85.1160%\n",
      "layer   3  Sparsity: 87.1328%\n",
      "total_backward_count 332860 real_backward_count 56473  16.966%\n",
      "lif layer 2 self.abs_max_v: 1938.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.958035/  2.057756, val:  57.50%, val_best:  59.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5643%\n",
      "layer   2  Sparsity: 85.5125%\n",
      "layer   3  Sparsity: 86.8402%\n",
      "total_backward_count 342650 real_backward_count 57789  16.865%\n",
      "fc layer 1 self.abs_max_out: 4985.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.942335/  2.056422, val:  54.58%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5990%\n",
      "layer   2  Sparsity: 85.2587%\n",
      "layer   3  Sparsity: 86.3558%\n",
      "total_backward_count 352440 real_backward_count 59158  16.785%\n",
      "lif layer 2 self.abs_max_v: 1939.5\n",
      "fc layer 1 self.abs_max_out: 5402.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.931579/  2.030515, val:  64.58%, val_best:  64.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5844%\n",
      "layer   2  Sparsity: 85.3157%\n",
      "layer   3  Sparsity: 86.6434%\n",
      "total_backward_count 362230 real_backward_count 60537  16.712%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.933319/  2.079944, val:  52.50%, val_best:  64.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6107%\n",
      "layer   2  Sparsity: 85.6936%\n",
      "layer   3  Sparsity: 86.9406%\n",
      "total_backward_count 372020 real_backward_count 61832  16.621%\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 2 self.abs_max_out: 1681.0\n",
      "lif layer 2 self.abs_max_v: 2028.0\n",
      "lif layer 2 self.abs_max_v: 2181.0\n",
      "lif layer 2 self.abs_max_v: 2242.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.950834/  2.041189, val:  52.92%, val_best:  64.58%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6616%\n",
      "layer   2  Sparsity: 85.8674%\n",
      "layer   3  Sparsity: 86.9261%\n",
      "total_backward_count 381810 real_backward_count 63170  16.545%\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.941607/  2.048887, val:  54.58%, val_best:  64.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6067%\n",
      "layer   2  Sparsity: 85.5126%\n",
      "layer   3  Sparsity: 86.4404%\n",
      "total_backward_count 391600 real_backward_count 64481  16.466%\n",
      "fc layer 2 self.abs_max_out: 1759.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.931771/  2.029094, val:  62.50%, val_best:  64.58%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6153%\n",
      "layer   2  Sparsity: 85.7028%\n",
      "layer   3  Sparsity: 86.8803%\n",
      "total_backward_count 401390 real_backward_count 65851  16.406%\n",
      "fc layer 2 self.abs_max_out: 1770.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.946389/  2.051707, val:  55.83%, val_best:  64.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5949%\n",
      "layer   2  Sparsity: 85.5101%\n",
      "layer   3  Sparsity: 87.2456%\n",
      "total_backward_count 411180 real_backward_count 67192  16.341%\n",
      "fc layer 2 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.922883/  2.046569, val:  55.42%, val_best:  64.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5818%\n",
      "layer   2  Sparsity: 85.3179%\n",
      "layer   3  Sparsity: 87.0867%\n",
      "total_backward_count 420970 real_backward_count 68469  16.265%\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.927371/  2.034551, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6525%\n",
      "layer   2  Sparsity: 84.5616%\n",
      "layer   3  Sparsity: 86.6246%\n",
      "total_backward_count 430760 real_backward_count 69746  16.191%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.911663/  2.024056, val:  61.67%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6250%\n",
      "layer   2  Sparsity: 84.6891%\n",
      "layer   3  Sparsity: 86.2694%\n",
      "total_backward_count 440550 real_backward_count 70995  16.115%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.899598/  2.025792, val:  49.17%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6276%\n",
      "layer   2  Sparsity: 84.6829%\n",
      "layer   3  Sparsity: 85.8038%\n",
      "total_backward_count 450340 real_backward_count 72305  16.056%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.907784/  2.033050, val:  51.67%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 86.6225%\n",
      "layer   2  Sparsity: 84.8152%\n",
      "layer   3  Sparsity: 86.0183%\n",
      "total_backward_count 460130 real_backward_count 73573  15.990%\n",
      "lif layer 1 self.abs_max_v: 7654.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.907941/  2.032631, val:  56.25%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6016%\n",
      "layer   2  Sparsity: 84.7396%\n",
      "layer   3  Sparsity: 86.2224%\n",
      "total_backward_count 469920 real_backward_count 74834  15.925%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.891017/  2.014273, val:  61.67%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6245%\n",
      "layer   2  Sparsity: 84.8322%\n",
      "layer   3  Sparsity: 86.2511%\n",
      "total_backward_count 479710 real_backward_count 76088  15.861%\n",
      "lif layer 1 self.abs_max_v: 7692.5\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.885281/  2.004443, val:  55.83%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5788%\n",
      "layer   2  Sparsity: 84.9293%\n",
      "layer   3  Sparsity: 86.1560%\n",
      "total_backward_count 489500 real_backward_count 77409  15.814%\n",
      "lif layer 1 self.abs_max_v: 8057.5\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.892736/  2.013181, val:  48.33%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6033%\n",
      "layer   2  Sparsity: 84.6338%\n",
      "layer   3  Sparsity: 86.0059%\n",
      "total_backward_count 499290 real_backward_count 78734  15.769%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.889770/  1.992316, val:  58.75%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5546%\n",
      "layer   2  Sparsity: 84.8326%\n",
      "layer   3  Sparsity: 85.7643%\n",
      "total_backward_count 509080 real_backward_count 79950  15.705%\n",
      "lif layer 2 self.abs_max_v: 2246.5\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.882015/  2.016299, val:  58.33%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6146%\n",
      "layer   2  Sparsity: 85.0591%\n",
      "layer   3  Sparsity: 85.8822%\n",
      "total_backward_count 518870 real_backward_count 81273  15.663%\n",
      "lif layer 2 self.abs_max_v: 2459.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.885893/  1.985824, val:  65.00%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5993%\n",
      "layer   2  Sparsity: 85.2939%\n",
      "layer   3  Sparsity: 85.9012%\n",
      "total_backward_count 528660 real_backward_count 82522  15.610%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.896096/  2.023062, val:  56.67%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5885%\n",
      "layer   2  Sparsity: 85.3218%\n",
      "layer   3  Sparsity: 86.2857%\n",
      "total_backward_count 538450 real_backward_count 83809  15.565%\n",
      "lif layer 2 self.abs_max_v: 2484.5\n",
      "lif layer 1 self.abs_max_v: 8132.5\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.914344/  2.021025, val:  62.92%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6256%\n",
      "layer   2  Sparsity: 85.2391%\n",
      "layer   3  Sparsity: 86.6895%\n",
      "total_backward_count 548240 real_backward_count 85078  15.518%\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.893803/  2.017963, val:  61.67%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6511%\n",
      "layer   2  Sparsity: 85.5071%\n",
      "layer   3  Sparsity: 86.4209%\n",
      "total_backward_count 558030 real_backward_count 86298  15.465%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.882808/  1.997974, val:  56.67%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6107%\n",
      "layer   2  Sparsity: 86.0394%\n",
      "layer   3  Sparsity: 86.1752%\n",
      "total_backward_count 567820 real_backward_count 87543  15.417%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.870127/  2.008292, val:  63.75%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5780%\n",
      "layer   2  Sparsity: 86.0272%\n",
      "layer   3  Sparsity: 86.3074%\n",
      "total_backward_count 577610 real_backward_count 88728  15.361%\n",
      "fc layer 1 self.abs_max_out: 5674.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.887233/  2.012670, val:  50.83%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5737%\n",
      "layer   2  Sparsity: 85.8119%\n",
      "layer   3  Sparsity: 86.3634%\n",
      "total_backward_count 587400 real_backward_count 89943  15.312%\n",
      "lif layer 1 self.abs_max_v: 8231.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.887098/  1.996966, val:  58.33%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6448%\n",
      "layer   2  Sparsity: 85.9190%\n",
      "layer   3  Sparsity: 86.5761%\n",
      "total_backward_count 597190 real_backward_count 91163  15.265%\n",
      "lif layer 1 self.abs_max_v: 8429.0\n",
      "lif layer 1 self.abs_max_v: 8435.5\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.878428/  2.006878, val:  59.17%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5787%\n",
      "layer   2  Sparsity: 86.1730%\n",
      "layer   3  Sparsity: 87.0035%\n",
      "total_backward_count 606980 real_backward_count 92395  15.222%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.899209/  2.022717, val:  64.17%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5459%\n",
      "layer   2  Sparsity: 85.8119%\n",
      "layer   3  Sparsity: 87.1112%\n",
      "total_backward_count 616770 real_backward_count 93645  15.183%\n",
      "lif layer 1 self.abs_max_v: 8642.5\n",
      "lif layer 1 self.abs_max_v: 8646.5\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.897904/  2.009294, val:  58.33%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6003%\n",
      "layer   2  Sparsity: 85.7273%\n",
      "layer   3  Sparsity: 86.5122%\n",
      "total_backward_count 626560 real_backward_count 94811  15.132%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.874194/  2.020928, val:  54.17%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5698%\n",
      "layer   2  Sparsity: 85.4249%\n",
      "layer   3  Sparsity: 86.2705%\n",
      "total_backward_count 636350 real_backward_count 96017  15.089%\n",
      "lif layer 1 self.abs_max_v: 9008.0\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.873810/  2.009851, val:  56.25%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5887%\n",
      "layer   2  Sparsity: 85.5838%\n",
      "layer   3  Sparsity: 86.6041%\n",
      "total_backward_count 646140 real_backward_count 97257  15.052%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.877120/  2.013934, val:  57.08%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5926%\n",
      "layer   2  Sparsity: 85.6408%\n",
      "layer   3  Sparsity: 86.2633%\n",
      "total_backward_count 655930 real_backward_count 98458  15.010%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.859125/  1.967712, val:  62.08%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5941%\n",
      "layer   2  Sparsity: 85.6470%\n",
      "layer   3  Sparsity: 85.6934%\n",
      "total_backward_count 665720 real_backward_count 99628  14.965%\n",
      "lif layer 1 self.abs_max_v: 9033.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.847865/  1.982153, val:  63.33%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5752%\n",
      "layer   2  Sparsity: 85.4658%\n",
      "layer   3  Sparsity: 86.3527%\n",
      "total_backward_count 675510 real_backward_count 100821  14.925%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.848330/  1.995817, val:  57.50%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6583%\n",
      "layer   2  Sparsity: 85.4242%\n",
      "layer   3  Sparsity: 86.3812%\n",
      "total_backward_count 685300 real_backward_count 101938  14.875%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.851703/  1.984193, val:  60.00%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6206%\n",
      "layer   2  Sparsity: 85.4859%\n",
      "layer   3  Sparsity: 86.1045%\n",
      "total_backward_count 695090 real_backward_count 103162  14.842%\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "lif layer 1 self.abs_max_v: 9301.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.834806/  1.966934, val:  52.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5910%\n",
      "layer   2  Sparsity: 85.5336%\n",
      "layer   3  Sparsity: 85.6528%\n",
      "total_backward_count 704880 real_backward_count 104374  14.807%\n",
      "lif layer 1 self.abs_max_v: 9386.5\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.824901/  1.947567, val:  64.58%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5990%\n",
      "layer   2  Sparsity: 85.3408%\n",
      "layer   3  Sparsity: 85.5358%\n",
      "total_backward_count 714670 real_backward_count 105539  14.768%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.827116/  1.978516, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6360%\n",
      "layer   2  Sparsity: 85.1759%\n",
      "layer   3  Sparsity: 85.2296%\n",
      "total_backward_count 724460 real_backward_count 106687  14.726%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.843041/  1.967987, val:  58.33%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5567%\n",
      "layer   2  Sparsity: 84.9686%\n",
      "layer   3  Sparsity: 85.8272%\n",
      "total_backward_count 734250 real_backward_count 107911  14.697%\n",
      "fc layer 1 self.abs_max_out: 5688.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.828132/  1.955078, val:  60.83%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5889%\n",
      "layer   2  Sparsity: 84.9403%\n",
      "layer   3  Sparsity: 85.3330%\n",
      "total_backward_count 744040 real_backward_count 108969  14.646%\n",
      "fc layer 1 self.abs_max_out: 5779.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.825982/  1.972470, val:  58.75%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5959%\n",
      "layer   2  Sparsity: 85.2593%\n",
      "layer   3  Sparsity: 85.3533%\n",
      "total_backward_count 753830 real_backward_count 110156  14.613%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.824661/  1.967105, val:  52.92%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6507%\n",
      "layer   2  Sparsity: 85.1911%\n",
      "layer   3  Sparsity: 85.6530%\n",
      "total_backward_count 763620 real_backward_count 111268  14.571%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.824868/  1.979885, val:  57.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5469%\n",
      "layer   2  Sparsity: 85.1112%\n",
      "layer   3  Sparsity: 85.9479%\n",
      "total_backward_count 773410 real_backward_count 112376  14.530%\n",
      "lif layer 2 self.abs_max_v: 2559.0\n",
      "lif layer 2 self.abs_max_v: 2561.5\n",
      "fc layer 1 self.abs_max_out: 5867.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.835194/  1.959975, val:  62.92%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6009%\n",
      "layer   2  Sparsity: 84.8995%\n",
      "layer   3  Sparsity: 86.0452%\n",
      "total_backward_count 783200 real_backward_count 113516  14.494%\n",
      "fc layer 1 self.abs_max_out: 5998.0\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.824589/  1.947658, val:  60.42%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6010%\n",
      "layer   2  Sparsity: 85.1450%\n",
      "layer   3  Sparsity: 86.2825%\n",
      "total_backward_count 792990 real_backward_count 114662  14.459%\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.828274/  2.003964, val:  50.42%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6179%\n",
      "layer   2  Sparsity: 85.2056%\n",
      "layer   3  Sparsity: 86.1468%\n",
      "total_backward_count 802780 real_backward_count 115773  14.422%\n",
      "lif layer 1 self.abs_max_v: 9617.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.852044/  1.961541, val:  62.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5942%\n",
      "layer   2  Sparsity: 85.2524%\n",
      "layer   3  Sparsity: 86.2726%\n",
      "total_backward_count 812570 real_backward_count 116885  14.385%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.838214/  1.981407, val:  57.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.6156%\n",
      "layer   2  Sparsity: 85.3919%\n",
      "layer   3  Sparsity: 86.0202%\n",
      "total_backward_count 822360 real_backward_count 118022  14.352%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.830754/  1.943408, val:  63.75%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5927%\n",
      "layer   2  Sparsity: 85.2630%\n",
      "layer   3  Sparsity: 85.8540%\n",
      "total_backward_count 832150 real_backward_count 119164  14.320%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.812598/  1.948570, val:  67.08%, val_best:  67.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6384%\n",
      "layer   2  Sparsity: 85.3181%\n",
      "layer   3  Sparsity: 85.1763%\n",
      "total_backward_count 841940 real_backward_count 120259  14.284%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.815517/  1.926750, val:  67.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6356%\n",
      "layer   2  Sparsity: 85.1228%\n",
      "layer   3  Sparsity: 85.3421%\n",
      "total_backward_count 851730 real_backward_count 121357  14.248%\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.806636/  1.943336, val:  67.08%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5991%\n",
      "layer   2  Sparsity: 85.3836%\n",
      "layer   3  Sparsity: 85.3161%\n",
      "total_backward_count 861520 real_backward_count 122472  14.216%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.805090/  1.922541, val:  62.08%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5968%\n",
      "layer   2  Sparsity: 84.9310%\n",
      "layer   3  Sparsity: 84.8813%\n",
      "total_backward_count 871310 real_backward_count 123578  14.183%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.795724/  1.947237, val:  70.42%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6047%\n",
      "layer   2  Sparsity: 84.7602%\n",
      "layer   3  Sparsity: 85.1002%\n",
      "total_backward_count 881100 real_backward_count 124702  14.153%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.802330/  1.943880, val:  58.75%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6556%\n",
      "layer   2  Sparsity: 84.8032%\n",
      "layer   3  Sparsity: 85.0481%\n",
      "total_backward_count 890890 real_backward_count 125802  14.121%\n",
      "fc layer 1 self.abs_max_out: 6209.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.799270/  1.918647, val:  62.92%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5956%\n",
      "layer   2  Sparsity: 84.6569%\n",
      "layer   3  Sparsity: 85.1545%\n",
      "total_backward_count 900680 real_backward_count 126907  14.090%\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "fc layer 3 self.abs_max_out: 332.0\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.798682/  1.919470, val:  64.58%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6218%\n",
      "layer   2  Sparsity: 84.8170%\n",
      "layer   3  Sparsity: 85.0691%\n",
      "total_backward_count 910470 real_backward_count 128019  14.061%\n",
      "lif layer 1 self.abs_max_v: 9863.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.802048/  1.946621, val:  62.92%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5966%\n",
      "layer   2  Sparsity: 85.1071%\n",
      "layer   3  Sparsity: 85.5688%\n",
      "total_backward_count 920260 real_backward_count 129099  14.029%\n",
      "lif layer 1 self.abs_max_v: 10274.0\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.809427/  1.940483, val:  67.50%, val_best:  70.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5645%\n",
      "layer   2  Sparsity: 85.5767%\n",
      "layer   3  Sparsity: 85.4527%\n",
      "total_backward_count 930050 real_backward_count 130158  13.995%\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.790294/  1.907440, val:  72.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.5947%\n",
      "layer   2  Sparsity: 85.4500%\n",
      "layer   3  Sparsity: 84.9354%\n",
      "total_backward_count 939840 real_backward_count 131291  13.970%\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.781500/  1.931664, val:  60.83%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.6304%\n",
      "layer   2  Sparsity: 85.5462%\n",
      "layer   3  Sparsity: 84.8487%\n",
      "total_backward_count 949630 real_backward_count 132348  13.937%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.780619/  1.919043, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5781%\n",
      "layer   2  Sparsity: 85.9694%\n",
      "layer   3  Sparsity: 84.8870%\n",
      "total_backward_count 959420 real_backward_count 133399  13.904%\n",
      "lif layer 1 self.abs_max_v: 10288.5\n",
      "lif layer 1 self.abs_max_v: 10886.5\n",
      "fc layer 1 self.abs_max_out: 6285.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.781507/  1.938089, val:  65.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5636%\n",
      "layer   2  Sparsity: 85.7039%\n",
      "layer   3  Sparsity: 85.3182%\n",
      "total_backward_count 969210 real_backward_count 134424  13.869%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.793836/  1.930640, val:  66.67%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5817%\n",
      "layer   2  Sparsity: 85.3904%\n",
      "layer   3  Sparsity: 85.3476%\n",
      "total_backward_count 979000 real_backward_count 135462  13.837%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.791052/  1.921118, val:  60.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6208%\n",
      "layer   2  Sparsity: 85.5621%\n",
      "layer   3  Sparsity: 85.2746%\n",
      "total_backward_count 988790 real_backward_count 136540  13.809%\n",
      "fc layer 1 self.abs_max_out: 6291.0\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.783538/  1.935577, val:  65.00%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6441%\n",
      "layer   2  Sparsity: 85.5430%\n",
      "layer   3  Sparsity: 84.7954%\n",
      "total_backward_count 998580 real_backward_count 137632  13.783%\n",
      "lif layer 2 self.abs_max_v: 2626.5\n",
      "lif layer 2 self.abs_max_v: 2665.5\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.778883/  1.906557, val:  61.25%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6427%\n",
      "layer   2  Sparsity: 85.2506%\n",
      "layer   3  Sparsity: 84.4154%\n",
      "total_backward_count 1008370 real_backward_count 138660  13.751%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.763937/  1.899586, val:  67.92%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.5313%\n",
      "layer   2  Sparsity: 85.2685%\n",
      "layer   3  Sparsity: 84.9582%\n",
      "total_backward_count 1018160 real_backward_count 139672  13.718%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.768684/  1.903195, val:  78.33%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5874%\n",
      "layer   2  Sparsity: 85.2461%\n",
      "layer   3  Sparsity: 84.8084%\n",
      "total_backward_count 1027950 real_backward_count 140713  13.689%\n",
      "lif layer 2 self.abs_max_v: 2787.0\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.765415/  1.914081, val:  63.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5503%\n",
      "layer   2  Sparsity: 85.2114%\n",
      "layer   3  Sparsity: 85.0655%\n",
      "total_backward_count 1037740 real_backward_count 141789  13.663%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.778359/  1.902337, val:  67.50%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6415%\n",
      "layer   2  Sparsity: 85.4045%\n",
      "layer   3  Sparsity: 85.2422%\n",
      "total_backward_count 1047530 real_backward_count 142747  13.627%\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "lif layer 2 self.abs_max_v: 2827.5\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.776260/  1.915525, val:  60.83%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5781%\n",
      "layer   2  Sparsity: 85.3307%\n",
      "layer   3  Sparsity: 85.3743%\n",
      "total_backward_count 1057320 real_backward_count 143749  13.596%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.768287/  1.909548, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6263%\n",
      "layer   2  Sparsity: 85.5196%\n",
      "layer   3  Sparsity: 85.5836%\n",
      "total_backward_count 1067110 real_backward_count 144795  13.569%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.776717/  1.907548, val:  72.50%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6016%\n",
      "layer   2  Sparsity: 85.4991%\n",
      "layer   3  Sparsity: 85.4300%\n",
      "total_backward_count 1076900 real_backward_count 145758  13.535%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.766303/  1.899206, val:  72.92%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5806%\n",
      "layer   2  Sparsity: 85.3357%\n",
      "layer   3  Sparsity: 85.3656%\n",
      "total_backward_count 1086690 real_backward_count 146784  13.507%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.781215/  1.925120, val:  68.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5895%\n",
      "layer   2  Sparsity: 85.2331%\n",
      "layer   3  Sparsity: 85.3532%\n",
      "total_backward_count 1096480 real_backward_count 147765  13.476%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.767432/  1.911707, val:  71.67%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5835%\n",
      "layer   2  Sparsity: 85.0967%\n",
      "layer   3  Sparsity: 85.2839%\n",
      "total_backward_count 1106270 real_backward_count 148736  13.445%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.763161/  1.900992, val:  65.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.6001%\n",
      "layer   2  Sparsity: 85.1446%\n",
      "layer   3  Sparsity: 84.7402%\n",
      "total_backward_count 1116060 real_backward_count 149701  13.413%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.767792/  1.922845, val:  60.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6138%\n",
      "layer   2  Sparsity: 85.1811%\n",
      "layer   3  Sparsity: 85.0941%\n",
      "total_backward_count 1125850 real_backward_count 150700  13.385%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.752424/  1.911499, val:  63.33%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6235%\n",
      "layer   2  Sparsity: 85.1828%\n",
      "layer   3  Sparsity: 85.2839%\n",
      "total_backward_count 1135640 real_backward_count 151646  13.353%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.770470/  1.905493, val:  64.58%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5900%\n",
      "layer   2  Sparsity: 85.1195%\n",
      "layer   3  Sparsity: 85.3995%\n",
      "total_backward_count 1145430 real_backward_count 152643  13.326%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.759848/  1.894900, val:  68.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6356%\n",
      "layer   2  Sparsity: 85.1895%\n",
      "layer   3  Sparsity: 85.0785%\n",
      "total_backward_count 1155220 real_backward_count 153600  13.296%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.756159/  1.891886, val:  57.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5907%\n",
      "layer   2  Sparsity: 85.4573%\n",
      "layer   3  Sparsity: 85.5171%\n",
      "total_backward_count 1165010 real_backward_count 154595  13.270%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.752851/  1.892203, val:  75.00%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.5741%\n",
      "layer   2  Sparsity: 84.9584%\n",
      "layer   3  Sparsity: 85.3849%\n",
      "total_backward_count 1174800 real_backward_count 155578  13.243%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.740297/  1.892983, val:  61.67%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6374%\n",
      "layer   2  Sparsity: 84.7161%\n",
      "layer   3  Sparsity: 85.1202%\n",
      "total_backward_count 1184590 real_backward_count 156518  13.213%\n",
      "lif layer 2 self.abs_max_v: 2863.5\n",
      "lif layer 2 self.abs_max_v: 2957.0\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.762922/  1.910684, val:  70.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5817%\n",
      "layer   2  Sparsity: 84.9892%\n",
      "layer   3  Sparsity: 85.4970%\n",
      "total_backward_count 1194380 real_backward_count 157498  13.187%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.764646/  1.923179, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6226%\n",
      "layer   2  Sparsity: 84.9926%\n",
      "layer   3  Sparsity: 85.4555%\n",
      "total_backward_count 1204170 real_backward_count 158524  13.165%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.781425/  1.900043, val:  70.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6303%\n",
      "layer   2  Sparsity: 84.9426%\n",
      "layer   3  Sparsity: 85.1132%\n",
      "total_backward_count 1213960 real_backward_count 159499  13.139%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.755104/  1.887647, val:  67.50%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6521%\n",
      "layer   2  Sparsity: 85.0233%\n",
      "layer   3  Sparsity: 84.8853%\n",
      "total_backward_count 1223750 real_backward_count 160478  13.114%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.750390/  1.909961, val:  70.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6259%\n",
      "layer   2  Sparsity: 85.1748%\n",
      "layer   3  Sparsity: 85.0721%\n",
      "total_backward_count 1233540 real_backward_count 161442  13.088%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.751141/  1.903713, val:  65.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6542%\n",
      "layer   2  Sparsity: 84.9661%\n",
      "layer   3  Sparsity: 84.8283%\n",
      "total_backward_count 1243330 real_backward_count 162433  13.064%\n",
      "fc layer 1 self.abs_max_out: 6394.0\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.738010/  1.868478, val:  72.92%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5644%\n",
      "layer   2  Sparsity: 84.8013%\n",
      "layer   3  Sparsity: 84.4179%\n",
      "total_backward_count 1253120 real_backward_count 163371  13.037%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.737431/  1.889989, val:  75.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6078%\n",
      "layer   2  Sparsity: 84.6691%\n",
      "layer   3  Sparsity: 84.7071%\n",
      "total_backward_count 1262910 real_backward_count 164333  13.012%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.743718/  1.890988, val:  76.67%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6032%\n",
      "layer   2  Sparsity: 84.4656%\n",
      "layer   3  Sparsity: 85.2767%\n",
      "total_backward_count 1272700 real_backward_count 165217  12.982%\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.743575/  1.891242, val:  60.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6268%\n",
      "layer   2  Sparsity: 84.6628%\n",
      "layer   3  Sparsity: 85.6217%\n",
      "total_backward_count 1282490 real_backward_count 166138  12.954%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.748437/  1.910018, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5828%\n",
      "layer   2  Sparsity: 84.6300%\n",
      "layer   3  Sparsity: 85.7407%\n",
      "total_backward_count 1292280 real_backward_count 167121  12.932%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.740897/  1.883005, val:  68.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6033%\n",
      "layer   2  Sparsity: 84.2936%\n",
      "layer   3  Sparsity: 85.0594%\n",
      "total_backward_count 1302070 real_backward_count 168051  12.906%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.736530/  1.888052, val:  64.17%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6283%\n",
      "layer   2  Sparsity: 84.1520%\n",
      "layer   3  Sparsity: 84.7112%\n",
      "total_backward_count 1311860 real_backward_count 169005  12.883%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.738132/  1.893216, val:  67.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6224%\n",
      "layer   2  Sparsity: 84.0509%\n",
      "layer   3  Sparsity: 84.5886%\n",
      "total_backward_count 1321650 real_backward_count 169931  12.857%\n",
      "fc layer 1 self.abs_max_out: 6570.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.737373/  1.864070, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5892%\n",
      "layer   2  Sparsity: 84.3134%\n",
      "layer   3  Sparsity: 84.6287%\n",
      "total_backward_count 1331440 real_backward_count 170869  12.833%\n",
      "fc layer 1 self.abs_max_out: 6582.0\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.728994/  1.863485, val:  72.50%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5303%\n",
      "layer   2  Sparsity: 84.5354%\n",
      "layer   3  Sparsity: 85.1608%\n",
      "total_backward_count 1341230 real_backward_count 171715  12.803%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.728869/  1.882586, val:  59.17%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5785%\n",
      "layer   2  Sparsity: 84.7614%\n",
      "layer   3  Sparsity: 85.0447%\n",
      "total_backward_count 1351020 real_backward_count 172606  12.776%\n",
      "fc layer 1 self.abs_max_out: 6763.0\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.726540/  1.875287, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6029%\n",
      "layer   2  Sparsity: 84.8356%\n",
      "layer   3  Sparsity: 85.1377%\n",
      "total_backward_count 1360810 real_backward_count 173575  12.755%\n",
      "fc layer 1 self.abs_max_out: 6964.0\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.733247/  1.876411, val:  71.67%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5891%\n",
      "layer   2  Sparsity: 84.6971%\n",
      "layer   3  Sparsity: 84.5978%\n",
      "total_backward_count 1370600 real_backward_count 174526  12.734%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.719485/  1.891165, val:  64.58%, val_best:  78.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.5747%\n",
      "layer   2  Sparsity: 84.5524%\n",
      "layer   3  Sparsity: 84.8953%\n",
      "total_backward_count 1380390 real_backward_count 175456  12.711%\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.723762/  1.874994, val:  70.42%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6012%\n",
      "layer   2  Sparsity: 84.6179%\n",
      "layer   3  Sparsity: 84.7594%\n",
      "total_backward_count 1390180 real_backward_count 176331  12.684%\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 387.0\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.720514/  1.869058, val:  65.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6260%\n",
      "layer   2  Sparsity: 84.8149%\n",
      "layer   3  Sparsity: 84.5885%\n",
      "total_backward_count 1399970 real_backward_count 177274  12.663%\n",
      "fc layer 1 self.abs_max_out: 7090.0\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.705032/  1.859932, val:  66.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6002%\n",
      "layer   2  Sparsity: 84.9824%\n",
      "layer   3  Sparsity: 84.7076%\n",
      "total_backward_count 1409760 real_backward_count 178169  12.638%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.717711/  1.871513, val:  63.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5518%\n",
      "layer   2  Sparsity: 85.0934%\n",
      "layer   3  Sparsity: 84.9913%\n",
      "total_backward_count 1419550 real_backward_count 179079  12.615%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.724846/  1.861868, val:  72.92%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6164%\n",
      "layer   2  Sparsity: 84.9836%\n",
      "layer   3  Sparsity: 85.1790%\n",
      "total_backward_count 1429340 real_backward_count 179987  12.592%\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.712216/  1.851797, val:  76.67%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5894%\n",
      "layer   2  Sparsity: 84.7132%\n",
      "layer   3  Sparsity: 84.4751%\n",
      "total_backward_count 1439130 real_backward_count 180901  12.570%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.693801/  1.850288, val:  72.92%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5515%\n",
      "layer   2  Sparsity: 85.0484%\n",
      "layer   3  Sparsity: 84.5438%\n",
      "total_backward_count 1448920 real_backward_count 181728  12.542%\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.703345/  1.870400, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6398%\n",
      "layer   2  Sparsity: 85.0467%\n",
      "layer   3  Sparsity: 84.2728%\n",
      "total_backward_count 1458710 real_backward_count 182626  12.520%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.708746/  1.855461, val:  72.50%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6182%\n",
      "layer   2  Sparsity: 84.9225%\n",
      "layer   3  Sparsity: 84.4859%\n",
      "total_backward_count 1468500 real_backward_count 183512  12.497%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.686902/  1.853602, val:  69.17%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 86.6135%\n",
      "layer   2  Sparsity: 85.0836%\n",
      "layer   3  Sparsity: 84.6974%\n",
      "total_backward_count 1478290 real_backward_count 184353  12.471%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.714888/  1.870733, val:  65.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5836%\n",
      "layer   2  Sparsity: 85.1548%\n",
      "layer   3  Sparsity: 85.1978%\n",
      "total_backward_count 1488080 real_backward_count 185225  12.447%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.705728/  1.848446, val:  67.92%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5883%\n",
      "layer   2  Sparsity: 85.1484%\n",
      "layer   3  Sparsity: 84.6025%\n",
      "total_backward_count 1497870 real_backward_count 186111  12.425%\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.685817/  1.846405, val:  72.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5803%\n",
      "layer   2  Sparsity: 85.0997%\n",
      "layer   3  Sparsity: 84.5250%\n",
      "total_backward_count 1507660 real_backward_count 187029  12.405%\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.687133/  1.859382, val:  68.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5860%\n",
      "layer   2  Sparsity: 85.2644%\n",
      "layer   3  Sparsity: 84.4098%\n",
      "total_backward_count 1517450 real_backward_count 187874  12.381%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.685402/  1.856901, val:  70.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6199%\n",
      "layer   2  Sparsity: 85.2417%\n",
      "layer   3  Sparsity: 84.4376%\n",
      "total_backward_count 1527240 real_backward_count 188754  12.359%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.683526/  1.822221, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5020%\n",
      "layer   2  Sparsity: 85.1594%\n",
      "layer   3  Sparsity: 83.8481%\n",
      "total_backward_count 1537030 real_backward_count 189619  12.337%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.677769/  1.838267, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6479%\n",
      "layer   2  Sparsity: 85.3797%\n",
      "layer   3  Sparsity: 84.1315%\n",
      "total_backward_count 1546820 real_backward_count 190500  12.316%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.696067/  1.860707, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5980%\n",
      "layer   2  Sparsity: 85.4327%\n",
      "layer   3  Sparsity: 84.5924%\n",
      "total_backward_count 1556610 real_backward_count 191385  12.295%\n",
      "lif layer 1 self.abs_max_v: 10938.5\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.703233/  1.849158, val:  70.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6025%\n",
      "layer   2  Sparsity: 85.3473%\n",
      "layer   3  Sparsity: 84.3895%\n",
      "total_backward_count 1566400 real_backward_count 192253  12.274%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.705781/  1.859017, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6014%\n",
      "layer   2  Sparsity: 85.3612%\n",
      "layer   3  Sparsity: 84.6536%\n",
      "total_backward_count 1576190 real_backward_count 193119  12.252%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.704350/  1.855823, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5923%\n",
      "layer   2  Sparsity: 85.5087%\n",
      "layer   3  Sparsity: 84.0628%\n",
      "total_backward_count 1585980 real_backward_count 193995  12.232%\n",
      "lif layer 1 self.abs_max_v: 11022.5\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.683785/  1.833341, val:  70.00%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5790%\n",
      "layer   2  Sparsity: 85.3647%\n",
      "layer   3  Sparsity: 83.4885%\n",
      "total_backward_count 1595770 real_backward_count 194860  12.211%\n",
      "lif layer 1 self.abs_max_v: 11071.5\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.674550/  1.855408, val:  67.92%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5765%\n",
      "layer   2  Sparsity: 85.3616%\n",
      "layer   3  Sparsity: 83.7496%\n",
      "total_backward_count 1605560 real_backward_count 195707  12.189%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.694892/  1.865417, val:  70.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6465%\n",
      "layer   2  Sparsity: 85.1276%\n",
      "layer   3  Sparsity: 84.3247%\n",
      "total_backward_count 1615350 real_backward_count 196542  12.167%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.680002/  1.841527, val:  67.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6129%\n",
      "layer   2  Sparsity: 85.1212%\n",
      "layer   3  Sparsity: 84.0048%\n",
      "total_backward_count 1625140 real_backward_count 197424  12.148%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.680711/  1.844991, val:  69.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5768%\n",
      "layer   2  Sparsity: 85.2827%\n",
      "layer   3  Sparsity: 83.8699%\n",
      "total_backward_count 1634930 real_backward_count 198254  12.126%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.679202/  1.847283, val:  60.83%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5848%\n",
      "layer   2  Sparsity: 85.2939%\n",
      "layer   3  Sparsity: 84.7861%\n",
      "total_backward_count 1644720 real_backward_count 199126  12.107%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.678253/  1.849535, val:  64.17%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5890%\n",
      "layer   2  Sparsity: 85.2616%\n",
      "layer   3  Sparsity: 84.5655%\n",
      "total_backward_count 1654510 real_backward_count 199987  12.087%\n",
      "lif layer 1 self.abs_max_v: 11432.0\n",
      "lif layer 2 self.abs_max_v: 2972.5\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.674929/  1.835470, val:  65.83%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6513%\n",
      "layer   2  Sparsity: 84.9696%\n",
      "layer   3  Sparsity: 84.4491%\n",
      "total_backward_count 1664300 real_backward_count 200852  12.068%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.675470/  1.850455, val:  71.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5824%\n",
      "layer   2  Sparsity: 84.8993%\n",
      "layer   3  Sparsity: 84.5797%\n",
      "total_backward_count 1674090 real_backward_count 201723  12.050%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.694861/  1.851728, val:  67.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5941%\n",
      "layer   2  Sparsity: 84.8863%\n",
      "layer   3  Sparsity: 84.8547%\n",
      "total_backward_count 1683880 real_backward_count 202569  12.030%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.694860/  1.847954, val:  62.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5528%\n",
      "layer   2  Sparsity: 84.6784%\n",
      "layer   3  Sparsity: 84.4704%\n",
      "total_backward_count 1693670 real_backward_count 203407  12.010%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.673789/  1.819550, val:  67.92%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5888%\n",
      "layer   2  Sparsity: 84.7932%\n",
      "layer   3  Sparsity: 84.1503%\n",
      "total_backward_count 1703460 real_backward_count 204209  11.988%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.669268/  1.842152, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5808%\n",
      "layer   2  Sparsity: 84.6934%\n",
      "layer   3  Sparsity: 84.3558%\n",
      "total_backward_count 1713250 real_backward_count 205018  11.967%\n",
      "lif layer 2 self.abs_max_v: 3004.0\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.676594/  1.823505, val:  73.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6157%\n",
      "layer   2  Sparsity: 84.4612%\n",
      "layer   3  Sparsity: 84.5417%\n",
      "total_backward_count 1723040 real_backward_count 205837  11.946%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.664838/  1.831159, val:  74.17%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5798%\n",
      "layer   2  Sparsity: 84.5846%\n",
      "layer   3  Sparsity: 84.3123%\n",
      "total_backward_count 1732830 real_backward_count 206656  11.926%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.657724/  1.831365, val:  74.17%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.6023%\n",
      "layer   2  Sparsity: 84.4767%\n",
      "layer   3  Sparsity: 84.2411%\n",
      "total_backward_count 1742620 real_backward_count 207498  11.907%\n",
      "lif layer 1 self.abs_max_v: 11444.0\n",
      "lif layer 2 self.abs_max_v: 3074.5\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.669858/  1.848583, val:  70.83%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5982%\n",
      "layer   2  Sparsity: 84.5290%\n",
      "layer   3  Sparsity: 84.2621%\n",
      "total_backward_count 1752410 real_backward_count 208274  11.885%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.672943/  1.833539, val:  77.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5832%\n",
      "layer   2  Sparsity: 84.4081%\n",
      "layer   3  Sparsity: 84.1219%\n",
      "total_backward_count 1762200 real_backward_count 209074  11.864%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.658596/  1.817999, val:  72.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5836%\n",
      "layer   2  Sparsity: 84.5984%\n",
      "layer   3  Sparsity: 84.0649%\n",
      "total_backward_count 1771990 real_backward_count 209903  11.846%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.661505/  1.814195, val:  75.00%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5637%\n",
      "layer   2  Sparsity: 84.7634%\n",
      "layer   3  Sparsity: 84.3147%\n",
      "total_backward_count 1781780 real_backward_count 210748  11.828%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.648139/  1.816929, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6113%\n",
      "layer   2  Sparsity: 84.7737%\n",
      "layer   3  Sparsity: 83.9936%\n",
      "total_backward_count 1791570 real_backward_count 211541  11.808%\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.656540/  1.816174, val:  68.33%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5624%\n",
      "layer   2  Sparsity: 84.8334%\n",
      "layer   3  Sparsity: 84.1802%\n",
      "total_backward_count 1801360 real_backward_count 212323  11.787%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.659164/  1.837679, val:  74.58%, val_best:  78.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.5943%\n",
      "layer   2  Sparsity: 84.9571%\n",
      "layer   3  Sparsity: 84.5857%\n",
      "total_backward_count 1811150 real_backward_count 213134  11.768%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.669465/  1.821029, val:  70.42%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5939%\n",
      "layer   2  Sparsity: 84.8047%\n",
      "layer   3  Sparsity: 84.5924%\n",
      "total_backward_count 1820940 real_backward_count 213961  11.750%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.654745/  1.790411, val:  73.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5999%\n",
      "layer   2  Sparsity: 84.5926%\n",
      "layer   3  Sparsity: 83.9545%\n",
      "total_backward_count 1830730 real_backward_count 214776  11.732%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.640786/  1.821458, val:  77.92%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.6119%\n",
      "layer   2  Sparsity: 84.6276%\n",
      "layer   3  Sparsity: 84.1613%\n",
      "total_backward_count 1840520 real_backward_count 215587  11.713%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.641709/  1.793523, val:  72.50%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5984%\n",
      "layer   2  Sparsity: 84.6744%\n",
      "layer   3  Sparsity: 84.4415%\n",
      "total_backward_count 1850310 real_backward_count 216350  11.693%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.639157/  1.802128, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5738%\n",
      "layer   2  Sparsity: 84.4923%\n",
      "layer   3  Sparsity: 84.4554%\n",
      "total_backward_count 1860100 real_backward_count 217171  11.675%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.648405/  1.789812, val:  74.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.6359%\n",
      "layer   2  Sparsity: 84.8460%\n",
      "layer   3  Sparsity: 84.5012%\n",
      "total_backward_count 1869890 real_backward_count 217977  11.657%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.642912/  1.809636, val:  69.17%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.6012%\n",
      "layer   2  Sparsity: 85.1468%\n",
      "layer   3  Sparsity: 84.3488%\n",
      "total_backward_count 1879680 real_backward_count 218803  11.640%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.648972/  1.824247, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.6733%\n",
      "layer   2  Sparsity: 85.2311%\n",
      "layer   3  Sparsity: 84.2330%\n",
      "total_backward_count 1889470 real_backward_count 219620  11.623%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.659405/  1.828444, val:  72.08%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.66 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.5955%\n",
      "layer   2  Sparsity: 85.2461%\n",
      "layer   3  Sparsity: 84.7660%\n",
      "total_backward_count 1899260 real_backward_count 220394  11.604%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.672743/  1.824542, val:  72.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5909%\n",
      "layer   2  Sparsity: 85.3122%\n",
      "layer   3  Sparsity: 85.3308%\n",
      "total_backward_count 1909050 real_backward_count 221130  11.583%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.669451/  1.827021, val:  66.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6398%\n",
      "layer   2  Sparsity: 85.2187%\n",
      "layer   3  Sparsity: 84.8377%\n",
      "total_backward_count 1918840 real_backward_count 221927  11.566%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.666766/  1.815391, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.6334%\n",
      "layer   2  Sparsity: 85.1627%\n",
      "layer   3  Sparsity: 84.9887%\n",
      "total_backward_count 1928630 real_backward_count 222739  11.549%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.648490/  1.821799, val:  64.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5642%\n",
      "layer   2  Sparsity: 85.5564%\n",
      "layer   3  Sparsity: 84.6207%\n",
      "total_backward_count 1938420 real_backward_count 223522  11.531%\n",
      "fc layer 2 self.abs_max_out: 1900.0\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.658211/  1.815063, val:  75.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.5981%\n",
      "layer   2  Sparsity: 85.5236%\n",
      "layer   3  Sparsity: 84.8994%\n",
      "total_backward_count 1948210 real_backward_count 224268  11.511%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.656543/  1.838949, val:  63.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.5575%\n",
      "layer   2  Sparsity: 85.2327%\n",
      "layer   3  Sparsity: 84.6916%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad42cc109e4e4786abb097976187efc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.65654</td></tr><tr><td>val_acc_best</td><td>0.83333</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>1.83895</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">expert-sweep-127</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sypcffri' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sypcffri</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_042701-sypcffri/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llxyi5j2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_084347-llxyi5j2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llxyi5j2' target=\"_blank\">earthy-sweep-132</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llxyi5j2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llxyi5j2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_084355_884', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "lif layer 1 self.abs_max_v: 348.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "lif layer 2 self.abs_max_v: 754.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 1 self.abs_max_out: 414.0\n",
      "lif layer 1 self.abs_max_v: 442.0\n",
      "fc layer 2 self.abs_max_out: 1072.0\n",
      "lif layer 2 self.abs_max_v: 1417.0\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "lif layer 1 self.abs_max_v: 534.0\n",
      "lif layer 2 self.abs_max_v: 1615.5\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 565.5\n",
      "lif layer 1 self.abs_max_v: 728.5\n",
      "lif layer 2 self.abs_max_v: 1711.0\n",
      "fc layer 1 self.abs_max_out: 512.0\n",
      "lif layer 1 self.abs_max_v: 843.5\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "fc layer 1 self.abs_max_out: 615.0\n",
      "lif layer 1 self.abs_max_v: 1037.0\n",
      "lif layer 2 self.abs_max_v: 1733.0\n",
      "fc layer 2 self.abs_max_out: 1317.0\n",
      "fc layer 1 self.abs_max_out: 786.0\n",
      "lif layer 1 self.abs_max_v: 1091.0\n",
      "lif layer 1 self.abs_max_v: 1245.5\n",
      "fc layer 1 self.abs_max_out: 907.0\n",
      "lif layer 2 self.abs_max_v: 1829.0\n",
      "fc layer 1 self.abs_max_out: 1000.0\n",
      "fc layer 2 self.abs_max_out: 1445.0\n",
      "lif layer 2 self.abs_max_v: 2168.5\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 1 self.abs_max_out: 1043.0\n",
      "lif layer 1 self.abs_max_v: 1328.0\n",
      "lif layer 1 self.abs_max_v: 1347.5\n",
      "lif layer 1 self.abs_max_v: 1433.0\n",
      "lif layer 1 self.abs_max_v: 1691.5\n",
      "lif layer 2 self.abs_max_v: 2178.5\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 2 self.abs_max_v: 2252.5\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "fc layer 1 self.abs_max_out: 1132.0\n",
      "fc layer 2 self.abs_max_out: 1484.0\n",
      "lif layer 2 self.abs_max_v: 2317.5\n",
      "fc layer 1 self.abs_max_out: 1178.0\n",
      "lif layer 1 self.abs_max_v: 1694.0\n",
      "fc layer 1 self.abs_max_out: 1317.0\n",
      "fc layer 1 self.abs_max_out: 1388.0\n",
      "lif layer 2 self.abs_max_v: 2397.0\n",
      "lif layer 1 self.abs_max_v: 1748.5\n",
      "lif layer 1 self.abs_max_v: 1935.5\n",
      "lif layer 1 self.abs_max_v: 2009.0\n",
      "lif layer 1 self.abs_max_v: 2144.0\n",
      "lif layer 1 self.abs_max_v: 2172.0\n",
      "fc layer 1 self.abs_max_out: 1510.0\n",
      "lif layer 1 self.abs_max_v: 2483.0\n",
      "fc layer 1 self.abs_max_out: 1610.0\n",
      "lif layer 1 self.abs_max_v: 2613.5\n",
      "fc layer 1 self.abs_max_out: 1745.0\n",
      "lif layer 2 self.abs_max_v: 2521.0\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "fc layer 2 self.abs_max_out: 1521.0\n",
      "fc layer 1 self.abs_max_out: 1951.0\n",
      "lif layer 2 self.abs_max_v: 2609.5\n",
      "lif layer 2 self.abs_max_v: 2650.5\n",
      "fc layer 2 self.abs_max_out: 1702.0\n",
      "lif layer 2 self.abs_max_v: 2651.0\n",
      "lif layer 1 self.abs_max_v: 2833.0\n",
      "lif layer 1 self.abs_max_v: 2840.5\n",
      "lif layer 2 self.abs_max_v: 2864.5\n",
      "lif layer 2 self.abs_max_v: 2880.5\n",
      "fc layer 1 self.abs_max_out: 2068.0\n",
      "lif layer 1 self.abs_max_v: 3038.5\n",
      "lif layer 2 self.abs_max_v: 2982.5\n",
      "lif layer 2 self.abs_max_v: 3005.0\n",
      "fc layer 2 self.abs_max_out: 1721.0\n",
      "lif layer 2 self.abs_max_v: 3036.0\n",
      "lif layer 2 self.abs_max_v: 3056.0\n",
      "lif layer 2 self.abs_max_v: 3061.0\n",
      "lif layer 2 self.abs_max_v: 3144.5\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "lif layer 2 self.abs_max_v: 3198.5\n",
      "fc layer 2 self.abs_max_out: 2093.0\n",
      "lif layer 2 self.abs_max_v: 3383.0\n",
      "lif layer 2 self.abs_max_v: 3407.5\n",
      "lif layer 2 self.abs_max_v: 3583.0\n",
      "lif layer 2 self.abs_max_v: 3613.5\n",
      "lif layer 2 self.abs_max_v: 3695.5\n",
      "lif layer 2 self.abs_max_v: 3848.5\n",
      "lif layer 2 self.abs_max_v: 3987.5\n",
      "fc layer 2 self.abs_max_out: 2247.0\n",
      "lif layer 1 self.abs_max_v: 3135.5\n",
      "fc layer 1 self.abs_max_out: 2109.0\n",
      "lif layer 1 self.abs_max_v: 3200.5\n",
      "fc layer 1 self.abs_max_out: 2123.0\n",
      "fc layer 1 self.abs_max_out: 2124.0\n",
      "fc layer 1 self.abs_max_out: 2155.0\n",
      "fc layer 1 self.abs_max_out: 2513.0\n",
      "lif layer 1 self.abs_max_v: 3217.0\n",
      "lif layer 1 self.abs_max_v: 3673.0\n",
      "fc layer 1 self.abs_max_out: 2690.0\n",
      "lif layer 2 self.abs_max_v: 4150.5\n",
      "lif layer 1 self.abs_max_v: 3741.5\n",
      "lif layer 1 self.abs_max_v: 3753.5\n",
      "lif layer 1 self.abs_max_v: 3756.0\n",
      "lif layer 1 self.abs_max_v: 3846.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "lif layer 2 self.abs_max_v: 4180.0\n",
      "lif layer 1 self.abs_max_v: 3938.5\n",
      "fc layer 1 self.abs_max_out: 2860.0\n",
      "fc layer 2 self.abs_max_out: 2333.0\n",
      "fc layer 2 self.abs_max_out: 2368.0\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "lif layer 1 self.abs_max_v: 4013.5\n",
      "lif layer 2 self.abs_max_v: 4294.5\n",
      "lif layer 1 self.abs_max_v: 4159.0\n",
      "lif layer 2 self.abs_max_v: 4297.0\n",
      "lif layer 1 self.abs_max_v: 4201.0\n",
      "lif layer 2 self.abs_max_v: 4442.5\n",
      "lif layer 1 self.abs_max_v: 4311.5\n",
      "fc layer 2 self.abs_max_out: 2438.0\n",
      "fc layer 2 self.abs_max_out: 2493.0\n",
      "fc layer 2 self.abs_max_out: 2625.0\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "lif layer 1 self.abs_max_v: 4468.0\n",
      "lif layer 1 self.abs_max_v: 5126.5\n",
      "lif layer 1 self.abs_max_v: 5349.0\n",
      "lif layer 2 self.abs_max_v: 4539.5\n",
      "fc layer 2 self.abs_max_out: 2634.0\n",
      "fc layer 2 self.abs_max_out: 2635.0\n",
      "fc layer 2 self.abs_max_out: 2664.0\n",
      "lif layer 2 self.abs_max_v: 4706.0\n",
      "fc layer 1 self.abs_max_out: 3458.0\n",
      "lif layer 1 self.abs_max_v: 5404.0\n",
      "lif layer 1 self.abs_max_v: 5596.0\n",
      "fc layer 2 self.abs_max_out: 2769.0\n",
      "fc layer 1 self.abs_max_out: 3709.0\n",
      "lif layer 2 self.abs_max_v: 4775.5\n",
      "lif layer 2 self.abs_max_v: 4878.0\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "fc layer 1 self.abs_max_out: 3762.0\n",
      "lif layer 1 self.abs_max_v: 6234.0\n",
      "lif layer 1 self.abs_max_v: 6551.0\n",
      "fc layer 1 self.abs_max_out: 3777.0\n",
      "fc layer 1 self.abs_max_out: 3780.0\n",
      "fc layer 1 self.abs_max_out: 3845.0\n",
      "lif layer 2 self.abs_max_v: 4894.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.831545/  2.021356, val:  33.33%, val_best:  33.33%, tr:  97.04%, tr_best:  97.04%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 63.9392%\n",
      "layer   3  Sparsity: 57.4622%\n",
      "total_backward_count 9790 real_backward_count 2176  22.227%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 6574.0\n",
      "fc layer 1 self.abs_max_out: 3888.0\n",
      "lif layer 2 self.abs_max_v: 4949.5\n",
      "lif layer 2 self.abs_max_v: 4956.0\n",
      "fc layer 3 self.abs_max_out: 782.0\n",
      "fc layer 2 self.abs_max_out: 2817.0\n",
      "lif layer 2 self.abs_max_v: 5122.0\n",
      "lif layer 2 self.abs_max_v: 5210.0\n",
      "fc layer 1 self.abs_max_out: 4125.0\n",
      "lif layer 1 self.abs_max_v: 6765.5\n",
      "fc layer 1 self.abs_max_out: 4320.0\n",
      "lif layer 1 self.abs_max_v: 7015.0\n",
      "fc layer 2 self.abs_max_out: 2851.0\n",
      "fc layer 2 self.abs_max_out: 2882.0\n",
      "lif layer 1 self.abs_max_v: 7450.5\n",
      "lif layer 2 self.abs_max_v: 5238.0\n",
      "lif layer 2 self.abs_max_v: 5399.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.801444/  1.974228, val:  41.25%, val_best:  41.25%, tr:  98.98%, tr_best:  98.98%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 67.7636%\n",
      "layer   3  Sparsity: 59.2768%\n",
      "total_backward_count 19580 real_backward_count 3831  19.566%\n",
      "fc layer 1 self.abs_max_out: 4492.0\n",
      "fc layer 2 self.abs_max_out: 2966.0\n",
      "fc layer 2 self.abs_max_out: 3113.0\n",
      "fc layer 2 self.abs_max_out: 3164.0\n",
      "fc layer 2 self.abs_max_out: 3249.0\n",
      "lif layer 2 self.abs_max_v: 5715.0\n",
      "lif layer 2 self.abs_max_v: 5750.5\n",
      "fc layer 2 self.abs_max_out: 3297.0\n",
      "fc layer 2 self.abs_max_out: 3386.0\n",
      "fc layer 2 self.abs_max_out: 3446.0\n",
      "fc layer 2 self.abs_max_out: 3461.0\n",
      "fc layer 2 self.abs_max_out: 3466.0\n",
      "fc layer 2 self.abs_max_out: 3481.0\n",
      "fc layer 1 self.abs_max_out: 4772.0\n",
      "lif layer 1 self.abs_max_v: 7788.0\n",
      "lif layer 1 self.abs_max_v: 8149.0\n",
      "lif layer 2 self.abs_max_v: 5795.5\n",
      "lif layer 2 self.abs_max_v: 5805.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.775862/  1.994172, val:  37.50%, val_best:  41.25%, tr:  98.57%, tr_best:  98.98%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 68.9882%\n",
      "layer   3  Sparsity: 58.9016%\n",
      "total_backward_count 29370 real_backward_count 5371  18.287%\n",
      "fc layer 2 self.abs_max_out: 3498.0\n",
      "fc layer 2 self.abs_max_out: 3512.0\n",
      "lif layer 2 self.abs_max_v: 5913.0\n",
      "lif layer 2 self.abs_max_v: 5971.5\n",
      "lif layer 2 self.abs_max_v: 6007.0\n",
      "lif layer 2 self.abs_max_v: 6103.5\n",
      "fc layer 3 self.abs_max_out: 830.0\n",
      "fc layer 1 self.abs_max_out: 5108.0\n",
      "fc layer 1 self.abs_max_out: 5285.0\n",
      "lif layer 1 self.abs_max_v: 8513.0\n",
      "lif layer 1 self.abs_max_v: 9052.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.767090/  1.949628, val:  43.75%, val_best:  43.75%, tr:  99.08%, tr_best:  99.08%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.4185%\n",
      "layer   3  Sparsity: 59.0939%\n",
      "total_backward_count 39160 real_backward_count 6887  17.587%\n",
      "fc layer 2 self.abs_max_out: 3568.0\n",
      "fc layer 2 self.abs_max_out: 3612.0\n",
      "fc layer 1 self.abs_max_out: 5652.0\n",
      "lif layer 1 self.abs_max_v: 9106.5\n",
      "lif layer 1 self.abs_max_v: 9651.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.743115/  1.954040, val:  41.67%, val_best:  43.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.0781%\n",
      "layer   3  Sparsity: 59.9412%\n",
      "total_backward_count 48950 real_backward_count 8274  16.903%\n",
      "lif layer 2 self.abs_max_v: 6129.0\n",
      "fc layer 2 self.abs_max_out: 3671.0\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "fc layer 1 self.abs_max_out: 5801.0\n",
      "lif layer 1 self.abs_max_v: 9996.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.719736/  1.954955, val:  38.75%, val_best:  43.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.5426%\n",
      "layer   3  Sparsity: 60.8436%\n",
      "total_backward_count 58740 real_backward_count 9695  16.505%\n",
      "fc layer 2 self.abs_max_out: 3683.0\n",
      "fc layer 2 self.abs_max_out: 3796.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.734715/  1.926719, val:  52.08%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 73.5669%\n",
      "layer   3  Sparsity: 60.5849%\n",
      "total_backward_count 68530 real_backward_count 10973  16.012%\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "fc layer 3 self.abs_max_out: 889.0\n",
      "fc layer 1 self.abs_max_out: 5856.0\n",
      "lif layer 1 self.abs_max_v: 10013.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.712726/  1.884363, val:  54.58%, val_best:  54.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 73.8920%\n",
      "layer   3  Sparsity: 60.7605%\n",
      "total_backward_count 78320 real_backward_count 12217  15.599%\n",
      "fc layer 3 self.abs_max_out: 911.0\n",
      "fc layer 2 self.abs_max_out: 3958.0\n",
      "fc layer 1 self.abs_max_out: 5918.0\n",
      "lif layer 1 self.abs_max_v: 10340.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.697863/  1.854190, val:  60.00%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 73.7097%\n",
      "layer   3  Sparsity: 61.4724%\n",
      "total_backward_count 88110 real_backward_count 13455  15.271%\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 963.0\n",
      "fc layer 1 self.abs_max_out: 5992.0\n",
      "lif layer 1 self.abs_max_v: 10531.0\n",
      "lif layer 2 self.abs_max_v: 6411.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.670251/  1.872076, val:  50.42%, val_best:  60.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 73.8207%\n",
      "layer   3  Sparsity: 61.4113%\n",
      "total_backward_count 97900 real_backward_count 14600  14.913%\n",
      "lif layer 2 self.abs_max_v: 6438.0\n",
      "fc layer 2 self.abs_max_out: 3972.0\n",
      "fc layer 1 self.abs_max_out: 6105.0\n",
      "lif layer 1 self.abs_max_v: 10760.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.662878/  1.875004, val:  48.75%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 73.3932%\n",
      "layer   3  Sparsity: 61.4689%\n",
      "total_backward_count 107690 real_backward_count 15716  14.594%\n",
      "lif layer 2 self.abs_max_v: 6497.5\n",
      "fc layer 1 self.abs_max_out: 6195.0\n",
      "lif layer 1 self.abs_max_v: 11104.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.670677/  1.853366, val:  56.67%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.7203%\n",
      "layer   3  Sparsity: 62.2837%\n",
      "total_backward_count 117480 real_backward_count 16876  14.365%\n",
      "lif layer 2 self.abs_max_v: 6557.5\n",
      "lif layer 2 self.abs_max_v: 6884.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.655075/  1.858955, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.6715%\n",
      "layer   3  Sparsity: 63.1894%\n",
      "total_backward_count 127270 real_backward_count 17892  14.058%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.660682/  1.865548, val:  49.58%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.1855%\n",
      "layer   3  Sparsity: 63.7816%\n",
      "total_backward_count 137060 real_backward_count 18935  13.815%\n",
      "fc layer 1 self.abs_max_out: 6284.0\n",
      "fc layer 1 self.abs_max_out: 6397.0\n",
      "lif layer 1 self.abs_max_v: 11481.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.646019/  1.830995, val:  60.42%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.8128%\n",
      "layer   3  Sparsity: 63.8655%\n",
      "total_backward_count 146850 real_backward_count 19912  13.559%\n",
      "fc layer 1 self.abs_max_out: 6509.0\n",
      "lif layer 1 self.abs_max_v: 11600.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.633651/  1.845694, val:  56.25%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.2836%\n",
      "layer   3  Sparsity: 64.1731%\n",
      "total_backward_count 156640 real_backward_count 20855  13.314%\n",
      "fc layer 3 self.abs_max_out: 968.0\n",
      "fc layer 1 self.abs_max_out: 6636.0\n",
      "lif layer 1 self.abs_max_v: 11799.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.620988/  1.825607, val:  57.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 72.0815%\n",
      "layer   3  Sparsity: 64.3130%\n",
      "total_backward_count 166430 real_backward_count 21819  13.110%\n",
      "fc layer 1 self.abs_max_out: 6706.0\n",
      "lif layer 1 self.abs_max_v: 11930.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.605355/  1.806438, val:  65.83%, val_best:  65.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.9851%\n",
      "layer   3  Sparsity: 64.6607%\n",
      "total_backward_count 176220 real_backward_count 22737  12.903%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.605718/  1.803471, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.3954%\n",
      "layer   3  Sparsity: 64.7934%\n",
      "total_backward_count 186010 real_backward_count 23618  12.697%\n",
      "fc layer 2 self.abs_max_out: 4008.0\n",
      "fc layer 2 self.abs_max_out: 4120.0\n",
      "fc layer 1 self.abs_max_out: 6731.0\n",
      "lif layer 1 self.abs_max_v: 12016.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.586917/  1.828531, val:  57.08%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.4447%\n",
      "layer   3  Sparsity: 64.8849%\n",
      "total_backward_count 195800 real_backward_count 24431  12.478%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.581154/  1.788838, val:  68.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.4479%\n",
      "layer   3  Sparsity: 64.6354%\n",
      "total_backward_count 205590 real_backward_count 25243  12.278%\n",
      "fc layer 1 self.abs_max_out: 6914.0\n",
      "lif layer 1 self.abs_max_v: 12349.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.575934/  1.779106, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.1604%\n",
      "layer   3  Sparsity: 65.3710%\n",
      "total_backward_count 215380 real_backward_count 25987  12.066%\n",
      "fc layer 1 self.abs_max_out: 6938.0\n",
      "lif layer 1 self.abs_max_v: 12459.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.572047/  1.751262, val:  72.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.1208%\n",
      "layer   3  Sparsity: 65.0698%\n",
      "total_backward_count 225170 real_backward_count 26673  11.846%\n",
      "fc layer 1 self.abs_max_out: 7088.0\n",
      "lif layer 1 self.abs_max_v: 12673.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.547256/  1.740304, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.3558%\n",
      "layer   3  Sparsity: 65.0527%\n",
      "total_backward_count 234960 real_backward_count 27376  11.651%\n",
      "fc layer 1 self.abs_max_out: 7150.0\n",
      "lif layer 1 self.abs_max_v: 12793.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.531217/  1.726099, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.6858%\n",
      "layer   3  Sparsity: 65.3754%\n",
      "total_backward_count 244750 real_backward_count 28031  11.453%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.527225/  1.705988, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.0989%\n",
      "layer   3  Sparsity: 65.1692%\n",
      "total_backward_count 254540 real_backward_count 28734  11.289%\n",
      "fc layer 2 self.abs_max_out: 4127.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.517663/  1.728481, val:  68.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.9093%\n",
      "layer   3  Sparsity: 65.5595%\n",
      "total_backward_count 264330 real_backward_count 29383  11.116%\n",
      "lif layer 2 self.abs_max_v: 7012.0\n",
      "lif layer 2 self.abs_max_v: 7016.0\n",
      "fc layer 2 self.abs_max_out: 4328.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.510968/  1.703756, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.3441%\n",
      "layer   3  Sparsity: 66.3028%\n",
      "total_backward_count 274120 real_backward_count 29960  10.930%\n",
      "fc layer 1 self.abs_max_out: 7373.0\n",
      "lif layer 1 self.abs_max_v: 13098.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.501157/  1.722316, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 71.1682%\n",
      "layer   3  Sparsity: 65.8386%\n",
      "total_backward_count 283910 real_backward_count 30528  10.753%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.493639/  1.686592, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.8036%\n",
      "layer   3  Sparsity: 65.5890%\n",
      "total_backward_count 293700 real_backward_count 31082  10.583%\n",
      "fc layer 3 self.abs_max_out: 980.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.483771/  1.678038, val:  74.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.5630%\n",
      "layer   3  Sparsity: 65.6995%\n",
      "total_backward_count 303490 real_backward_count 31597  10.411%\n",
      "lif layer 2 self.abs_max_v: 7084.0\n",
      "lif layer 2 self.abs_max_v: 7294.0\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.472127/  1.707570, val:  58.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.8072%\n",
      "layer   3  Sparsity: 65.9660%\n",
      "total_backward_count 313280 real_backward_count 32056  10.232%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.464939/  1.668801, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.6747%\n",
      "layer   3  Sparsity: 66.4042%\n",
      "total_backward_count 323070 real_backward_count 32587  10.087%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.457617/  1.666594, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.6860%\n",
      "layer   3  Sparsity: 66.1926%\n",
      "total_backward_count 332860 real_backward_count 32977   9.907%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.455861/  1.651363, val:  80.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.7403%\n",
      "layer   3  Sparsity: 66.3495%\n",
      "total_backward_count 342650 real_backward_count 33404   9.749%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.449213/  1.655813, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.3931%\n",
      "layer   3  Sparsity: 66.6953%\n",
      "total_backward_count 352440 real_backward_count 33841   9.602%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.441367/  1.627789, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.3096%\n",
      "layer   3  Sparsity: 67.2529%\n",
      "total_backward_count 362230 real_backward_count 34226   9.449%\n",
      "lif layer 2 self.abs_max_v: 7392.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.422070/  1.634678, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.4241%\n",
      "layer   3  Sparsity: 66.9340%\n",
      "total_backward_count 372020 real_backward_count 34580   9.295%\n",
      "fc layer 3 self.abs_max_out: 1018.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.423093/  1.642295, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1180%\n",
      "layer   3  Sparsity: 66.4776%\n",
      "total_backward_count 381810 real_backward_count 34948   9.153%\n",
      "fc layer 2 self.abs_max_out: 4360.0\n",
      "lif layer 2 self.abs_max_v: 7514.5\n",
      "fc layer 1 self.abs_max_out: 7512.0\n",
      "lif layer 1 self.abs_max_v: 13298.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.410560/  1.635244, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2563%\n",
      "layer   3  Sparsity: 66.5146%\n",
      "total_backward_count 391600 real_backward_count 35284   9.010%\n",
      "fc layer 2 self.abs_max_out: 4514.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.404038/  1.614478, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2726%\n",
      "layer   3  Sparsity: 65.7785%\n",
      "total_backward_count 401390 real_backward_count 35639   8.879%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.398970/  1.608541, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1880%\n",
      "layer   3  Sparsity: 65.7697%\n",
      "total_backward_count 411180 real_backward_count 35982   8.751%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.385306/  1.587693, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.5114%\n",
      "layer   3  Sparsity: 66.1336%\n",
      "total_backward_count 420970 real_backward_count 36297   8.622%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.377829/  1.595607, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.3740%\n",
      "layer   3  Sparsity: 66.7493%\n",
      "total_backward_count 430760 real_backward_count 36583   8.493%\n",
      "fc layer 2 self.abs_max_out: 4518.0\n",
      "fc layer 1 self.abs_max_out: 7634.0\n",
      "lif layer 1 self.abs_max_v: 13396.0\n",
      "fc layer 3 self.abs_max_out: 1021.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.382151/  1.603135, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1651%\n",
      "layer   3  Sparsity: 66.6517%\n",
      "total_backward_count 440550 real_backward_count 36863   8.367%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.387869/  1.588731, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0928%\n",
      "layer   3  Sparsity: 66.8406%\n",
      "total_backward_count 450340 real_backward_count 37150   8.249%\n",
      "fc layer 3 self.abs_max_out: 1026.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.376039/  1.606331, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1847%\n",
      "layer   3  Sparsity: 67.2513%\n",
      "total_backward_count 460130 real_backward_count 37429   8.134%\n",
      "lif layer 2 self.abs_max_v: 7586.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.370888/  1.582461, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.4913%\n",
      "layer   3  Sparsity: 66.8772%\n",
      "total_backward_count 469920 real_backward_count 37687   8.020%\n",
      "lif layer 2 self.abs_max_v: 7762.5\n",
      "fc layer 3 self.abs_max_out: 1035.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.365588/  1.586594, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1710%\n",
      "layer   3  Sparsity: 67.1559%\n",
      "total_backward_count 479710 real_backward_count 37960   7.913%\n",
      "fc layer 3 self.abs_max_out: 1050.0\n",
      "fc layer 3 self.abs_max_out: 1070.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.357949/  1.585020, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9531%\n",
      "layer   3  Sparsity: 67.1381%\n",
      "total_backward_count 489500 real_backward_count 38213   7.807%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.351026/  1.578270, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0881%\n",
      "layer   3  Sparsity: 67.4823%\n",
      "total_backward_count 499290 real_backward_count 38414   7.694%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.345620/  1.568609, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1581%\n",
      "layer   3  Sparsity: 67.6039%\n",
      "total_backward_count 509080 real_backward_count 38601   7.583%\n",
      "fc layer 1 self.abs_max_out: 7674.0\n",
      "lif layer 1 self.abs_max_v: 13517.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.343369/  1.576657, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9117%\n",
      "layer   3  Sparsity: 67.3987%\n",
      "total_backward_count 518870 real_backward_count 38805   7.479%\n",
      "fc layer 1 self.abs_max_out: 7727.0\n",
      "lif layer 1 self.abs_max_v: 13628.5\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.336606/  1.559584, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8880%\n",
      "layer   3  Sparsity: 66.9963%\n",
      "total_backward_count 528660 real_backward_count 39016   7.380%\n",
      "fc layer 1 self.abs_max_out: 7865.0\n",
      "lif layer 1 self.abs_max_v: 13861.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.327666/  1.573837, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9587%\n",
      "layer   3  Sparsity: 67.3861%\n",
      "total_backward_count 538450 real_backward_count 39220   7.284%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.320544/  1.547655, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9413%\n",
      "layer   3  Sparsity: 67.1342%\n",
      "total_backward_count 548240 real_backward_count 39415   7.189%\n",
      "fc layer 1 self.abs_max_out: 7874.0\n",
      "lif layer 1 self.abs_max_v: 13908.5\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.313536/  1.551981, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8057%\n",
      "layer   3  Sparsity: 66.6486%\n",
      "total_backward_count 558030 real_backward_count 39622   7.100%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.309496/  1.525686, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1449%\n",
      "layer   3  Sparsity: 66.9955%\n",
      "total_backward_count 567820 real_backward_count 39796   7.009%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.300758/  1.539563, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2410%\n",
      "layer   3  Sparsity: 66.9220%\n",
      "total_backward_count 577610 real_backward_count 39986   6.923%\n",
      "fc layer 3 self.abs_max_out: 1095.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.299824/  1.536253, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1757%\n",
      "layer   3  Sparsity: 67.3984%\n",
      "total_backward_count 587400 real_backward_count 40166   6.838%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.294906/  1.542881, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1971%\n",
      "layer   3  Sparsity: 67.5006%\n",
      "total_backward_count 597190 real_backward_count 40338   6.755%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.299005/  1.532808, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1916%\n",
      "layer   3  Sparsity: 67.7816%\n",
      "total_backward_count 606980 real_backward_count 40515   6.675%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.292323/  1.527769, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1998%\n",
      "layer   3  Sparsity: 67.9461%\n",
      "total_backward_count 616770 real_backward_count 40652   6.591%\n",
      "fc layer 3 self.abs_max_out: 1109.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.288427/  1.527286, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1011%\n",
      "layer   3  Sparsity: 67.7285%\n",
      "total_backward_count 626560 real_backward_count 40811   6.514%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.284777/  1.524747, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1554%\n",
      "layer   3  Sparsity: 67.7973%\n",
      "total_backward_count 636350 real_backward_count 40948   6.435%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.288740/  1.532696, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0363%\n",
      "layer   3  Sparsity: 67.4749%\n",
      "total_backward_count 646140 real_backward_count 41104   6.361%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.290809/  1.516372, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6347%\n",
      "layer   3  Sparsity: 67.3181%\n",
      "total_backward_count 655930 real_backward_count 41285   6.294%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.278032/  1.516851, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.5727%\n",
      "layer   3  Sparsity: 67.3747%\n",
      "total_backward_count 665720 real_backward_count 41444   6.225%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.281070/  1.532788, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7718%\n",
      "layer   3  Sparsity: 67.1691%\n",
      "total_backward_count 675510 real_backward_count 41587   6.156%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.279707/  1.533546, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8810%\n",
      "layer   3  Sparsity: 67.1395%\n",
      "total_backward_count 685300 real_backward_count 41751   6.092%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.273648/  1.500518, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6123%\n",
      "layer   3  Sparsity: 66.8617%\n",
      "total_backward_count 695090 real_backward_count 41889   6.026%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.275408/  1.512241, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6364%\n",
      "layer   3  Sparsity: 66.7141%\n",
      "total_backward_count 704880 real_backward_count 42022   5.962%\n",
      "fc layer 3 self.abs_max_out: 1123.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.266614/  1.510840, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7308%\n",
      "layer   3  Sparsity: 67.5888%\n",
      "total_backward_count 714670 real_backward_count 42143   5.897%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.260229/  1.495551, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7009%\n",
      "layer   3  Sparsity: 67.7074%\n",
      "total_backward_count 724460 real_backward_count 42253   5.832%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.256703/  1.504894, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8398%\n",
      "layer   3  Sparsity: 68.0019%\n",
      "total_backward_count 734250 real_backward_count 42402   5.775%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.251341/  1.491770, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7406%\n",
      "layer   3  Sparsity: 67.9758%\n",
      "total_backward_count 744040 real_backward_count 42501   5.712%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.247433/  1.495195, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6309%\n",
      "layer   3  Sparsity: 67.6046%\n",
      "total_backward_count 753830 real_backward_count 42630   5.655%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.236899/  1.478883, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8189%\n",
      "layer   3  Sparsity: 67.6785%\n",
      "total_backward_count 763620 real_backward_count 42735   5.596%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.231663/  1.477903, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0678%\n",
      "layer   3  Sparsity: 67.8306%\n",
      "total_backward_count 773410 real_backward_count 42835   5.538%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.229069/  1.477200, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0687%\n",
      "layer   3  Sparsity: 67.7773%\n",
      "total_backward_count 783200 real_backward_count 42952   5.484%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.227495/  1.477929, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2244%\n",
      "layer   3  Sparsity: 67.9283%\n",
      "total_backward_count 792990 real_backward_count 43033   5.427%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.217587/  1.472373, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1767%\n",
      "layer   3  Sparsity: 68.2637%\n",
      "total_backward_count 802780 real_backward_count 43137   5.373%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.212939/  1.457726, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9815%\n",
      "layer   3  Sparsity: 68.3801%\n",
      "total_backward_count 812570 real_backward_count 43226   5.320%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.211762/  1.463775, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8072%\n",
      "layer   3  Sparsity: 68.3148%\n",
      "total_backward_count 822360 real_backward_count 43294   5.265%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.210624/  1.459785, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9680%\n",
      "layer   3  Sparsity: 68.1694%\n",
      "total_backward_count 832150 real_backward_count 43366   5.211%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.208315/  1.471332, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1411%\n",
      "layer   3  Sparsity: 68.4549%\n",
      "total_backward_count 841940 real_backward_count 43432   5.159%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.208748/  1.472909, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.4224%\n",
      "layer   3  Sparsity: 68.7573%\n",
      "total_backward_count 851730 real_backward_count 43498   5.107%\n",
      "fc layer 3 self.abs_max_out: 1126.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.214776/  1.469315, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2647%\n",
      "layer   3  Sparsity: 68.8949%\n",
      "total_backward_count 861520 real_backward_count 43572   5.058%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.212734/  1.472854, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1425%\n",
      "layer   3  Sparsity: 68.6521%\n",
      "total_backward_count 871310 real_backward_count 43639   5.008%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.212935/  1.465046, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0741%\n",
      "layer   3  Sparsity: 68.4975%\n",
      "total_backward_count 881100 real_backward_count 43701   4.960%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.211511/  1.473561, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9457%\n",
      "layer   3  Sparsity: 68.6587%\n",
      "total_backward_count 890890 real_backward_count 43763   4.912%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.208662/  1.454320, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0172%\n",
      "layer   3  Sparsity: 68.3946%\n",
      "total_backward_count 900680 real_backward_count 43819   4.865%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.202329/  1.470429, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8448%\n",
      "layer   3  Sparsity: 68.3900%\n",
      "total_backward_count 910470 real_backward_count 43893   4.821%\n",
      "fc layer 1 self.abs_max_out: 7955.0\n",
      "lif layer 1 self.abs_max_v: 14064.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.201836/  1.463560, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.4885%\n",
      "layer   3  Sparsity: 68.4391%\n",
      "total_backward_count 920260 real_backward_count 43962   4.777%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.201751/  1.466849, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.4797%\n",
      "layer   3  Sparsity: 68.4576%\n",
      "total_backward_count 930050 real_backward_count 44034   4.735%\n",
      "fc layer 1 self.abs_max_out: 7966.0\n",
      "lif layer 1 self.abs_max_v: 14092.5\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.203202/  1.471718, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.3932%\n",
      "layer   3  Sparsity: 68.1877%\n",
      "total_backward_count 939840 real_backward_count 44105   4.693%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.196511/  1.467151, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.4532%\n",
      "layer   3  Sparsity: 68.4479%\n",
      "total_backward_count 949630 real_backward_count 44162   4.650%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.197976/  1.458784, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.5443%\n",
      "layer   3  Sparsity: 68.7746%\n",
      "total_backward_count 959420 real_backward_count 44215   4.609%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.195621/  1.464058, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.4045%\n",
      "layer   3  Sparsity: 68.7546%\n",
      "total_backward_count 969210 real_backward_count 44257   4.566%\n",
      "fc layer 3 self.abs_max_out: 1145.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.197158/  1.460992, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.4290%\n",
      "layer   3  Sparsity: 68.8741%\n",
      "total_backward_count 979000 real_backward_count 44296   4.525%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.192734/  1.461389, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.4499%\n",
      "layer   3  Sparsity: 68.7281%\n",
      "total_backward_count 988790 real_backward_count 44338   4.484%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.193615/  1.461019, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.5990%\n",
      "layer   3  Sparsity: 69.1409%\n",
      "total_backward_count 998580 real_backward_count 44395   4.446%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.189321/  1.458346, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6249%\n",
      "layer   3  Sparsity: 69.1117%\n",
      "total_backward_count 1008370 real_backward_count 44442   4.407%\n",
      "fc layer 3 self.abs_max_out: 1175.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.183672/  1.447691, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8030%\n",
      "layer   3  Sparsity: 69.0301%\n",
      "total_backward_count 1018160 real_backward_count 44493   4.370%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.187277/  1.454869, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1441%\n",
      "layer   3  Sparsity: 69.0295%\n",
      "total_backward_count 1027950 real_backward_count 44525   4.331%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.181739/  1.449518, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2301%\n",
      "layer   3  Sparsity: 68.9014%\n",
      "total_backward_count 1037740 real_backward_count 44557   4.294%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.180466/  1.445600, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9512%\n",
      "layer   3  Sparsity: 68.6648%\n",
      "total_backward_count 1047530 real_backward_count 44593   4.257%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.178014/  1.451589, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8511%\n",
      "layer   3  Sparsity: 68.7909%\n",
      "total_backward_count 1057320 real_backward_count 44638   4.222%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.183741/  1.449106, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8149%\n",
      "layer   3  Sparsity: 68.9247%\n",
      "total_backward_count 1067110 real_backward_count 44676   4.187%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.184383/  1.449644, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8498%\n",
      "layer   3  Sparsity: 68.8638%\n",
      "total_backward_count 1076900 real_backward_count 44716   4.152%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.186062/  1.446820, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.5797%\n",
      "layer   3  Sparsity: 69.1094%\n",
      "total_backward_count 1086690 real_backward_count 44772   4.120%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.188053/  1.456631, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6521%\n",
      "layer   3  Sparsity: 69.0113%\n",
      "total_backward_count 1096480 real_backward_count 44810   4.087%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.181048/  1.440723, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8029%\n",
      "layer   3  Sparsity: 68.9551%\n",
      "total_backward_count 1106270 real_backward_count 44842   4.053%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.173455/  1.448081, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9607%\n",
      "layer   3  Sparsity: 69.2261%\n",
      "total_backward_count 1116060 real_backward_count 44870   4.020%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.173959/  1.442188, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1252%\n",
      "layer   3  Sparsity: 69.2804%\n",
      "total_backward_count 1125850 real_backward_count 44904   3.988%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.175532/  1.446718, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9931%\n",
      "layer   3  Sparsity: 69.3445%\n",
      "total_backward_count 1135640 real_backward_count 44928   3.956%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.174105/  1.445702, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9415%\n",
      "layer   3  Sparsity: 69.5227%\n",
      "total_backward_count 1145430 real_backward_count 44960   3.925%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.171886/  1.431287, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0361%\n",
      "layer   3  Sparsity: 69.5502%\n",
      "total_backward_count 1155220 real_backward_count 44993   3.895%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.169509/  1.422755, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8253%\n",
      "layer   3  Sparsity: 69.6630%\n",
      "total_backward_count 1165010 real_backward_count 45030   3.865%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.164481/  1.422944, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8254%\n",
      "layer   3  Sparsity: 69.5200%\n",
      "total_backward_count 1174800 real_backward_count 45072   3.837%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.160116/  1.430304, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7229%\n",
      "layer   3  Sparsity: 69.4788%\n",
      "total_backward_count 1184590 real_backward_count 45108   3.808%\n",
      "fc layer 2 self.abs_max_out: 4601.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.163259/  1.428503, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7464%\n",
      "layer   3  Sparsity: 69.5500%\n",
      "total_backward_count 1194380 real_backward_count 45141   3.779%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.157101/  1.430502, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8440%\n",
      "layer   3  Sparsity: 69.4010%\n",
      "total_backward_count 1204170 real_backward_count 45162   3.750%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.160751/  1.434861, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8193%\n",
      "layer   3  Sparsity: 69.3259%\n",
      "total_backward_count 1213960 real_backward_count 45176   3.721%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.160539/  1.429512, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7842%\n",
      "layer   3  Sparsity: 69.2682%\n",
      "total_backward_count 1223750 real_backward_count 45183   3.692%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.159855/  1.423318, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7393%\n",
      "layer   3  Sparsity: 69.2621%\n",
      "total_backward_count 1233540 real_backward_count 45204   3.665%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.157137/  1.424931, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8856%\n",
      "layer   3  Sparsity: 69.2958%\n",
      "total_backward_count 1243330 real_backward_count 45231   3.638%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.155437/  1.429854, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8607%\n",
      "layer   3  Sparsity: 69.3702%\n",
      "total_backward_count 1253120 real_backward_count 45266   3.612%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.158063/  1.425985, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7120%\n",
      "layer   3  Sparsity: 69.2107%\n",
      "total_backward_count 1262910 real_backward_count 45303   3.587%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.158348/  1.431521, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6219%\n",
      "layer   3  Sparsity: 69.2510%\n",
      "total_backward_count 1272700 real_backward_count 45330   3.562%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.155805/  1.418510, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6674%\n",
      "layer   3  Sparsity: 69.4653%\n",
      "total_backward_count 1282490 real_backward_count 45360   3.537%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.151978/  1.423054, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6337%\n",
      "layer   3  Sparsity: 69.3390%\n",
      "total_backward_count 1292280 real_backward_count 45387   3.512%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.156309/  1.423136, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.6906%\n",
      "layer   3  Sparsity: 69.2878%\n",
      "total_backward_count 1302070 real_backward_count 45424   3.489%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.153538/  1.422301, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7534%\n",
      "layer   3  Sparsity: 69.3251%\n",
      "total_backward_count 1311860 real_backward_count 45459   3.465%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.150877/  1.420255, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7639%\n",
      "layer   3  Sparsity: 69.3406%\n",
      "total_backward_count 1321650 real_backward_count 45483   3.441%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.146402/  1.419644, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8102%\n",
      "layer   3  Sparsity: 69.2671%\n",
      "total_backward_count 1331440 real_backward_count 45499   3.417%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.150350/  1.427053, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7571%\n",
      "layer   3  Sparsity: 69.2081%\n",
      "total_backward_count 1341230 real_backward_count 45528   3.394%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.151386/  1.425731, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9056%\n",
      "layer   3  Sparsity: 69.1847%\n",
      "total_backward_count 1351020 real_backward_count 45564   3.373%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.147236/  1.428079, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1520%\n",
      "layer   3  Sparsity: 69.4591%\n",
      "total_backward_count 1360810 real_backward_count 45598   3.351%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.150623/  1.436661, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1978%\n",
      "layer   3  Sparsity: 69.4055%\n",
      "total_backward_count 1370600 real_backward_count 45629   3.329%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.145987/  1.427364, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.3110%\n",
      "layer   3  Sparsity: 69.5050%\n",
      "total_backward_count 1380390 real_backward_count 45661   3.308%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.142485/  1.424496, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.3368%\n",
      "layer   3  Sparsity: 69.6672%\n",
      "total_backward_count 1390180 real_backward_count 45679   3.286%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.141846/  1.422403, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.3566%\n",
      "layer   3  Sparsity: 69.6858%\n",
      "total_backward_count 1399970 real_backward_count 45698   3.264%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.141242/  1.421255, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2282%\n",
      "layer   3  Sparsity: 69.6314%\n",
      "total_backward_count 1409760 real_backward_count 45724   3.243%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.143034/  1.422033, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1354%\n",
      "layer   3  Sparsity: 69.6374%\n",
      "total_backward_count 1419550 real_backward_count 45753   3.223%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.135757/  1.407956, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1544%\n",
      "layer   3  Sparsity: 69.5223%\n",
      "total_backward_count 1429340 real_backward_count 45774   3.202%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.132941/  1.416921, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2260%\n",
      "layer   3  Sparsity: 69.5744%\n",
      "total_backward_count 1439130 real_backward_count 45794   3.182%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.130626/  1.412182, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2466%\n",
      "layer   3  Sparsity: 69.7036%\n",
      "total_backward_count 1448920 real_backward_count 45801   3.161%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.131370/  1.410415, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.2043%\n",
      "layer   3  Sparsity: 69.7042%\n",
      "total_backward_count 1458710 real_backward_count 45809   3.140%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.131850/  1.416908, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1347%\n",
      "layer   3  Sparsity: 69.5557%\n",
      "total_backward_count 1468500 real_backward_count 45833   3.121%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.130972/  1.410087, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1694%\n",
      "layer   3  Sparsity: 69.4850%\n",
      "total_backward_count 1478290 real_backward_count 45850   3.102%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.131619/  1.413483, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.1116%\n",
      "layer   3  Sparsity: 69.6277%\n",
      "total_backward_count 1488080 real_backward_count 45871   3.083%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.129655/  1.406920, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0562%\n",
      "layer   3  Sparsity: 69.5923%\n",
      "total_backward_count 1497870 real_backward_count 45887   3.063%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.130082/  1.406095, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0447%\n",
      "layer   3  Sparsity: 69.5704%\n",
      "total_backward_count 1507660 real_backward_count 45901   3.045%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.124023/  1.408208, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0418%\n",
      "layer   3  Sparsity: 69.7142%\n",
      "total_backward_count 1517450 real_backward_count 45914   3.026%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.129753/  1.410216, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0538%\n",
      "layer   3  Sparsity: 69.8396%\n",
      "total_backward_count 1527240 real_backward_count 45925   3.007%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.127112/  1.416481, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0299%\n",
      "layer   3  Sparsity: 69.7928%\n",
      "total_backward_count 1537030 real_backward_count 45934   2.988%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.129809/  1.415655, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9267%\n",
      "layer   3  Sparsity: 69.8087%\n",
      "total_backward_count 1546820 real_backward_count 45948   2.970%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.128846/  1.413595, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8683%\n",
      "layer   3  Sparsity: 69.7777%\n",
      "total_backward_count 1556610 real_backward_count 45951   2.952%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.129154/  1.413153, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8144%\n",
      "layer   3  Sparsity: 69.7886%\n",
      "total_backward_count 1566400 real_backward_count 45959   2.934%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.133304/  1.417497, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8086%\n",
      "layer   3  Sparsity: 69.7388%\n",
      "total_backward_count 1576190 real_backward_count 45962   2.916%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.134566/  1.415829, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8204%\n",
      "layer   3  Sparsity: 69.6682%\n",
      "total_backward_count 1585980 real_backward_count 45967   2.898%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.128001/  1.407881, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8808%\n",
      "layer   3  Sparsity: 69.6540%\n",
      "total_backward_count 1595770 real_backward_count 45986   2.882%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.128564/  1.410577, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.7706%\n",
      "layer   3  Sparsity: 69.6016%\n",
      "total_backward_count 1605560 real_backward_count 45994   2.865%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.126155/  1.407804, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8137%\n",
      "layer   3  Sparsity: 69.7453%\n",
      "total_backward_count 1615350 real_backward_count 46003   2.848%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.125133/  1.410516, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8768%\n",
      "layer   3  Sparsity: 69.6650%\n",
      "total_backward_count 1625140 real_backward_count 46015   2.831%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.128399/  1.411104, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9204%\n",
      "layer   3  Sparsity: 69.5511%\n",
      "total_backward_count 1634930 real_backward_count 46022   2.815%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.126872/  1.411812, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9512%\n",
      "layer   3  Sparsity: 69.6006%\n",
      "total_backward_count 1644720 real_backward_count 46024   2.798%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.127223/  1.411510, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9656%\n",
      "layer   3  Sparsity: 69.6900%\n",
      "total_backward_count 1654510 real_backward_count 46026   2.782%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.126739/  1.411344, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9292%\n",
      "layer   3  Sparsity: 69.6751%\n",
      "total_backward_count 1664300 real_backward_count 46028   2.766%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.129427/  1.415099, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.8977%\n",
      "layer   3  Sparsity: 69.6756%\n",
      "total_backward_count 1674090 real_backward_count 46035   2.750%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.128351/  1.415071, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9510%\n",
      "layer   3  Sparsity: 69.6276%\n",
      "total_backward_count 1683880 real_backward_count 46041   2.734%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.124721/  1.408813, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 69.9883%\n",
      "layer   3  Sparsity: 69.5537%\n",
      "total_backward_count 1693670 real_backward_count 46046   2.719%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.122044/  1.408900, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0141%\n",
      "layer   3  Sparsity: 69.5563%\n",
      "total_backward_count 1703460 real_backward_count 46048   2.703%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.121076/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0180%\n",
      "layer   3  Sparsity: 69.5603%\n",
      "total_backward_count 1713250 real_backward_count 46050   2.688%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.119967/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1723040 real_backward_count 46050   2.673%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1732830 real_backward_count 46050   2.658%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1742620 real_backward_count 46050   2.643%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1752410 real_backward_count 46050   2.628%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1762200 real_backward_count 46050   2.613%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1771990 real_backward_count 46050   2.599%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1781780 real_backward_count 46050   2.584%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1791570 real_backward_count 46050   2.570%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1801360 real_backward_count 46050   2.556%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1811150 real_backward_count 46050   2.543%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1820940 real_backward_count 46050   2.529%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1830730 real_backward_count 46050   2.515%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1840520 real_backward_count 46050   2.502%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1850310 real_backward_count 46050   2.489%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1860100 real_backward_count 46050   2.476%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1869890 real_backward_count 46050   2.463%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1879680 real_backward_count 46050   2.450%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1889470 real_backward_count 46050   2.437%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1899260 real_backward_count 46050   2.425%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1909050 real_backward_count 46050   2.412%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1918840 real_backward_count 46050   2.400%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1928630 real_backward_count 46050   2.388%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1938420 real_backward_count 46050   2.376%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.119965/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n",
      "total_backward_count 1948210 real_backward_count 46050   2.364%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.119966/  1.407743, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 90.8883%\n",
      "layer   2  Sparsity: 70.0179%\n",
      "layer   3  Sparsity: 69.5611%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ece85f118443729c577d3877deea33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.11997</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>1.40774</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-132</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llxyi5j2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/llxyi5j2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_084347-llxyi5j2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3bf2qi65 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_130059-3bf2qi65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3bf2qi65' target=\"_blank\">fancy-sweep-138</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3bf2qi65' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3bf2qi65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_130108_597', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0.75} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 220.0\n",
      "lif layer 1 self.abs_max_v: 220.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 384.0\n",
      "lif layer 2 self.abs_max_v: 384.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 1 self.abs_max_out: 304.0\n",
      "lif layer 1 self.abs_max_v: 379.0\n",
      "fc layer 2 self.abs_max_out: 414.0\n",
      "lif layer 2 self.abs_max_v: 512.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "fc layer 1 self.abs_max_out: 333.0\n",
      "lif layer 1 self.abs_max_v: 399.5\n",
      "lif layer 2 self.abs_max_v: 634.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "lif layer 1 self.abs_max_v: 446.0\n",
      "fc layer 2 self.abs_max_out: 541.0\n",
      "lif layer 2 self.abs_max_v: 710.0\n",
      "lif layer 1 self.abs_max_v: 528.0\n",
      "lif layer 2 self.abs_max_v: 754.0\n",
      "lif layer 2 self.abs_max_v: 756.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 348.0\n",
      "fc layer 1 self.abs_max_out: 392.0\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 622.5\n",
      "lif layer 2 self.abs_max_v: 848.5\n",
      "lif layer 1 self.abs_max_v: 643.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 2 self.abs_max_out: 559.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "lif layer 2 self.abs_max_v: 880.5\n",
      "fc layer 1 self.abs_max_out: 410.0\n",
      "fc layer 2 self.abs_max_out: 661.0\n",
      "fc layer 1 self.abs_max_out: 453.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "fc layer 3 self.abs_max_out: 284.0\n",
      "fc layer 1 self.abs_max_out: 461.0\n",
      "lif layer 1 self.abs_max_v: 692.5\n",
      "lif layer 2 self.abs_max_v: 892.0\n",
      "fc layer 1 self.abs_max_out: 469.0\n",
      "lif layer 1 self.abs_max_v: 750.5\n",
      "lif layer 1 self.abs_max_v: 799.5\n",
      "lif layer 2 self.abs_max_v: 908.5\n",
      "lif layer 1 self.abs_max_v: 822.0\n",
      "fc layer 1 self.abs_max_out: 481.0\n",
      "fc layer 1 self.abs_max_out: 513.0\n",
      "fc layer 1 self.abs_max_out: 540.0\n",
      "lif layer 2 self.abs_max_v: 1051.5\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "lif layer 1 self.abs_max_v: 897.0\n",
      "lif layer 1 self.abs_max_v: 923.5\n",
      "lif layer 1 self.abs_max_v: 964.5\n",
      "fc layer 2 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 1 self.abs_max_out: 582.0\n",
      "lif layer 2 self.abs_max_v: 1067.0\n",
      "fc layer 3 self.abs_max_out: 309.0\n",
      "fc layer 2 self.abs_max_out: 702.0\n",
      "lif layer 2 self.abs_max_v: 1154.0\n",
      "fc layer 1 self.abs_max_out: 606.0\n",
      "lif layer 1 self.abs_max_v: 999.5\n",
      "lif layer 1 self.abs_max_v: 1047.0\n",
      "fc layer 1 self.abs_max_out: 667.0\n",
      "fc layer 3 self.abs_max_out: 367.0\n",
      "lif layer 2 self.abs_max_v: 1163.0\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "lif layer 2 self.abs_max_v: 1188.0\n",
      "fc layer 1 self.abs_max_out: 671.0\n",
      "lif layer 1 self.abs_max_v: 1099.0\n",
      "lif layer 1 self.abs_max_v: 1099.5\n",
      "fc layer 1 self.abs_max_out: 673.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.348796/  2.353877, val:  12.50%, val_best:  12.50%, tr:  14.10%, tr_best:  14.10%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 84.8967%\n",
      "layer   2  Sparsity: 71.0621%\n",
      "layer   3  Sparsity: 72.6520%\n",
      "total_backward_count 9790 real_backward_count 8586  87.702%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.348796/  2.353877, val:  12.50%, val_best:  12.50%, tr:  14.10%, tr_best:  14.10%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 84.8967%\n",
      "layer   2  Sparsity: 71.0621%\n",
      "layer   3  Sparsity: 72.6520%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e0bd78dd024f2cb85477eeac548071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.14096</td></tr><tr><td>tr_epoch_loss</td><td>2.3488</td></tr><tr><td>val_acc_best</td><td>0.125</td></tr><tr><td>val_acc_now</td><td>0.125</td></tr><tr><td>val_loss</td><td>2.35388</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-138</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3bf2qi65' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3bf2qi65</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_130059-3bf2qi65/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3bf2qi65 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_18083/2692891570.py\", line 114, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_18083/2991991161.py\", line 973, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3bf2qi65 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_18083/2692891570.py\", line 114, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_18083/2991991161.py\", line 973, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wdn62czp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_130401-wdn62czp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wdn62czp' target=\"_blank\">sunny-sweep-139</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wdn62czp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wdn62czp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_130410_813', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "lif layer 1 self.abs_max_v: 503.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "lif layer 2 self.abs_max_v: 1446.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 1 self.abs_max_out: 660.0\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 2 self.abs_max_out: 1977.0\n",
      "lif layer 2 self.abs_max_v: 2371.5\n",
      "fc layer 1 self.abs_max_out: 859.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 2 self.abs_max_v: 2764.0\n",
      "fc layer 3 self.abs_max_out: 758.0\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 1206.5\n",
      "lif layer 2 self.abs_max_v: 3056.0\n",
      "fc layer 1 self.abs_max_out: 895.0\n",
      "lif layer 1 self.abs_max_v: 1498.5\n",
      "fc layer 2 self.abs_max_out: 2207.0\n",
      "fc layer 3 self.abs_max_out: 854.0\n",
      "fc layer 1 self.abs_max_out: 977.0\n",
      "lif layer 2 self.abs_max_v: 3382.5\n",
      "fc layer 1 self.abs_max_out: 1070.0\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "fc layer 1 self.abs_max_out: 1185.0\n",
      "lif layer 1 self.abs_max_v: 1603.0\n",
      "lif layer 1 self.abs_max_v: 1700.0\n",
      "lif layer 1 self.abs_max_v: 2009.0\n",
      "fc layer 1 self.abs_max_out: 1398.0\n",
      "fc layer 3 self.abs_max_out: 1053.0\n",
      "fc layer 2 self.abs_max_out: 2243.0\n",
      "fc layer 1 self.abs_max_out: 1554.0\n",
      "fc layer 2 self.abs_max_out: 2385.0\n",
      "lif layer 2 self.abs_max_v: 3718.0\n",
      "fc layer 2 self.abs_max_out: 2432.0\n",
      "lif layer 1 self.abs_max_v: 2153.5\n",
      "fc layer 1 self.abs_max_out: 1565.0\n",
      "lif layer 1 self.abs_max_v: 2258.0\n",
      "lif layer 2 self.abs_max_v: 3773.5\n",
      "lif layer 1 self.abs_max_v: 2265.5\n",
      "fc layer 1 self.abs_max_out: 1764.0\n",
      "lif layer 1 self.abs_max_v: 2360.5\n",
      "lif layer 1 self.abs_max_v: 2428.0\n",
      "fc layer 3 self.abs_max_out: 1193.0\n",
      "lif layer 2 self.abs_max_v: 3813.5\n",
      "lif layer 1 self.abs_max_v: 2828.5\n",
      "fc layer 2 self.abs_max_out: 2546.0\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "lif layer 2 self.abs_max_v: 4090.0\n",
      "fc layer 1 self.abs_max_out: 1796.0\n",
      "fc layer 2 self.abs_max_out: 2568.0\n",
      "fc layer 1 self.abs_max_out: 1805.0\n",
      "fc layer 1 self.abs_max_out: 1996.0\n",
      "fc layer 1 self.abs_max_out: 1997.0\n",
      "lif layer 1 self.abs_max_v: 2937.0\n",
      "fc layer 2 self.abs_max_out: 2599.0\n",
      "fc layer 1 self.abs_max_out: 2362.0\n",
      "fc layer 2 self.abs_max_out: 2664.0\n",
      "fc layer 2 self.abs_max_out: 2731.0\n",
      "lif layer 2 self.abs_max_v: 4090.5\n",
      "lif layer 2 self.abs_max_v: 4372.5\n",
      "lif layer 1 self.abs_max_v: 2983.5\n",
      "lif layer 1 self.abs_max_v: 3247.0\n",
      "lif layer 1 self.abs_max_v: 3917.5\n",
      "lif layer 2 self.abs_max_v: 4602.5\n",
      "fc layer 1 self.abs_max_out: 2730.0\n",
      "lif layer 2 self.abs_max_v: 4700.5\n",
      "fc layer 1 self.abs_max_out: 2894.0\n",
      "fc layer 2 self.abs_max_out: 2891.0\n",
      "lif layer 2 self.abs_max_v: 4892.5\n",
      "lif layer 1 self.abs_max_v: 4238.5\n",
      "fc layer 2 self.abs_max_out: 2934.0\n",
      "fc layer 2 self.abs_max_out: 3034.0\n",
      "fc layer 1 self.abs_max_out: 3571.0\n",
      "lif layer 1 self.abs_max_v: 4533.5\n",
      "lif layer 1 self.abs_max_v: 4770.0\n",
      "fc layer 2 self.abs_max_out: 3044.0\n",
      "fc layer 3 self.abs_max_out: 1199.0\n",
      "lif layer 2 self.abs_max_v: 5022.5\n",
      "fc layer 2 self.abs_max_out: 3505.0\n",
      "lif layer 2 self.abs_max_v: 5093.5\n",
      "lif layer 2 self.abs_max_v: 5355.5\n",
      "lif layer 2 self.abs_max_v: 5434.5\n",
      "lif layer 2 self.abs_max_v: 5486.5\n",
      "lif layer 2 self.abs_max_v: 5686.5\n",
      "fc layer 3 self.abs_max_out: 1221.0\n",
      "lif layer 1 self.abs_max_v: 4782.5\n",
      "lif layer 1 self.abs_max_v: 4987.0\n",
      "fc layer 1 self.abs_max_out: 4009.0\n",
      "lif layer 1 self.abs_max_v: 5116.0\n",
      "fc layer 3 self.abs_max_out: 1232.0\n",
      "lif layer 1 self.abs_max_v: 5128.5\n",
      "lif layer 1 self.abs_max_v: 5255.0\n",
      "fc layer 3 self.abs_max_out: 1375.0\n",
      "lif layer 1 self.abs_max_v: 5267.5\n",
      "fc layer 1 self.abs_max_out: 4071.0\n",
      "lif layer 1 self.abs_max_v: 5934.5\n",
      "lif layer 2 self.abs_max_v: 5972.0\n",
      "lif layer 1 self.abs_max_v: 5976.0\n",
      "fc layer 3 self.abs_max_out: 1414.0\n",
      "fc layer 2 self.abs_max_out: 3506.0\n",
      "fc layer 1 self.abs_max_out: 4243.0\n",
      "lif layer 2 self.abs_max_v: 6113.5\n",
      "lif layer 1 self.abs_max_v: 6605.5\n",
      "lif layer 1 self.abs_max_v: 6725.0\n",
      "fc layer 1 self.abs_max_out: 4266.0\n",
      "lif layer 1 self.abs_max_v: 6977.0\n",
      "fc layer 2 self.abs_max_out: 3646.0\n",
      "fc layer 1 self.abs_max_out: 4274.0\n",
      "fc layer 1 self.abs_max_out: 4519.0\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "lif layer 1 self.abs_max_v: 7599.5\n",
      "lif layer 1 self.abs_max_v: 8348.0\n",
      "lif layer 1 self.abs_max_v: 8921.0\n",
      "lif layer 2 self.abs_max_v: 6302.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.817325/  2.022479, val:  37.08%, val_best:  37.08%, tr:  96.12%, tr_best:  96.12%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1945%\n",
      "layer   2  Sparsity: 69.2500%\n",
      "layer   3  Sparsity: 60.4427%\n",
      "total_backward_count 9790 real_backward_count 2269  23.177%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 4796.0\n",
      "fc layer 2 self.abs_max_out: 3700.0\n",
      "fc layer 2 self.abs_max_out: 3729.0\n",
      "lif layer 2 self.abs_max_v: 6511.5\n",
      "fc layer 2 self.abs_max_out: 3735.0\n",
      "fc layer 2 self.abs_max_out: 3758.0\n",
      "fc layer 2 self.abs_max_out: 3807.0\n",
      "fc layer 1 self.abs_max_out: 4835.0\n",
      "fc layer 1 self.abs_max_out: 5021.0\n",
      "fc layer 3 self.abs_max_out: 1524.0\n",
      "fc layer 2 self.abs_max_out: 4086.0\n",
      "fc layer 2 self.abs_max_out: 4105.0\n",
      "fc layer 1 self.abs_max_out: 5040.0\n",
      "fc layer 1 self.abs_max_out: 5247.0\n",
      "lif layer 1 self.abs_max_v: 9463.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.768115/  2.000256, val:  40.00%, val_best:  40.00%, tr:  98.88%, tr_best:  98.88%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2108%\n",
      "layer   2  Sparsity: 69.3340%\n",
      "layer   3  Sparsity: 57.3774%\n",
      "total_backward_count 19580 real_backward_count 3879  19.811%\n",
      "fc layer 2 self.abs_max_out: 4145.0\n",
      "fc layer 2 self.abs_max_out: 4553.0\n",
      "fc layer 1 self.abs_max_out: 5871.0\n",
      "fc layer 3 self.abs_max_out: 1593.0\n",
      "lif layer 1 self.abs_max_v: 10273.5\n",
      "lif layer 1 self.abs_max_v: 10580.0\n",
      "lif layer 1 self.abs_max_v: 10582.0\n",
      "fc layer 1 self.abs_max_out: 6145.0\n",
      "lif layer 1 self.abs_max_v: 10986.5\n",
      "lif layer 2 self.abs_max_v: 6680.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.762212/  1.972621, val:  37.50%, val_best:  40.00%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2184%\n",
      "layer   2  Sparsity: 70.9726%\n",
      "layer   3  Sparsity: 57.7932%\n",
      "total_backward_count 29370 real_backward_count 5328  18.141%\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.743495/  1.957425, val:  40.83%, val_best:  40.83%, tr:  99.28%, tr_best:  99.28%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1967%\n",
      "layer   2  Sparsity: 71.9009%\n",
      "layer   3  Sparsity: 56.5008%\n",
      "total_backward_count 39160 real_backward_count 6748  17.232%\n",
      "lif layer 2 self.abs_max_v: 6680.5\n",
      "fc layer 1 self.abs_max_out: 6242.0\n",
      "lif layer 2 self.abs_max_v: 6719.5\n",
      "lif layer 2 self.abs_max_v: 7006.0\n",
      "fc layer 1 self.abs_max_out: 6351.0\n",
      "lif layer 1 self.abs_max_v: 11640.0\n",
      "lif layer 1 self.abs_max_v: 11892.0\n",
      "fc layer 1 self.abs_max_out: 6874.0\n",
      "lif layer 1 self.abs_max_v: 12360.5\n",
      "lif layer 1 self.abs_max_v: 12403.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.741349/  1.976974, val:  40.00%, val_best:  40.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2005%\n",
      "layer   2  Sparsity: 71.5148%\n",
      "layer   3  Sparsity: 56.6851%\n",
      "total_backward_count 48950 real_backward_count 8059  16.464%\n",
      "fc layer 1 self.abs_max_out: 7017.0\n",
      "lif layer 2 self.abs_max_v: 7163.5\n",
      "lif layer 2 self.abs_max_v: 7348.5\n",
      "fc layer 3 self.abs_max_out: 1680.0\n",
      "lif layer 2 self.abs_max_v: 7359.5\n",
      "lif layer 2 self.abs_max_v: 7598.0\n",
      "lif layer 2 self.abs_max_v: 7640.0\n",
      "lif layer 1 self.abs_max_v: 12571.5\n",
      "lif layer 1 self.abs_max_v: 12866.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.732645/  1.953887, val:  51.67%, val_best:  51.67%, tr:  99.18%, tr_best:  99.49%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1989%\n",
      "layer   2  Sparsity: 71.6232%\n",
      "layer   3  Sparsity: 58.6379%\n",
      "total_backward_count 58740 real_backward_count 9366  15.945%\n",
      "fc layer 1 self.abs_max_out: 7359.0\n",
      "lif layer 1 self.abs_max_v: 13529.0\n",
      "lif layer 1 self.abs_max_v: 13540.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.732114/  1.947401, val:  45.00%, val_best:  51.67%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.1888%\n",
      "layer   2  Sparsity: 71.3045%\n",
      "layer   3  Sparsity: 58.4920%\n",
      "total_backward_count 68530 real_backward_count 10636  15.520%\n",
      "fc layer 1 self.abs_max_out: 7752.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.723372/  1.926160, val:  52.08%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2093%\n",
      "layer   2  Sparsity: 71.3495%\n",
      "layer   3  Sparsity: 59.5781%\n",
      "total_backward_count 78320 real_backward_count 11878  15.166%\n",
      "fc layer 1 self.abs_max_out: 7877.0\n",
      "lif layer 1 self.abs_max_v: 13743.0\n",
      "lif layer 1 self.abs_max_v: 13876.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.742296/  1.928980, val:  50.00%, val_best:  52.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2180%\n",
      "layer   2  Sparsity: 71.0652%\n",
      "layer   3  Sparsity: 61.3945%\n",
      "total_backward_count 88110 real_backward_count 13132  14.904%\n",
      "lif layer 2 self.abs_max_v: 7645.0\n",
      "fc layer 1 self.abs_max_out: 7973.0\n",
      "lif layer 1 self.abs_max_v: 14480.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.725986/  1.972114, val:  46.67%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2091%\n",
      "layer   2  Sparsity: 70.2539%\n",
      "layer   3  Sparsity: 62.1555%\n",
      "total_backward_count 97900 real_backward_count 14319  14.626%\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.731800/  1.932283, val:  49.17%, val_best:  52.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2084%\n",
      "layer   2  Sparsity: 70.6000%\n",
      "layer   3  Sparsity: 62.3249%\n",
      "total_backward_count 107690 real_backward_count 15478  14.373%\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.731132/  1.924456, val:  60.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1825%\n",
      "layer   2  Sparsity: 70.6594%\n",
      "layer   3  Sparsity: 62.7674%\n",
      "total_backward_count 117480 real_backward_count 16648  14.171%\n",
      "fc layer 1 self.abs_max_out: 8249.0\n",
      "lif layer 1 self.abs_max_v: 15353.5\n",
      "fc layer 1 self.abs_max_out: 8461.0\n",
      "lif layer 1 self.abs_max_v: 16138.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.729784/  1.946184, val:  41.67%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2026%\n",
      "layer   2  Sparsity: 70.7555%\n",
      "layer   3  Sparsity: 62.9995%\n",
      "total_backward_count 127270 real_backward_count 17759  13.954%\n",
      "lif layer 2 self.abs_max_v: 8155.5\n",
      "fc layer 2 self.abs_max_out: 4667.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.721280/  1.917745, val:  47.08%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2116%\n",
      "layer   2  Sparsity: 70.5430%\n",
      "layer   3  Sparsity: 62.2127%\n",
      "total_backward_count 137060 real_backward_count 18905  13.793%\n",
      "fc layer 2 self.abs_max_out: 4668.0\n",
      "fc layer 2 self.abs_max_out: 4962.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.730946/  1.880680, val:  57.08%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1877%\n",
      "layer   2  Sparsity: 70.1651%\n",
      "layer   3  Sparsity: 63.1553%\n",
      "total_backward_count 146850 real_backward_count 19937  13.576%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.721506/  1.914865, val:  47.08%, val_best:  60.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1901%\n",
      "layer   2  Sparsity: 70.0840%\n",
      "layer   3  Sparsity: 64.6642%\n",
      "total_backward_count 156640 real_backward_count 21079  13.457%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.708434/  1.862586, val:  60.42%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2028%\n",
      "layer   2  Sparsity: 69.9056%\n",
      "layer   3  Sparsity: 64.3032%\n",
      "total_backward_count 166430 real_backward_count 22134  13.299%\n",
      "fc layer 2 self.abs_max_out: 5096.0\n",
      "fc layer 1 self.abs_max_out: 8529.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.709627/  1.857322, val:  68.75%, val_best:  68.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1748%\n",
      "layer   2  Sparsity: 69.6890%\n",
      "layer   3  Sparsity: 63.8958%\n",
      "total_backward_count 176220 real_backward_count 23158  13.142%\n",
      "fc layer 2 self.abs_max_out: 5316.0\n",
      "lif layer 1 self.abs_max_v: 16217.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.707349/  1.843235, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1858%\n",
      "layer   2  Sparsity: 69.4228%\n",
      "layer   3  Sparsity: 65.1719%\n",
      "total_backward_count 186010 real_backward_count 24179  12.999%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.697993/  1.861753, val:  47.92%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1815%\n",
      "layer   2  Sparsity: 69.5115%\n",
      "layer   3  Sparsity: 64.8890%\n",
      "total_backward_count 195800 real_backward_count 25171  12.855%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.703161/  1.867561, val:  55.83%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1894%\n",
      "layer   2  Sparsity: 69.6819%\n",
      "layer   3  Sparsity: 65.0577%\n",
      "total_backward_count 205590 real_backward_count 26149  12.719%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.699954/  1.853887, val:  70.42%, val_best:  75.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2033%\n",
      "layer   2  Sparsity: 69.5425%\n",
      "layer   3  Sparsity: 65.5059%\n",
      "total_backward_count 215380 real_backward_count 27122  12.593%\n",
      "lif layer 2 self.abs_max_v: 8492.5\n",
      "fc layer 1 self.abs_max_out: 8703.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.706246/  1.863901, val:  72.08%, val_best:  75.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1866%\n",
      "layer   2  Sparsity: 69.1303%\n",
      "layer   3  Sparsity: 65.6776%\n",
      "total_backward_count 225170 real_backward_count 28091  12.475%\n",
      "fc layer 1 self.abs_max_out: 8743.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.718849/  1.865969, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1678%\n",
      "layer   2  Sparsity: 69.3735%\n",
      "layer   3  Sparsity: 65.8767%\n",
      "total_backward_count 234960 real_backward_count 29095  12.383%\n",
      "fc layer 1 self.abs_max_out: 8791.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.704100/  1.855468, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.2021%\n",
      "layer   2  Sparsity: 69.7271%\n",
      "layer   3  Sparsity: 65.9369%\n",
      "total_backward_count 244750 real_backward_count 30078  12.289%\n",
      "fc layer 1 self.abs_max_out: 8918.0\n",
      "lif layer 1 self.abs_max_v: 16652.0\n",
      "fc layer 1 self.abs_max_out: 9185.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.708731/  1.845053, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1944%\n",
      "layer   2  Sparsity: 69.6223%\n",
      "layer   3  Sparsity: 65.6882%\n",
      "total_backward_count 254540 real_backward_count 31058  12.202%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.703475/  1.865124, val:  77.50%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1960%\n",
      "layer   2  Sparsity: 69.2521%\n",
      "layer   3  Sparsity: 66.2117%\n",
      "total_backward_count 264330 real_backward_count 31988  12.102%\n",
      "lif layer 1 self.abs_max_v: 17169.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.702382/  1.843015, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1694%\n",
      "layer   2  Sparsity: 68.9508%\n",
      "layer   3  Sparsity: 64.8404%\n",
      "total_backward_count 274120 real_backward_count 32917  12.008%\n",
      "fc layer 1 self.abs_max_out: 9316.0\n",
      "lif layer 1 self.abs_max_v: 17412.5\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.702417/  1.883520, val:  70.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1832%\n",
      "layer   2  Sparsity: 68.8109%\n",
      "layer   3  Sparsity: 66.6657%\n",
      "total_backward_count 283910 real_backward_count 33787  11.901%\n",
      "lif layer 2 self.abs_max_v: 8534.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.703986/  1.846593, val:  66.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1937%\n",
      "layer   2  Sparsity: 68.7368%\n",
      "layer   3  Sparsity: 65.3703%\n",
      "total_backward_count 293700 real_backward_count 34646  11.796%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.693462/  1.852590, val:  69.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2134%\n",
      "layer   2  Sparsity: 68.7351%\n",
      "layer   3  Sparsity: 65.5670%\n",
      "total_backward_count 303490 real_backward_count 35513  11.702%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.688683/  1.846173, val:  55.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1770%\n",
      "layer   2  Sparsity: 69.1934%\n",
      "layer   3  Sparsity: 65.6718%\n",
      "total_backward_count 313280 real_backward_count 36365  11.608%\n",
      "fc layer 1 self.abs_max_out: 9455.0\n",
      "lif layer 1 self.abs_max_v: 17545.0\n",
      "fc layer 1 self.abs_max_out: 9493.0\n",
      "lif layer 1 self.abs_max_v: 18265.5\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.678179/  1.857623, val:  64.58%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1916%\n",
      "layer   2  Sparsity: 68.4696%\n",
      "layer   3  Sparsity: 65.8345%\n",
      "total_backward_count 323070 real_backward_count 37222  11.521%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.673452/  1.816402, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2143%\n",
      "layer   2  Sparsity: 68.8347%\n",
      "layer   3  Sparsity: 66.1623%\n",
      "total_backward_count 332860 real_backward_count 38035  11.427%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.669367/  1.828369, val:  72.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1956%\n",
      "layer   2  Sparsity: 69.3446%\n",
      "layer   3  Sparsity: 66.9505%\n",
      "total_backward_count 342650 real_backward_count 38852  11.339%\n",
      "fc layer 1 self.abs_max_out: 9562.0\n",
      "fc layer 1 self.abs_max_out: 9656.0\n",
      "lif layer 1 self.abs_max_v: 18544.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.658899/  1.811594, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1871%\n",
      "layer   2  Sparsity: 69.2054%\n",
      "layer   3  Sparsity: 66.0746%\n",
      "total_backward_count 352440 real_backward_count 39645  11.249%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.657817/  1.827192, val:  75.42%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2161%\n",
      "layer   2  Sparsity: 69.0554%\n",
      "layer   3  Sparsity: 65.2849%\n",
      "total_backward_count 362230 real_backward_count 40413  11.157%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.663783/  1.839549, val:  68.75%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2116%\n",
      "layer   2  Sparsity: 68.6229%\n",
      "layer   3  Sparsity: 65.4144%\n",
      "total_backward_count 372020 real_backward_count 41171  11.067%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.658673/  1.816250, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1720%\n",
      "layer   2  Sparsity: 68.1770%\n",
      "layer   3  Sparsity: 65.3802%\n",
      "total_backward_count 381810 real_backward_count 41984  10.996%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.663613/  1.846200, val:  68.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1988%\n",
      "layer   2  Sparsity: 68.0916%\n",
      "layer   3  Sparsity: 65.8384%\n",
      "total_backward_count 391600 real_backward_count 42733  10.912%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.684215/  1.821379, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1883%\n",
      "layer   2  Sparsity: 68.2295%\n",
      "layer   3  Sparsity: 66.0242%\n",
      "total_backward_count 401390 real_backward_count 43477  10.832%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.669587/  1.821218, val:  72.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1981%\n",
      "layer   2  Sparsity: 68.3575%\n",
      "layer   3  Sparsity: 65.2842%\n",
      "total_backward_count 411180 real_backward_count 44236  10.758%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.655737/  1.809498, val:  75.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2090%\n",
      "layer   2  Sparsity: 68.6719%\n",
      "layer   3  Sparsity: 66.5137%\n",
      "total_backward_count 420970 real_backward_count 44990  10.687%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.657090/  1.805188, val:  80.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2191%\n",
      "layer   2  Sparsity: 68.3724%\n",
      "layer   3  Sparsity: 66.2324%\n",
      "total_backward_count 430760 real_backward_count 45711  10.612%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.653962/  1.798195, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2322%\n",
      "layer   2  Sparsity: 68.4144%\n",
      "layer   3  Sparsity: 66.3406%\n",
      "total_backward_count 440550 real_backward_count 46448  10.543%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.648724/  1.792985, val:  79.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1859%\n",
      "layer   2  Sparsity: 68.0501%\n",
      "layer   3  Sparsity: 66.2683%\n",
      "total_backward_count 450340 real_backward_count 47172  10.475%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.654798/  1.822671, val:  80.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1889%\n",
      "layer   2  Sparsity: 67.8845%\n",
      "layer   3  Sparsity: 65.6668%\n",
      "total_backward_count 460130 real_backward_count 47862  10.402%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.643432/  1.802618, val:  75.00%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2122%\n",
      "layer   2  Sparsity: 68.0812%\n",
      "layer   3  Sparsity: 66.1441%\n",
      "total_backward_count 469920 real_backward_count 48578  10.338%\n",
      "lif layer 2 self.abs_max_v: 8648.5\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.651130/  1.800806, val:  81.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1742%\n",
      "layer   2  Sparsity: 68.1451%\n",
      "layer   3  Sparsity: 66.4130%\n",
      "total_backward_count 479710 real_backward_count 49248  10.266%\n",
      "fc layer 1 self.abs_max_out: 9829.0\n",
      "lif layer 1 self.abs_max_v: 18776.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.641023/  1.780894, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1846%\n",
      "layer   2  Sparsity: 68.4231%\n",
      "layer   3  Sparsity: 65.5342%\n",
      "total_backward_count 489500 real_backward_count 49963  10.207%\n",
      "fc layer 1 self.abs_max_out: 10417.0\n",
      "lif layer 1 self.abs_max_v: 19449.5\n",
      "fc layer 1 self.abs_max_out: 10615.0\n",
      "lif layer 1 self.abs_max_v: 20340.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.631655/  1.800602, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.2043%\n",
      "layer   2  Sparsity: 68.2525%\n",
      "layer   3  Sparsity: 65.9656%\n",
      "total_backward_count 499290 real_backward_count 50627  10.140%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.632213/  1.781600, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1710%\n",
      "layer   2  Sparsity: 68.0685%\n",
      "layer   3  Sparsity: 65.6438%\n",
      "total_backward_count 509080 real_backward_count 51314  10.080%\n",
      "lif layer 2 self.abs_max_v: 9047.0\n",
      "fc layer 2 self.abs_max_out: 5339.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.627665/  1.783200, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1970%\n",
      "layer   2  Sparsity: 68.2698%\n",
      "layer   3  Sparsity: 66.1764%\n",
      "total_backward_count 518870 real_backward_count 51957  10.013%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.631159/  1.778831, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1851%\n",
      "layer   2  Sparsity: 68.0363%\n",
      "layer   3  Sparsity: 66.4836%\n",
      "total_backward_count 528660 real_backward_count 52613   9.952%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.619794/  1.760227, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1971%\n",
      "layer   2  Sparsity: 68.0417%\n",
      "layer   3  Sparsity: 65.2410%\n",
      "total_backward_count 538450 real_backward_count 53274   9.894%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.620550/  1.769298, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1999%\n",
      "layer   2  Sparsity: 68.2243%\n",
      "layer   3  Sparsity: 65.5699%\n",
      "total_backward_count 548240 real_backward_count 53953   9.841%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.612912/  1.779328, val:  74.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1786%\n",
      "layer   2  Sparsity: 68.4964%\n",
      "layer   3  Sparsity: 66.0117%\n",
      "total_backward_count 558030 real_backward_count 54603   9.785%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.626659/  1.786453, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 93.2015%\n",
      "layer   2  Sparsity: 68.6323%\n",
      "layer   3  Sparsity: 67.0889%\n",
      "total_backward_count 567820 real_backward_count 55212   9.724%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.635554/  1.770029, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.1961%\n",
      "layer   2  Sparsity: 68.0344%\n",
      "layer   3  Sparsity: 65.7116%\n",
      "total_backward_count 577610 real_backward_count 55878   9.674%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.613302/  1.772501, val:  73.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.2067%\n",
      "layer   2  Sparsity: 67.7035%\n",
      "layer   3  Sparsity: 65.6270%\n",
      "total_backward_count 587400 real_backward_count 56554   9.628%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.624842/  1.767510, val:  71.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2038%\n",
      "layer   2  Sparsity: 67.7628%\n",
      "layer   3  Sparsity: 66.5943%\n",
      "total_backward_count 597190 real_backward_count 57241   9.585%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.608686/  1.745118, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1834%\n",
      "layer   2  Sparsity: 68.1245%\n",
      "layer   3  Sparsity: 66.8739%\n",
      "total_backward_count 606980 real_backward_count 57908   9.540%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.594195/  1.771616, val:  82.08%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1926%\n",
      "layer   2  Sparsity: 68.0451%\n",
      "layer   3  Sparsity: 66.2989%\n",
      "total_backward_count 616770 real_backward_count 58559   9.494%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.600670/  1.758775, val:  77.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2148%\n",
      "layer   2  Sparsity: 68.2295%\n",
      "layer   3  Sparsity: 66.3651%\n",
      "total_backward_count 626560 real_backward_count 59151   9.441%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.593170/  1.771834, val:  73.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2196%\n",
      "layer   2  Sparsity: 67.9820%\n",
      "layer   3  Sparsity: 64.9606%\n",
      "total_backward_count 636350 real_backward_count 59821   9.401%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.602355/  1.766694, val:  77.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1908%\n",
      "layer   2  Sparsity: 67.8713%\n",
      "layer   3  Sparsity: 65.5744%\n",
      "total_backward_count 646140 real_backward_count 60471   9.359%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.603491/  1.738583, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1884%\n",
      "layer   2  Sparsity: 67.9919%\n",
      "layer   3  Sparsity: 67.0146%\n",
      "total_backward_count 655930 real_backward_count 61061   9.309%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.590965/  1.740057, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1794%\n",
      "layer   2  Sparsity: 68.0203%\n",
      "layer   3  Sparsity: 66.6912%\n",
      "total_backward_count 665720 real_backward_count 61647   9.260%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.584817/  1.741894, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1838%\n",
      "layer   2  Sparsity: 67.9385%\n",
      "layer   3  Sparsity: 65.3514%\n",
      "total_backward_count 675510 real_backward_count 62237   9.213%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.574340/  1.735307, val:  81.67%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1906%\n",
      "layer   2  Sparsity: 67.9182%\n",
      "layer   3  Sparsity: 66.1710%\n",
      "total_backward_count 685300 real_backward_count 62850   9.171%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.576801/  1.728148, val:  81.25%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1998%\n",
      "layer   2  Sparsity: 68.0558%\n",
      "layer   3  Sparsity: 67.3080%\n",
      "total_backward_count 695090 real_backward_count 63466   9.131%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.595775/  1.733322, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1849%\n",
      "layer   2  Sparsity: 68.1563%\n",
      "layer   3  Sparsity: 67.2887%\n",
      "total_backward_count 704880 real_backward_count 64031   9.084%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.590774/  1.758197, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1935%\n",
      "layer   2  Sparsity: 68.2476%\n",
      "layer   3  Sparsity: 67.5747%\n",
      "total_backward_count 714670 real_backward_count 64597   9.039%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.584917/  1.761307, val:  71.25%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1971%\n",
      "layer   2  Sparsity: 67.9747%\n",
      "layer   3  Sparsity: 66.4354%\n",
      "total_backward_count 724460 real_backward_count 65199   9.000%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.591964/  1.730138, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2056%\n",
      "layer   2  Sparsity: 67.8164%\n",
      "layer   3  Sparsity: 67.1164%\n",
      "total_backward_count 734250 real_backward_count 65787   8.960%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.579177/  1.729713, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1813%\n",
      "layer   2  Sparsity: 67.6766%\n",
      "layer   3  Sparsity: 66.4815%\n",
      "total_backward_count 744040 real_backward_count 66355   8.918%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.591647/  1.755288, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2089%\n",
      "layer   2  Sparsity: 67.7425%\n",
      "layer   3  Sparsity: 66.4749%\n",
      "total_backward_count 753830 real_backward_count 66897   8.874%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.595557/  1.767902, val:  61.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2041%\n",
      "layer   2  Sparsity: 67.7645%\n",
      "layer   3  Sparsity: 65.2761%\n",
      "total_backward_count 763620 real_backward_count 67470   8.836%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.590176/  1.735271, val:  77.50%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2084%\n",
      "layer   2  Sparsity: 67.6142%\n",
      "layer   3  Sparsity: 66.2128%\n",
      "total_backward_count 773410 real_backward_count 68090   8.804%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.583014/  1.739560, val:  81.67%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1921%\n",
      "layer   2  Sparsity: 67.8370%\n",
      "layer   3  Sparsity: 66.8431%\n",
      "total_backward_count 783200 real_backward_count 68652   8.766%\n",
      "fc layer 2 self.abs_max_out: 5437.0\n",
      "lif layer 2 self.abs_max_v: 9384.5\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.593201/  1.727336, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1756%\n",
      "layer   2  Sparsity: 67.4622%\n",
      "layer   3  Sparsity: 67.3288%\n",
      "total_backward_count 792990 real_backward_count 69241   8.732%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.562807/  1.711590, val:  77.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2045%\n",
      "layer   2  Sparsity: 67.5988%\n",
      "layer   3  Sparsity: 66.7437%\n",
      "total_backward_count 802780 real_backward_count 69767   8.691%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.569740/  1.693681, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.1926%\n",
      "layer   2  Sparsity: 67.8309%\n",
      "layer   3  Sparsity: 66.8426%\n",
      "total_backward_count 812570 real_backward_count 70334   8.656%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.558388/  1.724903, val:  75.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1801%\n",
      "layer   2  Sparsity: 68.1010%\n",
      "layer   3  Sparsity: 66.7740%\n",
      "total_backward_count 822360 real_backward_count 70873   8.618%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.560577/  1.703557, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1974%\n",
      "layer   2  Sparsity: 67.9397%\n",
      "layer   3  Sparsity: 66.7963%\n",
      "total_backward_count 832150 real_backward_count 71441   8.585%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.559369/  1.736349, val:  80.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2021%\n",
      "layer   2  Sparsity: 67.5550%\n",
      "layer   3  Sparsity: 67.2813%\n",
      "total_backward_count 841940 real_backward_count 71945   8.545%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.567443/  1.729690, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1828%\n",
      "layer   2  Sparsity: 67.5073%\n",
      "layer   3  Sparsity: 67.2039%\n",
      "total_backward_count 851730 real_backward_count 72506   8.513%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.572629/  1.708269, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1624%\n",
      "layer   2  Sparsity: 67.3879%\n",
      "layer   3  Sparsity: 66.9025%\n",
      "total_backward_count 861520 real_backward_count 73034   8.477%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.570369/  1.716084, val:  78.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1780%\n",
      "layer   2  Sparsity: 67.7587%\n",
      "layer   3  Sparsity: 66.0938%\n",
      "total_backward_count 871310 real_backward_count 73601   8.447%\n",
      "fc layer 3 self.abs_max_out: 1718.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.561996/  1.710092, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2049%\n",
      "layer   2  Sparsity: 67.7095%\n",
      "layer   3  Sparsity: 66.4611%\n",
      "total_backward_count 881100 real_backward_count 74140   8.414%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.555353/  1.733111, val:  78.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2007%\n",
      "layer   2  Sparsity: 68.0357%\n",
      "layer   3  Sparsity: 66.1969%\n",
      "total_backward_count 890890 real_backward_count 74684   8.383%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.564440/  1.728087, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1714%\n",
      "layer   2  Sparsity: 67.7464%\n",
      "layer   3  Sparsity: 67.4971%\n",
      "total_backward_count 900680 real_backward_count 75211   8.350%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.561950/  1.715631, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2104%\n",
      "layer   2  Sparsity: 67.6454%\n",
      "layer   3  Sparsity: 67.5845%\n",
      "total_backward_count 910470 real_backward_count 75704   8.315%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.571879/  1.741404, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2073%\n",
      "layer   2  Sparsity: 67.4517%\n",
      "layer   3  Sparsity: 67.5292%\n",
      "total_backward_count 920260 real_backward_count 76227   8.283%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.555319/  1.743200, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2130%\n",
      "layer   2  Sparsity: 67.6442%\n",
      "layer   3  Sparsity: 68.0840%\n",
      "total_backward_count 930050 real_backward_count 76737   8.251%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.541619/  1.699474, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2111%\n",
      "layer   2  Sparsity: 67.7409%\n",
      "layer   3  Sparsity: 66.9764%\n",
      "total_backward_count 939840 real_backward_count 77259   8.220%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.547709/  1.701361, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1720%\n",
      "layer   2  Sparsity: 67.3679%\n",
      "layer   3  Sparsity: 66.9537%\n",
      "total_backward_count 949630 real_backward_count 77773   8.190%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.547561/  1.692044, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2059%\n",
      "layer   2  Sparsity: 67.7397%\n",
      "layer   3  Sparsity: 67.2916%\n",
      "total_backward_count 959420 real_backward_count 78268   8.158%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.534522/  1.728383, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2065%\n",
      "layer   2  Sparsity: 67.5287%\n",
      "layer   3  Sparsity: 66.9251%\n",
      "total_backward_count 969210 real_backward_count 78788   8.129%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.544560/  1.689283, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2052%\n",
      "layer   2  Sparsity: 67.6888%\n",
      "layer   3  Sparsity: 67.9588%\n",
      "total_backward_count 979000 real_backward_count 79290   8.099%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.523944/  1.706000, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1904%\n",
      "layer   2  Sparsity: 67.7937%\n",
      "layer   3  Sparsity: 67.8721%\n",
      "total_backward_count 988790 real_backward_count 79820   8.072%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.528144/  1.701368, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1898%\n",
      "layer   2  Sparsity: 67.9063%\n",
      "layer   3  Sparsity: 68.1921%\n",
      "total_backward_count 998580 real_backward_count 80384   8.050%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.540075/  1.702885, val:  82.50%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2286%\n",
      "layer   2  Sparsity: 67.9755%\n",
      "layer   3  Sparsity: 68.0649%\n",
      "total_backward_count 1008370 real_backward_count 80884   8.021%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.538859/  1.689826, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1867%\n",
      "layer   2  Sparsity: 67.7388%\n",
      "layer   3  Sparsity: 67.6527%\n",
      "total_backward_count 1018160 real_backward_count 81373   7.992%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.526354/  1.675999, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2109%\n",
      "layer   2  Sparsity: 67.8791%\n",
      "layer   3  Sparsity: 68.7341%\n",
      "total_backward_count 1027950 real_backward_count 81904   7.968%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.540625/  1.714622, val:  72.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1907%\n",
      "layer   2  Sparsity: 67.7565%\n",
      "layer   3  Sparsity: 67.6636%\n",
      "total_backward_count 1037740 real_backward_count 82393   7.940%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.547011/  1.721059, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2020%\n",
      "layer   2  Sparsity: 67.7339%\n",
      "layer   3  Sparsity: 67.1245%\n",
      "total_backward_count 1047530 real_backward_count 82872   7.911%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.542556/  1.703168, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1915%\n",
      "layer   2  Sparsity: 67.4618%\n",
      "layer   3  Sparsity: 67.6505%\n",
      "total_backward_count 1057320 real_backward_count 83346   7.883%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.554893/  1.717326, val:  79.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1939%\n",
      "layer   2  Sparsity: 67.4543%\n",
      "layer   3  Sparsity: 67.0913%\n",
      "total_backward_count 1067110 real_backward_count 83855   7.858%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.544977/  1.722761, val:  81.25%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1731%\n",
      "layer   2  Sparsity: 67.3379%\n",
      "layer   3  Sparsity: 67.2154%\n",
      "total_backward_count 1076900 real_backward_count 84324   7.830%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.550193/  1.696160, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2067%\n",
      "layer   2  Sparsity: 67.5485%\n",
      "layer   3  Sparsity: 66.5249%\n",
      "total_backward_count 1086690 real_backward_count 84784   7.802%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.545317/  1.693172, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2090%\n",
      "layer   2  Sparsity: 67.5864%\n",
      "layer   3  Sparsity: 67.2070%\n",
      "total_backward_count 1096480 real_backward_count 85271   7.777%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.550402/  1.714576, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2104%\n",
      "layer   2  Sparsity: 67.4031%\n",
      "layer   3  Sparsity: 67.9035%\n",
      "total_backward_count 1106270 real_backward_count 85763   7.752%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.540297/  1.699330, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1661%\n",
      "layer   2  Sparsity: 67.4089%\n",
      "layer   3  Sparsity: 67.5559%\n",
      "total_backward_count 1116060 real_backward_count 86234   7.727%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.536980/  1.706912, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2086%\n",
      "layer   2  Sparsity: 67.5109%\n",
      "layer   3  Sparsity: 67.4659%\n",
      "total_backward_count 1125850 real_backward_count 86720   7.703%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.536069/  1.708341, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2003%\n",
      "layer   2  Sparsity: 67.5165%\n",
      "layer   3  Sparsity: 67.8963%\n",
      "total_backward_count 1135640 real_backward_count 87218   7.680%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.540742/  1.705114, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1956%\n",
      "layer   2  Sparsity: 67.5217%\n",
      "layer   3  Sparsity: 68.4534%\n",
      "total_backward_count 1145430 real_backward_count 87654   7.652%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.539310/  1.704336, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2035%\n",
      "layer   2  Sparsity: 67.7143%\n",
      "layer   3  Sparsity: 67.9723%\n",
      "total_backward_count 1155220 real_backward_count 88126   7.629%\n",
      "fc layer 1 self.abs_max_out: 10632.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.533569/  1.717968, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1778%\n",
      "layer   2  Sparsity: 67.3210%\n",
      "layer   3  Sparsity: 67.6462%\n",
      "total_backward_count 1165010 real_backward_count 88596   7.605%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.535941/  1.728296, val:  80.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1886%\n",
      "layer   2  Sparsity: 67.1895%\n",
      "layer   3  Sparsity: 67.5940%\n",
      "total_backward_count 1174800 real_backward_count 89071   7.582%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.545197/  1.709833, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1810%\n",
      "layer   2  Sparsity: 66.9873%\n",
      "layer   3  Sparsity: 67.0062%\n",
      "total_backward_count 1184590 real_backward_count 89535   7.558%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.521505/  1.712508, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.2013%\n",
      "layer   2  Sparsity: 67.3067%\n",
      "layer   3  Sparsity: 67.7095%\n",
      "total_backward_count 1194380 real_backward_count 90019   7.537%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.533758/  1.706948, val:  75.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1859%\n",
      "layer   2  Sparsity: 67.4492%\n",
      "layer   3  Sparsity: 67.3657%\n",
      "total_backward_count 1204170 real_backward_count 90493   7.515%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.523216/  1.689378, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 93.1968%\n",
      "layer   2  Sparsity: 67.2589%\n",
      "layer   3  Sparsity: 67.2520%\n",
      "total_backward_count 1213960 real_backward_count 90967   7.493%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.525949/  1.688017, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2134%\n",
      "layer   2  Sparsity: 67.2213%\n",
      "layer   3  Sparsity: 67.2891%\n",
      "total_backward_count 1223750 real_backward_count 91388   7.468%\n",
      "fc layer 1 self.abs_max_out: 10959.0\n",
      "lif layer 1 self.abs_max_v: 20343.5\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.524200/  1.679897, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1722%\n",
      "layer   2  Sparsity: 67.6528%\n",
      "layer   3  Sparsity: 66.7939%\n",
      "total_backward_count 1233540 real_backward_count 91817   7.443%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.523790/  1.686054, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2161%\n",
      "layer   2  Sparsity: 67.7878%\n",
      "layer   3  Sparsity: 67.3110%\n",
      "total_backward_count 1243330 real_backward_count 92249   7.420%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.524212/  1.692786, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 93.2095%\n",
      "layer   2  Sparsity: 67.6987%\n",
      "layer   3  Sparsity: 67.8658%\n",
      "total_backward_count 1253120 real_backward_count 92688   7.397%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.528426/  1.686271, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1794%\n",
      "layer   2  Sparsity: 67.6208%\n",
      "layer   3  Sparsity: 68.5797%\n",
      "total_backward_count 1262910 real_backward_count 93086   7.371%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.508575/  1.653988, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1992%\n",
      "layer   2  Sparsity: 67.5860%\n",
      "layer   3  Sparsity: 67.6408%\n",
      "total_backward_count 1272700 real_backward_count 93541   7.350%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.518338/  1.672001, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1788%\n",
      "layer   2  Sparsity: 67.3198%\n",
      "layer   3  Sparsity: 66.7604%\n",
      "total_backward_count 1282490 real_backward_count 94019   7.331%\n",
      "fc layer 3 self.abs_max_out: 1768.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.532583/  1.687048, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1906%\n",
      "layer   2  Sparsity: 67.1626%\n",
      "layer   3  Sparsity: 66.5874%\n",
      "total_backward_count 1292280 real_backward_count 94471   7.310%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.503661/  1.661093, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2030%\n",
      "layer   2  Sparsity: 67.2234%\n",
      "layer   3  Sparsity: 66.6145%\n",
      "total_backward_count 1302070 real_backward_count 94901   7.288%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.498790/  1.646026, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2022%\n",
      "layer   2  Sparsity: 67.4276%\n",
      "layer   3  Sparsity: 66.6546%\n",
      "total_backward_count 1311860 real_backward_count 95335   7.267%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.490603/  1.668865, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1834%\n",
      "layer   2  Sparsity: 67.2993%\n",
      "layer   3  Sparsity: 66.6207%\n",
      "total_backward_count 1321650 real_backward_count 95773   7.246%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.502593/  1.650431, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1823%\n",
      "layer   2  Sparsity: 67.2678%\n",
      "layer   3  Sparsity: 67.5534%\n",
      "total_backward_count 1331440 real_backward_count 96202   7.225%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.507331/  1.660298, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1969%\n",
      "layer   2  Sparsity: 67.2091%\n",
      "layer   3  Sparsity: 67.3145%\n",
      "total_backward_count 1341230 real_backward_count 96608   7.203%\n",
      "fc layer 1 self.abs_max_out: 10991.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.501026/  1.676657, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2116%\n",
      "layer   2  Sparsity: 67.0430%\n",
      "layer   3  Sparsity: 66.4302%\n",
      "total_backward_count 1351020 real_backward_count 97041   7.183%\n",
      "fc layer 1 self.abs_max_out: 11057.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.509387/  1.674524, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1848%\n",
      "layer   2  Sparsity: 67.3083%\n",
      "layer   3  Sparsity: 66.0354%\n",
      "total_backward_count 1360810 real_backward_count 97488   7.164%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.503882/  1.660318, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1994%\n",
      "layer   2  Sparsity: 67.1078%\n",
      "layer   3  Sparsity: 66.1991%\n",
      "total_backward_count 1370600 real_backward_count 97930   7.145%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.507794/  1.682666, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1739%\n",
      "layer   2  Sparsity: 67.1800%\n",
      "layer   3  Sparsity: 67.4782%\n",
      "total_backward_count 1380390 real_backward_count 98332   7.123%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.514519/  1.672837, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2024%\n",
      "layer   2  Sparsity: 67.4312%\n",
      "layer   3  Sparsity: 68.1456%\n",
      "total_backward_count 1390180 real_backward_count 98769   7.105%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.504054/  1.681424, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1836%\n",
      "layer   2  Sparsity: 67.8255%\n",
      "layer   3  Sparsity: 68.5209%\n",
      "total_backward_count 1399970 real_backward_count 99196   7.086%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.500658/  1.666561, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1758%\n",
      "layer   2  Sparsity: 67.7099%\n",
      "layer   3  Sparsity: 68.1182%\n",
      "total_backward_count 1409760 real_backward_count 99677   7.070%\n",
      "lif layer 2 self.abs_max_v: 9458.5\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.489615/  1.657512, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2048%\n",
      "layer   2  Sparsity: 67.6960%\n",
      "layer   3  Sparsity: 68.3316%\n",
      "total_backward_count 1419550 real_backward_count 100104   7.052%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.479282/  1.627758, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2084%\n",
      "layer   2  Sparsity: 67.5046%\n",
      "layer   3  Sparsity: 68.2514%\n",
      "total_backward_count 1429340 real_backward_count 100482   7.030%\n",
      "fc layer 3 self.abs_max_out: 1826.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.465560/  1.641647, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2061%\n",
      "layer   2  Sparsity: 67.5639%\n",
      "layer   3  Sparsity: 66.7281%\n",
      "total_backward_count 1439130 real_backward_count 100903   7.011%\n",
      "fc layer 1 self.abs_max_out: 11229.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.459223/  1.615271, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1904%\n",
      "layer   2  Sparsity: 67.3360%\n",
      "layer   3  Sparsity: 66.4029%\n",
      "total_backward_count 1448920 real_backward_count 101323   6.993%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.459144/  1.641280, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2018%\n",
      "layer   2  Sparsity: 67.3854%\n",
      "layer   3  Sparsity: 66.6764%\n",
      "total_backward_count 1458710 real_backward_count 101775   6.977%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.462302/  1.631838, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1959%\n",
      "layer   2  Sparsity: 67.3305%\n",
      "layer   3  Sparsity: 66.4587%\n",
      "total_backward_count 1468500 real_backward_count 102173   6.958%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.469822/  1.660905, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2262%\n",
      "layer   2  Sparsity: 67.3856%\n",
      "layer   3  Sparsity: 66.6327%\n",
      "total_backward_count 1478290 real_backward_count 102603   6.941%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.480755/  1.648032, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2011%\n",
      "layer   2  Sparsity: 67.8038%\n",
      "layer   3  Sparsity: 66.3955%\n",
      "total_backward_count 1488080 real_backward_count 103003   6.922%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.468035/  1.642182, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2042%\n",
      "layer   2  Sparsity: 67.4205%\n",
      "layer   3  Sparsity: 67.4570%\n",
      "total_backward_count 1497870 real_backward_count 103393   6.903%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.479628/  1.646282, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1903%\n",
      "layer   2  Sparsity: 67.1542%\n",
      "layer   3  Sparsity: 68.3927%\n",
      "total_backward_count 1507660 real_backward_count 103824   6.886%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.469852/  1.641556, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1844%\n",
      "layer   2  Sparsity: 67.0583%\n",
      "layer   3  Sparsity: 67.6226%\n",
      "total_backward_count 1517450 real_backward_count 104205   6.867%\n",
      "fc layer 1 self.abs_max_out: 11242.0\n",
      "lif layer 1 self.abs_max_v: 20669.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.467143/  1.662324, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2035%\n",
      "layer   2  Sparsity: 67.1966%\n",
      "layer   3  Sparsity: 67.0213%\n",
      "total_backward_count 1527240 real_backward_count 104609   6.850%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.476092/  1.672856, val:  77.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1954%\n",
      "layer   2  Sparsity: 66.8397%\n",
      "layer   3  Sparsity: 66.4995%\n",
      "total_backward_count 1537030 real_backward_count 105002   6.831%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.468094/  1.638605, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1835%\n",
      "layer   2  Sparsity: 66.9766%\n",
      "layer   3  Sparsity: 67.4551%\n",
      "total_backward_count 1546820 real_backward_count 105392   6.813%\n",
      "fc layer 1 self.abs_max_out: 11454.0\n",
      "fc layer 1 self.abs_max_out: 11497.0\n",
      "lif layer 1 self.abs_max_v: 21110.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.468150/  1.663503, val:  76.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2142%\n",
      "layer   2  Sparsity: 67.1788%\n",
      "layer   3  Sparsity: 66.7123%\n",
      "total_backward_count 1556610 real_backward_count 105828   6.799%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.468757/  1.637097, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.2033%\n",
      "layer   2  Sparsity: 67.1160%\n",
      "layer   3  Sparsity: 67.3512%\n",
      "total_backward_count 1566400 real_backward_count 106224   6.781%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.458099/  1.652734, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2141%\n",
      "layer   2  Sparsity: 67.3144%\n",
      "layer   3  Sparsity: 66.9824%\n",
      "total_backward_count 1576190 real_backward_count 106619   6.764%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.459331/  1.639284, val:  82.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 93.1944%\n",
      "layer   2  Sparsity: 67.1491%\n",
      "layer   3  Sparsity: 67.1953%\n",
      "total_backward_count 1585980 real_backward_count 106996   6.746%\n",
      "fc layer 1 self.abs_max_out: 11523.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.462942/  1.630294, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1924%\n",
      "layer   2  Sparsity: 67.0542%\n",
      "layer   3  Sparsity: 67.7465%\n",
      "total_backward_count 1595770 real_backward_count 107383   6.729%\n",
      "fc layer 1 self.abs_max_out: 11574.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.447021/  1.637975, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2004%\n",
      "layer   2  Sparsity: 67.1366%\n",
      "layer   3  Sparsity: 67.6452%\n",
      "total_backward_count 1605560 real_backward_count 107759   6.712%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.470656/  1.631047, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1832%\n",
      "layer   2  Sparsity: 67.1632%\n",
      "layer   3  Sparsity: 66.8575%\n",
      "total_backward_count 1615350 real_backward_count 108188   6.697%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.464458/  1.654043, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2003%\n",
      "layer   2  Sparsity: 67.0512%\n",
      "layer   3  Sparsity: 67.4107%\n",
      "total_backward_count 1625140 real_backward_count 108594   6.682%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.475936/  1.656522, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2011%\n",
      "layer   2  Sparsity: 67.1673%\n",
      "layer   3  Sparsity: 65.6844%\n",
      "total_backward_count 1634930 real_backward_count 108977   6.666%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.474900/  1.660262, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1844%\n",
      "layer   2  Sparsity: 67.1982%\n",
      "layer   3  Sparsity: 66.3658%\n",
      "total_backward_count 1644720 real_backward_count 109386   6.651%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.487588/  1.657188, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2177%\n",
      "layer   2  Sparsity: 67.4719%\n",
      "layer   3  Sparsity: 66.8198%\n",
      "total_backward_count 1654510 real_backward_count 109800   6.636%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.471009/  1.633130, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1969%\n",
      "layer   2  Sparsity: 66.9618%\n",
      "layer   3  Sparsity: 66.8494%\n",
      "total_backward_count 1664300 real_backward_count 110187   6.621%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.462059/  1.649575, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1970%\n",
      "layer   2  Sparsity: 66.9549%\n",
      "layer   3  Sparsity: 66.1491%\n",
      "total_backward_count 1674090 real_backward_count 110539   6.603%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.468709/  1.670075, val:  80.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1722%\n",
      "layer   2  Sparsity: 66.9628%\n",
      "layer   3  Sparsity: 66.5063%\n",
      "total_backward_count 1683880 real_backward_count 110948   6.589%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.468462/  1.630838, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1885%\n",
      "layer   2  Sparsity: 67.0286%\n",
      "layer   3  Sparsity: 66.5868%\n",
      "total_backward_count 1693670 real_backward_count 111335   6.574%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.449660/  1.630336, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1846%\n",
      "layer   2  Sparsity: 67.0526%\n",
      "layer   3  Sparsity: 65.8737%\n",
      "total_backward_count 1703460 real_backward_count 111718   6.558%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.447347/  1.631382, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1874%\n",
      "layer   2  Sparsity: 67.0718%\n",
      "layer   3  Sparsity: 66.8993%\n",
      "total_backward_count 1713250 real_backward_count 112137   6.545%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.446374/  1.618359, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1678%\n",
      "layer   2  Sparsity: 67.1014%\n",
      "layer   3  Sparsity: 67.1166%\n",
      "total_backward_count 1723040 real_backward_count 112546   6.532%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.447845/  1.631865, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.1931%\n",
      "layer   2  Sparsity: 67.1979%\n",
      "layer   3  Sparsity: 67.8402%\n",
      "total_backward_count 1732830 real_backward_count 112937   6.517%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.446203/  1.638966, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1929%\n",
      "layer   2  Sparsity: 66.8151%\n",
      "layer   3  Sparsity: 67.7068%\n",
      "total_backward_count 1742620 real_backward_count 113314   6.503%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.450760/  1.655495, val:  77.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1904%\n",
      "layer   2  Sparsity: 66.8324%\n",
      "layer   3  Sparsity: 65.7844%\n",
      "total_backward_count 1752410 real_backward_count 113715   6.489%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.451316/  1.633156, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.2057%\n",
      "layer   2  Sparsity: 66.9622%\n",
      "layer   3  Sparsity: 65.8618%\n",
      "total_backward_count 1762200 real_backward_count 114085   6.474%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.455271/  1.632222, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.1809%\n",
      "layer   2  Sparsity: 67.0757%\n",
      "layer   3  Sparsity: 66.6355%\n",
      "total_backward_count 1771990 real_backward_count 114506   6.462%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.444252/  1.630054, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2046%\n",
      "layer   2  Sparsity: 67.1396%\n",
      "layer   3  Sparsity: 67.4568%\n",
      "total_backward_count 1781780 real_backward_count 114853   6.446%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.456079/  1.639183, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1729%\n",
      "layer   2  Sparsity: 67.0506%\n",
      "layer   3  Sparsity: 68.2548%\n",
      "total_backward_count 1791570 real_backward_count 115202   6.430%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.446398/  1.632037, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1933%\n",
      "layer   2  Sparsity: 66.9614%\n",
      "layer   3  Sparsity: 66.6815%\n",
      "total_backward_count 1801360 real_backward_count 115570   6.416%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.449617/  1.624767, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1968%\n",
      "layer   2  Sparsity: 66.8909%\n",
      "layer   3  Sparsity: 67.0603%\n",
      "total_backward_count 1811150 real_backward_count 115920   6.400%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.440885/  1.617411, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1738%\n",
      "layer   2  Sparsity: 66.9452%\n",
      "layer   3  Sparsity: 68.0341%\n",
      "total_backward_count 1820940 real_backward_count 116301   6.387%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.446772/  1.615882, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1821%\n",
      "layer   2  Sparsity: 66.9508%\n",
      "layer   3  Sparsity: 67.4284%\n",
      "total_backward_count 1830730 real_backward_count 116690   6.374%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.432297/  1.659324, val:  74.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1946%\n",
      "layer   2  Sparsity: 67.0878%\n",
      "layer   3  Sparsity: 67.7253%\n",
      "total_backward_count 1840520 real_backward_count 117068   6.361%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.454349/  1.637483, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1865%\n",
      "layer   2  Sparsity: 67.0571%\n",
      "layer   3  Sparsity: 67.0007%\n",
      "total_backward_count 1850310 real_backward_count 117441   6.347%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.448075/  1.623847, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1730%\n",
      "layer   2  Sparsity: 67.1676%\n",
      "layer   3  Sparsity: 68.3027%\n",
      "total_backward_count 1860100 real_backward_count 117794   6.333%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.438035/  1.618061, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.1823%\n",
      "layer   2  Sparsity: 66.9873%\n",
      "layer   3  Sparsity: 67.9643%\n",
      "total_backward_count 1869890 real_backward_count 118127   6.317%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.455582/  1.637663, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.1931%\n",
      "layer   2  Sparsity: 66.8928%\n",
      "layer   3  Sparsity: 68.2855%\n",
      "total_backward_count 1879680 real_backward_count 118475   6.303%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.456691/  1.618451, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1848%\n",
      "layer   2  Sparsity: 66.7239%\n",
      "layer   3  Sparsity: 66.8605%\n",
      "total_backward_count 1889470 real_backward_count 118840   6.290%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.428742/  1.651508, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2063%\n",
      "layer   2  Sparsity: 67.0201%\n",
      "layer   3  Sparsity: 67.0223%\n",
      "total_backward_count 1899260 real_backward_count 119159   6.274%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.435008/  1.625631, val:  76.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1995%\n",
      "layer   2  Sparsity: 67.0693%\n",
      "layer   3  Sparsity: 66.3578%\n",
      "total_backward_count 1909050 real_backward_count 119501   6.260%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.434121/  1.648075, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1649%\n",
      "layer   2  Sparsity: 67.1605%\n",
      "layer   3  Sparsity: 67.3415%\n",
      "total_backward_count 1918840 real_backward_count 119844   6.246%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.430775/  1.607134, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 93.2054%\n",
      "layer   2  Sparsity: 67.0981%\n",
      "layer   3  Sparsity: 67.0131%\n",
      "total_backward_count 1928630 real_backward_count 120200   6.232%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.421044/  1.592391, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 93.2345%\n",
      "layer   2  Sparsity: 67.1491%\n",
      "layer   3  Sparsity: 66.9670%\n",
      "total_backward_count 1938420 real_backward_count 120552   6.219%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.433091/  1.624556, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1898%\n",
      "layer   2  Sparsity: 66.9522%\n",
      "layer   3  Sparsity: 66.9194%\n",
      "total_backward_count 1948210 real_backward_count 120899   6.206%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.442256/  1.589321, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.1993%\n",
      "layer   2  Sparsity: 66.9306%\n",
      "layer   3  Sparsity: 66.2734%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e873466521aa45ea9b7537e551335da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.44226</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>1.58932</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sweep-139</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wdn62czp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/wdn62czp</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_130401-wdn62czp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rrlsiskm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_172225-rrlsiskm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rrlsiskm' target=\"_blank\">lively-sweep-144</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rrlsiskm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rrlsiskm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_172234_802', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 3, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 119.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 149.0\n",
      "lif layer 1 self.abs_max_v: 169.5\n",
      "fc layer 1 self.abs_max_out: 154.0\n",
      "lif layer 1 self.abs_max_v: 197.5\n",
      "lif layer 1 self.abs_max_v: 200.0\n",
      "lif layer 1 self.abs_max_v: 230.0\n",
      "lif layer 1 self.abs_max_v: 244.0\n",
      "fc layer 1 self.abs_max_out: 206.0\n",
      "fc layer 1 self.abs_max_out: 208.0\n",
      "lif layer 1 self.abs_max_v: 245.0\n",
      "lif layer 1 self.abs_max_v: 269.5\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "lif layer 2 self.abs_max_v: 70.0\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 287.0\n",
      "lif layer 2 self.abs_max_v: 93.5\n",
      "lif layer 1 self.abs_max_v: 345.0\n",
      "fc layer 1 self.abs_max_out: 320.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "fc layer 2 self.abs_max_out: 111.0\n",
      "lif layer 2 self.abs_max_v: 117.5\n",
      "fc layer 1 self.abs_max_out: 459.0\n",
      "lif layer 1 self.abs_max_v: 577.5\n",
      "fc layer 2 self.abs_max_out: 173.0\n",
      "lif layer 2 self.abs_max_v: 224.0\n",
      "lif layer 2 self.abs_max_v: 263.0\n",
      "fc layer 1 self.abs_max_out: 466.0\n",
      "fc layer 2 self.abs_max_out: 196.0\n",
      "lif layer 2 self.abs_max_v: 303.0\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "fc layer 2 self.abs_max_out: 229.0\n",
      "lif layer 2 self.abs_max_v: 316.0\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 1 self.abs_max_out: 529.0\n",
      "lif layer 2 self.abs_max_v: 359.0\n",
      "fc layer 1 self.abs_max_out: 551.0\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "fc layer 1 self.abs_max_out: 764.0\n",
      "lif layer 1 self.abs_max_v: 764.0\n",
      "fc layer 2 self.abs_max_out: 299.0\n",
      "lif layer 2 self.abs_max_v: 360.5\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "fc layer 1 self.abs_max_out: 808.0\n",
      "lif layer 1 self.abs_max_v: 808.0\n",
      "lif layer 2 self.abs_max_v: 389.5\n",
      "lif layer 2 self.abs_max_v: 392.0\n",
      "lif layer 2 self.abs_max_v: 393.0\n",
      "lif layer 2 self.abs_max_v: 398.0\n",
      "lif layer 2 self.abs_max_v: 444.0\n",
      "lif layer 2 self.abs_max_v: 466.0\n",
      "lif layer 2 self.abs_max_v: 491.5\n",
      "fc layer 2 self.abs_max_out: 357.0\n",
      "fc layer 2 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "fc layer 2 self.abs_max_out: 543.0\n",
      "lif layer 2 self.abs_max_v: 543.0\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "lif layer 2 self.abs_max_v: 600.0\n",
      "fc layer 1 self.abs_max_out: 893.0\n",
      "lif layer 1 self.abs_max_v: 893.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "fc layer 1 self.abs_max_out: 961.0\n",
      "lif layer 1 self.abs_max_v: 961.0\n",
      "lif layer 2 self.abs_max_v: 618.5\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "fc layer 2 self.abs_max_out: 574.0\n",
      "fc layer 1 self.abs_max_out: 966.0\n",
      "lif layer 1 self.abs_max_v: 1016.5\n",
      "fc layer 1 self.abs_max_out: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1040.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "lif layer 2 self.abs_max_v: 669.5\n",
      "fc layer 1 self.abs_max_out: 1466.0\n",
      "lif layer 1 self.abs_max_v: 1466.0\n",
      "lif layer 2 self.abs_max_v: 682.5\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 2 self.abs_max_out: 661.0\n",
      "lif layer 2 self.abs_max_v: 788.5\n",
      "lif layer 2 self.abs_max_v: 799.0\n",
      "lif layer 2 self.abs_max_v: 853.5\n",
      "lif layer 2 self.abs_max_v: 954.0\n",
      "fc layer 2 self.abs_max_out: 662.0\n",
      "fc layer 2 self.abs_max_out: 703.0\n",
      "fc layer 2 self.abs_max_out: 708.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "fc layer 2 self.abs_max_out: 754.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "fc layer 2 self.abs_max_out: 763.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 2 self.abs_max_out: 839.0\n",
      "fc layer 2 self.abs_max_out: 867.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 982.5\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "fc layer 2 self.abs_max_out: 954.0\n",
      "lif layer 2 self.abs_max_v: 998.5\n",
      "fc layer 2 self.abs_max_out: 980.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "lif layer 2 self.abs_max_v: 1016.0\n",
      "fc layer 2 self.abs_max_out: 1027.0\n",
      "lif layer 2 self.abs_max_v: 1027.0\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "lif layer 2 self.abs_max_v: 1039.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "lif layer 2 self.abs_max_v: 1086.0\n",
      "fc layer 2 self.abs_max_out: 1177.0\n",
      "lif layer 2 self.abs_max_v: 1177.0\n",
      "fc layer 1 self.abs_max_out: 1525.0\n",
      "lif layer 1 self.abs_max_v: 1525.0\n",
      "fc layer 1 self.abs_max_out: 1588.0\n",
      "lif layer 1 self.abs_max_v: 1588.0\n",
      "fc layer 1 self.abs_max_out: 1683.0\n",
      "lif layer 1 self.abs_max_v: 1683.0\n",
      "fc layer 2 self.abs_max_out: 1251.0\n",
      "lif layer 2 self.abs_max_v: 1251.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 1 self.abs_max_out: 1744.0\n",
      "lif layer 1 self.abs_max_v: 1744.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "fc layer 2 self.abs_max_out: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1262.0\n",
      "fc layer 2 self.abs_max_out: 1348.0\n",
      "lif layer 2 self.abs_max_v: 1348.0\n",
      "fc layer 2 self.abs_max_out: 1367.0\n",
      "lif layer 2 self.abs_max_v: 1367.0\n",
      "fc layer 1 self.abs_max_out: 1937.0\n",
      "lif layer 1 self.abs_max_v: 1937.0\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "lif layer 1 self.abs_max_v: 1984.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "fc layer 2 self.abs_max_out: 1394.0\n",
      "lif layer 2 self.abs_max_v: 1394.0\n",
      "fc layer 1 self.abs_max_out: 2027.0\n",
      "lif layer 1 self.abs_max_v: 2027.0\n",
      "fc layer 1 self.abs_max_out: 2108.0\n",
      "lif layer 1 self.abs_max_v: 2108.0\n",
      "fc layer 1 self.abs_max_out: 2210.0\n",
      "lif layer 1 self.abs_max_v: 2210.0\n",
      "fc layer 2 self.abs_max_out: 1413.0\n",
      "lif layer 2 self.abs_max_v: 1413.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 1493.0\n",
      "lif layer 2 self.abs_max_v: 1493.0\n",
      "fc layer 2 self.abs_max_out: 1568.0\n",
      "lif layer 2 self.abs_max_v: 1568.0\n",
      "fc layer 2 self.abs_max_out: 1577.0\n",
      "lif layer 2 self.abs_max_v: 1577.0\n",
      "fc layer 2 self.abs_max_out: 1597.0\n",
      "lif layer 2 self.abs_max_v: 1597.0\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "lif layer 1 self.abs_max_v: 2212.0\n",
      "fc layer 1 self.abs_max_out: 2238.0\n",
      "lif layer 1 self.abs_max_v: 2238.0\n",
      "fc layer 2 self.abs_max_out: 1642.0\n",
      "lif layer 2 self.abs_max_v: 1642.0\n",
      "fc layer 2 self.abs_max_out: 1787.0\n",
      "lif layer 2 self.abs_max_v: 1787.0\n",
      "fc layer 1 self.abs_max_out: 2405.0\n",
      "lif layer 1 self.abs_max_v: 2405.0\n",
      "fc layer 1 self.abs_max_out: 2504.0\n",
      "lif layer 1 self.abs_max_v: 2504.0\n",
      "fc layer 2 self.abs_max_out: 1819.0\n",
      "lif layer 2 self.abs_max_v: 1819.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.027976/  2.112720, val:  41.67%, val_best:  41.67%, tr:  90.50%, tr_best:  90.50%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9260%\n",
      "layer   2  Sparsity: 82.1632%\n",
      "layer   3  Sparsity: 86.5263%\n",
      "total_backward_count 9790 real_backward_count 3058  31.236%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1831.0\n",
      "lif layer 2 self.abs_max_v: 1831.0\n",
      "fc layer 2 self.abs_max_out: 1843.0\n",
      "lif layer 2 self.abs_max_v: 1843.0\n",
      "fc layer 2 self.abs_max_out: 2007.0\n",
      "lif layer 2 self.abs_max_v: 2007.0\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "lif layer 2 self.abs_max_v: 2092.0\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "fc layer 1 self.abs_max_out: 2576.0\n",
      "lif layer 1 self.abs_max_v: 2576.0\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "lif layer 2 self.abs_max_v: 2343.0\n",
      "fc layer 1 self.abs_max_out: 2727.0\n",
      "lif layer 1 self.abs_max_v: 2727.0\n",
      "fc layer 1 self.abs_max_out: 2742.0\n",
      "lif layer 1 self.abs_max_v: 2742.0\n",
      "fc layer 1 self.abs_max_out: 2782.0\n",
      "lif layer 1 self.abs_max_v: 2782.0\n",
      "fc layer 1 self.abs_max_out: 2988.0\n",
      "lif layer 1 self.abs_max_v: 2988.0\n",
      "fc layer 2 self.abs_max_out: 2345.0\n",
      "lif layer 2 self.abs_max_v: 2345.0\n",
      "fc layer 2 self.abs_max_out: 2430.0\n",
      "lif layer 2 self.abs_max_v: 2430.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.964504/  2.090792, val:  41.25%, val_best:  41.67%, tr:  98.77%, tr_best:  98.77%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9358%\n",
      "layer   2  Sparsity: 79.8936%\n",
      "layer   3  Sparsity: 83.8907%\n",
      "total_backward_count 19580 real_backward_count 5088  25.986%\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.955808/  2.088611, val:  40.42%, val_best:  41.67%, tr:  98.37%, tr_best:  98.77%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9474%\n",
      "layer   2  Sparsity: 79.2491%\n",
      "layer   3  Sparsity: 83.4847%\n",
      "total_backward_count 29370 real_backward_count 6948  23.657%\n",
      "fc layer 1 self.abs_max_out: 3003.0\n",
      "lif layer 1 self.abs_max_v: 3003.0\n",
      "fc layer 1 self.abs_max_out: 3038.0\n",
      "lif layer 1 self.abs_max_v: 3038.0\n",
      "fc layer 1 self.abs_max_out: 3143.0\n",
      "lif layer 1 self.abs_max_v: 3143.0\n",
      "fc layer 1 self.abs_max_out: 3253.0\n",
      "lif layer 1 self.abs_max_v: 3253.0\n",
      "fc layer 1 self.abs_max_out: 3649.0\n",
      "lif layer 1 self.abs_max_v: 3649.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.935523/  2.074393, val:  39.58%, val_best:  41.67%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9346%\n",
      "layer   2  Sparsity: 78.9228%\n",
      "layer   3  Sparsity: 82.5019%\n",
      "total_backward_count 39160 real_backward_count 8727  22.285%\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 2 self.abs_max_out: 2446.0\n",
      "lif layer 2 self.abs_max_v: 2446.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.923546/  2.062269, val:  44.58%, val_best:  44.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9255%\n",
      "layer   2  Sparsity: 78.3670%\n",
      "layer   3  Sparsity: 81.7885%\n",
      "total_backward_count 48950 real_backward_count 10394  21.234%\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 1 self.abs_max_out: 3719.0\n",
      "lif layer 1 self.abs_max_v: 3719.0\n",
      "fc layer 3 self.abs_max_out: 252.0\n",
      "lif layer 1 self.abs_max_v: 3890.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.900185/  2.071986, val:  52.92%, val_best:  52.92%, tr:  99.18%, tr_best:  99.69%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9331%\n",
      "layer   2  Sparsity: 78.4487%\n",
      "layer   3  Sparsity: 81.9719%\n",
      "total_backward_count 58740 real_backward_count 12053  20.519%\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 1 self.abs_max_out: 3743.0\n",
      "fc layer 2 self.abs_max_out: 2488.0\n",
      "lif layer 2 self.abs_max_v: 2488.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.897340/  2.061320, val:  49.58%, val_best:  52.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9397%\n",
      "layer   2  Sparsity: 78.2169%\n",
      "layer   3  Sparsity: 81.3101%\n",
      "total_backward_count 68530 real_backward_count 13693  19.981%\n",
      "fc layer 1 self.abs_max_out: 3771.0\n",
      "fc layer 1 self.abs_max_out: 4416.0\n",
      "lif layer 1 self.abs_max_v: 4416.0\n",
      "lif layer 2 self.abs_max_v: 2584.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.870497/  2.022528, val:  55.00%, val_best:  55.00%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9359%\n",
      "layer   2  Sparsity: 78.0084%\n",
      "layer   3  Sparsity: 81.2833%\n",
      "total_backward_count 78320 real_backward_count 15227  19.442%\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.850646/  2.000900, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9327%\n",
      "layer   2  Sparsity: 77.9637%\n",
      "layer   3  Sparsity: 81.1814%\n",
      "total_backward_count 88110 real_backward_count 16772  19.035%\n",
      "fc layer 2 self.abs_max_out: 2516.0\n",
      "fc layer 2 self.abs_max_out: 2534.0\n",
      "fc layer 2 self.abs_max_out: 2545.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.830660/  2.006481, val:  52.50%, val_best:  57.08%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9275%\n",
      "layer   2  Sparsity: 77.9491%\n",
      "layer   3  Sparsity: 81.3157%\n",
      "total_backward_count 97900 real_backward_count 18293  18.685%\n",
      "lif layer 2 self.abs_max_v: 2601.5\n",
      "fc layer 1 self.abs_max_out: 4759.0\n",
      "lif layer 1 self.abs_max_v: 4759.0\n",
      "lif layer 2 self.abs_max_v: 2681.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.812940/  1.963974, val:  55.00%, val_best:  57.08%, tr:  99.59%, tr_best:  99.69%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9214%\n",
      "layer   2  Sparsity: 78.0967%\n",
      "layer   3  Sparsity: 80.9561%\n",
      "total_backward_count 107690 real_backward_count 19777  18.365%\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "lif layer 2 self.abs_max_v: 2727.0\n",
      "fc layer 1 self.abs_max_out: 4900.0\n",
      "lif layer 1 self.abs_max_v: 4900.0\n",
      "lif layer 2 self.abs_max_v: 2795.5\n",
      "lif layer 2 self.abs_max_v: 2808.5\n",
      "lif layer 1 self.abs_max_v: 4903.0\n",
      "lif layer 1 self.abs_max_v: 5023.5\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.810991/  1.960376, val:  69.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9191%\n",
      "layer   2  Sparsity: 77.7918%\n",
      "layer   3  Sparsity: 80.8216%\n",
      "total_backward_count 117480 real_backward_count 21260  18.097%\n",
      "lif layer 2 self.abs_max_v: 2842.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.795851/  1.960184, val:  49.58%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9297%\n",
      "layer   2  Sparsity: 77.5602%\n",
      "layer   3  Sparsity: 81.0803%\n",
      "total_backward_count 127270 real_backward_count 22632  17.783%\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 1 self.abs_max_out: 4971.0\n",
      "lif layer 1 self.abs_max_v: 5030.0\n",
      "lif layer 1 self.abs_max_v: 5071.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.782151/  1.953950, val:  50.42%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9042%\n",
      "layer   2  Sparsity: 77.2957%\n",
      "layer   3  Sparsity: 80.3873%\n",
      "total_backward_count 137060 real_backward_count 24028  17.531%\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "lif layer 1 self.abs_max_v: 5131.0\n",
      "fc layer 2 self.abs_max_out: 2564.0\n",
      "lif layer 2 self.abs_max_v: 2973.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.766738/  1.916919, val:  65.42%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9154%\n",
      "layer   2  Sparsity: 77.3419%\n",
      "layer   3  Sparsity: 80.6673%\n",
      "total_backward_count 146850 real_backward_count 25373  17.278%\n",
      "lif layer 1 self.abs_max_v: 5519.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.764396/  1.921950, val:  62.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9159%\n",
      "layer   2  Sparsity: 77.1902%\n",
      "layer   3  Sparsity: 80.5130%\n",
      "total_backward_count 156640 real_backward_count 26754  17.080%\n",
      "fc layer 2 self.abs_max_out: 2669.0\n",
      "lif layer 2 self.abs_max_v: 3051.5\n",
      "fc layer 1 self.abs_max_out: 5216.0\n",
      "fc layer 2 self.abs_max_out: 2731.0\n",
      "lif layer 1 self.abs_max_v: 5654.5\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.746781/  1.896825, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9251%\n",
      "layer   2  Sparsity: 77.0790%\n",
      "layer   3  Sparsity: 80.2662%\n",
      "total_backward_count 166430 real_backward_count 28014  16.832%\n",
      "lif layer 1 self.abs_max_v: 5866.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.760358/  1.910170, val:  72.50%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9344%\n",
      "layer   2  Sparsity: 77.2744%\n",
      "layer   3  Sparsity: 80.3592%\n",
      "total_backward_count 176220 real_backward_count 29293  16.623%\n",
      "lif layer 2 self.abs_max_v: 3078.0\n",
      "fc layer 2 self.abs_max_out: 2760.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.743097/  1.890566, val:  68.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9299%\n",
      "layer   2  Sparsity: 77.1027%\n",
      "layer   3  Sparsity: 80.1716%\n",
      "total_backward_count 186010 real_backward_count 30536  16.416%\n",
      "lif layer 2 self.abs_max_v: 3218.0\n",
      "fc layer 2 self.abs_max_out: 2836.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.730906/  1.905047, val:  51.25%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9335%\n",
      "layer   2  Sparsity: 76.8760%\n",
      "layer   3  Sparsity: 80.3797%\n",
      "total_backward_count 195800 real_backward_count 31739  16.210%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.731101/  1.896067, val:  53.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9422%\n",
      "layer   2  Sparsity: 76.8097%\n",
      "layer   3  Sparsity: 80.2551%\n",
      "total_backward_count 205590 real_backward_count 32919  16.012%\n",
      "fc layer 2 self.abs_max_out: 2851.0\n",
      "fc layer 1 self.abs_max_out: 5479.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.717519/  1.872041, val:  73.75%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9325%\n",
      "layer   2  Sparsity: 76.8699%\n",
      "layer   3  Sparsity: 79.7921%\n",
      "total_backward_count 215380 real_backward_count 34128  15.845%\n",
      "fc layer 1 self.abs_max_out: 5494.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "fc layer 2 self.abs_max_out: 2962.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.704101/  1.853956, val:  69.17%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9272%\n",
      "layer   2  Sparsity: 76.6582%\n",
      "layer   3  Sparsity: 80.0349%\n",
      "total_backward_count 225170 real_backward_count 35318  15.685%\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.694493/  1.852805, val:  68.33%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9375%\n",
      "layer   2  Sparsity: 76.7493%\n",
      "layer   3  Sparsity: 80.5942%\n",
      "total_backward_count 234960 real_backward_count 36472  15.523%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.692161/  1.844962, val:  71.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9352%\n",
      "layer   2  Sparsity: 76.8709%\n",
      "layer   3  Sparsity: 80.2627%\n",
      "total_backward_count 244750 real_backward_count 37631  15.375%\n",
      "fc layer 1 self.abs_max_out: 5600.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.686781/  1.815350, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9214%\n",
      "layer   2  Sparsity: 76.4619%\n",
      "layer   3  Sparsity: 79.6071%\n",
      "total_backward_count 254540 real_backward_count 38752  15.224%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.680766/  1.839872, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9331%\n",
      "layer   2  Sparsity: 76.4709%\n",
      "layer   3  Sparsity: 79.6927%\n",
      "total_backward_count 264330 real_backward_count 39854  15.077%\n",
      "fc layer 1 self.abs_max_out: 5673.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.678138/  1.830214, val:  77.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9239%\n",
      "layer   2  Sparsity: 76.3651%\n",
      "layer   3  Sparsity: 79.4881%\n",
      "total_backward_count 274120 real_backward_count 40956  14.941%\n",
      "fc layer 2 self.abs_max_out: 3010.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.660548/  1.852267, val:  75.00%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9273%\n",
      "layer   2  Sparsity: 76.4060%\n",
      "layer   3  Sparsity: 79.8696%\n",
      "total_backward_count 283910 real_backward_count 41995  14.792%\n",
      "lif layer 2 self.abs_max_v: 3234.0\n",
      "lif layer 2 self.abs_max_v: 3410.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "lif layer 1 self.abs_max_v: 6109.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.670586/  1.837162, val:  70.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9119%\n",
      "layer   2  Sparsity: 76.4611%\n",
      "layer   3  Sparsity: 79.3501%\n",
      "total_backward_count 293700 real_backward_count 43007  14.643%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.650034/  1.825108, val:  76.67%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9189%\n",
      "layer   2  Sparsity: 76.5945%\n",
      "layer   3  Sparsity: 79.2106%\n",
      "total_backward_count 303490 real_backward_count 44050  14.514%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.648513/  1.787698, val:  67.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9205%\n",
      "layer   2  Sparsity: 76.4257%\n",
      "layer   3  Sparsity: 79.1720%\n",
      "total_backward_count 313280 real_backward_count 45085  14.391%\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 1 self.abs_max_out: 5759.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.627919/  1.821471, val:  70.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9219%\n",
      "layer   2  Sparsity: 76.4838%\n",
      "layer   3  Sparsity: 79.0957%\n",
      "total_backward_count 323070 real_backward_count 46081  14.263%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.638957/  1.810429, val:  63.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9263%\n",
      "layer   2  Sparsity: 76.3145%\n",
      "layer   3  Sparsity: 78.9829%\n",
      "total_backward_count 332860 real_backward_count 47068  14.140%\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 1 self.abs_max_out: 5760.0\n",
      "lif layer 1 self.abs_max_v: 6218.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.621627/  1.770608, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9101%\n",
      "layer   2  Sparsity: 76.2420%\n",
      "layer   3  Sparsity: 79.0059%\n",
      "total_backward_count 342650 real_backward_count 48042  14.021%\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "lif layer 1 self.abs_max_v: 6480.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.627180/  1.792498, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9239%\n",
      "layer   2  Sparsity: 76.2457%\n",
      "layer   3  Sparsity: 79.4046%\n",
      "total_backward_count 352440 real_backward_count 48961  13.892%\n",
      "fc layer 1 self.abs_max_out: 5763.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.613478/  1.778705, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9293%\n",
      "layer   2  Sparsity: 76.1802%\n",
      "layer   3  Sparsity: 79.3599%\n",
      "total_backward_count 362230 real_backward_count 49924  13.782%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.596114/  1.826205, val:  62.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9304%\n",
      "layer   2  Sparsity: 75.9066%\n",
      "layer   3  Sparsity: 79.1446%\n",
      "total_backward_count 372020 real_backward_count 50816  13.659%\n",
      "lif layer 1 self.abs_max_v: 6492.5\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.619102/  1.782801, val:  77.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9456%\n",
      "layer   2  Sparsity: 76.0998%\n",
      "layer   3  Sparsity: 79.2364%\n",
      "total_backward_count 381810 real_backward_count 51771  13.559%\n",
      "lif layer 1 self.abs_max_v: 6522.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.611674/  1.764329, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9336%\n",
      "layer   2  Sparsity: 76.1251%\n",
      "layer   3  Sparsity: 79.0672%\n",
      "total_backward_count 391600 real_backward_count 52668  13.449%\n",
      "fc layer 1 self.abs_max_out: 5785.0\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.612874/  1.776460, val:  72.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9321%\n",
      "layer   2  Sparsity: 75.9797%\n",
      "layer   3  Sparsity: 79.0391%\n",
      "total_backward_count 401390 real_backward_count 53521  13.334%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.600631/  1.762241, val:  70.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9208%\n",
      "layer   2  Sparsity: 76.2338%\n",
      "layer   3  Sparsity: 78.9272%\n",
      "total_backward_count 411180 real_backward_count 54401  13.230%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.574417/  1.736724, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9291%\n",
      "layer   2  Sparsity: 76.0284%\n",
      "layer   3  Sparsity: 78.8165%\n",
      "total_backward_count 420970 real_backward_count 55249  13.124%\n",
      "fc layer 1 self.abs_max_out: 5984.0\n",
      "fc layer 2 self.abs_max_out: 3020.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.569239/  1.754203, val:  67.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9380%\n",
      "layer   2  Sparsity: 75.8663%\n",
      "layer   3  Sparsity: 78.7406%\n",
      "total_backward_count 430760 real_backward_count 56102  13.024%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.565320/  1.727308, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9396%\n",
      "layer   2  Sparsity: 75.9779%\n",
      "layer   3  Sparsity: 78.6342%\n",
      "total_backward_count 440550 real_backward_count 56909  12.918%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.553953/  1.728866, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9322%\n",
      "layer   2  Sparsity: 76.0884%\n",
      "layer   3  Sparsity: 78.5391%\n",
      "total_backward_count 450340 real_backward_count 57754  12.825%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.572203/  1.739363, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9352%\n",
      "layer   2  Sparsity: 76.1284%\n",
      "layer   3  Sparsity: 78.7360%\n",
      "total_backward_count 460130 real_backward_count 58582  12.732%\n",
      "fc layer 1 self.abs_max_out: 5990.0\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.560219/  1.730215, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9242%\n",
      "layer   2  Sparsity: 76.0674%\n",
      "layer   3  Sparsity: 78.7606%\n",
      "total_backward_count 469920 real_backward_count 59405  12.642%\n",
      "fc layer 1 self.abs_max_out: 6010.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "fc layer 2 self.abs_max_out: 3074.0\n",
      "lif layer 2 self.abs_max_v: 3428.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.549463/  1.741814, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9361%\n",
      "layer   2  Sparsity: 75.7263%\n",
      "layer   3  Sparsity: 78.6240%\n",
      "total_backward_count 479710 real_backward_count 60194  12.548%\n",
      "fc layer 1 self.abs_max_out: 6025.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.550282/  1.711501, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9251%\n",
      "layer   2  Sparsity: 75.9010%\n",
      "layer   3  Sparsity: 79.3881%\n",
      "total_backward_count 489500 real_backward_count 61012  12.464%\n",
      "fc layer 1 self.abs_max_out: 6035.0\n",
      "lif layer 1 self.abs_max_v: 6796.5\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.544055/  1.745217, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9294%\n",
      "layer   2  Sparsity: 75.9199%\n",
      "layer   3  Sparsity: 79.1182%\n",
      "total_backward_count 499290 real_backward_count 61811  12.380%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.540900/  1.697253, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9146%\n",
      "layer   2  Sparsity: 75.7879%\n",
      "layer   3  Sparsity: 78.6358%\n",
      "total_backward_count 509080 real_backward_count 62616  12.300%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.529221/  1.711516, val:  77.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9296%\n",
      "layer   2  Sparsity: 76.1137%\n",
      "layer   3  Sparsity: 78.7039%\n",
      "total_backward_count 518870 real_backward_count 63359  12.211%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.537101/  1.701846, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9280%\n",
      "layer   2  Sparsity: 75.9732%\n",
      "layer   3  Sparsity: 78.6701%\n",
      "total_backward_count 528660 real_backward_count 64116  12.128%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.517522/  1.687185, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9396%\n",
      "layer   2  Sparsity: 75.7974%\n",
      "layer   3  Sparsity: 78.6283%\n",
      "total_backward_count 538450 real_backward_count 64868  12.047%\n",
      "fc layer 1 self.abs_max_out: 6063.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.517566/  1.684608, val:  83.33%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9318%\n",
      "layer   2  Sparsity: 75.7723%\n",
      "layer   3  Sparsity: 79.0616%\n",
      "total_backward_count 548240 real_backward_count 65631  11.971%\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 3095.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.510767/  1.674684, val:  78.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9304%\n",
      "layer   2  Sparsity: 75.7305%\n",
      "layer   3  Sparsity: 78.9018%\n",
      "total_backward_count 558030 real_backward_count 66385  11.896%\n",
      "fc layer 1 self.abs_max_out: 6134.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.493060/  1.698003, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9169%\n",
      "layer   2  Sparsity: 75.7491%\n",
      "layer   3  Sparsity: 78.8932%\n",
      "total_backward_count 567820 real_backward_count 67084  11.814%\n",
      "fc layer 1 self.abs_max_out: 6141.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.501811/  1.684731, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9251%\n",
      "layer   2  Sparsity: 75.8625%\n",
      "layer   3  Sparsity: 78.8122%\n",
      "total_backward_count 577610 real_backward_count 67816  11.741%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.496979/  1.684516, val:  70.42%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9154%\n",
      "layer   2  Sparsity: 75.9534%\n",
      "layer   3  Sparsity: 78.9405%\n",
      "total_backward_count 587400 real_backward_count 68507  11.663%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.502609/  1.663891, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9351%\n",
      "layer   2  Sparsity: 75.7710%\n",
      "layer   3  Sparsity: 78.5373%\n",
      "total_backward_count 597190 real_backward_count 69240  11.594%\n",
      "fc layer 1 self.abs_max_out: 6174.0\n",
      "lif layer 2 self.abs_max_v: 3818.5\n",
      "lif layer 2 self.abs_max_v: 3956.5\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.496328/  1.666119, val:  80.83%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9206%\n",
      "layer   2  Sparsity: 75.7715%\n",
      "layer   3  Sparsity: 79.0110%\n",
      "total_backward_count 606980 real_backward_count 69921  11.519%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.483844/  1.677059, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9196%\n",
      "layer   2  Sparsity: 75.8862%\n",
      "layer   3  Sparsity: 78.8845%\n",
      "total_backward_count 616770 real_backward_count 70614  11.449%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.509258/  1.656874, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9283%\n",
      "layer   2  Sparsity: 75.9568%\n",
      "layer   3  Sparsity: 78.7251%\n",
      "total_backward_count 626560 real_backward_count 71317  11.382%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.502269/  1.670601, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 94.9229%\n",
      "layer   2  Sparsity: 75.8605%\n",
      "layer   3  Sparsity: 78.9048%\n",
      "total_backward_count 636350 real_backward_count 72012  11.316%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.475526/  1.668867, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9219%\n",
      "layer   2  Sparsity: 75.6267%\n",
      "layer   3  Sparsity: 78.9059%\n",
      "total_backward_count 646140 real_backward_count 72673  11.247%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.485176/  1.670414, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9281%\n",
      "layer   2  Sparsity: 75.3728%\n",
      "layer   3  Sparsity: 78.9118%\n",
      "total_backward_count 655930 real_backward_count 73394  11.189%\n",
      "fc layer 2 self.abs_max_out: 3100.0\n",
      "fc layer 1 self.abs_max_out: 6214.0\n",
      "fc layer 2 self.abs_max_out: 3155.0\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.495436/  1.675401, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9356%\n",
      "layer   2  Sparsity: 75.3263%\n",
      "layer   3  Sparsity: 79.0365%\n",
      "total_backward_count 665720 real_backward_count 74073  11.127%\n",
      "fc layer 1 self.abs_max_out: 6243.0\n",
      "lif layer 1 self.abs_max_v: 6903.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.490673/  1.668437, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9158%\n",
      "layer   2  Sparsity: 75.4193%\n",
      "layer   3  Sparsity: 78.6651%\n",
      "total_backward_count 675510 real_backward_count 74716  11.061%\n",
      "lif layer 1 self.abs_max_v: 6912.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.482713/  1.680471, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9361%\n",
      "layer   2  Sparsity: 75.3914%\n",
      "layer   3  Sparsity: 78.9068%\n",
      "total_backward_count 685300 real_backward_count 75324  10.991%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.488901/  1.646581, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9287%\n",
      "layer   2  Sparsity: 75.5017%\n",
      "layer   3  Sparsity: 79.4013%\n",
      "total_backward_count 695090 real_backward_count 75943  10.926%\n",
      "fc layer 2 self.abs_max_out: 3220.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.471268/  1.684033, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.9244%\n",
      "layer   2  Sparsity: 75.3664%\n",
      "layer   3  Sparsity: 78.8856%\n",
      "total_backward_count 704880 real_backward_count 76562  10.862%\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "fc layer 2 self.abs_max_out: 3224.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.471847/  1.652395, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9296%\n",
      "layer   2  Sparsity: 75.4119%\n",
      "layer   3  Sparsity: 78.9352%\n",
      "total_backward_count 714670 real_backward_count 77200  10.802%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.469591/  1.639944, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9348%\n",
      "layer   2  Sparsity: 75.4996%\n",
      "layer   3  Sparsity: 78.9759%\n",
      "total_backward_count 724460 real_backward_count 77848  10.746%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.461586/  1.662112, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9163%\n",
      "layer   2  Sparsity: 75.4013%\n",
      "layer   3  Sparsity: 79.0217%\n",
      "total_backward_count 734250 real_backward_count 78461  10.686%\n",
      "fc layer 2 self.abs_max_out: 3325.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.471930/  1.639663, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9266%\n",
      "layer   2  Sparsity: 75.3629%\n",
      "layer   3  Sparsity: 78.9290%\n",
      "total_backward_count 744040 real_backward_count 79076  10.628%\n",
      "fc layer 1 self.abs_max_out: 6295.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.461352/  1.639894, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9188%\n",
      "layer   2  Sparsity: 75.2884%\n",
      "layer   3  Sparsity: 78.7883%\n",
      "total_backward_count 753830 real_backward_count 79662  10.568%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.465046/  1.641186, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9364%\n",
      "layer   2  Sparsity: 75.5299%\n",
      "layer   3  Sparsity: 78.9743%\n",
      "total_backward_count 763620 real_backward_count 80274  10.512%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.452123/  1.631307, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9211%\n",
      "layer   2  Sparsity: 75.5437%\n",
      "layer   3  Sparsity: 78.9434%\n",
      "total_backward_count 773410 real_backward_count 80841  10.453%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.439294/  1.618911, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.9402%\n",
      "layer   2  Sparsity: 75.5878%\n",
      "layer   3  Sparsity: 79.0616%\n",
      "total_backward_count 783200 real_backward_count 81439  10.398%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.441525/  1.612034, val:  77.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9341%\n",
      "layer   2  Sparsity: 75.4946%\n",
      "layer   3  Sparsity: 79.4052%\n",
      "total_backward_count 792990 real_backward_count 82001  10.341%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.434750/  1.615273, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9312%\n",
      "layer   2  Sparsity: 75.3211%\n",
      "layer   3  Sparsity: 79.1742%\n",
      "total_backward_count 802780 real_backward_count 82567  10.285%\n",
      "fc layer 2 self.abs_max_out: 3348.0\n",
      "lif layer 1 self.abs_max_v: 7098.5\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.427486/  1.615742, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9222%\n",
      "layer   2  Sparsity: 75.2187%\n",
      "layer   3  Sparsity: 79.3387%\n",
      "total_backward_count 812570 real_backward_count 83116  10.229%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.430032/  1.619566, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.9320%\n",
      "layer   2  Sparsity: 75.2856%\n",
      "layer   3  Sparsity: 79.3392%\n",
      "total_backward_count 822360 real_backward_count 83652  10.172%\n",
      "fc layer 2 self.abs_max_out: 3349.0\n",
      "fc layer 2 self.abs_max_out: 3529.0\n",
      "fc layer 1 self.abs_max_out: 6369.0\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.433681/  1.597930, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9231%\n",
      "layer   2  Sparsity: 75.2737%\n",
      "layer   3  Sparsity: 79.0088%\n",
      "total_backward_count 832150 real_backward_count 84177  10.116%\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "fc layer 1 self.abs_max_out: 6393.0\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.424511/  1.630513, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9437%\n",
      "layer   2  Sparsity: 75.3894%\n",
      "layer   3  Sparsity: 79.0829%\n",
      "total_backward_count 841940 real_backward_count 84736  10.064%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.440931/  1.646335, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9332%\n",
      "layer   2  Sparsity: 75.2656%\n",
      "layer   3  Sparsity: 78.9880%\n",
      "total_backward_count 851730 real_backward_count 85262  10.010%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.439100/  1.638274, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9289%\n",
      "layer   2  Sparsity: 75.2993%\n",
      "layer   3  Sparsity: 78.9216%\n",
      "total_backward_count 861520 real_backward_count 85778   9.957%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.443256/  1.644933, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9308%\n",
      "layer   2  Sparsity: 75.3930%\n",
      "layer   3  Sparsity: 79.1717%\n",
      "total_backward_count 871310 real_backward_count 86359   9.911%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.447671/  1.627640, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9255%\n",
      "layer   2  Sparsity: 75.3671%\n",
      "layer   3  Sparsity: 79.1060%\n",
      "total_backward_count 881100 real_backward_count 86919   9.865%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.425933/  1.610394, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9411%\n",
      "layer   2  Sparsity: 75.3308%\n",
      "layer   3  Sparsity: 79.0750%\n",
      "total_backward_count 890890 real_backward_count 87453   9.816%\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.429630/  1.620821, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9324%\n",
      "layer   2  Sparsity: 75.2880%\n",
      "layer   3  Sparsity: 79.2678%\n",
      "total_backward_count 900680 real_backward_count 87966   9.767%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.422978/  1.611681, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9284%\n",
      "layer   2  Sparsity: 75.1339%\n",
      "layer   3  Sparsity: 78.5822%\n",
      "total_backward_count 910470 real_backward_count 88507   9.721%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.412325/  1.606827, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9215%\n",
      "layer   2  Sparsity: 75.1567%\n",
      "layer   3  Sparsity: 78.6856%\n",
      "total_backward_count 920260 real_backward_count 88989   9.670%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.406334/  1.584171, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9223%\n",
      "layer   2  Sparsity: 75.2612%\n",
      "layer   3  Sparsity: 78.9950%\n",
      "total_backward_count 930050 real_backward_count 89500   9.623%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.397506/  1.602763, val:  85.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9219%\n",
      "layer   2  Sparsity: 75.2562%\n",
      "layer   3  Sparsity: 78.6240%\n",
      "total_backward_count 939840 real_backward_count 90031   9.579%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.405563/  1.601901, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9322%\n",
      "layer   2  Sparsity: 75.2751%\n",
      "layer   3  Sparsity: 78.7957%\n",
      "total_backward_count 949630 real_backward_count 90566   9.537%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.399957/  1.614868, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9158%\n",
      "layer   2  Sparsity: 75.0822%\n",
      "layer   3  Sparsity: 78.9259%\n",
      "total_backward_count 959420 real_backward_count 91046   9.490%\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.409035/  1.632464, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9177%\n",
      "layer   2  Sparsity: 75.1185%\n",
      "layer   3  Sparsity: 79.2792%\n",
      "total_backward_count 969210 real_backward_count 91558   9.447%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.413743/  1.624581, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9121%\n",
      "layer   2  Sparsity: 75.1811%\n",
      "layer   3  Sparsity: 79.1018%\n",
      "total_backward_count 979000 real_backward_count 92043   9.402%\n",
      "fc layer 1 self.abs_max_out: 6410.0\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.424825/  1.606949, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9238%\n",
      "layer   2  Sparsity: 75.1177%\n",
      "layer   3  Sparsity: 79.5219%\n",
      "total_backward_count 988790 real_backward_count 92492   9.354%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.415397/  1.613472, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9348%\n",
      "layer   2  Sparsity: 75.2424%\n",
      "layer   3  Sparsity: 79.9631%\n",
      "total_backward_count 998580 real_backward_count 92970   9.310%\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "fc layer 1 self.abs_max_out: 6424.0\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.413629/  1.596507, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9298%\n",
      "layer   2  Sparsity: 75.2265%\n",
      "layer   3  Sparsity: 79.6317%\n",
      "total_backward_count 1008370 real_backward_count 93491   9.271%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.408368/  1.591731, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9183%\n",
      "layer   2  Sparsity: 75.2605%\n",
      "layer   3  Sparsity: 79.7646%\n",
      "total_backward_count 1018160 real_backward_count 93976   9.230%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.399628/  1.584207, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9203%\n",
      "layer   2  Sparsity: 75.2305%\n",
      "layer   3  Sparsity: 79.6290%\n",
      "total_backward_count 1027950 real_backward_count 94457   9.189%\n",
      "fc layer 1 self.abs_max_out: 6478.0\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.388700/  1.601425, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9248%\n",
      "layer   2  Sparsity: 75.3350%\n",
      "layer   3  Sparsity: 79.5845%\n",
      "total_backward_count 1037740 real_backward_count 94909   9.146%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.400984/  1.589435, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9461%\n",
      "layer   2  Sparsity: 75.1628%\n",
      "layer   3  Sparsity: 79.2216%\n",
      "total_backward_count 1047530 real_backward_count 95386   9.106%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.404712/  1.587909, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9188%\n",
      "layer   2  Sparsity: 75.0892%\n",
      "layer   3  Sparsity: 79.3975%\n",
      "total_backward_count 1057320 real_backward_count 95861   9.066%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.403538/  1.602967, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9312%\n",
      "layer   2  Sparsity: 75.1057%\n",
      "layer   3  Sparsity: 79.8404%\n",
      "total_backward_count 1067110 real_backward_count 96281   9.023%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.403477/  1.593083, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9310%\n",
      "layer   2  Sparsity: 75.1222%\n",
      "layer   3  Sparsity: 80.1666%\n",
      "total_backward_count 1076900 real_backward_count 96720   8.981%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.402327/  1.611661, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9275%\n",
      "layer   2  Sparsity: 75.1793%\n",
      "layer   3  Sparsity: 79.6731%\n",
      "total_backward_count 1086690 real_backward_count 97184   8.943%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.394063/  1.584482, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9270%\n",
      "layer   2  Sparsity: 75.2061%\n",
      "layer   3  Sparsity: 79.4466%\n",
      "total_backward_count 1096480 real_backward_count 97629   8.904%\n",
      "fc layer 1 self.abs_max_out: 6480.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.395196/  1.581461, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9138%\n",
      "layer   2  Sparsity: 75.0922%\n",
      "layer   3  Sparsity: 79.5505%\n",
      "total_backward_count 1106270 real_backward_count 98083   8.866%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.414118/  1.608647, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9229%\n",
      "layer   2  Sparsity: 75.0525%\n",
      "layer   3  Sparsity: 79.3823%\n",
      "total_backward_count 1116060 real_backward_count 98494   8.825%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.402725/  1.599190, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 94.9224%\n",
      "layer   2  Sparsity: 75.0352%\n",
      "layer   3  Sparsity: 79.3564%\n",
      "total_backward_count 1125850 real_backward_count 98967   8.790%\n",
      "fc layer 1 self.abs_max_out: 6538.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.391308/  1.591631, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9274%\n",
      "layer   2  Sparsity: 75.0092%\n",
      "layer   3  Sparsity: 79.4888%\n",
      "total_backward_count 1135640 real_backward_count 99399   8.753%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.374770/  1.556069, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9259%\n",
      "layer   2  Sparsity: 75.0970%\n",
      "layer   3  Sparsity: 79.6399%\n",
      "total_backward_count 1145430 real_backward_count 99806   8.713%\n",
      "fc layer 1 self.abs_max_out: 6540.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.360152/  1.574465, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9266%\n",
      "layer   2  Sparsity: 75.0584%\n",
      "layer   3  Sparsity: 79.8241%\n",
      "total_backward_count 1155220 real_backward_count 100235   8.677%\n",
      "fc layer 1 self.abs_max_out: 6551.0\n",
      "lif layer 1 self.abs_max_v: 7216.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.367735/  1.563075, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9362%\n",
      "layer   2  Sparsity: 75.1785%\n",
      "layer   3  Sparsity: 79.3549%\n",
      "total_backward_count 1165010 real_backward_count 100668   8.641%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.368256/  1.569455, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9316%\n",
      "layer   2  Sparsity: 75.1292%\n",
      "layer   3  Sparsity: 79.6666%\n",
      "total_backward_count 1174800 real_backward_count 101093   8.605%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.371406/  1.587277, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9409%\n",
      "layer   2  Sparsity: 75.1311%\n",
      "layer   3  Sparsity: 79.5445%\n",
      "total_backward_count 1184590 real_backward_count 101525   8.570%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.369609/  1.558123, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9320%\n",
      "layer   2  Sparsity: 75.1011%\n",
      "layer   3  Sparsity: 79.4758%\n",
      "total_backward_count 1194380 real_backward_count 101948   8.536%\n",
      "fc layer 2 self.abs_max_out: 3573.0\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.353660/  1.556852, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9392%\n",
      "layer   2  Sparsity: 75.1451%\n",
      "layer   3  Sparsity: 80.0630%\n",
      "total_backward_count 1204170 real_backward_count 102325   8.498%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.352617/  1.548357, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9419%\n",
      "layer   2  Sparsity: 75.0766%\n",
      "layer   3  Sparsity: 80.0853%\n",
      "total_backward_count 1213960 real_backward_count 102732   8.463%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.346413/  1.549946, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9465%\n",
      "layer   2  Sparsity: 75.1690%\n",
      "layer   3  Sparsity: 80.1365%\n",
      "total_backward_count 1223750 real_backward_count 103138   8.428%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.347703/  1.543404, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9392%\n",
      "layer   2  Sparsity: 75.1670%\n",
      "layer   3  Sparsity: 80.1096%\n",
      "total_backward_count 1233540 real_backward_count 103548   8.394%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.338066/  1.559602, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9317%\n",
      "layer   2  Sparsity: 75.0959%\n",
      "layer   3  Sparsity: 80.1947%\n",
      "total_backward_count 1243330 real_backward_count 103978   8.363%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.340821/  1.563949, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9138%\n",
      "layer   2  Sparsity: 75.0188%\n",
      "layer   3  Sparsity: 80.2577%\n",
      "total_backward_count 1253120 real_backward_count 104398   8.331%\n",
      "lif layer 1 self.abs_max_v: 7373.5\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.361236/  1.565783, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9365%\n",
      "layer   2  Sparsity: 74.9591%\n",
      "layer   3  Sparsity: 80.0991%\n",
      "total_backward_count 1262910 real_backward_count 104778   8.297%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.367927/  1.557876, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9330%\n",
      "layer   2  Sparsity: 74.8730%\n",
      "layer   3  Sparsity: 79.8638%\n",
      "total_backward_count 1272700 real_backward_count 105216   8.267%\n",
      "lif layer 2 self.abs_max_v: 3966.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.362440/  1.547275, val:  78.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9411%\n",
      "layer   2  Sparsity: 74.9442%\n",
      "layer   3  Sparsity: 79.9137%\n",
      "total_backward_count 1282490 real_backward_count 105615   8.235%\n",
      "lif layer 2 self.abs_max_v: 3975.0\n",
      "lif layer 2 self.abs_max_v: 4045.5\n",
      "fc layer 1 self.abs_max_out: 6559.0\n",
      "lif layer 2 self.abs_max_v: 4078.5\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.352703/  1.543961, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9219%\n",
      "layer   2  Sparsity: 75.0338%\n",
      "layer   3  Sparsity: 80.0042%\n",
      "total_backward_count 1292280 real_backward_count 105993   8.202%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.360207/  1.538098, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9370%\n",
      "layer   2  Sparsity: 75.0972%\n",
      "layer   3  Sparsity: 80.0377%\n",
      "total_backward_count 1302070 real_backward_count 106347   8.168%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.352857/  1.535469, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9344%\n",
      "layer   2  Sparsity: 75.1589%\n",
      "layer   3  Sparsity: 79.6927%\n",
      "total_backward_count 1311860 real_backward_count 106704   8.134%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.363114/  1.546582, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9196%\n",
      "layer   2  Sparsity: 74.9839%\n",
      "layer   3  Sparsity: 79.8827%\n",
      "total_backward_count 1321650 real_backward_count 107123   8.105%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.359749/  1.552994, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9282%\n",
      "layer   2  Sparsity: 75.0646%\n",
      "layer   3  Sparsity: 80.1370%\n",
      "total_backward_count 1331440 real_backward_count 107532   8.076%\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 7811.0\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.367132/  1.568273, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9151%\n",
      "layer   2  Sparsity: 75.1280%\n",
      "layer   3  Sparsity: 80.4185%\n",
      "total_backward_count 1341230 real_backward_count 107901   8.045%\n",
      "fc layer 1 self.abs_max_out: 6582.0\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.360132/  1.558335, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9152%\n",
      "layer   2  Sparsity: 75.1833%\n",
      "layer   3  Sparsity: 80.2982%\n",
      "total_backward_count 1351020 real_backward_count 108302   8.016%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.360703/  1.553984, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9361%\n",
      "layer   2  Sparsity: 75.1929%\n",
      "layer   3  Sparsity: 80.4814%\n",
      "total_backward_count 1360810 real_backward_count 108649   7.984%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.366244/  1.563383, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9190%\n",
      "layer   2  Sparsity: 75.2530%\n",
      "layer   3  Sparsity: 80.4258%\n",
      "total_backward_count 1370600 real_backward_count 109022   7.954%\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.358380/  1.566715, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9273%\n",
      "layer   2  Sparsity: 75.1381%\n",
      "layer   3  Sparsity: 80.5342%\n",
      "total_backward_count 1380390 real_backward_count 109381   7.924%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.370513/  1.551696, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9334%\n",
      "layer   2  Sparsity: 75.1681%\n",
      "layer   3  Sparsity: 80.6279%\n",
      "total_backward_count 1390180 real_backward_count 109721   7.893%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.366447/  1.577413, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9316%\n",
      "layer   2  Sparsity: 75.1104%\n",
      "layer   3  Sparsity: 80.3887%\n",
      "total_backward_count 1399970 real_backward_count 110075   7.863%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.372172/  1.553126, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9337%\n",
      "layer   2  Sparsity: 74.9231%\n",
      "layer   3  Sparsity: 80.3634%\n",
      "total_backward_count 1409760 real_backward_count 110422   7.833%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.345148/  1.543800, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9125%\n",
      "layer   2  Sparsity: 74.8219%\n",
      "layer   3  Sparsity: 80.4790%\n",
      "total_backward_count 1419550 real_backward_count 110749   7.802%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.350863/  1.540296, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9324%\n",
      "layer   2  Sparsity: 75.0116%\n",
      "layer   3  Sparsity: 80.4990%\n",
      "total_backward_count 1429340 real_backward_count 111055   7.770%\n",
      "fc layer 1 self.abs_max_out: 6589.0\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.342011/  1.550163, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9147%\n",
      "layer   2  Sparsity: 74.9973%\n",
      "layer   3  Sparsity: 80.3331%\n",
      "total_backward_count 1439130 real_backward_count 111387   7.740%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.344482/  1.552264, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9164%\n",
      "layer   2  Sparsity: 75.1298%\n",
      "layer   3  Sparsity: 80.3967%\n",
      "total_backward_count 1448920 real_backward_count 111732   7.711%\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "fc layer 1 self.abs_max_out: 6597.0\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.347468/  1.558527, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9380%\n",
      "layer   2  Sparsity: 74.9799%\n",
      "layer   3  Sparsity: 80.5833%\n",
      "total_backward_count 1458710 real_backward_count 112077   7.683%\n",
      "fc layer 1 self.abs_max_out: 6631.0\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.344296/  1.539905, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9306%\n",
      "layer   2  Sparsity: 74.9038%\n",
      "layer   3  Sparsity: 80.4638%\n",
      "total_backward_count 1468500 real_backward_count 112381   7.653%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.344489/  1.542407, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9250%\n",
      "layer   2  Sparsity: 74.9511%\n",
      "layer   3  Sparsity: 80.5333%\n",
      "total_backward_count 1478290 real_backward_count 112691   7.623%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.344102/  1.550703, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9308%\n",
      "layer   2  Sparsity: 75.0299%\n",
      "layer   3  Sparsity: 80.7411%\n",
      "total_backward_count 1488080 real_backward_count 113021   7.595%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.332676/  1.542059, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9232%\n",
      "layer   2  Sparsity: 75.1283%\n",
      "layer   3  Sparsity: 80.7332%\n",
      "total_backward_count 1497870 real_backward_count 113337   7.567%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.335889/  1.538505, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9198%\n",
      "layer   2  Sparsity: 75.1108%\n",
      "layer   3  Sparsity: 80.7989%\n",
      "total_backward_count 1507660 real_backward_count 113655   7.539%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.327135/  1.532323, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9227%\n",
      "layer   2  Sparsity: 75.0901%\n",
      "layer   3  Sparsity: 80.7071%\n",
      "total_backward_count 1517450 real_backward_count 113984   7.512%\n",
      "fc layer 1 self.abs_max_out: 6636.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.331270/  1.544163, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9343%\n",
      "layer   2  Sparsity: 75.1007%\n",
      "layer   3  Sparsity: 80.8554%\n",
      "total_backward_count 1527240 real_backward_count 114311   7.485%\n",
      "fc layer 1 self.abs_max_out: 6638.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.332544/  1.542294, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9117%\n",
      "layer   2  Sparsity: 75.0292%\n",
      "layer   3  Sparsity: 80.6886%\n",
      "total_backward_count 1537030 real_backward_count 114604   7.456%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.323299/  1.535854, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9508%\n",
      "layer   2  Sparsity: 74.9617%\n",
      "layer   3  Sparsity: 80.4365%\n",
      "total_backward_count 1546820 real_backward_count 114904   7.428%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.324335/  1.538052, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9269%\n",
      "layer   2  Sparsity: 74.9677%\n",
      "layer   3  Sparsity: 80.4809%\n",
      "total_backward_count 1556610 real_backward_count 115231   7.403%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.337797/  1.549648, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9364%\n",
      "layer   2  Sparsity: 74.9308%\n",
      "layer   3  Sparsity: 80.3151%\n",
      "total_backward_count 1566400 real_backward_count 115551   7.377%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.329440/  1.549670, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9295%\n",
      "layer   2  Sparsity: 74.8064%\n",
      "layer   3  Sparsity: 80.4904%\n",
      "total_backward_count 1576190 real_backward_count 115880   7.352%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.336631/  1.553537, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9264%\n",
      "layer   2  Sparsity: 74.8427%\n",
      "layer   3  Sparsity: 80.3178%\n",
      "total_backward_count 1585980 real_backward_count 116190   7.326%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.342780/  1.537626, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9152%\n",
      "layer   2  Sparsity: 74.9228%\n",
      "layer   3  Sparsity: 80.3934%\n",
      "total_backward_count 1595770 real_backward_count 116497   7.300%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.339476/  1.552565, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9213%\n",
      "layer   2  Sparsity: 74.8840%\n",
      "layer   3  Sparsity: 80.2703%\n",
      "total_backward_count 1605560 real_backward_count 116791   7.274%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.350207/  1.577765, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9467%\n",
      "layer   2  Sparsity: 74.9399%\n",
      "layer   3  Sparsity: 80.2531%\n",
      "total_backward_count 1615350 real_backward_count 117103   7.249%\n",
      "fc layer 1 self.abs_max_out: 6646.0\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.346186/  1.555380, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9265%\n",
      "layer   2  Sparsity: 74.9122%\n",
      "layer   3  Sparsity: 80.2688%\n",
      "total_backward_count 1625140 real_backward_count 117397   7.224%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.331669/  1.542223, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9265%\n",
      "layer   2  Sparsity: 74.8787%\n",
      "layer   3  Sparsity: 80.3342%\n",
      "total_backward_count 1634930 real_backward_count 117672   7.197%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.332508/  1.542425, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 94.9239%\n",
      "layer   2  Sparsity: 74.9023%\n",
      "layer   3  Sparsity: 80.3048%\n",
      "total_backward_count 1644720 real_backward_count 117982   7.173%\n",
      "fc layer 1 self.abs_max_out: 6659.0\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.326688/  1.544505, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 94.9296%\n",
      "layer   2  Sparsity: 74.9196%\n",
      "layer   3  Sparsity: 80.2748%\n",
      "total_backward_count 1654510 real_backward_count 118266   7.148%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.330658/  1.529030, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9393%\n",
      "layer   2  Sparsity: 75.0204%\n",
      "layer   3  Sparsity: 80.1841%\n",
      "total_backward_count 1664300 real_backward_count 118569   7.124%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.322615/  1.531133, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9148%\n",
      "layer   2  Sparsity: 74.9285%\n",
      "layer   3  Sparsity: 80.2479%\n",
      "total_backward_count 1674090 real_backward_count 118844   7.099%\n",
      "fc layer 1 self.abs_max_out: 6666.0\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.319918/  1.539145, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9150%\n",
      "layer   2  Sparsity: 74.8578%\n",
      "layer   3  Sparsity: 80.4924%\n",
      "total_backward_count 1683880 real_backward_count 119137   7.075%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.313179/  1.521320, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9357%\n",
      "layer   2  Sparsity: 74.8323%\n",
      "layer   3  Sparsity: 80.3398%\n",
      "total_backward_count 1693670 real_backward_count 119446   7.052%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.294876/  1.493351, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9347%\n",
      "layer   2  Sparsity: 74.9654%\n",
      "layer   3  Sparsity: 80.0228%\n",
      "total_backward_count 1703460 real_backward_count 119778   7.031%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.289302/  1.516786, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9224%\n",
      "layer   2  Sparsity: 74.8471%\n",
      "layer   3  Sparsity: 79.8971%\n",
      "total_backward_count 1713250 real_backward_count 120083   7.009%\n",
      "fc layer 2 self.abs_max_out: 3594.0\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.311485/  1.520383, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9350%\n",
      "layer   2  Sparsity: 74.7442%\n",
      "layer   3  Sparsity: 80.0685%\n",
      "total_backward_count 1723040 real_backward_count 120389   6.987%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.312414/  1.509511, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9362%\n",
      "layer   2  Sparsity: 74.8307%\n",
      "layer   3  Sparsity: 80.3968%\n",
      "total_backward_count 1732830 real_backward_count 120672   6.964%\n",
      "fc layer 1 self.abs_max_out: 6675.0\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.299240/  1.510959, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9197%\n",
      "layer   2  Sparsity: 74.9201%\n",
      "layer   3  Sparsity: 80.4201%\n",
      "total_backward_count 1742620 real_backward_count 120962   6.941%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.299027/  1.503841, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9261%\n",
      "layer   2  Sparsity: 74.8477%\n",
      "layer   3  Sparsity: 80.1834%\n",
      "total_backward_count 1752410 real_backward_count 121278   6.921%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.293256/  1.518412, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9130%\n",
      "layer   2  Sparsity: 74.8095%\n",
      "layer   3  Sparsity: 80.2167%\n",
      "total_backward_count 1762200 real_backward_count 121583   6.900%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.294251/  1.493965, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9254%\n",
      "layer   2  Sparsity: 74.8740%\n",
      "layer   3  Sparsity: 80.3564%\n",
      "total_backward_count 1771990 real_backward_count 121899   6.879%\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.286636/  1.502732, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9135%\n",
      "layer   2  Sparsity: 74.7978%\n",
      "layer   3  Sparsity: 80.5389%\n",
      "total_backward_count 1781780 real_backward_count 122183   6.857%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.288982/  1.513752, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9256%\n",
      "layer   2  Sparsity: 74.7756%\n",
      "layer   3  Sparsity: 80.2413%\n",
      "total_backward_count 1791570 real_backward_count 122451   6.835%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.289670/  1.511817, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9140%\n",
      "layer   2  Sparsity: 74.7390%\n",
      "layer   3  Sparsity: 80.2017%\n",
      "total_backward_count 1801360 real_backward_count 122728   6.813%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.284801/  1.496311, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9371%\n",
      "layer   2  Sparsity: 74.8117%\n",
      "layer   3  Sparsity: 80.4593%\n",
      "total_backward_count 1811150 real_backward_count 123010   6.792%\n",
      "fc layer 1 self.abs_max_out: 6720.0\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.272290/  1.477060, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9363%\n",
      "layer   2  Sparsity: 74.8045%\n",
      "layer   3  Sparsity: 80.4254%\n",
      "total_backward_count 1820940 real_backward_count 123280   6.770%\n",
      "fc layer 1 self.abs_max_out: 6730.0\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.275006/  1.485620, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 94.9264%\n",
      "layer   2  Sparsity: 74.8053%\n",
      "layer   3  Sparsity: 80.2986%\n",
      "total_backward_count 1830730 real_backward_count 123559   6.749%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.275909/  1.495618, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9344%\n",
      "layer   2  Sparsity: 74.8157%\n",
      "layer   3  Sparsity: 80.3000%\n",
      "total_backward_count 1840520 real_backward_count 123841   6.729%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.290990/  1.505314, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9401%\n",
      "layer   2  Sparsity: 74.8358%\n",
      "layer   3  Sparsity: 80.3203%\n",
      "total_backward_count 1850310 real_backward_count 124123   6.708%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.288027/  1.501335, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9217%\n",
      "layer   2  Sparsity: 74.6286%\n",
      "layer   3  Sparsity: 80.2401%\n",
      "total_backward_count 1860100 real_backward_count 124383   6.687%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.279048/  1.505905, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9283%\n",
      "layer   2  Sparsity: 74.6758%\n",
      "layer   3  Sparsity: 80.3851%\n",
      "total_backward_count 1869890 real_backward_count 124647   6.666%\n",
      "fc layer 3 self.abs_max_out: 548.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.285698/  1.497148, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9340%\n",
      "layer   2  Sparsity: 74.7718%\n",
      "layer   3  Sparsity: 80.4779%\n",
      "total_backward_count 1879680 real_backward_count 124912   6.645%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.277565/  1.498304, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9435%\n",
      "layer   2  Sparsity: 74.7412%\n",
      "layer   3  Sparsity: 80.1322%\n",
      "total_backward_count 1889470 real_backward_count 125177   6.625%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.275985/  1.499030, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9288%\n",
      "layer   2  Sparsity: 74.7605%\n",
      "layer   3  Sparsity: 80.3161%\n",
      "total_backward_count 1899260 real_backward_count 125404   6.603%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.269424/  1.492308, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9250%\n",
      "layer   2  Sparsity: 74.8162%\n",
      "layer   3  Sparsity: 80.3946%\n",
      "total_backward_count 1909050 real_backward_count 125658   6.582%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.276499/  1.491093, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9345%\n",
      "layer   2  Sparsity: 74.8189%\n",
      "layer   3  Sparsity: 80.5132%\n",
      "total_backward_count 1918840 real_backward_count 125905   6.562%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.273688/  1.482792, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 94.9331%\n",
      "layer   2  Sparsity: 74.7507%\n",
      "layer   3  Sparsity: 80.6237%\n",
      "total_backward_count 1928630 real_backward_count 126148   6.541%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.278855/  1.492112, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9182%\n",
      "layer   2  Sparsity: 74.8803%\n",
      "layer   3  Sparsity: 80.6528%\n",
      "total_backward_count 1938420 real_backward_count 126417   6.522%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.262625/  1.481999, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 94.9236%\n",
      "layer   2  Sparsity: 74.8351%\n",
      "layer   3  Sparsity: 80.6520%\n",
      "total_backward_count 1948210 real_backward_count 126670   6.502%\n",
      "fc layer 1 self.abs_max_out: 6777.0\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.273685/  1.497507, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 94.9194%\n",
      "layer   2  Sparsity: 74.6685%\n",
      "layer   3  Sparsity: 80.6708%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0017052bd6ad45558f296a89d8f90b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.27368</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>1.49751</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-144</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rrlsiskm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rrlsiskm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_172225-rrlsiskm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pwswxmpw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_214155-pwswxmpw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pwswxmpw' target=\"_blank\">clear-sweep-149</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pwswxmpw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pwswxmpw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251117_214204_745', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 553.0\n",
      "lif layer 1 self.abs_max_v: 553.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 256.0\n",
      "lif layer 2 self.abs_max_v: 256.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "lif layer 1 self.abs_max_v: 791.5\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 1 self.abs_max_out: 749.0\n",
      "lif layer 1 self.abs_max_v: 1090.0\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "lif layer 2 self.abs_max_v: 1062.0\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 1 self.abs_max_out: 833.0\n",
      "fc layer 2 self.abs_max_out: 797.0\n",
      "lif layer 2 self.abs_max_v: 1085.0\n",
      "fc layer 1 self.abs_max_out: 843.0\n",
      "lif layer 1 self.abs_max_v: 1183.5\n",
      "lif layer 2 self.abs_max_v: 1100.5\n",
      "fc layer 1 self.abs_max_out: 921.0\n",
      "lif layer 1 self.abs_max_v: 1405.0\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "lif layer 2 self.abs_max_v: 1529.5\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 1 self.abs_max_out: 1038.0\n",
      "lif layer 1 self.abs_max_v: 1486.5\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "lif layer 2 self.abs_max_v: 1600.5\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 1 self.abs_max_out: 1178.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "lif layer 2 self.abs_max_v: 1765.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 1 self.abs_max_out: 1368.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 1624.5\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 1 self.abs_max_out: 1595.0\n",
      "lif layer 1 self.abs_max_v: 1819.0\n",
      "fc layer 2 self.abs_max_out: 1470.0\n",
      "lif layer 2 self.abs_max_v: 2261.0\n",
      "fc layer 1 self.abs_max_out: 2158.0\n",
      "lif layer 1 self.abs_max_v: 2158.0\n",
      "lif layer 2 self.abs_max_v: 2389.5\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "lif layer 2 self.abs_max_v: 2697.0\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "lif layer 1 self.abs_max_v: 2340.0\n",
      "fc layer 2 self.abs_max_out: 1754.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "lif layer 1 self.abs_max_v: 2460.0\n",
      "lif layer 1 self.abs_max_v: 2515.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "lif layer 2 self.abs_max_v: 2702.5\n",
      "lif layer 1 self.abs_max_v: 2572.5\n",
      "lif layer 1 self.abs_max_v: 2596.5\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "lif layer 1 self.abs_max_v: 2762.5\n",
      "lif layer 2 self.abs_max_v: 2919.0\n",
      "lif layer 2 self.abs_max_v: 3214.5\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "fc layer 1 self.abs_max_out: 3142.0\n",
      "lif layer 1 self.abs_max_v: 3142.0\n",
      "fc layer 3 self.abs_max_out: 748.0\n",
      "fc layer 2 self.abs_max_out: 2394.0\n",
      "lif layer 1 self.abs_max_v: 3242.5\n",
      "fc layer 3 self.abs_max_out: 802.0\n",
      "fc layer 1 self.abs_max_out: 3345.0\n",
      "lif layer 1 self.abs_max_v: 3412.5\n",
      "lif layer 2 self.abs_max_v: 3221.0\n",
      "lif layer 2 self.abs_max_v: 3233.0\n",
      "fc layer 3 self.abs_max_out: 804.0\n",
      "lif layer 2 self.abs_max_v: 3581.5\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "fc layer 3 self.abs_max_out: 968.0\n",
      "fc layer 3 self.abs_max_out: 1140.0\n",
      "lif layer 1 self.abs_max_v: 3422.5\n",
      "fc layer 2 self.abs_max_out: 2475.0\n",
      "lif layer 2 self.abs_max_v: 3609.0\n",
      "fc layer 2 self.abs_max_out: 2487.0\n",
      "fc layer 2 self.abs_max_out: 2671.0\n",
      "fc layer 1 self.abs_max_out: 3381.0\n",
      "lif layer 1 self.abs_max_v: 3960.5\n",
      "lif layer 2 self.abs_max_v: 3700.0\n",
      "lif layer 2 self.abs_max_v: 3781.0\n",
      "fc layer 1 self.abs_max_out: 3952.0\n",
      "fc layer 2 self.abs_max_out: 2780.0\n",
      "fc layer 2 self.abs_max_out: 2783.0\n",
      "fc layer 1 self.abs_max_out: 4142.0\n",
      "lif layer 1 self.abs_max_v: 4142.0\n",
      "fc layer 1 self.abs_max_out: 4356.0\n",
      "lif layer 1 self.abs_max_v: 4356.0\n",
      "fc layer 2 self.abs_max_out: 2789.0\n",
      "lif layer 2 self.abs_max_v: 3862.5\n",
      "lif layer 2 self.abs_max_v: 3867.5\n",
      "lif layer 2 self.abs_max_v: 3945.0\n",
      "lif layer 2 self.abs_max_v: 3964.5\n",
      "lif layer 2 self.abs_max_v: 3966.5\n",
      "fc layer 1 self.abs_max_out: 4464.0\n",
      "lif layer 1 self.abs_max_v: 4464.0\n",
      "lif layer 2 self.abs_max_v: 4019.0\n",
      "fc layer 1 self.abs_max_out: 4664.0\n",
      "lif layer 1 self.abs_max_v: 4664.0\n",
      "fc layer 1 self.abs_max_out: 4743.0\n",
      "lif layer 1 self.abs_max_v: 4743.0\n",
      "fc layer 1 self.abs_max_out: 4851.0\n",
      "lif layer 1 self.abs_max_v: 4851.0\n",
      "fc layer 2 self.abs_max_out: 2792.0\n",
      "fc layer 1 self.abs_max_out: 5304.0\n",
      "lif layer 1 self.abs_max_v: 5304.0\n",
      "fc layer 1 self.abs_max_out: 5518.0\n",
      "lif layer 1 self.abs_max_v: 5518.0\n",
      "fc layer 2 self.abs_max_out: 2844.0\n",
      "lif layer 2 self.abs_max_v: 4288.5\n",
      "fc layer 2 self.abs_max_out: 2893.0\n",
      "fc layer 2 self.abs_max_out: 3172.0\n",
      "lif layer 2 self.abs_max_v: 4383.5\n",
      "fc layer 2 self.abs_max_out: 3246.0\n",
      "fc layer 2 self.abs_max_out: 3377.0\n",
      "lif layer 2 self.abs_max_v: 4727.0\n",
      "fc layer 1 self.abs_max_out: 5559.0\n",
      "lif layer 1 self.abs_max_v: 5559.0\n",
      "fc layer 2 self.abs_max_out: 3395.0\n",
      "fc layer 1 self.abs_max_out: 5581.0\n",
      "lif layer 1 self.abs_max_v: 5581.0\n",
      "lif layer 2 self.abs_max_v: 5035.0\n",
      "fc layer 1 self.abs_max_out: 5795.0\n",
      "lif layer 1 self.abs_max_v: 5795.0\n",
      "fc layer 2 self.abs_max_out: 3518.0\n",
      "lif layer 1 self.abs_max_v: 5809.0\n",
      "fc layer 1 self.abs_max_out: 5992.0\n",
      "lif layer 1 self.abs_max_v: 5992.0\n",
      "fc layer 2 self.abs_max_out: 3548.0\n",
      "lif layer 1 self.abs_max_v: 6002.5\n",
      "lif layer 2 self.abs_max_v: 5233.0\n",
      "fc layer 2 self.abs_max_out: 3595.0\n",
      "lif layer 1 self.abs_max_v: 6254.5\n",
      "lif layer 1 self.abs_max_v: 6257.5\n",
      "fc layer 1 self.abs_max_out: 6062.0\n",
      "fc layer 1 self.abs_max_out: 6097.0\n",
      "fc layer 2 self.abs_max_out: 3703.0\n",
      "lif layer 1 self.abs_max_v: 6830.5\n",
      "fc layer 1 self.abs_max_out: 6223.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.994115/  2.083418, val:  34.58%, val_best:  34.58%, tr:  88.46%, tr_best:  88.46%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7830%\n",
      "layer   2  Sparsity: 78.1665%\n",
      "layer   3  Sparsity: 80.2010%\n",
      "total_backward_count 9790 real_backward_count 3299  33.698%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3783.0\n",
      "lif layer 2 self.abs_max_v: 5359.5\n",
      "fc layer 2 self.abs_max_out: 4070.0\n",
      "lif layer 2 self.abs_max_v: 5364.5\n",
      "lif layer 1 self.abs_max_v: 7016.5\n",
      "lif layer 1 self.abs_max_v: 7217.0\n",
      "fc layer 1 self.abs_max_out: 6231.0\n",
      "lif layer 1 self.abs_max_v: 7241.5\n",
      "lif layer 2 self.abs_max_v: 5637.5\n",
      "fc layer 1 self.abs_max_out: 6296.0\n",
      "fc layer 1 self.abs_max_out: 6668.0\n",
      "fc layer 1 self.abs_max_out: 6834.0\n",
      "lif layer 2 self.abs_max_v: 5700.5\n",
      "lif layer 1 self.abs_max_v: 7604.0\n",
      "lif layer 2 self.abs_max_v: 5870.0\n",
      "lif layer 2 self.abs_max_v: 6151.0\n",
      "lif layer 2 self.abs_max_v: 6306.5\n",
      "lif layer 2 self.abs_max_v: 6409.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.957284/  2.073248, val:  49.58%, val_best:  49.58%, tr:  98.37%, tr_best:  98.37%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8033%\n",
      "layer   2  Sparsity: 76.2870%\n",
      "layer   3  Sparsity: 77.8099%\n",
      "total_backward_count 19580 real_backward_count 5385  27.503%\n",
      "fc layer 1 self.abs_max_out: 6918.0\n",
      "fc layer 1 self.abs_max_out: 7150.0\n",
      "fc layer 1 self.abs_max_out: 7215.0\n",
      "fc layer 2 self.abs_max_out: 4139.0\n",
      "lif layer 1 self.abs_max_v: 7631.0\n",
      "lif layer 1 self.abs_max_v: 8374.5\n",
      "lif layer 1 self.abs_max_v: 8408.5\n",
      "lif layer 1 self.abs_max_v: 8409.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.969832/  2.082859, val:  41.67%, val_best:  49.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8088%\n",
      "layer   2  Sparsity: 76.0235%\n",
      "layer   3  Sparsity: 77.2374%\n",
      "total_backward_count 29370 real_backward_count 7312  24.896%\n",
      "fc layer 1 self.abs_max_out: 7341.0\n",
      "fc layer 2 self.abs_max_out: 4193.0\n",
      "fc layer 1 self.abs_max_out: 7473.0\n",
      "fc layer 1 self.abs_max_out: 7721.0\n",
      "lif layer 1 self.abs_max_v: 8621.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.974037/  2.086406, val:  43.75%, val_best:  49.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7927%\n",
      "layer   2  Sparsity: 75.6436%\n",
      "layer   3  Sparsity: 77.1330%\n",
      "total_backward_count 39160 real_backward_count 9074  23.172%\n",
      "fc layer 1 self.abs_max_out: 7821.0\n",
      "lif layer 1 self.abs_max_v: 8813.5\n",
      "fc layer 1 self.abs_max_out: 8162.0\n",
      "lif layer 2 self.abs_max_v: 6628.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.973919/  2.088389, val:  42.08%, val_best:  49.58%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7933%\n",
      "layer   2  Sparsity: 74.9065%\n",
      "layer   3  Sparsity: 77.0331%\n",
      "total_backward_count 48950 real_backward_count 10703  21.865%\n",
      "fc layer 2 self.abs_max_out: 4247.0\n",
      "fc layer 2 self.abs_max_out: 4287.0\n",
      "lif layer 1 self.abs_max_v: 9048.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.973949/  2.083592, val:  56.67%, val_best:  56.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7851%\n",
      "layer   2  Sparsity: 74.0251%\n",
      "layer   3  Sparsity: 75.6853%\n",
      "total_backward_count 58740 real_backward_count 12295  20.931%\n",
      "lif layer 1 self.abs_max_v: 9214.5\n",
      "lif layer 2 self.abs_max_v: 6662.0\n",
      "lif layer 2 self.abs_max_v: 6727.5\n",
      "lif layer 2 self.abs_max_v: 6746.5\n",
      "fc layer 2 self.abs_max_out: 4294.0\n",
      "fc layer 1 self.abs_max_out: 8414.0\n",
      "lif layer 1 self.abs_max_v: 9253.5\n",
      "lif layer 1 self.abs_max_v: 9651.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.973299/  2.088312, val:  48.33%, val_best:  56.67%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7783%\n",
      "layer   2  Sparsity: 74.7558%\n",
      "layer   3  Sparsity: 76.1010%\n",
      "total_backward_count 68530 real_backward_count 13858  20.222%\n",
      "lif layer 2 self.abs_max_v: 6777.0\n",
      "lif layer 2 self.abs_max_v: 6871.5\n",
      "lif layer 2 self.abs_max_v: 7007.5\n",
      "fc layer 1 self.abs_max_out: 8713.0\n",
      "lif layer 1 self.abs_max_v: 10243.5\n",
      "lif layer 1 self.abs_max_v: 10594.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.962481/  2.075435, val:  50.42%, val_best:  56.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7981%\n",
      "layer   2  Sparsity: 74.4148%\n",
      "layer   3  Sparsity: 76.0300%\n",
      "total_backward_count 78320 real_backward_count 15358  19.609%\n",
      "fc layer 2 self.abs_max_out: 4515.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.968085/  2.068056, val:  53.33%, val_best:  56.67%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8081%\n",
      "layer   2  Sparsity: 74.4840%\n",
      "layer   3  Sparsity: 75.5335%\n",
      "total_backward_count 88110 real_backward_count 16909  19.191%\n",
      "fc layer 1 self.abs_max_out: 8769.0\n",
      "lif layer 1 self.abs_max_v: 11190.5\n",
      "lif layer 1 self.abs_max_v: 11695.5\n",
      "lif layer 2 self.abs_max_v: 7027.0\n",
      "lif layer 2 self.abs_max_v: 7335.0\n",
      "fc layer 1 self.abs_max_out: 9080.0\n",
      "lif layer 2 self.abs_max_v: 7424.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.966252/  2.092455, val:  47.08%, val_best:  56.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7987%\n",
      "layer   2  Sparsity: 74.4157%\n",
      "layer   3  Sparsity: 75.7207%\n",
      "total_backward_count 97900 real_backward_count 18303  18.696%\n",
      "lif layer 2 self.abs_max_v: 7451.5\n",
      "lif layer 2 self.abs_max_v: 7518.0\n",
      "lif layer 2 self.abs_max_v: 7537.5\n",
      "lif layer 2 self.abs_max_v: 7541.0\n",
      "lif layer 2 self.abs_max_v: 7564.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.964847/  2.090978, val:  45.83%, val_best:  56.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8050%\n",
      "layer   2  Sparsity: 73.7329%\n",
      "layer   3  Sparsity: 75.6901%\n",
      "total_backward_count 107690 real_backward_count 19720  18.312%\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.970048/  2.085992, val:  60.83%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7691%\n",
      "layer   2  Sparsity: 73.6384%\n",
      "layer   3  Sparsity: 75.3454%\n",
      "total_backward_count 117480 real_backward_count 21123  17.980%\n",
      "fc layer 1 self.abs_max_out: 9341.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.978179/  2.081693, val:  38.75%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7981%\n",
      "layer   2  Sparsity: 73.9820%\n",
      "layer   3  Sparsity: 76.9053%\n",
      "total_backward_count 127270 real_backward_count 22482  17.665%\n",
      "fc layer 2 self.abs_max_out: 4574.0\n",
      "fc layer 2 self.abs_max_out: 4661.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.970721/  2.087220, val:  51.67%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8031%\n",
      "layer   2  Sparsity: 73.5062%\n",
      "layer   3  Sparsity: 76.7922%\n",
      "total_backward_count 137060 real_backward_count 23900  17.438%\n",
      "lif layer 2 self.abs_max_v: 7597.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.971518/  2.064154, val:  54.17%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7759%\n",
      "layer   2  Sparsity: 73.7357%\n",
      "layer   3  Sparsity: 76.5916%\n",
      "total_backward_count 146850 real_backward_count 25187  17.152%\n",
      "lif layer 1 self.abs_max_v: 12147.0\n",
      "lif layer 1 self.abs_max_v: 13196.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.967060/  2.080679, val:  52.08%, val_best:  60.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7794%\n",
      "layer   2  Sparsity: 73.6178%\n",
      "layer   3  Sparsity: 76.3882%\n",
      "total_backward_count 156640 real_backward_count 26530  16.937%\n",
      "lif layer 2 self.abs_max_v: 7674.5\n",
      "lif layer 2 self.abs_max_v: 7695.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.960695/  2.074767, val:  55.00%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7951%\n",
      "layer   2  Sparsity: 73.6633%\n",
      "layer   3  Sparsity: 76.8304%\n",
      "total_backward_count 166430 real_backward_count 27896  16.761%\n",
      "fc layer 1 self.abs_max_out: 9711.0\n",
      "lif layer 2 self.abs_max_v: 7856.0\n",
      "lif layer 2 self.abs_max_v: 7879.0\n",
      "lif layer 2 self.abs_max_v: 7940.5\n",
      "lif layer 2 self.abs_max_v: 8053.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.969471/  2.059888, val:  63.33%, val_best:  63.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7649%\n",
      "layer   2  Sparsity: 73.6641%\n",
      "layer   3  Sparsity: 77.2862%\n",
      "total_backward_count 176220 real_backward_count 29258  16.603%\n",
      "lif layer 2 self.abs_max_v: 8090.5\n",
      "lif layer 2 self.abs_max_v: 8196.5\n",
      "lif layer 2 self.abs_max_v: 8201.0\n",
      "lif layer 2 self.abs_max_v: 8283.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.958906/  2.069978, val:  54.58%, val_best:  63.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7780%\n",
      "layer   2  Sparsity: 73.8593%\n",
      "layer   3  Sparsity: 77.6091%\n",
      "total_backward_count 186010 real_backward_count 30590  16.445%\n",
      "fc layer 2 self.abs_max_out: 4718.0\n",
      "lif layer 1 self.abs_max_v: 13410.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.958697/  2.073040, val:  42.50%, val_best:  63.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7782%\n",
      "layer   2  Sparsity: 73.6845%\n",
      "layer   3  Sparsity: 77.1811%\n",
      "total_backward_count 195800 real_backward_count 31861  16.272%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.954254/  2.080790, val:  41.67%, val_best:  63.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7819%\n",
      "layer   2  Sparsity: 73.8983%\n",
      "layer   3  Sparsity: 77.4457%\n",
      "total_backward_count 205590 real_backward_count 33188  16.143%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.964329/  2.063483, val:  56.67%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7957%\n",
      "layer   2  Sparsity: 73.7365%\n",
      "layer   3  Sparsity: 77.9915%\n",
      "total_backward_count 215380 real_backward_count 34537  16.035%\n",
      "fc layer 1 self.abs_max_out: 10029.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.958925/  2.050581, val:  61.67%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7781%\n",
      "layer   2  Sparsity: 73.3699%\n",
      "layer   3  Sparsity: 77.9443%\n",
      "total_backward_count 225170 real_backward_count 35859  15.925%\n",
      "lif layer 1 self.abs_max_v: 13524.0\n",
      "lif layer 1 self.abs_max_v: 14317.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.960454/  2.047225, val:  57.50%, val_best:  63.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7609%\n",
      "layer   2  Sparsity: 73.3407%\n",
      "layer   3  Sparsity: 77.8280%\n",
      "total_backward_count 234960 real_backward_count 37169  15.819%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.950121/  2.045345, val:  63.75%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7947%\n",
      "layer   2  Sparsity: 73.1056%\n",
      "layer   3  Sparsity: 77.4217%\n",
      "total_backward_count 244750 real_backward_count 38488  15.725%\n",
      "fc layer 1 self.abs_max_out: 10100.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.953058/  2.043890, val:  70.42%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7855%\n",
      "layer   2  Sparsity: 73.2076%\n",
      "layer   3  Sparsity: 77.3020%\n",
      "total_backward_count 254540 real_backward_count 39803  15.637%\n",
      "fc layer 1 self.abs_max_out: 10122.0\n",
      "fc layer 2 self.abs_max_out: 4753.0\n",
      "lif layer 1 self.abs_max_v: 14745.5\n",
      "lif layer 1 self.abs_max_v: 15633.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.957328/  2.054466, val:  61.25%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7870%\n",
      "layer   2  Sparsity: 73.4374%\n",
      "layer   3  Sparsity: 77.5192%\n",
      "total_backward_count 264330 real_backward_count 41092  15.546%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.946516/  2.043087, val:  68.33%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7581%\n",
      "layer   2  Sparsity: 73.3260%\n",
      "layer   3  Sparsity: 77.5299%\n",
      "total_backward_count 274120 real_backward_count 42382  15.461%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.935576/  2.042825, val:  51.25%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7763%\n",
      "layer   2  Sparsity: 73.2084%\n",
      "layer   3  Sparsity: 77.8190%\n",
      "total_backward_count 283910 real_backward_count 43605  15.359%\n",
      "fc layer 1 self.abs_max_out: 10273.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.941690/  2.035142, val:  55.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7839%\n",
      "layer   2  Sparsity: 73.4108%\n",
      "layer   3  Sparsity: 77.9379%\n",
      "total_backward_count 293700 real_backward_count 44871  15.278%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.940238/  2.038411, val:  66.25%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8010%\n",
      "layer   2  Sparsity: 73.1221%\n",
      "layer   3  Sparsity: 77.3807%\n",
      "total_backward_count 303490 real_backward_count 46156  15.208%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.946263/  2.046982, val:  49.17%, val_best:  70.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7670%\n",
      "layer   2  Sparsity: 73.2112%\n",
      "layer   3  Sparsity: 77.8192%\n",
      "total_backward_count 313280 real_backward_count 47409  15.133%\n",
      "fc layer 1 self.abs_max_out: 10456.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.939780/  2.054157, val:  52.08%, val_best:  70.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7816%\n",
      "layer   2  Sparsity: 73.4014%\n",
      "layer   3  Sparsity: 77.7491%\n",
      "total_backward_count 323070 real_backward_count 48638  15.055%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.928518/  2.025156, val:  62.08%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.8072%\n",
      "layer   2  Sparsity: 73.5333%\n",
      "layer   3  Sparsity: 77.1554%\n",
      "total_backward_count 332860 real_backward_count 49908  14.994%\n",
      "fc layer 2 self.abs_max_out: 4941.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.931688/  2.037027, val:  57.08%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7884%\n",
      "layer   2  Sparsity: 73.2611%\n",
      "layer   3  Sparsity: 77.2951%\n",
      "total_backward_count 342650 real_backward_count 51104  14.914%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.929175/  2.026239, val:  62.92%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7761%\n",
      "layer   2  Sparsity: 73.2184%\n",
      "layer   3  Sparsity: 77.8220%\n",
      "total_backward_count 352440 real_backward_count 52297  14.839%\n",
      "fc layer 1 self.abs_max_out: 10504.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.927674/  2.026064, val:  66.25%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8100%\n",
      "layer   2  Sparsity: 73.1345%\n",
      "layer   3  Sparsity: 77.8489%\n",
      "total_backward_count 362230 real_backward_count 53451  14.756%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.937794/  2.043641, val:  61.67%, val_best:  70.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8023%\n",
      "layer   2  Sparsity: 72.9747%\n",
      "layer   3  Sparsity: 78.1194%\n",
      "total_backward_count 372020 real_backward_count 54576  14.670%\n",
      "fc layer 1 self.abs_max_out: 10744.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.934980/  2.029507, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7617%\n",
      "layer   2  Sparsity: 73.2248%\n",
      "layer   3  Sparsity: 77.9062%\n",
      "total_backward_count 381810 real_backward_count 55790  14.612%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.940261/  2.042071, val:  64.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7931%\n",
      "layer   2  Sparsity: 73.3188%\n",
      "layer   3  Sparsity: 78.8815%\n",
      "total_backward_count 391600 real_backward_count 56962  14.546%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.956482/  2.047125, val:  64.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7795%\n",
      "layer   2  Sparsity: 73.2534%\n",
      "layer   3  Sparsity: 79.5434%\n",
      "total_backward_count 401390 real_backward_count 58173  14.493%\n",
      "fc layer 1 self.abs_max_out: 10788.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.953217/  2.039045, val:  79.17%, val_best:  79.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7880%\n",
      "layer   2  Sparsity: 73.0230%\n",
      "layer   3  Sparsity: 79.0083%\n",
      "total_backward_count 411180 real_backward_count 59403  14.447%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.950610/  2.048519, val:  63.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8087%\n",
      "layer   2  Sparsity: 72.9621%\n",
      "layer   3  Sparsity: 78.8876%\n",
      "total_backward_count 420970 real_backward_count 60558  14.385%\n",
      "lif layer 1 self.abs_max_v: 15872.5\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.947324/  2.036856, val:  72.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8146%\n",
      "layer   2  Sparsity: 72.6392%\n",
      "layer   3  Sparsity: 78.4547%\n",
      "total_backward_count 430760 real_backward_count 61713  14.327%\n",
      "fc layer 1 self.abs_max_out: 11020.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.949576/  2.030090, val:  77.08%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8252%\n",
      "layer   2  Sparsity: 73.0600%\n",
      "layer   3  Sparsity: 78.2978%\n",
      "total_backward_count 440550 real_backward_count 62913  14.281%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.936552/  2.025697, val:  67.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7740%\n",
      "layer   2  Sparsity: 73.0523%\n",
      "layer   3  Sparsity: 77.7116%\n",
      "total_backward_count 450340 real_backward_count 64033  14.219%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.941666/  2.039861, val:  56.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7710%\n",
      "layer   2  Sparsity: 73.1808%\n",
      "layer   3  Sparsity: 77.1828%\n",
      "total_backward_count 460130 real_backward_count 65093  14.147%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.924216/  2.022556, val:  63.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8015%\n",
      "layer   2  Sparsity: 73.0955%\n",
      "layer   3  Sparsity: 76.4572%\n",
      "total_backward_count 469920 real_backward_count 66191  14.086%\n",
      "fc layer 1 self.abs_max_out: 11028.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.918558/  2.011336, val:  79.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7653%\n",
      "layer   2  Sparsity: 72.8683%\n",
      "layer   3  Sparsity: 76.8682%\n",
      "total_backward_count 479710 real_backward_count 67283  14.026%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.915364/  2.002874, val:  64.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7708%\n",
      "layer   2  Sparsity: 72.9663%\n",
      "layer   3  Sparsity: 77.0068%\n",
      "total_backward_count 489500 real_backward_count 68390  13.971%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.917267/  2.024071, val:  68.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7926%\n",
      "layer   2  Sparsity: 73.2100%\n",
      "layer   3  Sparsity: 77.6355%\n",
      "total_backward_count 499290 real_backward_count 69495  13.919%\n",
      "fc layer 1 self.abs_max_out: 11261.0\n",
      "lif layer 1 self.abs_max_v: 15904.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.919322/  2.018476, val:  70.00%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7648%\n",
      "layer   2  Sparsity: 72.8463%\n",
      "layer   3  Sparsity: 77.5231%\n",
      "total_backward_count 509080 real_backward_count 70543  13.857%\n",
      "fc layer 1 self.abs_max_out: 11314.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.921256/  2.013179, val:  67.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7852%\n",
      "layer   2  Sparsity: 72.9596%\n",
      "layer   3  Sparsity: 76.8924%\n",
      "total_backward_count 518870 real_backward_count 71648  13.808%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.913402/  2.006166, val:  73.33%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7729%\n",
      "layer   2  Sparsity: 72.5379%\n",
      "layer   3  Sparsity: 77.3491%\n",
      "total_backward_count 528660 real_backward_count 72732  13.758%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.916895/  2.014332, val:  73.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7869%\n",
      "layer   2  Sparsity: 72.7425%\n",
      "layer   3  Sparsity: 76.2747%\n",
      "total_backward_count 538450 real_backward_count 73808  13.707%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.925447/  2.014670, val:  64.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7933%\n",
      "layer   2  Sparsity: 72.9769%\n",
      "layer   3  Sparsity: 76.1853%\n",
      "total_backward_count 548240 real_backward_count 74925  13.666%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.901792/  2.015292, val:  53.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7656%\n",
      "layer   2  Sparsity: 72.9224%\n",
      "layer   3  Sparsity: 76.2721%\n",
      "total_backward_count 558030 real_backward_count 75988  13.617%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.910143/  1.998223, val:  71.25%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7939%\n",
      "layer   2  Sparsity: 72.8253%\n",
      "layer   3  Sparsity: 76.5759%\n",
      "total_backward_count 567820 real_backward_count 77080  13.575%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.899448/  2.001713, val:  63.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7888%\n",
      "layer   2  Sparsity: 73.0151%\n",
      "layer   3  Sparsity: 77.2402%\n",
      "total_backward_count 577610 real_backward_count 78113  13.523%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.902729/  2.001362, val:  58.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7975%\n",
      "layer   2  Sparsity: 72.6526%\n",
      "layer   3  Sparsity: 76.6173%\n",
      "total_backward_count 587400 real_backward_count 79106  13.467%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.899248/  2.005836, val:  57.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7944%\n",
      "layer   2  Sparsity: 72.8385%\n",
      "layer   3  Sparsity: 76.2032%\n",
      "total_backward_count 597190 real_backward_count 80179  13.426%\n",
      "fc layer 1 self.abs_max_out: 11372.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.901197/  1.985520, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7667%\n",
      "layer   2  Sparsity: 72.7297%\n",
      "layer   3  Sparsity: 76.7631%\n",
      "total_backward_count 606980 real_backward_count 81235  13.383%\n",
      "fc layer 1 self.abs_max_out: 11558.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.899751/  2.008253, val:  57.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7840%\n",
      "layer   2  Sparsity: 72.8452%\n",
      "layer   3  Sparsity: 76.9510%\n",
      "total_backward_count 616770 real_backward_count 82284  13.341%\n",
      "fc layer 3 self.abs_max_out: 1199.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.902461/  2.003474, val:  71.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8062%\n",
      "layer   2  Sparsity: 72.5954%\n",
      "layer   3  Sparsity: 77.4209%\n",
      "total_backward_count 626560 real_backward_count 83277  13.291%\n",
      "fc layer 1 self.abs_max_out: 11655.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.904620/  2.017549, val:  68.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8105%\n",
      "layer   2  Sparsity: 72.5394%\n",
      "layer   3  Sparsity: 77.1628%\n",
      "total_backward_count 636350 real_backward_count 84333  13.253%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.896706/  1.993591, val:  77.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7800%\n",
      "layer   2  Sparsity: 72.3187%\n",
      "layer   3  Sparsity: 76.2848%\n",
      "total_backward_count 646140 real_backward_count 85394  13.216%\n",
      "fc layer 2 self.abs_max_out: 4942.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.893114/  2.004569, val:  62.08%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7798%\n",
      "layer   2  Sparsity: 72.6687%\n",
      "layer   3  Sparsity: 76.4909%\n",
      "total_backward_count 655930 real_backward_count 86404  13.173%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.892320/  1.993958, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7676%\n",
      "layer   2  Sparsity: 72.8030%\n",
      "layer   3  Sparsity: 76.2332%\n",
      "total_backward_count 665720 real_backward_count 87427  13.133%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.884022/  1.989270, val:  80.42%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7749%\n",
      "layer   2  Sparsity: 72.8221%\n",
      "layer   3  Sparsity: 75.9549%\n",
      "total_backward_count 675510 real_backward_count 88464  13.096%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.886409/  1.993214, val:  71.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7821%\n",
      "layer   2  Sparsity: 72.6008%\n",
      "layer   3  Sparsity: 75.8683%\n",
      "total_backward_count 685300 real_backward_count 89525  13.064%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.869804/  1.982043, val:  75.00%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7885%\n",
      "layer   2  Sparsity: 72.7214%\n",
      "layer   3  Sparsity: 75.8236%\n",
      "total_backward_count 695090 real_backward_count 90526  13.024%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.872760/  1.993362, val:  77.50%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.7762%\n",
      "layer   2  Sparsity: 72.6297%\n",
      "layer   3  Sparsity: 76.1491%\n",
      "total_backward_count 704880 real_backward_count 91470  12.977%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.885011/  1.983027, val:  80.00%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7846%\n",
      "layer   2  Sparsity: 72.6925%\n",
      "layer   3  Sparsity: 76.4844%\n",
      "total_backward_count 714670 real_backward_count 92474  12.939%\n",
      "fc layer 1 self.abs_max_out: 11721.0\n",
      "fc layer 3 self.abs_max_out: 1228.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.884138/  2.002736, val:  68.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7874%\n",
      "layer   2  Sparsity: 72.8250%\n",
      "layer   3  Sparsity: 77.7065%\n",
      "total_backward_count 724460 real_backward_count 93392  12.891%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.899419/  1.986811, val:  77.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7967%\n",
      "layer   2  Sparsity: 73.1094%\n",
      "layer   3  Sparsity: 77.8941%\n",
      "total_backward_count 734250 real_backward_count 94458  12.865%\n",
      "fc layer 1 self.abs_max_out: 11876.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.880975/  1.970105, val:  81.25%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7779%\n",
      "layer   2  Sparsity: 72.8028%\n",
      "layer   3  Sparsity: 76.1269%\n",
      "total_backward_count 744040 real_backward_count 95401  12.822%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.875357/  1.966017, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7972%\n",
      "layer   2  Sparsity: 72.8755%\n",
      "layer   3  Sparsity: 76.0500%\n",
      "total_backward_count 753830 real_backward_count 96374  12.785%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.860156/  1.977235, val:  63.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7958%\n",
      "layer   2  Sparsity: 72.5454%\n",
      "layer   3  Sparsity: 76.0582%\n",
      "total_backward_count 763620 real_backward_count 97387  12.753%\n",
      "fc layer 1 self.abs_max_out: 11884.0\n",
      "lif layer 1 self.abs_max_v: 15918.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.867142/  1.973761, val:  70.83%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8028%\n",
      "layer   2  Sparsity: 72.5555%\n",
      "layer   3  Sparsity: 76.8249%\n",
      "total_backward_count 773410 real_backward_count 98391  12.722%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.863908/  1.991253, val:  79.17%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7859%\n",
      "layer   2  Sparsity: 72.5399%\n",
      "layer   3  Sparsity: 76.3798%\n",
      "total_backward_count 783200 real_backward_count 99331  12.683%\n",
      "fc layer 2 self.abs_max_out: 4961.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.878503/  1.987467, val:  75.42%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7705%\n",
      "layer   2  Sparsity: 72.4635%\n",
      "layer   3  Sparsity: 76.7424%\n",
      "total_backward_count 792990 real_backward_count 100354  12.655%\n",
      "lif layer 1 self.abs_max_v: 16061.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.886588/  1.985467, val:  67.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7978%\n",
      "layer   2  Sparsity: 72.5839%\n",
      "layer   3  Sparsity: 76.5217%\n",
      "total_backward_count 802780 real_backward_count 101338  12.623%\n",
      "fc layer 2 self.abs_max_out: 5100.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.881374/  1.961696, val:  72.50%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7865%\n",
      "layer   2  Sparsity: 72.4534%\n",
      "layer   3  Sparsity: 76.6443%\n",
      "total_backward_count 812570 real_backward_count 102341  12.595%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.878563/  1.976081, val:  74.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7684%\n",
      "layer   2  Sparsity: 72.5096%\n",
      "layer   3  Sparsity: 77.2261%\n",
      "total_backward_count 822360 real_backward_count 103313  12.563%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.873031/  1.950856, val:  69.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7868%\n",
      "layer   2  Sparsity: 72.5542%\n",
      "layer   3  Sparsity: 76.7372%\n",
      "total_backward_count 832150 real_backward_count 104249  12.528%\n",
      "lif layer 1 self.abs_max_v: 16365.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.853625/  1.963199, val:  74.58%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7863%\n",
      "layer   2  Sparsity: 72.5127%\n",
      "layer   3  Sparsity: 76.3725%\n",
      "total_backward_count 841940 real_backward_count 105171  12.492%\n",
      "fc layer 3 self.abs_max_out: 1241.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.867458/  1.965194, val:  75.42%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7703%\n",
      "layer   2  Sparsity: 72.8266%\n",
      "layer   3  Sparsity: 76.3873%\n",
      "total_backward_count 851730 real_backward_count 106104  12.457%\n",
      "lif layer 1 self.abs_max_v: 17265.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.877760/  1.977507, val:  81.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7505%\n",
      "layer   2  Sparsity: 72.4151%\n",
      "layer   3  Sparsity: 76.9130%\n",
      "total_backward_count 861520 real_backward_count 106977  12.417%\n",
      "fc layer 2 self.abs_max_out: 5173.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.875273/  1.956747, val:  75.00%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7661%\n",
      "layer   2  Sparsity: 72.1988%\n",
      "layer   3  Sparsity: 75.8864%\n",
      "total_backward_count 871310 real_backward_count 107936  12.388%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.862795/  1.960716, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7944%\n",
      "layer   2  Sparsity: 72.1858%\n",
      "layer   3  Sparsity: 76.2763%\n",
      "total_backward_count 881100 real_backward_count 108892  12.359%\n",
      "lif layer 1 self.abs_max_v: 17600.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.856871/  1.963553, val:  67.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7917%\n",
      "layer   2  Sparsity: 72.3533%\n",
      "layer   3  Sparsity: 76.5967%\n",
      "total_backward_count 890890 real_backward_count 109828  12.328%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.860114/  1.963091, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7630%\n",
      "layer   2  Sparsity: 72.5079%\n",
      "layer   3  Sparsity: 75.8767%\n",
      "total_backward_count 900680 real_backward_count 110768  12.298%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.860537/  1.965719, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8051%\n",
      "layer   2  Sparsity: 72.3923%\n",
      "layer   3  Sparsity: 76.5505%\n",
      "total_backward_count 910470 real_backward_count 111677  12.266%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.869463/  1.974369, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8026%\n",
      "layer   2  Sparsity: 72.3513%\n",
      "layer   3  Sparsity: 76.2581%\n",
      "total_backward_count 920260 real_backward_count 112643  12.240%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.856053/  1.952916, val:  75.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8078%\n",
      "layer   2  Sparsity: 72.4795%\n",
      "layer   3  Sparsity: 75.9893%\n",
      "total_backward_count 930050 real_backward_count 113571  12.211%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.860709/  1.961107, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8020%\n",
      "layer   2  Sparsity: 72.4524%\n",
      "layer   3  Sparsity: 76.9092%\n",
      "total_backward_count 939840 real_backward_count 114490  12.182%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.871809/  1.981778, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7611%\n",
      "layer   2  Sparsity: 72.5436%\n",
      "layer   3  Sparsity: 76.5623%\n",
      "total_backward_count 949630 real_backward_count 115470  12.159%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.872238/  1.963875, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7939%\n",
      "layer   2  Sparsity: 72.6884%\n",
      "layer   3  Sparsity: 76.5751%\n",
      "total_backward_count 959420 real_backward_count 116377  12.130%\n",
      "fc layer 1 self.abs_max_out: 12203.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.869898/  1.984591, val:  69.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7944%\n",
      "layer   2  Sparsity: 72.5159%\n",
      "layer   3  Sparsity: 76.9222%\n",
      "total_backward_count 969210 real_backward_count 117305  12.103%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.864057/  1.958063, val:  80.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7956%\n",
      "layer   2  Sparsity: 72.1715%\n",
      "layer   3  Sparsity: 76.7127%\n",
      "total_backward_count 979000 real_backward_count 118231  12.077%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.874960/  1.977757, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7850%\n",
      "layer   2  Sparsity: 72.4035%\n",
      "layer   3  Sparsity: 77.1616%\n",
      "total_backward_count 988790 real_backward_count 119126  12.048%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.879850/  1.964403, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7794%\n",
      "layer   2  Sparsity: 72.6493%\n",
      "layer   3  Sparsity: 77.3150%\n",
      "total_backward_count 998580 real_backward_count 120037  12.021%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.865577/  1.974457, val:  71.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8233%\n",
      "layer   2  Sparsity: 72.4883%\n",
      "layer   3  Sparsity: 76.8368%\n",
      "total_backward_count 1008370 real_backward_count 120924  11.992%\n",
      "fc layer 1 self.abs_max_out: 12220.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.866511/  1.967157, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7809%\n",
      "layer   2  Sparsity: 72.1798%\n",
      "layer   3  Sparsity: 76.1999%\n",
      "total_backward_count 1018160 real_backward_count 121880  11.971%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.873092/  1.959195, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8067%\n",
      "layer   2  Sparsity: 72.5454%\n",
      "layer   3  Sparsity: 76.4309%\n",
      "total_backward_count 1027950 real_backward_count 122789  11.945%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.870440/  1.973499, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7797%\n",
      "layer   2  Sparsity: 72.1062%\n",
      "layer   3  Sparsity: 76.9681%\n",
      "total_backward_count 1037740 real_backward_count 123687  11.919%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.878312/  1.978362, val:  71.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7948%\n",
      "layer   2  Sparsity: 72.0816%\n",
      "layer   3  Sparsity: 77.5340%\n",
      "total_backward_count 1047530 real_backward_count 124498  11.885%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.879867/  1.987148, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7857%\n",
      "layer   2  Sparsity: 72.0614%\n",
      "layer   3  Sparsity: 77.2866%\n",
      "total_backward_count 1057320 real_backward_count 125429  11.863%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.891522/  1.972322, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7790%\n",
      "layer   2  Sparsity: 72.0069%\n",
      "layer   3  Sparsity: 77.0102%\n",
      "total_backward_count 1067110 real_backward_count 126303  11.836%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.877651/  1.967860, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7601%\n",
      "layer   2  Sparsity: 71.9548%\n",
      "layer   3  Sparsity: 77.1574%\n",
      "total_backward_count 1076900 real_backward_count 127190  11.811%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.874818/  1.956787, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7989%\n",
      "layer   2  Sparsity: 72.2634%\n",
      "layer   3  Sparsity: 75.9898%\n",
      "total_backward_count 1086690 real_backward_count 128019  11.781%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.863033/  1.966817, val:  63.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7958%\n",
      "layer   2  Sparsity: 72.0917%\n",
      "layer   3  Sparsity: 75.7112%\n",
      "total_backward_count 1096480 real_backward_count 128888  11.755%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.872108/  1.966836, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8049%\n",
      "layer   2  Sparsity: 71.9283%\n",
      "layer   3  Sparsity: 76.3790%\n",
      "total_backward_count 1106270 real_backward_count 129793  11.732%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.861652/  1.977187, val:  77.92%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7522%\n",
      "layer   2  Sparsity: 72.2710%\n",
      "layer   3  Sparsity: 76.7462%\n",
      "total_backward_count 1116060 real_backward_count 130633  11.705%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.872786/  1.976278, val:  76.25%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7988%\n",
      "layer   2  Sparsity: 72.2350%\n",
      "layer   3  Sparsity: 76.9270%\n",
      "total_backward_count 1125850 real_backward_count 131512  11.681%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.865602/  1.980332, val:  79.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7875%\n",
      "layer   2  Sparsity: 72.2023%\n",
      "layer   3  Sparsity: 76.4979%\n",
      "total_backward_count 1135640 real_backward_count 132415  11.660%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.870873/  1.976308, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7850%\n",
      "layer   2  Sparsity: 72.4620%\n",
      "layer   3  Sparsity: 76.8707%\n",
      "total_backward_count 1145430 real_backward_count 133293  11.637%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.865277/  1.945521, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7991%\n",
      "layer   2  Sparsity: 72.5160%\n",
      "layer   3  Sparsity: 76.6450%\n",
      "total_backward_count 1155220 real_backward_count 134187  11.616%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.859934/  1.961594, val:  66.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7667%\n",
      "layer   2  Sparsity: 72.3583%\n",
      "layer   3  Sparsity: 76.2599%\n",
      "total_backward_count 1165010 real_backward_count 135021  11.590%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.860053/  1.961795, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7776%\n",
      "layer   2  Sparsity: 72.4587%\n",
      "layer   3  Sparsity: 77.0604%\n",
      "total_backward_count 1174800 real_backward_count 135903  11.568%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.861397/  1.972358, val:  72.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7732%\n",
      "layer   2  Sparsity: 72.7107%\n",
      "layer   3  Sparsity: 76.9350%\n",
      "total_backward_count 1184590 real_backward_count 136775  11.546%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.874987/  1.985674, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7922%\n",
      "layer   2  Sparsity: 72.4613%\n",
      "layer   3  Sparsity: 76.9887%\n",
      "total_backward_count 1194380 real_backward_count 137642  11.524%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.872281/  1.969632, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7766%\n",
      "layer   2  Sparsity: 72.4628%\n",
      "layer   3  Sparsity: 77.3960%\n",
      "total_backward_count 1204170 real_backward_count 138540  11.505%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.864084/  1.950795, val:  85.83%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7870%\n",
      "layer   2  Sparsity: 72.3478%\n",
      "layer   3  Sparsity: 76.2057%\n",
      "total_backward_count 1213960 real_backward_count 139451  11.487%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.858997/  1.945139, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8007%\n",
      "layer   2  Sparsity: 72.2467%\n",
      "layer   3  Sparsity: 76.5239%\n",
      "total_backward_count 1223750 real_backward_count 140312  11.466%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.852684/  1.959555, val:  73.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7614%\n",
      "layer   2  Sparsity: 72.1788%\n",
      "layer   3  Sparsity: 76.3342%\n",
      "total_backward_count 1233540 real_backward_count 141196  11.446%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.849901/  1.973486, val:  77.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8120%\n",
      "layer   2  Sparsity: 72.2764%\n",
      "layer   3  Sparsity: 76.9016%\n",
      "total_backward_count 1243330 real_backward_count 142067  11.426%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.863502/  1.957743, val:  77.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8015%\n",
      "layer   2  Sparsity: 72.3906%\n",
      "layer   3  Sparsity: 76.1775%\n",
      "total_backward_count 1253120 real_backward_count 142944  11.407%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.849437/  1.959841, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7681%\n",
      "layer   2  Sparsity: 72.2608%\n",
      "layer   3  Sparsity: 76.6246%\n",
      "total_backward_count 1262910 real_backward_count 143768  11.384%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.854147/  1.968966, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7867%\n",
      "layer   2  Sparsity: 72.5572%\n",
      "layer   3  Sparsity: 77.2404%\n",
      "total_backward_count 1272700 real_backward_count 144626  11.364%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.866834/  1.961215, val:  73.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7721%\n",
      "layer   2  Sparsity: 72.0896%\n",
      "layer   3  Sparsity: 78.0922%\n",
      "total_backward_count 1282490 real_backward_count 145434  11.340%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.867191/  1.958124, val:  74.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7802%\n",
      "layer   2  Sparsity: 71.9819%\n",
      "layer   3  Sparsity: 77.4151%\n",
      "total_backward_count 1292280 real_backward_count 146269  11.319%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.850855/  1.967433, val:  67.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7920%\n",
      "layer   2  Sparsity: 72.1982%\n",
      "layer   3  Sparsity: 76.7606%\n",
      "total_backward_count 1302070 real_backward_count 147111  11.298%\n",
      "fc layer 1 self.abs_max_out: 12238.0\n",
      "fc layer 1 self.abs_max_out: 12370.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.854655/  1.959466, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7928%\n",
      "layer   2  Sparsity: 71.9884%\n",
      "layer   3  Sparsity: 76.4667%\n",
      "total_backward_count 1311860 real_backward_count 147951  11.278%\n",
      "fc layer 1 self.abs_max_out: 12384.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.850744/  1.957519, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7762%\n",
      "layer   2  Sparsity: 72.2166%\n",
      "layer   3  Sparsity: 76.1923%\n",
      "total_backward_count 1321650 real_backward_count 148793  11.258%\n",
      "fc layer 2 self.abs_max_out: 5244.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.842922/  1.934887, val:  83.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7661%\n",
      "layer   2  Sparsity: 72.3162%\n",
      "layer   3  Sparsity: 75.8848%\n",
      "total_backward_count 1331440 real_backward_count 149648  11.240%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.842611/  1.941289, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7895%\n",
      "layer   2  Sparsity: 72.2475%\n",
      "layer   3  Sparsity: 76.5813%\n",
      "total_backward_count 1341230 real_backward_count 150480  11.220%\n",
      "fc layer 1 self.abs_max_out: 12416.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.846739/  1.944810, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8029%\n",
      "layer   2  Sparsity: 72.2167%\n",
      "layer   3  Sparsity: 76.7853%\n",
      "total_backward_count 1351020 real_backward_count 151301  11.199%\n",
      "fc layer 2 self.abs_max_out: 5247.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.848643/  1.953644, val:  77.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7771%\n",
      "layer   2  Sparsity: 72.3228%\n",
      "layer   3  Sparsity: 76.6072%\n",
      "total_backward_count 1360810 real_backward_count 152165  11.182%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.849697/  1.953008, val:  80.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7917%\n",
      "layer   2  Sparsity: 72.0365%\n",
      "layer   3  Sparsity: 75.8542%\n",
      "total_backward_count 1370600 real_backward_count 152983  11.162%\n",
      "fc layer 1 self.abs_max_out: 12427.0\n",
      "fc layer 2 self.abs_max_out: 5316.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.858058/  1.970174, val:  74.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7561%\n",
      "layer   2  Sparsity: 71.7785%\n",
      "layer   3  Sparsity: 75.9455%\n",
      "total_backward_count 1380390 real_backward_count 153836  11.144%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.860875/  1.961701, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7934%\n",
      "layer   2  Sparsity: 71.8946%\n",
      "layer   3  Sparsity: 76.2597%\n",
      "total_backward_count 1390180 real_backward_count 154680  11.127%\n",
      "fc layer 2 self.abs_max_out: 5476.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.852219/  1.942790, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7687%\n",
      "layer   2  Sparsity: 71.9026%\n",
      "layer   3  Sparsity: 76.2674%\n",
      "total_backward_count 1399970 real_backward_count 155539  11.110%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.833401/  1.945998, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7634%\n",
      "layer   2  Sparsity: 71.9810%\n",
      "layer   3  Sparsity: 75.8518%\n",
      "total_backward_count 1409760 real_backward_count 156386  11.093%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.831587/  1.936851, val:  73.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8017%\n",
      "layer   2  Sparsity: 72.3112%\n",
      "layer   3  Sparsity: 75.5283%\n",
      "total_backward_count 1419550 real_backward_count 157221  11.075%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.829738/  1.937552, val:  69.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7981%\n",
      "layer   2  Sparsity: 72.3428%\n",
      "layer   3  Sparsity: 75.2904%\n",
      "total_backward_count 1429340 real_backward_count 158073  11.059%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.837510/  1.948434, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7959%\n",
      "layer   2  Sparsity: 72.1951%\n",
      "layer   3  Sparsity: 75.7535%\n",
      "total_backward_count 1439130 real_backward_count 158936  11.044%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.841016/  1.953886, val:  69.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7796%\n",
      "layer   2  Sparsity: 72.1482%\n",
      "layer   3  Sparsity: 75.5034%\n",
      "total_backward_count 1448920 real_backward_count 159766  11.027%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.836895/  1.936900, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7940%\n",
      "layer   2  Sparsity: 72.2895%\n",
      "layer   3  Sparsity: 75.7048%\n",
      "total_backward_count 1458710 real_backward_count 160640  11.012%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.841161/  1.947239, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7886%\n",
      "layer   2  Sparsity: 72.0796%\n",
      "layer   3  Sparsity: 76.5341%\n",
      "total_backward_count 1468500 real_backward_count 161480  10.996%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.833272/  1.943669, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8182%\n",
      "layer   2  Sparsity: 72.2167%\n",
      "layer   3  Sparsity: 75.9320%\n",
      "total_backward_count 1478290 real_backward_count 162329  10.981%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.836868/  1.950766, val:  66.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7931%\n",
      "layer   2  Sparsity: 72.1607%\n",
      "layer   3  Sparsity: 76.0550%\n",
      "total_backward_count 1488080 real_backward_count 163148  10.964%\n",
      "fc layer 2 self.abs_max_out: 5615.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.824419/  1.931933, val:  77.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7933%\n",
      "layer   2  Sparsity: 72.0635%\n",
      "layer   3  Sparsity: 75.8538%\n",
      "total_backward_count 1497870 real_backward_count 163967  10.947%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.820332/  1.922176, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7812%\n",
      "layer   2  Sparsity: 72.2784%\n",
      "layer   3  Sparsity: 75.7672%\n",
      "total_backward_count 1507660 real_backward_count 164803  10.931%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.828622/  1.937145, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7739%\n",
      "layer   2  Sparsity: 71.7836%\n",
      "layer   3  Sparsity: 75.5737%\n",
      "total_backward_count 1517450 real_backward_count 165618  10.914%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.825837/  1.941333, val:  80.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7967%\n",
      "layer   2  Sparsity: 72.0578%\n",
      "layer   3  Sparsity: 75.4378%\n",
      "total_backward_count 1527240 real_backward_count 166410  10.896%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.829503/  1.936963, val:  75.00%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7916%\n",
      "layer   2  Sparsity: 72.0802%\n",
      "layer   3  Sparsity: 75.4990%\n",
      "total_backward_count 1537030 real_backward_count 167230  10.880%\n",
      "fc layer 2 self.abs_max_out: 5690.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.824980/  1.933226, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7755%\n",
      "layer   2  Sparsity: 71.8900%\n",
      "layer   3  Sparsity: 75.0845%\n",
      "total_backward_count 1546820 real_backward_count 168022  10.862%\n",
      "fc layer 1 self.abs_max_out: 12638.0\n",
      "lif layer 1 self.abs_max_v: 17646.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.821020/  1.931961, val:  78.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8049%\n",
      "layer   2  Sparsity: 71.9303%\n",
      "layer   3  Sparsity: 75.4472%\n",
      "total_backward_count 1556610 real_backward_count 168831  10.846%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.818869/  1.931777, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7959%\n",
      "layer   2  Sparsity: 71.8771%\n",
      "layer   3  Sparsity: 75.8363%\n",
      "total_backward_count 1566400 real_backward_count 169584  10.826%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.819768/  1.920966, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8113%\n",
      "layer   2  Sparsity: 72.1767%\n",
      "layer   3  Sparsity: 75.9681%\n",
      "total_backward_count 1576190 real_backward_count 170357  10.808%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.809553/  1.924890, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7880%\n",
      "layer   2  Sparsity: 72.3785%\n",
      "layer   3  Sparsity: 76.6178%\n",
      "total_backward_count 1585980 real_backward_count 171131  10.790%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.829097/  1.942685, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7782%\n",
      "layer   2  Sparsity: 72.4419%\n",
      "layer   3  Sparsity: 75.8605%\n",
      "total_backward_count 1595770 real_backward_count 171939  10.775%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.825705/  1.933916, val:  70.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7883%\n",
      "layer   2  Sparsity: 72.4135%\n",
      "layer   3  Sparsity: 76.4363%\n",
      "total_backward_count 1605560 real_backward_count 172734  10.758%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.821800/  1.930672, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7781%\n",
      "layer   2  Sparsity: 72.1425%\n",
      "layer   3  Sparsity: 76.3438%\n",
      "total_backward_count 1615350 real_backward_count 173584  10.746%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.818057/  1.923697, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7903%\n",
      "layer   2  Sparsity: 72.2933%\n",
      "layer   3  Sparsity: 75.8335%\n",
      "total_backward_count 1625140 real_backward_count 174376  10.730%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.811075/  1.919759, val:  74.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7923%\n",
      "layer   2  Sparsity: 72.2946%\n",
      "layer   3  Sparsity: 75.7163%\n",
      "total_backward_count 1634930 real_backward_count 175139  10.712%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.819186/  1.932559, val:  74.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7700%\n",
      "layer   2  Sparsity: 72.1846%\n",
      "layer   3  Sparsity: 75.9929%\n",
      "total_backward_count 1644720 real_backward_count 175968  10.699%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.830353/  1.924167, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8114%\n",
      "layer   2  Sparsity: 72.3406%\n",
      "layer   3  Sparsity: 76.2128%\n",
      "total_backward_count 1654510 real_backward_count 176811  10.687%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.828068/  1.937520, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7895%\n",
      "layer   2  Sparsity: 72.4043%\n",
      "layer   3  Sparsity: 75.8262%\n",
      "total_backward_count 1664300 real_backward_count 177570  10.669%\n",
      "lif layer 1 self.abs_max_v: 17895.5\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.823625/  1.938519, val:  73.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7897%\n",
      "layer   2  Sparsity: 72.3883%\n",
      "layer   3  Sparsity: 75.3112%\n",
      "total_backward_count 1674090 real_backward_count 178367  10.655%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.812256/  1.935866, val:  68.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7626%\n",
      "layer   2  Sparsity: 72.2422%\n",
      "layer   3  Sparsity: 74.4069%\n",
      "total_backward_count 1683880 real_backward_count 179119  10.637%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.810450/  1.927187, val:  73.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7799%\n",
      "layer   2  Sparsity: 72.0676%\n",
      "layer   3  Sparsity: 74.3863%\n",
      "total_backward_count 1693670 real_backward_count 179925  10.623%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.809418/  1.933305, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7731%\n",
      "layer   2  Sparsity: 72.1938%\n",
      "layer   3  Sparsity: 75.1239%\n",
      "total_backward_count 1703460 real_backward_count 180700  10.608%\n",
      "fc layer 1 self.abs_max_out: 12690.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.820352/  1.922536, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.7743%\n",
      "layer   2  Sparsity: 72.2618%\n",
      "layer   3  Sparsity: 75.9407%\n",
      "total_backward_count 1713250 real_backward_count 181493  10.593%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.804893/  1.913900, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7577%\n",
      "layer   2  Sparsity: 72.3690%\n",
      "layer   3  Sparsity: 75.7032%\n",
      "total_backward_count 1723040 real_backward_count 182289  10.579%\n",
      "fc layer 1 self.abs_max_out: 12699.0\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.818882/  1.929443, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7858%\n",
      "layer   2  Sparsity: 72.0399%\n",
      "layer   3  Sparsity: 75.0387%\n",
      "total_backward_count 1732830 real_backward_count 183077  10.565%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.821704/  1.936220, val:  71.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7872%\n",
      "layer   2  Sparsity: 72.1333%\n",
      "layer   3  Sparsity: 75.0352%\n",
      "total_backward_count 1742620 real_backward_count 183869  10.551%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.830132/  1.953119, val:  72.50%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7768%\n",
      "layer   2  Sparsity: 72.1058%\n",
      "layer   3  Sparsity: 75.3027%\n",
      "total_backward_count 1752410 real_backward_count 184672  10.538%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.824419/  1.925086, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7963%\n",
      "layer   2  Sparsity: 72.0114%\n",
      "layer   3  Sparsity: 75.7776%\n",
      "total_backward_count 1762200 real_backward_count 185495  10.526%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.824805/  1.927415, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7685%\n",
      "layer   2  Sparsity: 71.8491%\n",
      "layer   3  Sparsity: 75.6013%\n",
      "total_backward_count 1771990 real_backward_count 186287  10.513%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.821869/  1.932787, val:  72.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7960%\n",
      "layer   2  Sparsity: 71.8595%\n",
      "layer   3  Sparsity: 75.3258%\n",
      "total_backward_count 1781780 real_backward_count 187073  10.499%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.811818/  1.917142, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7623%\n",
      "layer   2  Sparsity: 72.0370%\n",
      "layer   3  Sparsity: 74.6779%\n",
      "total_backward_count 1791570 real_backward_count 187843  10.485%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.811179/  1.920915, val:  73.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7822%\n",
      "layer   2  Sparsity: 72.0697%\n",
      "layer   3  Sparsity: 74.8808%\n",
      "total_backward_count 1801360 real_backward_count 188600  10.470%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.813446/  1.928533, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7840%\n",
      "layer   2  Sparsity: 72.0482%\n",
      "layer   3  Sparsity: 75.0776%\n",
      "total_backward_count 1811150 real_backward_count 189373  10.456%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.801690/  1.916796, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7615%\n",
      "layer   2  Sparsity: 72.1916%\n",
      "layer   3  Sparsity: 74.9374%\n",
      "total_backward_count 1820940 real_backward_count 190175  10.444%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.803024/  1.916148, val:  74.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7760%\n",
      "layer   2  Sparsity: 72.1597%\n",
      "layer   3  Sparsity: 74.9117%\n",
      "total_backward_count 1830730 real_backward_count 190973  10.432%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.800497/  1.934072, val:  73.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7847%\n",
      "layer   2  Sparsity: 72.1823%\n",
      "layer   3  Sparsity: 75.2739%\n",
      "total_backward_count 1840520 real_backward_count 191776  10.420%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.797388/  1.896346, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7798%\n",
      "layer   2  Sparsity: 72.3915%\n",
      "layer   3  Sparsity: 75.1080%\n",
      "total_backward_count 1850310 real_backward_count 192550  10.406%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.785429/  1.899608, val:  77.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7623%\n",
      "layer   2  Sparsity: 72.1992%\n",
      "layer   3  Sparsity: 75.0886%\n",
      "total_backward_count 1860100 real_backward_count 193302  10.392%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.784351/  1.897816, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7682%\n",
      "layer   2  Sparsity: 72.0930%\n",
      "layer   3  Sparsity: 74.6235%\n",
      "total_backward_count 1869890 real_backward_count 194064  10.378%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.788213/  1.899044, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7847%\n",
      "layer   2  Sparsity: 72.2396%\n",
      "layer   3  Sparsity: 74.5968%\n",
      "total_backward_count 1879680 real_backward_count 194822  10.365%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.786820/  1.898624, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7739%\n",
      "layer   2  Sparsity: 72.1020%\n",
      "layer   3  Sparsity: 74.7923%\n",
      "total_backward_count 1889470 real_backward_count 195587  10.351%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.779809/  1.922485, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8012%\n",
      "layer   2  Sparsity: 72.0435%\n",
      "layer   3  Sparsity: 75.2267%\n",
      "total_backward_count 1899260 real_backward_count 196322  10.337%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.792722/  1.904946, val:  74.17%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7867%\n",
      "layer   2  Sparsity: 71.8718%\n",
      "layer   3  Sparsity: 75.0274%\n",
      "total_backward_count 1909050 real_backward_count 197082  10.324%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.780719/  1.905282, val:  77.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7523%\n",
      "layer   2  Sparsity: 71.9686%\n",
      "layer   3  Sparsity: 74.9409%\n",
      "total_backward_count 1918840 real_backward_count 197857  10.311%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.794966/  1.917818, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7961%\n",
      "layer   2  Sparsity: 71.8845%\n",
      "layer   3  Sparsity: 74.9317%\n",
      "total_backward_count 1928630 real_backward_count 198648  10.300%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.799280/  1.890435, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8285%\n",
      "layer   2  Sparsity: 72.0217%\n",
      "layer   3  Sparsity: 75.8485%\n",
      "total_backward_count 1938420 real_backward_count 199449  10.289%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.789246/  1.899337, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7812%\n",
      "layer   2  Sparsity: 72.1391%\n",
      "layer   3  Sparsity: 75.3134%\n",
      "total_backward_count 1948210 real_backward_count 200231  10.278%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.789637/  1.912153, val:  77.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7922%\n",
      "layer   2  Sparsity: 71.8737%\n",
      "layer   3  Sparsity: 75.6839%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb832b4d1064156bf51250d653e3569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.78964</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.775</td></tr><tr><td>val_loss</td><td>1.91215</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-sweep-149</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pwswxmpw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pwswxmpw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_214155-pwswxmpw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 13rdziie with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_020032-13rdziie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/13rdziie' target=\"_blank\">laced-sweep-154</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/13rdziie' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/13rdziie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251118_020041_829', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 475.0\n",
      "lif layer 1 self.abs_max_v: 475.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 403.0\n",
      "lif layer 2 self.abs_max_v: 403.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 84.0\n",
      "fc layer 1 self.abs_max_out: 600.0\n",
      "lif layer 1 self.abs_max_v: 636.5\n",
      "fc layer 2 self.abs_max_out: 626.0\n",
      "lif layer 2 self.abs_max_v: 669.5\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "lif layer 1 self.abs_max_v: 721.0\n",
      "lif layer 2 self.abs_max_v: 842.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 775.5\n",
      "fc layer 2 self.abs_max_out: 752.0\n",
      "lif layer 2 self.abs_max_v: 1084.5\n",
      "fc layer 1 self.abs_max_out: 703.0\n",
      "lif layer 1 self.abs_max_v: 862.5\n",
      "fc layer 1 self.abs_max_out: 825.0\n",
      "lif layer 1 self.abs_max_v: 967.0\n",
      "fc layer 1 self.abs_max_out: 932.0\n",
      "lif layer 2 self.abs_max_v: 1089.5\n",
      "fc layer 1 self.abs_max_out: 1202.0\n",
      "lif layer 1 self.abs_max_v: 1202.0\n",
      "fc layer 2 self.abs_max_out: 756.0\n",
      "fc layer 2 self.abs_max_out: 845.0\n",
      "fc layer 2 self.abs_max_out: 903.0\n",
      "lif layer 2 self.abs_max_v: 1194.5\n",
      "fc layer 1 self.abs_max_out: 1277.0\n",
      "lif layer 1 self.abs_max_v: 1277.0\n",
      "lif layer 2 self.abs_max_v: 1311.5\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 1 self.abs_max_out: 1368.0\n",
      "lif layer 1 self.abs_max_v: 1368.0\n",
      "fc layer 1 self.abs_max_out: 1522.0\n",
      "lif layer 1 self.abs_max_v: 1522.0\n",
      "fc layer 2 self.abs_max_out: 1089.0\n",
      "lif layer 2 self.abs_max_v: 1416.0\n",
      "fc layer 2 self.abs_max_out: 1170.0\n",
      "lif layer 1 self.abs_max_v: 1757.0\n",
      "lif layer 2 self.abs_max_v: 1465.0\n",
      "lif layer 2 self.abs_max_v: 1637.0\n",
      "lif layer 1 self.abs_max_v: 1793.0\n",
      "lif layer 1 self.abs_max_v: 2019.5\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "lif layer 1 self.abs_max_v: 2084.0\n",
      "fc layer 3 self.abs_max_out: 347.0\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "fc layer 1 self.abs_max_out: 1619.0\n",
      "lif layer 2 self.abs_max_v: 1854.5\n",
      "fc layer 1 self.abs_max_out: 1807.0\n",
      "fc layer 3 self.abs_max_out: 424.0\n",
      "fc layer 1 self.abs_max_out: 1819.0\n",
      "lif layer 2 self.abs_max_v: 1933.5\n",
      "fc layer 1 self.abs_max_out: 1879.0\n",
      "fc layer 2 self.abs_max_out: 1314.0\n",
      "fc layer 1 self.abs_max_out: 2002.0\n",
      "lif layer 1 self.abs_max_v: 2104.5\n",
      "lif layer 2 self.abs_max_v: 1955.0\n",
      "lif layer 1 self.abs_max_v: 2310.5\n",
      "lif layer 2 self.abs_max_v: 2002.5\n",
      "fc layer 1 self.abs_max_out: 2020.0\n",
      "lif layer 1 self.abs_max_v: 2359.5\n",
      "lif layer 1 self.abs_max_v: 2915.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "fc layer 1 self.abs_max_out: 2150.0\n",
      "lif layer 1 self.abs_max_v: 3096.0\n",
      "fc layer 1 self.abs_max_out: 2447.0\n",
      "fc layer 2 self.abs_max_out: 1328.0\n",
      "fc layer 2 self.abs_max_out: 1389.0\n",
      "lif layer 1 self.abs_max_v: 3236.0\n",
      "lif layer 1 self.abs_max_v: 3380.0\n",
      "lif layer 2 self.abs_max_v: 2017.0\n",
      "lif layer 2 self.abs_max_v: 2085.5\n",
      "lif layer 2 self.abs_max_v: 2087.5\n",
      "fc layer 2 self.abs_max_out: 1394.0\n",
      "lif layer 2 self.abs_max_v: 2120.5\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "fc layer 1 self.abs_max_out: 2495.0\n",
      "fc layer 1 self.abs_max_out: 2529.0\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "lif layer 2 self.abs_max_v: 2128.5\n",
      "lif layer 2 self.abs_max_v: 2171.5\n",
      "fc layer 1 self.abs_max_out: 2698.0\n",
      "lif layer 2 self.abs_max_v: 2218.0\n",
      "lif layer 2 self.abs_max_v: 2360.0\n",
      "lif layer 1 self.abs_max_v: 3436.5\n",
      "fc layer 2 self.abs_max_out: 1635.0\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1758.0\n",
      "fc layer 2 self.abs_max_out: 1783.0\n",
      "lif layer 2 self.abs_max_v: 2394.5\n",
      "lif layer 2 self.abs_max_v: 2474.5\n",
      "lif layer 2 self.abs_max_v: 2511.0\n",
      "lif layer 2 self.abs_max_v: 2533.0\n",
      "lif layer 1 self.abs_max_v: 3580.0\n",
      "lif layer 1 self.abs_max_v: 3593.5\n",
      "lif layer 1 self.abs_max_v: 3610.5\n",
      "lif layer 1 self.abs_max_v: 3637.5\n",
      "lif layer 1 self.abs_max_v: 3818.0\n",
      "lif layer 1 self.abs_max_v: 3939.5\n",
      "lif layer 1 self.abs_max_v: 4380.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 4468.5\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "lif layer 1 self.abs_max_v: 4492.5\n",
      "fc layer 1 self.abs_max_out: 2709.0\n",
      "lif layer 1 self.abs_max_v: 4504.5\n",
      "fc layer 1 self.abs_max_out: 2806.0\n",
      "lif layer 1 self.abs_max_v: 4994.0\n",
      "fc layer 1 self.abs_max_out: 2838.0\n",
      "fc layer 1 self.abs_max_out: 2853.0\n",
      "fc layer 1 self.abs_max_out: 3063.0\n",
      "fc layer 1 self.abs_max_out: 3115.0\n",
      "lif layer 1 self.abs_max_v: 5169.5\n",
      "lif layer 1 self.abs_max_v: 5234.0\n",
      "lif layer 1 self.abs_max_v: 5491.0\n",
      "lif layer 1 self.abs_max_v: 5696.5\n",
      "lif layer 1 self.abs_max_v: 5937.5\n",
      "fc layer 1 self.abs_max_out: 3335.0\n",
      "lif layer 1 self.abs_max_v: 6304.0\n",
      "fc layer 2 self.abs_max_out: 1813.0\n",
      "fc layer 1 self.abs_max_out: 3376.0\n",
      "fc layer 1 self.abs_max_out: 3450.0\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 1 self.abs_max_out: 3527.0\n",
      "fc layer 2 self.abs_max_out: 1887.0\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "fc layer 2 self.abs_max_out: 2071.0\n",
      "fc layer 2 self.abs_max_out: 2106.0\n",
      "fc layer 1 self.abs_max_out: 3577.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.904561/  2.015035, val:  32.08%, val_best:  32.08%, tr:  92.44%, tr_best:  92.44%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.3599%\n",
      "layer   2  Sparsity: 76.4146%\n",
      "layer   3  Sparsity: 71.2332%\n",
      "total_backward_count 9790 real_backward_count 2830  28.907%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2563.5\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "lif layer 2 self.abs_max_v: 2627.0\n",
      "lif layer 2 self.abs_max_v: 2699.5\n",
      "lif layer 2 self.abs_max_v: 2735.5\n",
      "lif layer 2 self.abs_max_v: 2872.0\n",
      "lif layer 1 self.abs_max_v: 6325.5\n",
      "fc layer 1 self.abs_max_out: 3615.0\n",
      "lif layer 2 self.abs_max_v: 3084.5\n",
      "fc layer 1 self.abs_max_out: 3686.0\n",
      "lif layer 1 self.abs_max_v: 6479.5\n",
      "fc layer 2 self.abs_max_out: 2121.0\n",
      "fc layer 1 self.abs_max_out: 3720.0\n",
      "fc layer 1 self.abs_max_out: 3939.0\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.865588/  2.025114, val:  40.00%, val_best:  40.00%, tr:  98.26%, tr_best:  98.26%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3392%\n",
      "layer   2  Sparsity: 77.3420%\n",
      "layer   3  Sparsity: 70.4420%\n",
      "total_backward_count 19580 real_backward_count 4664  23.820%\n",
      "lif layer 2 self.abs_max_v: 3141.5\n",
      "lif layer 1 self.abs_max_v: 6575.5\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "lif layer 1 self.abs_max_v: 6702.0\n",
      "fc layer 1 self.abs_max_out: 4008.0\n",
      "lif layer 1 self.abs_max_v: 7014.0\n",
      "lif layer 1 self.abs_max_v: 7313.0\n",
      "fc layer 1 self.abs_max_out: 4032.0\n",
      "fc layer 1 self.abs_max_out: 4160.0\n",
      "fc layer 1 self.abs_max_out: 4219.0\n",
      "lif layer 1 self.abs_max_v: 7421.0\n",
      "fc layer 1 self.abs_max_out: 4400.0\n",
      "lif layer 1 self.abs_max_v: 7624.0\n",
      "lif layer 1 self.abs_max_v: 7714.0\n",
      "lif layer 1 self.abs_max_v: 8210.0\n",
      "fc layer 1 self.abs_max_out: 4807.0\n",
      "fc layer 1 self.abs_max_out: 4811.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.868120/  2.015500, val:  49.17%, val_best:  49.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 77.4416%\n",
      "layer   3  Sparsity: 70.7872%\n",
      "total_backward_count 29370 real_backward_count 6391  21.760%\n",
      "fc layer 2 self.abs_max_out: 2244.0\n",
      "fc layer 2 self.abs_max_out: 2309.0\n",
      "lif layer 2 self.abs_max_v: 3309.0\n",
      "fc layer 1 self.abs_max_out: 5051.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.872253/  2.012265, val:  38.75%, val_best:  49.17%, tr:  99.18%, tr_best:  99.18%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3457%\n",
      "layer   2  Sparsity: 78.1393%\n",
      "layer   3  Sparsity: 71.0774%\n",
      "total_backward_count 39160 real_backward_count 7966  20.342%\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "lif layer 1 self.abs_max_v: 8223.0\n",
      "lif layer 1 self.abs_max_v: 8287.5\n",
      "lif layer 1 self.abs_max_v: 8697.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.879027/  2.008798, val:  46.67%, val_best:  49.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 77.8917%\n",
      "layer   3  Sparsity: 71.4387%\n",
      "total_backward_count 48950 real_backward_count 9416  19.236%\n",
      "fc layer 2 self.abs_max_out: 2350.0\n",
      "fc layer 2 self.abs_max_out: 2358.0\n",
      "fc layer 2 self.abs_max_out: 2446.0\n",
      "lif layer 1 self.abs_max_v: 8799.0\n",
      "lif layer 2 self.abs_max_v: 3313.0\n",
      "fc layer 1 self.abs_max_out: 5245.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.872730/  2.010396, val:  52.08%, val_best:  52.08%, tr:  99.49%, tr_best:  99.69%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 77.4174%\n",
      "layer   3  Sparsity: 71.3700%\n",
      "total_backward_count 58740 real_backward_count 10802  18.390%\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "lif layer 2 self.abs_max_v: 3356.5\n",
      "lif layer 2 self.abs_max_v: 3387.5\n",
      "lif layer 2 self.abs_max_v: 3480.0\n",
      "fc layer 2 self.abs_max_out: 2550.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.878999/  2.005811, val:  50.00%, val_best:  52.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3125%\n",
      "layer   2  Sparsity: 77.8329%\n",
      "layer   3  Sparsity: 71.7737%\n",
      "total_backward_count 68530 real_backward_count 12158  17.741%\n",
      "lif layer 1 self.abs_max_v: 9168.5\n",
      "fc layer 2 self.abs_max_out: 2581.0\n",
      "fc layer 2 self.abs_max_out: 2659.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.877364/  1.993108, val:  50.42%, val_best:  52.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3242%\n",
      "layer   2  Sparsity: 77.8537%\n",
      "layer   3  Sparsity: 71.6354%\n",
      "total_backward_count 78320 real_backward_count 13421  17.136%\n",
      "lif layer 1 self.abs_max_v: 9336.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.879087/  2.003623, val:  54.17%, val_best:  54.17%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 77.8301%\n",
      "layer   3  Sparsity: 72.3307%\n",
      "total_backward_count 88110 real_backward_count 14765  16.757%\n",
      "lif layer 2 self.abs_max_v: 3533.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.873575/  1.994253, val:  49.58%, val_best:  54.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 77.4205%\n",
      "layer   3  Sparsity: 72.4689%\n",
      "total_backward_count 97900 real_backward_count 16066  16.411%\n",
      "fc layer 2 self.abs_max_out: 2669.0\n",
      "lif layer 1 self.abs_max_v: 9438.0\n",
      "fc layer 1 self.abs_max_out: 5368.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.857869/  1.977090, val:  58.33%, val_best:  58.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3585%\n",
      "layer   2  Sparsity: 76.6625%\n",
      "layer   3  Sparsity: 71.8609%\n",
      "total_backward_count 107690 real_backward_count 17339  16.101%\n",
      "lif layer 2 self.abs_max_v: 3752.0\n",
      "fc layer 1 self.abs_max_out: 5492.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.862945/  1.971817, val:  60.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 77.0416%\n",
      "layer   3  Sparsity: 71.7614%\n",
      "total_backward_count 117480 real_backward_count 18624  15.853%\n",
      "fc layer 1 self.abs_max_out: 5556.0\n",
      "fc layer 2 self.abs_max_out: 2716.0\n",
      "fc layer 1 self.abs_max_out: 5613.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.852840/  1.986205, val:  42.08%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3343%\n",
      "layer   2  Sparsity: 77.0950%\n",
      "layer   3  Sparsity: 72.1009%\n",
      "total_backward_count 127270 real_backward_count 19795  15.554%\n",
      "lif layer 1 self.abs_max_v: 9734.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.847205/  1.980049, val:  48.33%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3433%\n",
      "layer   2  Sparsity: 76.2612%\n",
      "layer   3  Sparsity: 72.4980%\n",
      "total_backward_count 137060 real_backward_count 20963  15.295%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.846987/  1.957071, val:  57.92%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3454%\n",
      "layer   2  Sparsity: 75.8834%\n",
      "layer   3  Sparsity: 72.8838%\n",
      "total_backward_count 146850 real_backward_count 22071  15.030%\n",
      "fc layer 1 self.abs_max_out: 5722.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.850730/  1.966757, val:  59.17%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3256%\n",
      "layer   2  Sparsity: 75.8932%\n",
      "layer   3  Sparsity: 72.9967%\n",
      "total_backward_count 156640 real_backward_count 23215  14.821%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.835854/  1.946027, val:  51.25%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2795%\n",
      "layer   2  Sparsity: 76.3155%\n",
      "layer   3  Sparsity: 73.5782%\n",
      "total_backward_count 166430 real_backward_count 24371  14.643%\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "lif layer 1 self.abs_max_v: 10181.5\n",
      "fc layer 1 self.abs_max_out: 6126.0\n",
      "lif layer 1 self.abs_max_v: 10326.0\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.820755/  1.932088, val:  61.25%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3207%\n",
      "layer   2  Sparsity: 76.1981%\n",
      "layer   3  Sparsity: 73.4172%\n",
      "total_backward_count 176220 real_backward_count 25495  14.468%\n",
      "lif layer 1 self.abs_max_v: 10751.0\n",
      "lif layer 2 self.abs_max_v: 3845.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.823021/  1.922961, val:  59.17%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3354%\n",
      "layer   2  Sparsity: 75.5052%\n",
      "layer   3  Sparsity: 73.0353%\n",
      "total_backward_count 186010 real_backward_count 26618  14.310%\n",
      "fc layer 3 self.abs_max_out: 620.0\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.810904/  1.942456, val:  48.75%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3349%\n",
      "layer   2  Sparsity: 75.5449%\n",
      "layer   3  Sparsity: 72.3967%\n",
      "total_backward_count 195800 real_backward_count 27649  14.121%\n",
      "lif layer 2 self.abs_max_v: 4183.0\n",
      "fc layer 1 self.abs_max_out: 6174.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.807871/  1.925716, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3081%\n",
      "layer   2  Sparsity: 75.1493%\n",
      "layer   3  Sparsity: 73.3217%\n",
      "total_backward_count 205590 real_backward_count 28696  13.958%\n",
      "lif layer 1 self.abs_max_v: 11003.5\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.801667/  1.923324, val:  62.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3738%\n",
      "layer   2  Sparsity: 75.5570%\n",
      "layer   3  Sparsity: 73.3114%\n",
      "total_backward_count 215380 real_backward_count 29774  13.824%\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "lif layer 1 self.abs_max_v: 11089.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.801746/  1.910355, val:  57.08%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3379%\n",
      "layer   2  Sparsity: 74.9411%\n",
      "layer   3  Sparsity: 72.7338%\n",
      "total_backward_count 225170 real_backward_count 30771  13.666%\n",
      "lif layer 1 self.abs_max_v: 11154.0\n",
      "lif layer 2 self.abs_max_v: 4226.0\n",
      "fc layer 1 self.abs_max_out: 6426.0\n",
      "fc layer 2 self.abs_max_out: 2808.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.798006/  1.917028, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 74.9985%\n",
      "layer   3  Sparsity: 72.7826%\n",
      "total_backward_count 234960 real_backward_count 31739  13.508%\n",
      "lif layer 1 self.abs_max_v: 11268.5\n",
      "fc layer 1 self.abs_max_out: 6480.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.796401/  1.912978, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3679%\n",
      "layer   2  Sparsity: 75.4719%\n",
      "layer   3  Sparsity: 72.5783%\n",
      "total_backward_count 244750 real_backward_count 32717  13.368%\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.792084/  1.922888, val:  61.67%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3476%\n",
      "layer   2  Sparsity: 75.9490%\n",
      "layer   3  Sparsity: 73.4082%\n",
      "total_backward_count 254540 real_backward_count 33761  13.264%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.783316/  1.895497, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3467%\n",
      "layer   2  Sparsity: 75.5988%\n",
      "layer   3  Sparsity: 73.6617%\n",
      "total_backward_count 264330 real_backward_count 34749  13.146%\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.772755/  1.910446, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3408%\n",
      "layer   2  Sparsity: 75.2381%\n",
      "layer   3  Sparsity: 73.5014%\n",
      "total_backward_count 274120 real_backward_count 35730  13.034%\n",
      "fc layer 1 self.abs_max_out: 6550.0\n",
      "fc layer 2 self.abs_max_out: 2876.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.784153/  1.917843, val:  61.67%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3173%\n",
      "layer   2  Sparsity: 75.2271%\n",
      "layer   3  Sparsity: 72.9001%\n",
      "total_backward_count 283910 real_backward_count 36597  12.890%\n",
      "lif layer 2 self.abs_max_v: 4291.5\n",
      "fc layer 2 self.abs_max_out: 2906.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.781812/  1.910324, val:  58.33%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3423%\n",
      "layer   2  Sparsity: 75.0884%\n",
      "layer   3  Sparsity: 73.4560%\n",
      "total_backward_count 293700 real_backward_count 37556  12.787%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.778122/  1.887832, val:  68.75%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 74.8326%\n",
      "layer   3  Sparsity: 73.4338%\n",
      "total_backward_count 303490 real_backward_count 38473  12.677%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.771808/  1.926796, val:  64.17%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3777%\n",
      "layer   2  Sparsity: 74.7759%\n",
      "layer   3  Sparsity: 73.3804%\n",
      "total_backward_count 313280 real_backward_count 39393  12.574%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.764871/  1.899580, val:  60.42%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3057%\n",
      "layer   2  Sparsity: 75.0667%\n",
      "layer   3  Sparsity: 73.3066%\n",
      "total_backward_count 323070 real_backward_count 40254  12.460%\n",
      "fc layer 1 self.abs_max_out: 6667.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.754015/  1.881753, val:  65.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3417%\n",
      "layer   2  Sparsity: 75.4678%\n",
      "layer   3  Sparsity: 73.9118%\n",
      "total_backward_count 332860 real_backward_count 41156  12.364%\n",
      "lif layer 1 self.abs_max_v: 11424.0\n",
      "fc layer 1 self.abs_max_out: 6746.0\n",
      "fc layer 1 self.abs_max_out: 6889.0\n",
      "lif layer 2 self.abs_max_v: 4530.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.749001/  1.891125, val:  57.92%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3143%\n",
      "layer   2  Sparsity: 75.4555%\n",
      "layer   3  Sparsity: 73.7189%\n",
      "total_backward_count 342650 real_backward_count 42024  12.264%\n",
      "lif layer 1 self.abs_max_v: 11787.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.749401/  1.896108, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3625%\n",
      "layer   2  Sparsity: 74.9663%\n",
      "layer   3  Sparsity: 72.8773%\n",
      "total_backward_count 352440 real_backward_count 42874  12.165%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.741490/  1.872905, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3480%\n",
      "layer   2  Sparsity: 74.5619%\n",
      "layer   3  Sparsity: 73.3570%\n",
      "total_backward_count 362230 real_backward_count 43751  12.078%\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "lif layer 2 self.abs_max_v: 4620.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.725389/  1.882084, val:  61.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 74.9184%\n",
      "layer   3  Sparsity: 73.5425%\n",
      "total_backward_count 372020 real_backward_count 44514  11.965%\n",
      "fc layer 3 self.abs_max_out: 718.0\n",
      "fc layer 3 self.abs_max_out: 727.0\n",
      "fc layer 1 self.abs_max_out: 6919.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.725989/  1.865297, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3158%\n",
      "layer   2  Sparsity: 74.5757%\n",
      "layer   3  Sparsity: 73.8240%\n",
      "total_backward_count 381810 real_backward_count 45326  11.871%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.718618/  1.860858, val:  70.00%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3330%\n",
      "layer   2  Sparsity: 74.4397%\n",
      "layer   3  Sparsity: 73.5441%\n",
      "total_backward_count 391600 real_backward_count 46130  11.780%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.711230/  1.848224, val:  68.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3621%\n",
      "layer   2  Sparsity: 74.3825%\n",
      "layer   3  Sparsity: 73.0608%\n",
      "total_backward_count 401390 real_backward_count 46938  11.694%\n",
      "fc layer 1 self.abs_max_out: 7094.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.712667/  1.853692, val:  65.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3357%\n",
      "layer   2  Sparsity: 74.3210%\n",
      "layer   3  Sparsity: 72.8849%\n",
      "total_backward_count 411180 real_backward_count 47708  11.603%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.704619/  1.835382, val:  69.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 74.2376%\n",
      "layer   3  Sparsity: 73.0082%\n",
      "total_backward_count 420970 real_backward_count 48473  11.515%\n",
      "fc layer 3 self.abs_max_out: 729.0\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "fc layer 3 self.abs_max_out: 747.0\n",
      "fc layer 1 self.abs_max_out: 7642.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.702134/  1.849605, val:  69.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3415%\n",
      "layer   2  Sparsity: 74.4912%\n",
      "layer   3  Sparsity: 72.7063%\n",
      "total_backward_count 430760 real_backward_count 49223  11.427%\n",
      "lif layer 2 self.abs_max_v: 4720.5\n",
      "lif layer 2 self.abs_max_v: 4754.5\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.696068/  1.833901, val:  73.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3327%\n",
      "layer   2  Sparsity: 74.0944%\n",
      "layer   3  Sparsity: 72.4581%\n",
      "total_backward_count 440550 real_backward_count 49929  11.333%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.690539/  1.841562, val:  63.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 74.4249%\n",
      "layer   3  Sparsity: 72.8085%\n",
      "total_backward_count 450340 real_backward_count 50674  11.252%\n",
      "lif layer 2 self.abs_max_v: 5024.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.695223/  1.837910, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3312%\n",
      "layer   2  Sparsity: 74.7149%\n",
      "layer   3  Sparsity: 72.6189%\n",
      "total_backward_count 460130 real_backward_count 51343  11.158%\n",
      "fc layer 2 self.abs_max_out: 3105.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.691490/  1.847662, val:  64.58%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 74.7897%\n",
      "layer   3  Sparsity: 72.8213%\n",
      "total_backward_count 469920 real_backward_count 52047  11.076%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.682538/  1.834343, val:  70.83%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3594%\n",
      "layer   2  Sparsity: 74.6213%\n",
      "layer   3  Sparsity: 73.0849%\n",
      "total_backward_count 479710 real_backward_count 52708  10.987%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.677760/  1.819583, val:  63.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3317%\n",
      "layer   2  Sparsity: 74.3737%\n",
      "layer   3  Sparsity: 72.9769%\n",
      "total_backward_count 489500 real_backward_count 53381  10.905%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.660393/  1.806084, val:  72.92%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 74.2241%\n",
      "layer   3  Sparsity: 72.6863%\n",
      "total_backward_count 499290 real_backward_count 54094  10.834%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.661397/  1.810447, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3353%\n",
      "layer   2  Sparsity: 74.0845%\n",
      "layer   3  Sparsity: 73.2816%\n",
      "total_backward_count 509080 real_backward_count 54762  10.757%\n",
      "fc layer 3 self.abs_max_out: 759.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.647516/  1.804129, val:  68.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 74.0008%\n",
      "layer   3  Sparsity: 73.2382%\n",
      "total_backward_count 518870 real_backward_count 55484  10.693%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.647717/  1.794464, val:  73.33%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3163%\n",
      "layer   2  Sparsity: 73.9416%\n",
      "layer   3  Sparsity: 72.6416%\n",
      "total_backward_count 528660 real_backward_count 56120  10.616%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.653502/  1.806089, val:  68.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3340%\n",
      "layer   2  Sparsity: 74.0276%\n",
      "layer   3  Sparsity: 72.7320%\n",
      "total_backward_count 538450 real_backward_count 56741  10.538%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.662534/  1.802700, val:  75.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3274%\n",
      "layer   2  Sparsity: 73.6554%\n",
      "layer   3  Sparsity: 72.6591%\n",
      "total_backward_count 548240 real_backward_count 57372  10.465%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.655917/  1.813358, val:  70.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3386%\n",
      "layer   2  Sparsity: 73.9881%\n",
      "layer   3  Sparsity: 73.1182%\n",
      "total_backward_count 558030 real_backward_count 57992  10.392%\n",
      "lif layer 1 self.abs_max_v: 12197.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.645300/  1.786717, val:  63.33%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3535%\n",
      "layer   2  Sparsity: 74.0729%\n",
      "layer   3  Sparsity: 73.0449%\n",
      "total_backward_count 567820 real_backward_count 58564  10.314%\n",
      "fc layer 1 self.abs_max_out: 7722.0\n",
      "fc layer 1 self.abs_max_out: 7910.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.643369/  1.789943, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3322%\n",
      "layer   2  Sparsity: 74.0703%\n",
      "layer   3  Sparsity: 73.1751%\n",
      "total_backward_count 577610 real_backward_count 59176  10.245%\n",
      "fc layer 1 self.abs_max_out: 7937.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.635293/  1.800464, val:  65.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 73.9297%\n",
      "layer   3  Sparsity: 72.9338%\n",
      "total_backward_count 587400 real_backward_count 59710  10.165%\n",
      "lif layer 2 self.abs_max_v: 5059.0\n",
      "lif layer 2 self.abs_max_v: 5097.0\n",
      "lif layer 2 self.abs_max_v: 5174.0\n",
      "fc layer 1 self.abs_max_out: 7997.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.636320/  1.801088, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3436%\n",
      "layer   2  Sparsity: 74.1113%\n",
      "layer   3  Sparsity: 72.8156%\n",
      "total_backward_count 597190 real_backward_count 60246  10.088%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.628529/  1.791947, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3487%\n",
      "layer   2  Sparsity: 74.1479%\n",
      "layer   3  Sparsity: 72.6715%\n",
      "total_backward_count 606980 real_backward_count 60834  10.022%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.625328/  1.787660, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2906%\n",
      "layer   2  Sparsity: 74.2063%\n",
      "layer   3  Sparsity: 73.1112%\n",
      "total_backward_count 616770 real_backward_count 61391   9.954%\n",
      "fc layer 1 self.abs_max_out: 8101.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.623643/  1.801662, val:  67.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 74.3613%\n",
      "layer   3  Sparsity: 72.6871%\n",
      "total_backward_count 626560 real_backward_count 61943   9.886%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.617589/  1.783027, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3683%\n",
      "layer   2  Sparsity: 74.2736%\n",
      "layer   3  Sparsity: 73.1656%\n",
      "total_backward_count 636350 real_backward_count 62493   9.821%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.605618/  1.777145, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3233%\n",
      "layer   2  Sparsity: 74.1927%\n",
      "layer   3  Sparsity: 73.6642%\n",
      "total_backward_count 646140 real_backward_count 63001   9.750%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.606332/  1.773980, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.50 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3300%\n",
      "layer   2  Sparsity: 74.0849%\n",
      "layer   3  Sparsity: 73.3173%\n",
      "total_backward_count 655930 real_backward_count 63539   9.687%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.600804/  1.754730, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3041%\n",
      "layer   2  Sparsity: 74.0715%\n",
      "layer   3  Sparsity: 72.9169%\n",
      "total_backward_count 665720 real_backward_count 64058   9.622%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.590531/  1.751840, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3483%\n",
      "layer   2  Sparsity: 73.9957%\n",
      "layer   3  Sparsity: 72.4884%\n",
      "total_backward_count 675510 real_backward_count 64558   9.557%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.593148/  1.773834, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3217%\n",
      "layer   2  Sparsity: 74.1596%\n",
      "layer   3  Sparsity: 72.9432%\n",
      "total_backward_count 685300 real_backward_count 65037   9.490%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.595328/  1.760710, val:  72.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 73.9748%\n",
      "layer   3  Sparsity: 72.6998%\n",
      "total_backward_count 695090 real_backward_count 65537   9.429%\n",
      "fc layer 3 self.abs_max_out: 765.0\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.591119/  1.769182, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3388%\n",
      "layer   2  Sparsity: 74.0536%\n",
      "layer   3  Sparsity: 72.6835%\n",
      "total_backward_count 704880 real_backward_count 66032   9.368%\n",
      "lif layer 1 self.abs_max_v: 12244.5\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.588463/  1.744284, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3409%\n",
      "layer   2  Sparsity: 73.8543%\n",
      "layer   3  Sparsity: 72.5850%\n",
      "total_backward_count 714670 real_backward_count 66508   9.306%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.580869/  1.744580, val:  72.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3058%\n",
      "layer   2  Sparsity: 73.8676%\n",
      "layer   3  Sparsity: 72.6608%\n",
      "total_backward_count 724460 real_backward_count 66992   9.247%\n",
      "lif layer 1 self.abs_max_v: 12333.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.574042/  1.744460, val:  72.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3068%\n",
      "layer   2  Sparsity: 73.9641%\n",
      "layer   3  Sparsity: 72.7174%\n",
      "total_backward_count 734250 real_backward_count 67468   9.189%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.575199/  1.736186, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2863%\n",
      "layer   2  Sparsity: 73.8731%\n",
      "layer   3  Sparsity: 72.3642%\n",
      "total_backward_count 744040 real_backward_count 67944   9.132%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.568126/  1.727863, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3509%\n",
      "layer   2  Sparsity: 74.1227%\n",
      "layer   3  Sparsity: 72.2657%\n",
      "total_backward_count 753830 real_backward_count 68384   9.072%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.558556/  1.739784, val:  64.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 86.3268%\n",
      "layer   2  Sparsity: 74.3023%\n",
      "layer   3  Sparsity: 72.8771%\n",
      "total_backward_count 763620 real_backward_count 68770   9.006%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.557708/  1.732216, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3537%\n",
      "layer   2  Sparsity: 74.2547%\n",
      "layer   3  Sparsity: 72.4826%\n",
      "total_backward_count 773410 real_backward_count 69183   8.945%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.547075/  1.723678, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3138%\n",
      "layer   2  Sparsity: 74.0796%\n",
      "layer   3  Sparsity: 72.5493%\n",
      "total_backward_count 783200 real_backward_count 69592   8.886%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.551892/  1.725336, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 74.1594%\n",
      "layer   3  Sparsity: 72.5370%\n",
      "total_backward_count 792990 real_backward_count 70024   8.830%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.551941/  1.725368, val:  76.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3414%\n",
      "layer   2  Sparsity: 73.9537%\n",
      "layer   3  Sparsity: 72.1989%\n",
      "total_backward_count 802780 real_backward_count 70416   8.772%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.552815/  1.723148, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3336%\n",
      "layer   2  Sparsity: 73.9985%\n",
      "layer   3  Sparsity: 71.9560%\n",
      "total_backward_count 812570 real_backward_count 70807   8.714%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.550487/  1.747282, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3339%\n",
      "layer   2  Sparsity: 73.8105%\n",
      "layer   3  Sparsity: 71.7366%\n",
      "total_backward_count 822360 real_backward_count 71232   8.662%\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.543461/  1.724816, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3401%\n",
      "layer   2  Sparsity: 74.0142%\n",
      "layer   3  Sparsity: 72.1505%\n",
      "total_backward_count 832150 real_backward_count 71590   8.603%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.545774/  1.739067, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3605%\n",
      "layer   2  Sparsity: 74.3677%\n",
      "layer   3  Sparsity: 72.0498%\n",
      "total_backward_count 841940 real_backward_count 71992   8.551%\n",
      "lif layer 2 self.abs_max_v: 5239.5\n",
      "lif layer 2 self.abs_max_v: 5343.5\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.557568/  1.734960, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3079%\n",
      "layer   2  Sparsity: 74.2126%\n",
      "layer   3  Sparsity: 72.1913%\n",
      "total_backward_count 851730 real_backward_count 72366   8.496%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.562382/  1.736706, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3506%\n",
      "layer   2  Sparsity: 74.2538%\n",
      "layer   3  Sparsity: 72.6680%\n",
      "total_backward_count 861520 real_backward_count 72742   8.443%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.552788/  1.732171, val:  68.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3556%\n",
      "layer   2  Sparsity: 74.0333%\n",
      "layer   3  Sparsity: 71.9796%\n",
      "total_backward_count 871310 real_backward_count 73175   8.398%\n",
      "lif layer 1 self.abs_max_v: 12453.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.547533/  1.721380, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3377%\n",
      "layer   2  Sparsity: 73.9184%\n",
      "layer   3  Sparsity: 72.4178%\n",
      "total_backward_count 881100 real_backward_count 73547   8.347%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.546296/  1.722404, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3362%\n",
      "layer   2  Sparsity: 73.9239%\n",
      "layer   3  Sparsity: 73.1482%\n",
      "total_backward_count 890890 real_backward_count 73892   8.294%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.544563/  1.719212, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2931%\n",
      "layer   2  Sparsity: 73.8829%\n",
      "layer   3  Sparsity: 72.6507%\n",
      "total_backward_count 900680 real_backward_count 74239   8.243%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.546230/  1.726526, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3298%\n",
      "layer   2  Sparsity: 74.2607%\n",
      "layer   3  Sparsity: 72.3916%\n",
      "total_backward_count 910470 real_backward_count 74582   8.192%\n",
      "lif layer 1 self.abs_max_v: 13490.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.536320/  1.715890, val:  72.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3350%\n",
      "layer   2  Sparsity: 74.2392%\n",
      "layer   3  Sparsity: 72.6950%\n",
      "total_backward_count 920260 real_backward_count 74903   8.139%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.536103/  1.727079, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3206%\n",
      "layer   2  Sparsity: 74.1973%\n",
      "layer   3  Sparsity: 72.8626%\n",
      "total_backward_count 930050 real_backward_count 75271   8.093%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.533931/  1.712113, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2899%\n",
      "layer   2  Sparsity: 74.5268%\n",
      "layer   3  Sparsity: 72.4263%\n",
      "total_backward_count 939840 real_backward_count 75611   8.045%\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.523622/  1.728982, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3063%\n",
      "layer   2  Sparsity: 74.3585%\n",
      "layer   3  Sparsity: 72.3323%\n",
      "total_backward_count 949630 real_backward_count 75977   8.001%\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.520068/  1.707971, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3215%\n",
      "layer   2  Sparsity: 74.5438%\n",
      "layer   3  Sparsity: 72.6943%\n",
      "total_backward_count 959420 real_backward_count 76282   7.951%\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.518477/  1.700083, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2930%\n",
      "layer   2  Sparsity: 74.0524%\n",
      "layer   3  Sparsity: 72.4582%\n",
      "total_backward_count 969210 real_backward_count 76597   7.903%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.515464/  1.712223, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3234%\n",
      "layer   2  Sparsity: 73.9982%\n",
      "layer   3  Sparsity: 72.5726%\n",
      "total_backward_count 979000 real_backward_count 76908   7.856%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.522921/  1.697891, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3598%\n",
      "layer   2  Sparsity: 74.1471%\n",
      "layer   3  Sparsity: 72.6544%\n",
      "total_backward_count 988790 real_backward_count 77220   7.810%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.517042/  1.698799, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3281%\n",
      "layer   2  Sparsity: 74.1637%\n",
      "layer   3  Sparsity: 72.8638%\n",
      "total_backward_count 998580 real_backward_count 77521   7.763%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.510889/  1.700209, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 74.1232%\n",
      "layer   3  Sparsity: 72.7637%\n",
      "total_backward_count 1008370 real_backward_count 77792   7.715%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.510343/  1.694680, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3183%\n",
      "layer   2  Sparsity: 74.2586%\n",
      "layer   3  Sparsity: 73.1343%\n",
      "total_backward_count 1018160 real_backward_count 78051   7.666%\n",
      "lif layer 2 self.abs_max_v: 5604.5\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.512169/  1.702283, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3369%\n",
      "layer   2  Sparsity: 73.9014%\n",
      "layer   3  Sparsity: 72.6845%\n",
      "total_backward_count 1027950 real_backward_count 78344   7.621%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.511100/  1.695568, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3501%\n",
      "layer   2  Sparsity: 74.0258%\n",
      "layer   3  Sparsity: 72.4985%\n",
      "total_backward_count 1037740 real_backward_count 78603   7.574%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.503114/  1.693093, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3182%\n",
      "layer   2  Sparsity: 74.2590%\n",
      "layer   3  Sparsity: 72.1632%\n",
      "total_backward_count 1047530 real_backward_count 78894   7.531%\n",
      "fc layer 3 self.abs_max_out: 842.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.500009/  1.706082, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3324%\n",
      "layer   2  Sparsity: 74.2266%\n",
      "layer   3  Sparsity: 72.2270%\n",
      "total_backward_count 1057320 real_backward_count 79171   7.488%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.496927/  1.693306, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3035%\n",
      "layer   2  Sparsity: 74.3387%\n",
      "layer   3  Sparsity: 72.6515%\n",
      "total_backward_count 1067110 real_backward_count 79462   7.446%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.496163/  1.698022, val:  77.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3076%\n",
      "layer   2  Sparsity: 74.1290%\n",
      "layer   3  Sparsity: 73.1940%\n",
      "total_backward_count 1076900 real_backward_count 79689   7.400%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.502258/  1.689679, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3637%\n",
      "layer   2  Sparsity: 74.2259%\n",
      "layer   3  Sparsity: 73.1654%\n",
      "total_backward_count 1086690 real_backward_count 79961   7.358%\n",
      "fc layer 1 self.abs_max_out: 8141.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.509050/  1.689512, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3031%\n",
      "layer   2  Sparsity: 73.9299%\n",
      "layer   3  Sparsity: 72.7977%\n",
      "total_backward_count 1096480 real_backward_count 80251   7.319%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.504078/  1.683065, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3565%\n",
      "layer   2  Sparsity: 74.0556%\n",
      "layer   3  Sparsity: 73.0874%\n",
      "total_backward_count 1106270 real_backward_count 80509   7.278%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.493758/  1.696830, val:  72.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3365%\n",
      "layer   2  Sparsity: 74.4667%\n",
      "layer   3  Sparsity: 73.0030%\n",
      "total_backward_count 1116060 real_backward_count 80754   7.236%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.501247/  1.685306, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3261%\n",
      "layer   2  Sparsity: 74.5061%\n",
      "layer   3  Sparsity: 73.4459%\n",
      "total_backward_count 1125850 real_backward_count 81004   7.195%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.492209/  1.695357, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3047%\n",
      "layer   2  Sparsity: 74.5541%\n",
      "layer   3  Sparsity: 73.8802%\n",
      "total_backward_count 1135640 real_backward_count 81225   7.152%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.489787/  1.694295, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3151%\n",
      "layer   2  Sparsity: 74.4127%\n",
      "layer   3  Sparsity: 74.0272%\n",
      "total_backward_count 1145430 real_backward_count 81427   7.109%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.483307/  1.679753, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3383%\n",
      "layer   2  Sparsity: 74.5027%\n",
      "layer   3  Sparsity: 73.7304%\n",
      "total_backward_count 1155220 real_backward_count 81636   7.067%\n",
      "fc layer 1 self.abs_max_out: 8167.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.481360/  1.674163, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3311%\n",
      "layer   2  Sparsity: 74.5767%\n",
      "layer   3  Sparsity: 73.5744%\n",
      "total_backward_count 1165010 real_backward_count 81865   7.027%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.474173/  1.685335, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3779%\n",
      "layer   2  Sparsity: 74.4635%\n",
      "layer   3  Sparsity: 73.4957%\n",
      "total_backward_count 1174800 real_backward_count 82097   6.988%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.482484/  1.687584, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3366%\n",
      "layer   2  Sparsity: 74.6378%\n",
      "layer   3  Sparsity: 73.5785%\n",
      "total_backward_count 1184590 real_backward_count 82299   6.947%\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.484313/  1.685179, val:  77.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3407%\n",
      "layer   2  Sparsity: 74.3370%\n",
      "layer   3  Sparsity: 73.5943%\n",
      "total_backward_count 1194380 real_backward_count 82497   6.907%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.485984/  1.688409, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3551%\n",
      "layer   2  Sparsity: 74.2770%\n",
      "layer   3  Sparsity: 73.5539%\n",
      "total_backward_count 1204170 real_backward_count 82717   6.869%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.482522/  1.685009, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3231%\n",
      "layer   2  Sparsity: 74.1578%\n",
      "layer   3  Sparsity: 73.2285%\n",
      "total_backward_count 1213960 real_backward_count 82918   6.830%\n",
      "fc layer 1 self.abs_max_out: 8328.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.486729/  1.678098, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3334%\n",
      "layer   2  Sparsity: 74.0679%\n",
      "layer   3  Sparsity: 73.5319%\n",
      "total_backward_count 1223750 real_backward_count 83139   6.794%\n",
      "fc layer 2 self.abs_max_out: 3108.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.487587/  1.685579, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3626%\n",
      "layer   2  Sparsity: 74.1867%\n",
      "layer   3  Sparsity: 73.3918%\n",
      "total_backward_count 1233540 real_backward_count 83340   6.756%\n",
      "fc layer 3 self.abs_max_out: 855.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.479751/  1.680590, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3858%\n",
      "layer   2  Sparsity: 74.1295%\n",
      "layer   3  Sparsity: 72.9574%\n",
      "total_backward_count 1243330 real_backward_count 83538   6.719%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.483784/  1.682103, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3263%\n",
      "layer   2  Sparsity: 73.9281%\n",
      "layer   3  Sparsity: 72.9584%\n",
      "total_backward_count 1253120 real_backward_count 83724   6.681%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.475092/  1.669029, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3586%\n",
      "layer   2  Sparsity: 73.9544%\n",
      "layer   3  Sparsity: 73.2210%\n",
      "total_backward_count 1262910 real_backward_count 83935   6.646%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.472742/  1.669956, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3090%\n",
      "layer   2  Sparsity: 74.2142%\n",
      "layer   3  Sparsity: 73.7360%\n",
      "total_backward_count 1272700 real_backward_count 84140   6.611%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.464738/  1.674016, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3631%\n",
      "layer   2  Sparsity: 74.3245%\n",
      "layer   3  Sparsity: 73.2706%\n",
      "total_backward_count 1282490 real_backward_count 84342   6.576%\n",
      "fc layer 3 self.abs_max_out: 868.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.460995/  1.666858, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3278%\n",
      "layer   2  Sparsity: 74.1467%\n",
      "layer   3  Sparsity: 73.3617%\n",
      "total_backward_count 1292280 real_backward_count 84523   6.541%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.464994/  1.659839, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3214%\n",
      "layer   2  Sparsity: 74.0124%\n",
      "layer   3  Sparsity: 73.2702%\n",
      "total_backward_count 1302070 real_backward_count 84693   6.504%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.460602/  1.662812, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3374%\n",
      "layer   2  Sparsity: 74.2926%\n",
      "layer   3  Sparsity: 73.1034%\n",
      "total_backward_count 1311860 real_backward_count 84885   6.471%\n",
      "fc layer 3 self.abs_max_out: 873.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.460159/  1.658086, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3627%\n",
      "layer   2  Sparsity: 74.2688%\n",
      "layer   3  Sparsity: 73.1526%\n",
      "total_backward_count 1321650 real_backward_count 85057   6.436%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.453074/  1.662686, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3325%\n",
      "layer   2  Sparsity: 74.2448%\n",
      "layer   3  Sparsity: 72.9282%\n",
      "total_backward_count 1331440 real_backward_count 85202   6.399%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.453098/  1.651250, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3424%\n",
      "layer   2  Sparsity: 74.2518%\n",
      "layer   3  Sparsity: 73.1941%\n",
      "total_backward_count 1341230 real_backward_count 85362   6.364%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.455344/  1.662851, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3496%\n",
      "layer   2  Sparsity: 74.2790%\n",
      "layer   3  Sparsity: 72.8007%\n",
      "total_backward_count 1351020 real_backward_count 85543   6.332%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.451470/  1.657442, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3279%\n",
      "layer   2  Sparsity: 74.2931%\n",
      "layer   3  Sparsity: 72.6902%\n",
      "total_backward_count 1360810 real_backward_count 85714   6.299%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.447808/  1.655147, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3411%\n",
      "layer   2  Sparsity: 74.2318%\n",
      "layer   3  Sparsity: 72.9293%\n",
      "total_backward_count 1370600 real_backward_count 85864   6.265%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.440206/  1.650163, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3190%\n",
      "layer   2  Sparsity: 74.2659%\n",
      "layer   3  Sparsity: 72.9415%\n",
      "total_backward_count 1380390 real_backward_count 86008   6.231%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.431000/  1.643003, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3559%\n",
      "layer   2  Sparsity: 74.0694%\n",
      "layer   3  Sparsity: 72.7692%\n",
      "total_backward_count 1390180 real_backward_count 86155   6.197%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.438934/  1.656252, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3515%\n",
      "layer   2  Sparsity: 74.0278%\n",
      "layer   3  Sparsity: 73.0149%\n",
      "total_backward_count 1399970 real_backward_count 86339   6.167%\n",
      "fc layer 2 self.abs_max_out: 3128.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.433300/  1.643860, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2864%\n",
      "layer   2  Sparsity: 74.0988%\n",
      "layer   3  Sparsity: 73.3998%\n",
      "total_backward_count 1409760 real_backward_count 86464   6.133%\n",
      "fc layer 3 self.abs_max_out: 886.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.428922/  1.642269, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3405%\n",
      "layer   2  Sparsity: 73.9131%\n",
      "layer   3  Sparsity: 73.1926%\n",
      "total_backward_count 1419550 real_backward_count 86625   6.102%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.428884/  1.633690, val:  72.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3323%\n",
      "layer   2  Sparsity: 74.2364%\n",
      "layer   3  Sparsity: 73.0971%\n",
      "total_backward_count 1429340 real_backward_count 86775   6.071%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.421558/  1.634129, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3318%\n",
      "layer   2  Sparsity: 74.4772%\n",
      "layer   3  Sparsity: 73.2946%\n",
      "total_backward_count 1439130 real_backward_count 86930   6.040%\n",
      "fc layer 3 self.abs_max_out: 898.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.422487/  1.642833, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3289%\n",
      "layer   2  Sparsity: 74.2697%\n",
      "layer   3  Sparsity: 73.6019%\n",
      "total_backward_count 1448920 real_backward_count 87087   6.010%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.427759/  1.644884, val:  78.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3204%\n",
      "layer   2  Sparsity: 74.1390%\n",
      "layer   3  Sparsity: 73.7618%\n",
      "total_backward_count 1458710 real_backward_count 87236   5.980%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.426110/  1.638358, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3307%\n",
      "layer   2  Sparsity: 74.2752%\n",
      "layer   3  Sparsity: 73.9537%\n",
      "total_backward_count 1468500 real_backward_count 87375   5.950%\n",
      "fc layer 1 self.abs_max_out: 8363.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.421320/  1.647590, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 74.3369%\n",
      "layer   3  Sparsity: 73.8040%\n",
      "total_backward_count 1478290 real_backward_count 87492   5.918%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.428146/  1.640857, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3448%\n",
      "layer   2  Sparsity: 74.2792%\n",
      "layer   3  Sparsity: 73.6418%\n",
      "total_backward_count 1488080 real_backward_count 87642   5.890%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.427289/  1.640290, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3282%\n",
      "layer   2  Sparsity: 74.0489%\n",
      "layer   3  Sparsity: 73.2139%\n",
      "total_backward_count 1497870 real_backward_count 87781   5.860%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.428491/  1.644017, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3333%\n",
      "layer   2  Sparsity: 73.9952%\n",
      "layer   3  Sparsity: 73.5278%\n",
      "total_backward_count 1507660 real_backward_count 87923   5.832%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.427831/  1.636459, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3642%\n",
      "layer   2  Sparsity: 74.1764%\n",
      "layer   3  Sparsity: 73.6091%\n",
      "total_backward_count 1517450 real_backward_count 88045   5.802%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.425892/  1.636873, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.2999%\n",
      "layer   2  Sparsity: 74.2616%\n",
      "layer   3  Sparsity: 73.4440%\n",
      "total_backward_count 1527240 real_backward_count 88188   5.774%\n",
      "fc layer 1 self.abs_max_out: 8423.0\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.425850/  1.638950, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3447%\n",
      "layer   2  Sparsity: 74.2391%\n",
      "layer   3  Sparsity: 73.6162%\n",
      "total_backward_count 1537030 real_backward_count 88359   5.749%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.428817/  1.647121, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3001%\n",
      "layer   2  Sparsity: 74.1109%\n",
      "layer   3  Sparsity: 73.4740%\n",
      "total_backward_count 1546820 real_backward_count 88513   5.722%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.434093/  1.657644, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3402%\n",
      "layer   2  Sparsity: 74.0737%\n",
      "layer   3  Sparsity: 73.5095%\n",
      "total_backward_count 1556610 real_backward_count 88651   5.695%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.429574/  1.635667, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3253%\n",
      "layer   2  Sparsity: 74.0468%\n",
      "layer   3  Sparsity: 73.5305%\n",
      "total_backward_count 1566400 real_backward_count 88777   5.668%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.425845/  1.647612, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3443%\n",
      "layer   2  Sparsity: 74.0806%\n",
      "layer   3  Sparsity: 73.8361%\n",
      "total_backward_count 1576190 real_backward_count 88874   5.639%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.420627/  1.629191, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3023%\n",
      "layer   2  Sparsity: 74.3088%\n",
      "layer   3  Sparsity: 73.9337%\n",
      "total_backward_count 1585980 real_backward_count 88969   5.610%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.416675/  1.629397, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3355%\n",
      "layer   2  Sparsity: 74.1223%\n",
      "layer   3  Sparsity: 73.5286%\n",
      "total_backward_count 1595770 real_backward_count 89083   5.582%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.418327/  1.634915, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 74.3232%\n",
      "layer   3  Sparsity: 73.3005%\n",
      "total_backward_count 1605560 real_backward_count 89203   5.556%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.417368/  1.643817, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.2903%\n",
      "layer   2  Sparsity: 74.3513%\n",
      "layer   3  Sparsity: 73.3689%\n",
      "total_backward_count 1615350 real_backward_count 89310   5.529%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.426162/  1.650833, val:  75.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3084%\n",
      "layer   2  Sparsity: 74.1294%\n",
      "layer   3  Sparsity: 73.3683%\n",
      "total_backward_count 1625140 real_backward_count 89411   5.502%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.425055/  1.646169, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3134%\n",
      "layer   2  Sparsity: 74.2158%\n",
      "layer   3  Sparsity: 73.4114%\n",
      "total_backward_count 1634930 real_backward_count 89535   5.476%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.427150/  1.639072, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3286%\n",
      "layer   2  Sparsity: 74.0546%\n",
      "layer   3  Sparsity: 73.4676%\n",
      "total_backward_count 1644720 real_backward_count 89623   5.449%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.423570/  1.635338, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3690%\n",
      "layer   2  Sparsity: 74.0550%\n",
      "layer   3  Sparsity: 73.4928%\n",
      "total_backward_count 1654510 real_backward_count 89733   5.424%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.420105/  1.633427, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3504%\n",
      "layer   2  Sparsity: 73.9358%\n",
      "layer   3  Sparsity: 73.0379%\n",
      "total_backward_count 1664300 real_backward_count 89880   5.400%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.420101/  1.636640, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.3495%\n",
      "layer   2  Sparsity: 73.9412%\n",
      "layer   3  Sparsity: 72.9809%\n",
      "total_backward_count 1674090 real_backward_count 89965   5.374%\n",
      "fc layer 1 self.abs_max_out: 8499.0\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.414665/  1.624274, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3696%\n",
      "layer   2  Sparsity: 74.1956%\n",
      "layer   3  Sparsity: 73.2684%\n",
      "total_backward_count 1683880 real_backward_count 90064   5.349%\n",
      "lif layer 1 self.abs_max_v: 13638.5\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.410724/  1.628333, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3602%\n",
      "layer   2  Sparsity: 74.0558%\n",
      "layer   3  Sparsity: 73.3072%\n",
      "total_backward_count 1693670 real_backward_count 90179   5.324%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.403522/  1.612768, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3412%\n",
      "layer   2  Sparsity: 73.9015%\n",
      "layer   3  Sparsity: 73.3109%\n",
      "total_backward_count 1703460 real_backward_count 90262   5.299%\n",
      "fc layer 3 self.abs_max_out: 902.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.408087/  1.618817, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 74.1351%\n",
      "layer   3  Sparsity: 73.3519%\n",
      "total_backward_count 1713250 real_backward_count 90342   5.273%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.405433/  1.620316, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2967%\n",
      "layer   2  Sparsity: 74.1336%\n",
      "layer   3  Sparsity: 73.5815%\n",
      "total_backward_count 1723040 real_backward_count 90421   5.248%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.409508/  1.622319, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3398%\n",
      "layer   2  Sparsity: 73.9432%\n",
      "layer   3  Sparsity: 73.4583%\n",
      "total_backward_count 1732830 real_backward_count 90510   5.223%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.403816/  1.624730, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3123%\n",
      "layer   2  Sparsity: 73.8995%\n",
      "layer   3  Sparsity: 73.3696%\n",
      "total_backward_count 1742620 real_backward_count 90585   5.198%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.406607/  1.621111, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3472%\n",
      "layer   2  Sparsity: 73.7950%\n",
      "layer   3  Sparsity: 72.9637%\n",
      "total_backward_count 1752410 real_backward_count 90695   5.175%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.405206/  1.623453, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3422%\n",
      "layer   2  Sparsity: 74.1163%\n",
      "layer   3  Sparsity: 73.1769%\n",
      "total_backward_count 1762200 real_backward_count 90798   5.153%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.410628/  1.635809, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 86.3571%\n",
      "layer   2  Sparsity: 74.0545%\n",
      "layer   3  Sparsity: 73.2970%\n",
      "total_backward_count 1771990 real_backward_count 90917   5.131%\n",
      "fc layer 3 self.abs_max_out: 904.0\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.402866/  1.630083, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 86.3210%\n",
      "layer   2  Sparsity: 74.1389%\n",
      "layer   3  Sparsity: 73.5046%\n",
      "total_backward_count 1781780 real_backward_count 91003   5.107%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.406004/  1.626300, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3381%\n",
      "layer   2  Sparsity: 74.0711%\n",
      "layer   3  Sparsity: 73.3986%\n",
      "total_backward_count 1791570 real_backward_count 91100   5.085%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.406457/  1.632523, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3218%\n",
      "layer   2  Sparsity: 74.2744%\n",
      "layer   3  Sparsity: 73.3792%\n",
      "total_backward_count 1801360 real_backward_count 91194   5.063%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.403917/  1.632765, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3481%\n",
      "layer   2  Sparsity: 74.1920%\n",
      "layer   3  Sparsity: 73.8553%\n",
      "total_backward_count 1811150 real_backward_count 91279   5.040%\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.402155/  1.623027, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3277%\n",
      "layer   2  Sparsity: 74.0837%\n",
      "layer   3  Sparsity: 74.0194%\n",
      "total_backward_count 1820940 real_backward_count 91379   5.018%\n",
      "fc layer 3 self.abs_max_out: 911.0\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.392531/  1.617914, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3375%\n",
      "layer   2  Sparsity: 73.9403%\n",
      "layer   3  Sparsity: 73.9452%\n",
      "total_backward_count 1830730 real_backward_count 91486   4.997%\n",
      "fc layer 1 self.abs_max_out: 8505.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.390747/  1.619897, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3426%\n",
      "layer   2  Sparsity: 73.9243%\n",
      "layer   3  Sparsity: 73.4493%\n",
      "total_backward_count 1840520 real_backward_count 91572   4.975%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.389825/  1.614721, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3387%\n",
      "layer   2  Sparsity: 74.0090%\n",
      "layer   3  Sparsity: 73.3404%\n",
      "total_backward_count 1850310 real_backward_count 91647   4.953%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.390860/  1.621641, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3410%\n",
      "layer   2  Sparsity: 74.2150%\n",
      "layer   3  Sparsity: 72.9986%\n",
      "total_backward_count 1860100 real_backward_count 91734   4.932%\n",
      "fc layer 1 self.abs_max_out: 8529.0\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.394769/  1.616339, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 86.3368%\n",
      "layer   2  Sparsity: 74.1908%\n",
      "layer   3  Sparsity: 73.2267%\n",
      "total_backward_count 1869890 real_backward_count 91819   4.910%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.397886/  1.621318, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 86.3118%\n",
      "layer   2  Sparsity: 74.0815%\n",
      "layer   3  Sparsity: 72.8793%\n",
      "total_backward_count 1879680 real_backward_count 91929   4.891%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.396050/  1.623885, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3478%\n",
      "layer   2  Sparsity: 74.2631%\n",
      "layer   3  Sparsity: 73.1256%\n",
      "total_backward_count 1889470 real_backward_count 92025   4.870%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.390279/  1.614742, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3283%\n",
      "layer   2  Sparsity: 74.0956%\n",
      "layer   3  Sparsity: 73.5454%\n",
      "total_backward_count 1899260 real_backward_count 92104   4.849%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.383425/  1.614991, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3530%\n",
      "layer   2  Sparsity: 74.2256%\n",
      "layer   3  Sparsity: 73.6409%\n",
      "total_backward_count 1909050 real_backward_count 92176   4.828%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.384294/  1.602390, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.3601%\n",
      "layer   2  Sparsity: 74.1459%\n",
      "layer   3  Sparsity: 73.7749%\n",
      "total_backward_count 1918840 real_backward_count 92240   4.807%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.383166/  1.606287, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3452%\n",
      "layer   2  Sparsity: 74.3489%\n",
      "layer   3  Sparsity: 73.8569%\n",
      "total_backward_count 1928630 real_backward_count 92312   4.786%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.383844/  1.613931, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.2914%\n",
      "layer   2  Sparsity: 74.4709%\n",
      "layer   3  Sparsity: 73.6068%\n",
      "total_backward_count 1938420 real_backward_count 92391   4.766%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.383933/  1.612257, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3140%\n",
      "layer   2  Sparsity: 74.4965%\n",
      "layer   3  Sparsity: 73.8174%\n",
      "total_backward_count 1948210 real_backward_count 92469   4.746%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.384742/  1.616752, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 86.3181%\n",
      "layer   2  Sparsity: 74.5478%\n",
      "layer   3  Sparsity: 74.0034%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079eb45a13664c80b74ea0597d0e2627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.38474</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.81667</td></tr><tr><td>val_loss</td><td>1.61675</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-154</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/13rdziie' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/13rdziie</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_020032-13rdziie/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7x4xonr7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_061909-7x4xonr7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7x4xonr7' target=\"_blank\">fancy-sweep-159</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7x4xonr7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7x4xonr7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251118_061918_227', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 125.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 193.0\n",
      "fc layer 1 self.abs_max_out: 214.0\n",
      "lif layer 1 self.abs_max_v: 284.0\n",
      "lif layer 1 self.abs_max_v: 310.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 1 self.abs_max_out: 221.0\n",
      "lif layer 1 self.abs_max_v: 320.5\n",
      "fc layer 2 self.abs_max_out: 123.0\n",
      "lif layer 2 self.abs_max_v: 132.0\n",
      "fc layer 1 self.abs_max_out: 385.0\n",
      "lif layer 1 self.abs_max_v: 439.0\n",
      "fc layer 1 self.abs_max_out: 466.0\n",
      "lif layer 1 self.abs_max_v: 470.0\n",
      "fc layer 2 self.abs_max_out: 167.0\n",
      "lif layer 2 self.abs_max_v: 213.5\n",
      "fc layer 1 self.abs_max_out: 604.0\n",
      "lif layer 1 self.abs_max_v: 604.0\n",
      "fc layer 2 self.abs_max_out: 205.0\n",
      "lif layer 2 self.abs_max_v: 270.5\n",
      "fc layer 3 self.abs_max_out: 16.0\n",
      "fc layer 1 self.abs_max_out: 644.0\n",
      "lif layer 1 self.abs_max_v: 644.0\n",
      "lif layer 2 self.abs_max_v: 306.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "lif layer 2 self.abs_max_v: 332.0\n",
      "fc layer 2 self.abs_max_out: 253.0\n",
      "lif layer 2 self.abs_max_v: 352.0\n",
      "fc layer 2 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 444.5\n",
      "fc layer 1 self.abs_max_out: 804.0\n",
      "lif layer 1 self.abs_max_v: 804.0\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "lif layer 2 self.abs_max_v: 451.0\n",
      "fc layer 1 self.abs_max_out: 1223.0\n",
      "lif layer 1 self.abs_max_v: 1223.0\n",
      "fc layer 2 self.abs_max_out: 362.0\n",
      "lif layer 2 self.abs_max_v: 494.5\n",
      "fc layer 1 self.abs_max_out: 1346.0\n",
      "lif layer 1 self.abs_max_v: 1346.0\n",
      "fc layer 2 self.abs_max_out: 425.0\n",
      "lif layer 2 self.abs_max_v: 503.5\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 3 self.abs_max_out: 82.0\n",
      "fc layer 2 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 539.5\n",
      "fc layer 2 self.abs_max_out: 478.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "fc layer 2 self.abs_max_out: 529.0\n",
      "lif layer 2 self.abs_max_v: 540.0\n",
      "lif layer 2 self.abs_max_v: 591.0\n",
      "fc layer 3 self.abs_max_out: 95.0\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "lif layer 2 self.abs_max_v: 591.5\n",
      "lif layer 2 self.abs_max_v: 622.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "lif layer 2 self.abs_max_v: 704.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 2 self.abs_max_out: 701.0\n",
      "lif layer 2 self.abs_max_v: 712.5\n",
      "lif layer 2 self.abs_max_v: 715.5\n",
      "lif layer 2 self.abs_max_v: 721.0\n",
      "lif layer 2 self.abs_max_v: 799.5\n",
      "lif layer 2 self.abs_max_v: 820.5\n",
      "fc layer 2 self.abs_max_out: 761.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "fc layer 2 self.abs_max_out: 783.0\n",
      "lif layer 2 self.abs_max_v: 853.0\n",
      "lif layer 2 self.abs_max_v: 859.5\n",
      "fc layer 2 self.abs_max_out: 808.0\n",
      "fc layer 1 self.abs_max_out: 1517.0\n",
      "lif layer 1 self.abs_max_v: 1517.0\n",
      "lif layer 2 self.abs_max_v: 924.0\n",
      "fc layer 1 self.abs_max_out: 1659.0\n",
      "lif layer 1 self.abs_max_v: 1659.0\n",
      "fc layer 1 self.abs_max_out: 1782.0\n",
      "lif layer 1 self.abs_max_v: 1782.0\n",
      "fc layer 2 self.abs_max_out: 824.0\n",
      "lif layer 2 self.abs_max_v: 927.0\n",
      "lif layer 2 self.abs_max_v: 951.5\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 2 self.abs_max_out: 968.0\n",
      "lif layer 2 self.abs_max_v: 968.0\n",
      "fc layer 2 self.abs_max_out: 1009.0\n",
      "lif layer 2 self.abs_max_v: 1009.0\n",
      "fc layer 3 self.abs_max_out: 213.0\n",
      "fc layer 1 self.abs_max_out: 1870.0\n",
      "lif layer 1 self.abs_max_v: 1870.0\n",
      "fc layer 2 self.abs_max_out: 1044.0\n",
      "lif layer 2 self.abs_max_v: 1044.0\n",
      "lif layer 2 self.abs_max_v: 1047.0\n",
      "lif layer 2 self.abs_max_v: 1060.0\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "lif layer 2 self.abs_max_v: 1063.0\n",
      "lif layer 2 self.abs_max_v: 1087.5\n",
      "fc layer 2 self.abs_max_out: 1055.0\n",
      "fc layer 2 self.abs_max_out: 1083.0\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "lif layer 2 self.abs_max_v: 1116.0\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 1 self.abs_max_out: 1969.0\n",
      "lif layer 1 self.abs_max_v: 1969.0\n",
      "lif layer 2 self.abs_max_v: 1238.0\n",
      "lif layer 2 self.abs_max_v: 1322.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 1329.5\n",
      "fc layer 1 self.abs_max_out: 2006.0\n",
      "lif layer 1 self.abs_max_v: 2006.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "lif layer 2 self.abs_max_v: 1346.5\n",
      "lif layer 2 self.abs_max_v: 1381.5\n",
      "lif layer 2 self.abs_max_v: 1403.5\n",
      "fc layer 2 self.abs_max_out: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1220.0\n",
      "fc layer 2 self.abs_max_out: 1229.0\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "lif layer 2 self.abs_max_v: 1474.0\n",
      "fc layer 1 self.abs_max_out: 2023.0\n",
      "lif layer 1 self.abs_max_v: 2023.0\n",
      "fc layer 2 self.abs_max_out: 1283.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "lif layer 2 self.abs_max_v: 1483.0\n",
      "lif layer 2 self.abs_max_v: 1490.0\n",
      "lif layer 2 self.abs_max_v: 1558.0\n",
      "lif layer 2 self.abs_max_v: 1568.5\n",
      "lif layer 2 self.abs_max_v: 1689.5\n",
      "lif layer 2 self.abs_max_v: 1792.0\n",
      "lif layer 2 self.abs_max_v: 1800.0\n",
      "lif layer 2 self.abs_max_v: 1858.0\n",
      "lif layer 2 self.abs_max_v: 1940.0\n",
      "fc layer 1 self.abs_max_out: 2072.0\n",
      "lif layer 1 self.abs_max_v: 2072.0\n",
      "fc layer 3 self.abs_max_out: 350.0\n",
      "lif layer 1 self.abs_max_v: 2118.0\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "fc layer 2 self.abs_max_out: 1311.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "fc layer 2 self.abs_max_out: 1334.0\n",
      "fc layer 2 self.abs_max_out: 1373.0\n",
      "fc layer 1 self.abs_max_out: 2180.0\n",
      "lif layer 1 self.abs_max_v: 2180.0\n",
      "fc layer 2 self.abs_max_out: 1378.0\n",
      "lif layer 1 self.abs_max_v: 2326.5\n",
      "fc layer 2 self.abs_max_out: 1408.0\n",
      "lif layer 1 self.abs_max_v: 2439.5\n",
      "fc layer 2 self.abs_max_out: 1451.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "fc layer 1 self.abs_max_out: 2183.0\n",
      "fc layer 1 self.abs_max_out: 2325.0\n",
      "fc layer 2 self.abs_max_out: 1484.0\n",
      "fc layer 1 self.abs_max_out: 2363.0\n",
      "fc layer 2 self.abs_max_out: 1496.0\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "fc layer 2 self.abs_max_out: 1555.0\n",
      "lif layer 2 self.abs_max_v: 1976.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "lif layer 2 self.abs_max_v: 1989.0\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 2 self.abs_max_out: 1597.0\n",
      "fc layer 2 self.abs_max_out: 1727.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 1 self.abs_max_out: 2569.0\n",
      "lif layer 1 self.abs_max_v: 2569.0\n",
      "fc layer 1 self.abs_max_out: 2588.0\n",
      "lif layer 1 self.abs_max_v: 2588.0\n",
      "lif layer 1 self.abs_max_v: 2709.0\n",
      "lif layer 1 self.abs_max_v: 2826.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "lif layer 2 self.abs_max_v: 2023.5\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 1 self.abs_max_out: 2678.0\n",
      "fc layer 2 self.abs_max_out: 1859.0\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "lif layer 2 self.abs_max_v: 2063.0\n",
      "lif layer 2 self.abs_max_v: 2112.5\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "lif layer 2 self.abs_max_v: 2244.0\n",
      "fc layer 1 self.abs_max_out: 2764.0\n",
      "lif layer 2 self.abs_max_v: 2528.0\n",
      "lif layer 2 self.abs_max_v: 2553.5\n",
      "fc layer 1 self.abs_max_out: 2867.0\n",
      "lif layer 1 self.abs_max_v: 2867.0\n",
      "lif layer 1 self.abs_max_v: 3186.5\n",
      "fc layer 3 self.abs_max_out: 418.0\n",
      "fc layer 1 self.abs_max_out: 2907.0\n",
      "fc layer 1 self.abs_max_out: 2960.0\n",
      "fc layer 1 self.abs_max_out: 3020.0\n",
      "lif layer 1 self.abs_max_v: 3236.5\n",
      "lif layer 1 self.abs_max_v: 3284.5\n",
      "lif layer 1 self.abs_max_v: 3396.5\n",
      "lif layer 1 self.abs_max_v: 3421.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.747848/  1.940628, val:  31.67%, val_best:  31.67%, tr:  97.55%, tr_best:  97.55%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.8933%\n",
      "layer   2  Sparsity: 76.7603%\n",
      "layer   3  Sparsity: 81.4571%\n",
      "total_backward_count 9790 real_backward_count 2062  21.062%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 2608.0\n",
      "lif layer 2 self.abs_max_v: 2643.0\n",
      "fc layer 2 self.abs_max_out: 1998.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "fc layer 2 self.abs_max_out: 2007.0\n",
      "fc layer 1 self.abs_max_out: 3193.0\n",
      "fc layer 2 self.abs_max_out: 2094.0\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "fc layer 2 self.abs_max_out: 2118.0\n",
      "fc layer 2 self.abs_max_out: 2121.0\n",
      "fc layer 1 self.abs_max_out: 3223.0\n",
      "fc layer 2 self.abs_max_out: 2193.0\n",
      "lif layer 1 self.abs_max_v: 3455.0\n",
      "fc layer 3 self.abs_max_out: 508.0\n",
      "fc layer 1 self.abs_max_out: 3495.0\n",
      "lif layer 1 self.abs_max_v: 3495.0\n",
      "lif layer 2 self.abs_max_v: 2654.0\n",
      "lif layer 2 self.abs_max_v: 2789.5\n",
      "lif layer 2 self.abs_max_v: 2866.0\n",
      "lif layer 2 self.abs_max_v: 2954.5\n",
      "lif layer 2 self.abs_max_v: 2965.0\n",
      "lif layer 2 self.abs_max_v: 3060.5\n",
      "fc layer 1 self.abs_max_out: 3547.0\n",
      "lif layer 1 self.abs_max_v: 3547.0\n",
      "lif layer 2 self.abs_max_v: 3122.5\n",
      "lif layer 2 self.abs_max_v: 3172.5\n",
      "lif layer 2 self.abs_max_v: 3527.5\n",
      "lif layer 1 self.abs_max_v: 3916.0\n",
      "lif layer 1 self.abs_max_v: 3974.5\n",
      "lif layer 1 self.abs_max_v: 4251.5\n",
      "fc layer 1 self.abs_max_out: 3774.0\n",
      "fc layer 1 self.abs_max_out: 3891.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.585349/  1.899922, val:  39.17%, val_best:  39.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8881%\n",
      "layer   2  Sparsity: 75.0719%\n",
      "layer   3  Sparsity: 78.8165%\n",
      "total_backward_count 19580 real_backward_count 3611  18.442%\n",
      "lif layer 2 self.abs_max_v: 3540.5\n",
      "fc layer 3 self.abs_max_out: 537.0\n",
      "lif layer 2 self.abs_max_v: 3642.0\n",
      "lif layer 1 self.abs_max_v: 4253.5\n",
      "lif layer 1 self.abs_max_v: 4353.0\n",
      "lif layer 2 self.abs_max_v: 3647.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.538057/  1.845299, val:  42.08%, val_best:  42.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8908%\n",
      "layer   2  Sparsity: 74.2678%\n",
      "layer   3  Sparsity: 78.5602%\n",
      "total_backward_count 29370 real_backward_count 5120  17.433%\n",
      "fc layer 2 self.abs_max_out: 2218.0\n",
      "lif layer 1 self.abs_max_v: 4741.5\n",
      "lif layer 1 self.abs_max_v: 4784.5\n",
      "fc layer 1 self.abs_max_out: 3900.0\n",
      "fc layer 2 self.abs_max_out: 2248.0\n",
      "fc layer 2 self.abs_max_out: 2374.0\n",
      "lif layer 2 self.abs_max_v: 3675.5\n",
      "fc layer 1 self.abs_max_out: 4331.0\n",
      "fc layer 2 self.abs_max_out: 2448.0\n",
      "lif layer 2 self.abs_max_v: 3708.5\n",
      "lif layer 1 self.abs_max_v: 4816.0\n",
      "lif layer 2 self.abs_max_v: 3754.5\n",
      "lif layer 2 self.abs_max_v: 3761.0\n",
      "lif layer 2 self.abs_max_v: 3809.5\n",
      "lif layer 2 self.abs_max_v: 3889.0\n",
      "lif layer 2 self.abs_max_v: 3914.5\n",
      "lif layer 2 self.abs_max_v: 3922.0\n",
      "lif layer 2 self.abs_max_v: 4091.0\n",
      "lif layer 2 self.abs_max_v: 4189.5\n",
      "lif layer 2 self.abs_max_v: 4439.0\n",
      "lif layer 2 self.abs_max_v: 4584.5\n",
      "lif layer 1 self.abs_max_v: 5061.0\n",
      "lif layer 1 self.abs_max_v: 5377.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.513016/  1.886078, val:  37.92%, val_best:  42.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8970%\n",
      "layer   2  Sparsity: 74.1932%\n",
      "layer   3  Sparsity: 78.8032%\n",
      "total_backward_count 39160 real_backward_count 6590  16.828%\n",
      "fc layer 1 self.abs_max_out: 4367.0\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2472.0\n",
      "fc layer 2 self.abs_max_out: 2514.0\n",
      "lif layer 2 self.abs_max_v: 4698.0\n",
      "fc layer 2 self.abs_max_out: 2564.0\n",
      "fc layer 2 self.abs_max_out: 2607.0\n",
      "fc layer 2 self.abs_max_out: 2616.0\n",
      "lif layer 2 self.abs_max_v: 4708.5\n",
      "lif layer 1 self.abs_max_v: 5414.5\n",
      "lif layer 2 self.abs_max_v: 4744.5\n",
      "fc layer 2 self.abs_max_out: 2643.0\n",
      "fc layer 2 self.abs_max_out: 2693.0\n",
      "lif layer 2 self.abs_max_v: 4779.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.465302/  1.783410, val:  49.58%, val_best:  49.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8929%\n",
      "layer   2  Sparsity: 73.7677%\n",
      "layer   3  Sparsity: 78.4576%\n",
      "total_backward_count 48950 real_backward_count 8059  16.464%\n",
      "lif layer 1 self.abs_max_v: 5570.0\n",
      "lif layer 1 self.abs_max_v: 5638.0\n",
      "lif layer 2 self.abs_max_v: 4792.0\n",
      "fc layer 2 self.abs_max_out: 2753.0\n",
      "lif layer 2 self.abs_max_v: 5068.5\n",
      "fc layer 1 self.abs_max_out: 4588.0\n",
      "lif layer 1 self.abs_max_v: 5800.0\n",
      "lif layer 1 self.abs_max_v: 6240.0\n",
      "lif layer 1 self.abs_max_v: 6362.0\n",
      "lif layer 1 self.abs_max_v: 6993.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.414217/  1.781381, val:  49.17%, val_best:  49.58%, tr:  99.28%, tr_best:  99.80%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8813%\n",
      "layer   2  Sparsity: 73.9480%\n",
      "layer   3  Sparsity: 78.7120%\n",
      "total_backward_count 58740 real_backward_count 9468  16.118%\n",
      "fc layer 1 self.abs_max_out: 4633.0\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 3 self.abs_max_out: 620.0\n",
      "fc layer 2 self.abs_max_out: 2777.0\n",
      "lif layer 2 self.abs_max_v: 5180.0\n",
      "lif layer 2 self.abs_max_v: 5346.0\n",
      "fc layer 2 self.abs_max_out: 2787.0\n",
      "lif layer 2 self.abs_max_v: 5371.0\n",
      "lif layer 1 self.abs_max_v: 7019.5\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.394282/  1.752574, val:  45.42%, val_best:  49.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8871%\n",
      "layer   2  Sparsity: 74.4154%\n",
      "layer   3  Sparsity: 78.5128%\n",
      "total_backward_count 68530 real_backward_count 10840  15.818%\n",
      "fc layer 1 self.abs_max_out: 4811.0\n",
      "lif layer 2 self.abs_max_v: 5411.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.361152/  1.720710, val:  39.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8879%\n",
      "layer   2  Sparsity: 74.0595%\n",
      "layer   3  Sparsity: 78.3943%\n",
      "total_backward_count 78320 real_backward_count 12166  15.534%\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "fc layer 1 self.abs_max_out: 5002.0\n",
      "lif layer 1 self.abs_max_v: 7194.0\n",
      "fc layer 2 self.abs_max_out: 2866.0\n",
      "lif layer 1 self.abs_max_v: 7803.5\n",
      "lif layer 1 self.abs_max_v: 7891.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.328813/  1.629896, val:  58.33%, val_best:  58.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8929%\n",
      "layer   2  Sparsity: 73.8229%\n",
      "layer   3  Sparsity: 78.0221%\n",
      "total_backward_count 88110 real_backward_count 13606  15.442%\n",
      "fc layer 1 self.abs_max_out: 5073.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.303659/  1.697966, val:  47.08%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8961%\n",
      "layer   2  Sparsity: 73.3579%\n",
      "layer   3  Sparsity: 77.9323%\n",
      "total_backward_count 97900 real_backward_count 14884  15.203%\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 1 self.abs_max_out: 5187.0\n",
      "fc layer 2 self.abs_max_out: 2916.0\n",
      "lif layer 1 self.abs_max_v: 8235.5\n",
      "lif layer 1 self.abs_max_v: 8311.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.259525/  1.598689, val:  59.58%, val_best:  59.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9020%\n",
      "layer   2  Sparsity: 73.6880%\n",
      "layer   3  Sparsity: 78.1616%\n",
      "total_backward_count 107690 real_backward_count 16206  15.049%\n",
      "fc layer 3 self.abs_max_out: 699.0\n",
      "fc layer 1 self.abs_max_out: 5193.0\n",
      "lif layer 1 self.abs_max_v: 8390.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.256586/  1.616011, val:  52.50%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9015%\n",
      "layer   2  Sparsity: 73.9835%\n",
      "layer   3  Sparsity: 77.7672%\n",
      "total_backward_count 117480 real_backward_count 17488  14.886%\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "lif layer 1 self.abs_max_v: 8736.5\n",
      "lif layer 1 self.abs_max_v: 8842.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.222877/  1.581798, val:  55.83%, val_best:  59.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8957%\n",
      "layer   2  Sparsity: 73.5746%\n",
      "layer   3  Sparsity: 77.8221%\n",
      "total_backward_count 127270 real_backward_count 18696  14.690%\n",
      "fc layer 2 self.abs_max_out: 2959.0\n",
      "fc layer 2 self.abs_max_out: 3047.0\n",
      "lif layer 2 self.abs_max_v: 5589.0\n",
      "fc layer 2 self.abs_max_out: 3067.0\n",
      "fc layer 2 self.abs_max_out: 3207.0\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "fc layer 2 self.abs_max_out: 3208.0\n",
      "lif layer 2 self.abs_max_v: 5858.5\n",
      "lif layer 2 self.abs_max_v: 6045.5\n",
      "lif layer 2 self.abs_max_v: 6139.0\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "fc layer 3 self.abs_max_out: 751.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.203299/  1.623145, val:  44.58%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8937%\n",
      "layer   2  Sparsity: 73.2655%\n",
      "layer   3  Sparsity: 77.0859%\n",
      "total_backward_count 137060 real_backward_count 19916  14.531%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.173012/  1.579563, val:  54.17%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8905%\n",
      "layer   2  Sparsity: 73.2164%\n",
      "layer   3  Sparsity: 76.8147%\n",
      "total_backward_count 146850 real_backward_count 21090  14.362%\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "lif layer 1 self.abs_max_v: 8867.0\n",
      "lif layer 1 self.abs_max_v: 8905.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.179356/  1.536638, val:  55.42%, val_best:  59.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8936%\n",
      "layer   2  Sparsity: 73.3988%\n",
      "layer   3  Sparsity: 77.5687%\n",
      "total_backward_count 156640 real_backward_count 22254  14.207%\n",
      "fc layer 1 self.abs_max_out: 5313.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.136670/  1.486286, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8766%\n",
      "layer   2  Sparsity: 72.9372%\n",
      "layer   3  Sparsity: 76.9683%\n",
      "total_backward_count 166430 real_backward_count 23426  14.076%\n",
      "fc layer 1 self.abs_max_out: 5377.0\n",
      "lif layer 1 self.abs_max_v: 9024.0\n",
      "lif layer 1 self.abs_max_v: 9075.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.128382/  1.457584, val:  71.67%, val_best:  71.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8881%\n",
      "layer   2  Sparsity: 73.1569%\n",
      "layer   3  Sparsity: 77.0456%\n",
      "total_backward_count 176220 real_backward_count 24548  13.930%\n",
      "fc layer 1 self.abs_max_out: 5436.0\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "fc layer 3 self.abs_max_out: 848.0\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.094608/  1.457270, val:  60.83%, val_best:  71.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8928%\n",
      "layer   2  Sparsity: 72.9994%\n",
      "layer   3  Sparsity: 76.9628%\n",
      "total_backward_count 186010 real_backward_count 25720  13.827%\n",
      "lif layer 1 self.abs_max_v: 9299.0\n",
      "lif layer 1 self.abs_max_v: 9452.5\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.048656/  1.456833, val:  49.17%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8954%\n",
      "layer   2  Sparsity: 72.8484%\n",
      "layer   3  Sparsity: 76.7330%\n",
      "total_backward_count 195800 real_backward_count 26811  13.693%\n",
      "fc layer 1 self.abs_max_out: 5559.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.029054/  1.414979, val:  59.17%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8822%\n",
      "layer   2  Sparsity: 73.0542%\n",
      "layer   3  Sparsity: 76.8965%\n",
      "total_backward_count 205590 real_backward_count 27879  13.560%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.053054/  1.415501, val:  68.33%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9104%\n",
      "layer   2  Sparsity: 72.6553%\n",
      "layer   3  Sparsity: 77.1523%\n",
      "total_backward_count 215380 real_backward_count 28946  13.440%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.045600/  1.402948, val:  65.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9058%\n",
      "layer   2  Sparsity: 72.6184%\n",
      "layer   3  Sparsity: 76.8763%\n",
      "total_backward_count 225170 real_backward_count 30063  13.351%\n",
      "fc layer 3 self.abs_max_out: 876.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.015239/  1.407683, val:  59.58%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8810%\n",
      "layer   2  Sparsity: 73.0969%\n",
      "layer   3  Sparsity: 77.3233%\n",
      "total_backward_count 234960 real_backward_count 31120  13.245%\n",
      "fc layer 1 self.abs_max_out: 5606.0\n",
      "lif layer 1 self.abs_max_v: 9576.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.022589/  1.366261, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9053%\n",
      "layer   2  Sparsity: 72.7997%\n",
      "layer   3  Sparsity: 77.1153%\n",
      "total_backward_count 244750 real_backward_count 32057  13.098%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.023889/  1.339583, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8969%\n",
      "layer   2  Sparsity: 72.9625%\n",
      "layer   3  Sparsity: 77.5022%\n",
      "total_backward_count 254540 real_backward_count 33106  13.006%\n",
      "fc layer 1 self.abs_max_out: 5735.0\n",
      "lif layer 1 self.abs_max_v: 9670.5\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.006354/  1.382594, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8861%\n",
      "layer   2  Sparsity: 72.7852%\n",
      "layer   3  Sparsity: 77.2823%\n",
      "total_backward_count 264330 real_backward_count 34044  12.879%\n",
      "fc layer 3 self.abs_max_out: 908.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.998543/  1.331795, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 73.1350%\n",
      "layer   3  Sparsity: 77.4966%\n",
      "total_backward_count 274120 real_backward_count 34982  12.762%\n",
      "lif layer 1 self.abs_max_v: 9741.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.982139/  1.323564, val:  72.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8886%\n",
      "layer   2  Sparsity: 73.0562%\n",
      "layer   3  Sparsity: 77.2194%\n",
      "total_backward_count 283910 real_backward_count 35921  12.652%\n",
      "fc layer 3 self.abs_max_out: 918.0\n",
      "lif layer 1 self.abs_max_v: 9868.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.990056/  1.307619, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 72.8998%\n",
      "layer   3  Sparsity: 77.4745%\n",
      "total_backward_count 293700 real_backward_count 36782  12.524%\n",
      "lif layer 1 self.abs_max_v: 9922.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  0.977067/  1.318765, val:  77.50%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8917%\n",
      "layer   2  Sparsity: 72.8120%\n",
      "layer   3  Sparsity: 77.1743%\n",
      "total_backward_count 303490 real_backward_count 37641  12.403%\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 3 self.abs_max_out: 927.0\n",
      "fc layer 3 self.abs_max_out: 935.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  0.979999/  1.318405, val:  74.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9162%\n",
      "layer   2  Sparsity: 72.8670%\n",
      "layer   3  Sparsity: 77.0331%\n",
      "total_backward_count 313280 real_backward_count 38513  12.293%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  0.965086/  1.335282, val:  71.67%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 72.7104%\n",
      "layer   3  Sparsity: 78.0171%\n",
      "total_backward_count 323070 real_backward_count 39389  12.192%\n",
      "fc layer 1 self.abs_max_out: 5871.0\n",
      "lif layer 1 self.abs_max_v: 10030.0\n",
      "lif layer 1 self.abs_max_v: 10118.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  0.982372/  1.294216, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8915%\n",
      "layer   2  Sparsity: 72.6569%\n",
      "layer   3  Sparsity: 77.9187%\n",
      "total_backward_count 332860 real_backward_count 40200  12.077%\n",
      "fc layer 2 self.abs_max_out: 3285.0\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.934092/  1.237623, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8871%\n",
      "layer   2  Sparsity: 72.1539%\n",
      "layer   3  Sparsity: 77.3932%\n",
      "total_backward_count 342650 real_backward_count 40967  11.956%\n",
      "fc layer 3 self.abs_max_out: 937.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.907809/  1.254091, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8982%\n",
      "layer   2  Sparsity: 72.7264%\n",
      "layer   3  Sparsity: 77.4251%\n",
      "total_backward_count 352440 real_backward_count 41802  11.861%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.908439/  1.250840, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8930%\n",
      "layer   2  Sparsity: 72.2770%\n",
      "layer   3  Sparsity: 77.3484%\n",
      "total_backward_count 362230 real_backward_count 42532  11.742%\n",
      "fc layer 1 self.abs_max_out: 6001.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.904258/  1.303614, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8961%\n",
      "layer   2  Sparsity: 72.4426%\n",
      "layer   3  Sparsity: 77.2357%\n",
      "total_backward_count 372020 real_backward_count 43306  11.641%\n",
      "fc layer 2 self.abs_max_out: 3409.0\n",
      "fc layer 2 self.abs_max_out: 3454.0\n",
      "fc layer 1 self.abs_max_out: 6002.0\n",
      "fc layer 3 self.abs_max_out: 945.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.904929/  1.252153, val:  74.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8894%\n",
      "layer   2  Sparsity: 72.4678%\n",
      "layer   3  Sparsity: 77.5390%\n",
      "total_backward_count 381810 real_backward_count 44075  11.544%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.883598/  1.251443, val:  77.08%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8955%\n",
      "layer   2  Sparsity: 72.4948%\n",
      "layer   3  Sparsity: 77.9381%\n",
      "total_backward_count 391600 real_backward_count 44841  11.451%\n",
      "fc layer 3 self.abs_max_out: 1014.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.890709/  1.215201, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9110%\n",
      "layer   2  Sparsity: 72.1963%\n",
      "layer   3  Sparsity: 77.5875%\n",
      "total_backward_count 401390 real_backward_count 45577  11.355%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.881743/  1.221740, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8849%\n",
      "layer   2  Sparsity: 72.5821%\n",
      "layer   3  Sparsity: 77.2195%\n",
      "total_backward_count 411180 real_backward_count 46283  11.256%\n",
      "fc layer 3 self.abs_max_out: 1036.0\n",
      "fc layer 3 self.abs_max_out: 1051.0\n",
      "fc layer 3 self.abs_max_out: 1077.0\n",
      "fc layer 1 self.abs_max_out: 6091.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.872540/  1.261286, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9000%\n",
      "layer   2  Sparsity: 72.3246%\n",
      "layer   3  Sparsity: 76.8119%\n",
      "total_backward_count 420970 real_backward_count 46952  11.153%\n",
      "fc layer 1 self.abs_max_out: 6169.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.869643/  1.192452, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8986%\n",
      "layer   2  Sparsity: 72.1057%\n",
      "layer   3  Sparsity: 76.4445%\n",
      "total_backward_count 430760 real_backward_count 47640  11.060%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.848366/  1.190728, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8965%\n",
      "layer   2  Sparsity: 72.3836%\n",
      "layer   3  Sparsity: 76.8578%\n",
      "total_backward_count 440550 real_backward_count 48322  10.969%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.852023/  1.231701, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8981%\n",
      "layer   2  Sparsity: 72.1379%\n",
      "layer   3  Sparsity: 77.8176%\n",
      "total_backward_count 450340 real_backward_count 49014  10.884%\n",
      "fc layer 1 self.abs_max_out: 6172.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.850705/  1.233303, val:  75.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8898%\n",
      "layer   2  Sparsity: 72.4219%\n",
      "layer   3  Sparsity: 77.5711%\n",
      "total_backward_count 460130 real_backward_count 49675  10.796%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.842052/  1.225649, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8881%\n",
      "layer   2  Sparsity: 72.3243%\n",
      "layer   3  Sparsity: 77.9608%\n",
      "total_backward_count 469920 real_backward_count 50302  10.704%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.852512/  1.220828, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9065%\n",
      "layer   2  Sparsity: 72.1121%\n",
      "layer   3  Sparsity: 77.8012%\n",
      "total_backward_count 479710 real_backward_count 50971  10.625%\n",
      "lif layer 1 self.abs_max_v: 10145.5\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.846863/  1.163700, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9066%\n",
      "layer   2  Sparsity: 71.9377%\n",
      "layer   3  Sparsity: 77.2860%\n",
      "total_backward_count 489500 real_backward_count 51615  10.544%\n",
      "lif layer 1 self.abs_max_v: 10146.0\n",
      "lif layer 1 self.abs_max_v: 10538.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.854855/  1.213109, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9058%\n",
      "layer   2  Sparsity: 72.1421%\n",
      "layer   3  Sparsity: 77.0734%\n",
      "total_backward_count 499290 real_backward_count 52234  10.462%\n",
      "fc layer 2 self.abs_max_out: 3486.0\n",
      "fc layer 1 self.abs_max_out: 6339.0\n",
      "lif layer 1 self.abs_max_v: 10726.0\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.859060/  1.212907, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8952%\n",
      "layer   2  Sparsity: 71.7694%\n",
      "layer   3  Sparsity: 77.0804%\n",
      "total_backward_count 509080 real_backward_count 52804  10.372%\n",
      "fc layer 2 self.abs_max_out: 3519.0\n",
      "fc layer 1 self.abs_max_out: 6358.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.841999/  1.141099, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9029%\n",
      "layer   2  Sparsity: 71.9049%\n",
      "layer   3  Sparsity: 77.6242%\n",
      "total_backward_count 518870 real_backward_count 53376  10.287%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.811743/  1.147613, val:  81.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8852%\n",
      "layer   2  Sparsity: 71.8644%\n",
      "layer   3  Sparsity: 77.4836%\n",
      "total_backward_count 528660 real_backward_count 53947  10.204%\n",
      "fc layer 1 self.abs_max_out: 6385.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.834697/  1.142266, val:  85.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8811%\n",
      "layer   2  Sparsity: 71.8134%\n",
      "layer   3  Sparsity: 77.5412%\n",
      "total_backward_count 538450 real_backward_count 54549  10.131%\n",
      "fc layer 2 self.abs_max_out: 3540.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.812378/  1.166910, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8877%\n",
      "layer   2  Sparsity: 71.6547%\n",
      "layer   3  Sparsity: 77.7842%\n",
      "total_backward_count 548240 real_backward_count 55124  10.055%\n",
      "lif layer 1 self.abs_max_v: 10744.5\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.827528/  1.179355, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8989%\n",
      "layer   2  Sparsity: 71.5864%\n",
      "layer   3  Sparsity: 78.0723%\n",
      "total_backward_count 558030 real_backward_count 55687   9.979%\n",
      "fc layer 2 self.abs_max_out: 3680.0\n",
      "lif layer 1 self.abs_max_v: 10915.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.820507/  1.175155, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9079%\n",
      "layer   2  Sparsity: 71.6337%\n",
      "layer   3  Sparsity: 77.5644%\n",
      "total_backward_count 567820 real_backward_count 56280   9.912%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.808451/  1.193782, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9073%\n",
      "layer   2  Sparsity: 71.6494%\n",
      "layer   3  Sparsity: 78.0094%\n",
      "total_backward_count 577610 real_backward_count 56829   9.839%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.806406/  1.240913, val:  69.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9073%\n",
      "layer   2  Sparsity: 71.8867%\n",
      "layer   3  Sparsity: 78.1211%\n",
      "total_backward_count 587400 real_backward_count 57354   9.764%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.810251/  1.176417, val:  77.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8891%\n",
      "layer   2  Sparsity: 71.6840%\n",
      "layer   3  Sparsity: 78.0178%\n",
      "total_backward_count 597190 real_backward_count 57839   9.685%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.800696/  1.158280, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9000%\n",
      "layer   2  Sparsity: 71.7970%\n",
      "layer   3  Sparsity: 77.4612%\n",
      "total_backward_count 606980 real_backward_count 58293   9.604%\n",
      "fc layer 1 self.abs_max_out: 6405.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.774695/  1.164773, val:  74.17%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8790%\n",
      "layer   2  Sparsity: 71.3847%\n",
      "layer   3  Sparsity: 77.7100%\n",
      "total_backward_count 616770 real_backward_count 58799   9.533%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.798468/  1.148133, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.8845%\n",
      "layer   2  Sparsity: 71.5952%\n",
      "layer   3  Sparsity: 78.4605%\n",
      "total_backward_count 626560 real_backward_count 59303   9.465%\n",
      "fc layer 1 self.abs_max_out: 6467.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.787896/  1.109081, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9032%\n",
      "layer   2  Sparsity: 71.8042%\n",
      "layer   3  Sparsity: 78.1479%\n",
      "total_backward_count 636350 real_backward_count 59771   9.393%\n",
      "fc layer 1 self.abs_max_out: 6512.0\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.762934/  1.223152, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8875%\n",
      "layer   2  Sparsity: 71.5235%\n",
      "layer   3  Sparsity: 78.4256%\n",
      "total_backward_count 646140 real_backward_count 60262   9.326%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.761148/  1.099766, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8880%\n",
      "layer   2  Sparsity: 71.6115%\n",
      "layer   3  Sparsity: 78.0564%\n",
      "total_backward_count 655930 real_backward_count 60719   9.257%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.752771/  1.095777, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8905%\n",
      "layer   2  Sparsity: 71.3216%\n",
      "layer   3  Sparsity: 78.1977%\n",
      "total_backward_count 665720 real_backward_count 61197   9.193%\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.745645/  1.096077, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8996%\n",
      "layer   2  Sparsity: 71.2972%\n",
      "layer   3  Sparsity: 78.4264%\n",
      "total_backward_count 675510 real_backward_count 61624   9.123%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.741207/  1.130693, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8872%\n",
      "layer   2  Sparsity: 71.3183%\n",
      "layer   3  Sparsity: 77.9658%\n",
      "total_backward_count 685300 real_backward_count 62052   9.055%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.744265/  1.100130, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.40 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8888%\n",
      "layer   2  Sparsity: 71.4301%\n",
      "layer   3  Sparsity: 78.0511%\n",
      "total_backward_count 695090 real_backward_count 62531   8.996%\n",
      "fc layer 3 self.abs_max_out: 1121.0\n",
      "fc layer 3 self.abs_max_out: 1131.0\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.724368/  1.125851, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8920%\n",
      "layer   2  Sparsity: 71.4361%\n",
      "layer   3  Sparsity: 78.0226%\n",
      "total_backward_count 704880 real_backward_count 63010   8.939%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.737889/  1.075532, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8972%\n",
      "layer   2  Sparsity: 71.3546%\n",
      "layer   3  Sparsity: 77.7864%\n",
      "total_backward_count 714670 real_backward_count 63426   8.875%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.735910/  1.106414, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8745%\n",
      "layer   2  Sparsity: 71.5458%\n",
      "layer   3  Sparsity: 77.8464%\n",
      "total_backward_count 724460 real_backward_count 63864   8.815%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.733729/  1.112782, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8828%\n",
      "layer   2  Sparsity: 71.4089%\n",
      "layer   3  Sparsity: 78.3225%\n",
      "total_backward_count 734250 real_backward_count 64269   8.753%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.727818/  1.048448, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8816%\n",
      "layer   2  Sparsity: 71.4125%\n",
      "layer   3  Sparsity: 77.2988%\n",
      "total_backward_count 744040 real_backward_count 64693   8.695%\n",
      "lif layer 1 self.abs_max_v: 11111.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.721767/  1.126377, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8992%\n",
      "layer   2  Sparsity: 71.4413%\n",
      "layer   3  Sparsity: 77.8412%\n",
      "total_backward_count 753830 real_backward_count 65084   8.634%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.724864/  1.117532, val:  75.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8970%\n",
      "layer   2  Sparsity: 71.3772%\n",
      "layer   3  Sparsity: 77.6545%\n",
      "total_backward_count 763620 real_backward_count 65484   8.575%\n",
      "fc layer 1 self.abs_max_out: 6570.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.715857/  1.083157, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9002%\n",
      "layer   2  Sparsity: 71.4065%\n",
      "layer   3  Sparsity: 78.2988%\n",
      "total_backward_count 773410 real_backward_count 65860   8.516%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.697922/  1.040186, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8889%\n",
      "layer   2  Sparsity: 71.4681%\n",
      "layer   3  Sparsity: 78.0667%\n",
      "total_backward_count 783200 real_backward_count 66249   8.459%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.711197/  1.017958, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8960%\n",
      "layer   2  Sparsity: 71.3415%\n",
      "layer   3  Sparsity: 78.7485%\n",
      "total_backward_count 792990 real_backward_count 66634   8.403%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.694652/  1.117511, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9009%\n",
      "layer   2  Sparsity: 71.3417%\n",
      "layer   3  Sparsity: 78.6473%\n",
      "total_backward_count 802780 real_backward_count 66960   8.341%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.720441/  1.140077, val:  76.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8896%\n",
      "layer   2  Sparsity: 71.2818%\n",
      "layer   3  Sparsity: 78.6627%\n",
      "total_backward_count 812570 real_backward_count 67321   8.285%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.701299/  1.099023, val:  77.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 71.1075%\n",
      "layer   3  Sparsity: 78.1137%\n",
      "total_backward_count 822360 real_backward_count 67693   8.232%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.699744/  1.046371, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8921%\n",
      "layer   2  Sparsity: 71.3240%\n",
      "layer   3  Sparsity: 78.3869%\n",
      "total_backward_count 832150 real_backward_count 68065   8.179%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.701842/  1.091541, val:  79.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9133%\n",
      "layer   2  Sparsity: 71.2863%\n",
      "layer   3  Sparsity: 78.6411%\n",
      "total_backward_count 841940 real_backward_count 68442   8.129%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.705211/  1.055991, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8732%\n",
      "layer   2  Sparsity: 71.2101%\n",
      "layer   3  Sparsity: 78.2420%\n",
      "total_backward_count 851730 real_backward_count 68813   8.079%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.683211/  1.056864, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9033%\n",
      "layer   2  Sparsity: 71.6068%\n",
      "layer   3  Sparsity: 78.0449%\n",
      "total_backward_count 861520 real_backward_count 69185   8.031%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.714062/  1.083864, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9131%\n",
      "layer   2  Sparsity: 71.5950%\n",
      "layer   3  Sparsity: 77.9549%\n",
      "total_backward_count 871310 real_backward_count 69560   7.983%\n",
      "fc layer 1 self.abs_max_out: 6593.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.716728/  1.058253, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8892%\n",
      "layer   2  Sparsity: 71.3246%\n",
      "layer   3  Sparsity: 78.3033%\n",
      "total_backward_count 881100 real_backward_count 69911   7.935%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.700311/  1.080077, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8884%\n",
      "layer   2  Sparsity: 71.5326%\n",
      "layer   3  Sparsity: 78.4931%\n",
      "total_backward_count 890890 real_backward_count 70279   7.889%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.700800/  1.052402, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8757%\n",
      "layer   2  Sparsity: 71.5055%\n",
      "layer   3  Sparsity: 78.6059%\n",
      "total_backward_count 900680 real_backward_count 70608   7.839%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.713197/  1.058440, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8878%\n",
      "layer   2  Sparsity: 71.5888%\n",
      "layer   3  Sparsity: 78.4188%\n",
      "total_backward_count 910470 real_backward_count 70916   7.789%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.707676/  1.063988, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8896%\n",
      "layer   2  Sparsity: 71.4082%\n",
      "layer   3  Sparsity: 78.5845%\n",
      "total_backward_count 920260 real_backward_count 71254   7.743%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.697448/  1.051985, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8918%\n",
      "layer   2  Sparsity: 71.6530%\n",
      "layer   3  Sparsity: 79.0098%\n",
      "total_backward_count 930050 real_backward_count 71577   7.696%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.691204/  1.031412, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8702%\n",
      "layer   2  Sparsity: 71.5040%\n",
      "layer   3  Sparsity: 78.9753%\n",
      "total_backward_count 939840 real_backward_count 71921   7.652%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.699848/  1.068270, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8789%\n",
      "layer   2  Sparsity: 71.4382%\n",
      "layer   3  Sparsity: 79.1370%\n",
      "total_backward_count 949630 real_backward_count 72269   7.610%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.694825/  1.031747, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8824%\n",
      "layer   2  Sparsity: 71.5152%\n",
      "layer   3  Sparsity: 78.5994%\n",
      "total_backward_count 959420 real_backward_count 72614   7.569%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.683802/  1.058068, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8773%\n",
      "layer   2  Sparsity: 71.6900%\n",
      "layer   3  Sparsity: 78.9879%\n",
      "total_backward_count 969210 real_backward_count 72926   7.524%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.689500/  1.066897, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8926%\n",
      "layer   2  Sparsity: 71.6672%\n",
      "layer   3  Sparsity: 79.4150%\n",
      "total_backward_count 979000 real_backward_count 73251   7.482%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.677334/  1.047078, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9030%\n",
      "layer   2  Sparsity: 71.6956%\n",
      "layer   3  Sparsity: 79.2384%\n",
      "total_backward_count 988790 real_backward_count 73565   7.440%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.673317/  1.036715, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8891%\n",
      "layer   2  Sparsity: 71.8308%\n",
      "layer   3  Sparsity: 79.4015%\n",
      "total_backward_count 998580 real_backward_count 73884   7.399%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.669535/  1.014779, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8901%\n",
      "layer   2  Sparsity: 71.9883%\n",
      "layer   3  Sparsity: 79.4473%\n",
      "total_backward_count 1008370 real_backward_count 74169   7.355%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.675013/  1.005998, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8898%\n",
      "layer   2  Sparsity: 71.8055%\n",
      "layer   3  Sparsity: 78.8277%\n",
      "total_backward_count 1018160 real_backward_count 74445   7.312%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.659208/  1.023565, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9064%\n",
      "layer   2  Sparsity: 71.6975%\n",
      "layer   3  Sparsity: 79.1605%\n",
      "total_backward_count 1027950 real_backward_count 74746   7.271%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.660303/  1.054185, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9005%\n",
      "layer   2  Sparsity: 71.4524%\n",
      "layer   3  Sparsity: 79.3862%\n",
      "total_backward_count 1037740 real_backward_count 75018   7.229%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.655245/  1.042053, val:  81.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8856%\n",
      "layer   2  Sparsity: 71.5846%\n",
      "layer   3  Sparsity: 79.3697%\n",
      "total_backward_count 1047530 real_backward_count 75298   7.188%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.665392/  1.046307, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8936%\n",
      "layer   2  Sparsity: 71.8865%\n",
      "layer   3  Sparsity: 79.5229%\n",
      "total_backward_count 1057320 real_backward_count 75584   7.149%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.636013/  1.020101, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8701%\n",
      "layer   2  Sparsity: 71.8630%\n",
      "layer   3  Sparsity: 79.5546%\n",
      "total_backward_count 1067110 real_backward_count 75852   7.108%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.640536/  1.034386, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8832%\n",
      "layer   2  Sparsity: 71.5939%\n",
      "layer   3  Sparsity: 79.2559%\n",
      "total_backward_count 1076900 real_backward_count 76101   7.067%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.647379/  1.040025, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9084%\n",
      "layer   2  Sparsity: 71.4089%\n",
      "layer   3  Sparsity: 79.0447%\n",
      "total_backward_count 1086690 real_backward_count 76370   7.028%\n",
      "fc layer 1 self.abs_max_out: 6624.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.653563/  1.016287, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8821%\n",
      "layer   2  Sparsity: 71.4368%\n",
      "layer   3  Sparsity: 78.9865%\n",
      "total_backward_count 1096480 real_backward_count 76637   6.989%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.638582/  1.024722, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9114%\n",
      "layer   2  Sparsity: 71.6654%\n",
      "layer   3  Sparsity: 79.0715%\n",
      "total_backward_count 1106270 real_backward_count 76902   6.951%\n",
      "fc layer 1 self.abs_max_out: 6645.0\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.639029/  1.026800, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8882%\n",
      "layer   2  Sparsity: 71.7564%\n",
      "layer   3  Sparsity: 79.0831%\n",
      "total_backward_count 1116060 real_backward_count 77180   6.915%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.638013/  1.031143, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8910%\n",
      "layer   2  Sparsity: 71.6950%\n",
      "layer   3  Sparsity: 79.1924%\n",
      "total_backward_count 1125850 real_backward_count 77451   6.879%\n",
      "fc layer 1 self.abs_max_out: 6647.0\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.630148/  1.050030, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8843%\n",
      "layer   2  Sparsity: 71.7543%\n",
      "layer   3  Sparsity: 79.4747%\n",
      "total_backward_count 1135640 real_backward_count 77697   6.842%\n",
      "lif layer 1 self.abs_max_v: 11119.5\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.645676/  1.023511, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8775%\n",
      "layer   2  Sparsity: 71.6805%\n",
      "layer   3  Sparsity: 79.2621%\n",
      "total_backward_count 1145430 real_backward_count 77977   6.808%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.638096/  1.028064, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8981%\n",
      "layer   2  Sparsity: 71.6546%\n",
      "layer   3  Sparsity: 79.1568%\n",
      "total_backward_count 1155220 real_backward_count 78226   6.772%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.618868/  1.000318, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8941%\n",
      "layer   2  Sparsity: 71.8362%\n",
      "layer   3  Sparsity: 79.2847%\n",
      "total_backward_count 1165010 real_backward_count 78503   6.738%\n",
      "fc layer 1 self.abs_max_out: 6661.0\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.610802/  0.990880, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9110%\n",
      "layer   2  Sparsity: 72.0713%\n",
      "layer   3  Sparsity: 79.1675%\n",
      "total_backward_count 1174800 real_backward_count 78725   6.701%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.618236/  1.017072, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8851%\n",
      "layer   2  Sparsity: 72.0832%\n",
      "layer   3  Sparsity: 79.0774%\n",
      "total_backward_count 1184590 real_backward_count 78953   6.665%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.620135/  0.999315, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8969%\n",
      "layer   2  Sparsity: 71.8247%\n",
      "layer   3  Sparsity: 78.8535%\n",
      "total_backward_count 1194380 real_backward_count 79185   6.630%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.603807/  1.024382, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9013%\n",
      "layer   2  Sparsity: 71.7494%\n",
      "layer   3  Sparsity: 79.2462%\n",
      "total_backward_count 1204170 real_backward_count 79407   6.594%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.609674/  1.012615, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8864%\n",
      "layer   2  Sparsity: 71.6858%\n",
      "layer   3  Sparsity: 79.0447%\n",
      "total_backward_count 1213960 real_backward_count 79645   6.561%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.612374/  0.997659, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8981%\n",
      "layer   2  Sparsity: 71.7295%\n",
      "layer   3  Sparsity: 79.2999%\n",
      "total_backward_count 1223750 real_backward_count 79866   6.526%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.607631/  0.995727, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9018%\n",
      "layer   2  Sparsity: 71.8112%\n",
      "layer   3  Sparsity: 79.0402%\n",
      "total_backward_count 1233540 real_backward_count 80075   6.491%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.596898/  0.997097, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9125%\n",
      "layer   2  Sparsity: 71.9306%\n",
      "layer   3  Sparsity: 79.3720%\n",
      "total_backward_count 1243330 real_backward_count 80306   6.459%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.590509/  1.011910, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8893%\n",
      "layer   2  Sparsity: 72.0696%\n",
      "layer   3  Sparsity: 79.2379%\n",
      "total_backward_count 1253120 real_backward_count 80547   6.428%\n",
      "lif layer 1 self.abs_max_v: 11138.0\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.608354/  0.981172, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8987%\n",
      "layer   2  Sparsity: 71.7189%\n",
      "layer   3  Sparsity: 79.4062%\n",
      "total_backward_count 1262910 real_backward_count 80770   6.396%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.604308/  1.003448, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.8811%\n",
      "layer   2  Sparsity: 71.5861%\n",
      "layer   3  Sparsity: 79.4103%\n",
      "total_backward_count 1272700 real_backward_count 80989   6.364%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.597432/  0.988358, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.9015%\n",
      "layer   2  Sparsity: 71.7303%\n",
      "layer   3  Sparsity: 79.8662%\n",
      "total_backward_count 1282490 real_backward_count 81166   6.329%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.599955/  0.993516, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9064%\n",
      "layer   2  Sparsity: 71.7745%\n",
      "layer   3  Sparsity: 79.5211%\n",
      "total_backward_count 1292280 real_backward_count 81392   6.298%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.591918/  1.033499, val:  80.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8896%\n",
      "layer   2  Sparsity: 71.8515%\n",
      "layer   3  Sparsity: 79.3356%\n",
      "total_backward_count 1302070 real_backward_count 81617   6.268%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.598123/  0.984871, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8884%\n",
      "layer   2  Sparsity: 71.6964%\n",
      "layer   3  Sparsity: 79.3825%\n",
      "total_backward_count 1311860 real_backward_count 81842   6.239%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.596769/  0.976199, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9042%\n",
      "layer   2  Sparsity: 71.5951%\n",
      "layer   3  Sparsity: 79.7187%\n",
      "total_backward_count 1321650 real_backward_count 82036   6.207%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.603780/  0.978320, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8963%\n",
      "layer   2  Sparsity: 71.7955%\n",
      "layer   3  Sparsity: 80.1197%\n",
      "total_backward_count 1331440 real_backward_count 82250   6.178%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.598940/  0.983075, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9058%\n",
      "layer   2  Sparsity: 71.6952%\n",
      "layer   3  Sparsity: 80.0029%\n",
      "total_backward_count 1341230 real_backward_count 82472   6.149%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.585571/  0.964288, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 71.9233%\n",
      "layer   3  Sparsity: 79.9150%\n",
      "total_backward_count 1351020 real_backward_count 82709   6.122%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.580324/  1.017779, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8999%\n",
      "layer   2  Sparsity: 71.6391%\n",
      "layer   3  Sparsity: 79.7613%\n",
      "total_backward_count 1360810 real_backward_count 82921   6.094%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.568423/  0.959363, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9020%\n",
      "layer   2  Sparsity: 71.7446%\n",
      "layer   3  Sparsity: 79.6845%\n",
      "total_backward_count 1370600 real_backward_count 83135   6.066%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.572961/  1.001303, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8870%\n",
      "layer   2  Sparsity: 71.7525%\n",
      "layer   3  Sparsity: 79.7578%\n",
      "total_backward_count 1380390 real_backward_count 83343   6.038%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.575412/  0.971123, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9032%\n",
      "layer   2  Sparsity: 71.7372%\n",
      "layer   3  Sparsity: 80.0445%\n",
      "total_backward_count 1390180 real_backward_count 83519   6.008%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.582357/  0.978903, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8980%\n",
      "layer   2  Sparsity: 71.6675%\n",
      "layer   3  Sparsity: 79.7302%\n",
      "total_backward_count 1399970 real_backward_count 83707   5.979%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.581149/  0.973258, val:  85.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8760%\n",
      "layer   2  Sparsity: 71.5278%\n",
      "layer   3  Sparsity: 79.3895%\n",
      "total_backward_count 1409760 real_backward_count 83919   5.953%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.571918/  0.959708, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 71.4345%\n",
      "layer   3  Sparsity: 79.4030%\n",
      "total_backward_count 1419550 real_backward_count 84090   5.924%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.558796/  0.958779, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8979%\n",
      "layer   2  Sparsity: 71.5555%\n",
      "layer   3  Sparsity: 79.7614%\n",
      "total_backward_count 1429340 real_backward_count 84267   5.896%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.563268/  0.946124, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8902%\n",
      "layer   2  Sparsity: 71.6668%\n",
      "layer   3  Sparsity: 79.6737%\n",
      "total_backward_count 1439130 real_backward_count 84440   5.867%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.555967/  0.955927, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8936%\n",
      "layer   2  Sparsity: 71.5916%\n",
      "layer   3  Sparsity: 80.2176%\n",
      "total_backward_count 1448920 real_backward_count 84604   5.839%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.563718/  0.947811, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8954%\n",
      "layer   2  Sparsity: 71.7304%\n",
      "layer   3  Sparsity: 80.0357%\n",
      "total_backward_count 1458710 real_backward_count 84785   5.812%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.552683/  0.960682, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8980%\n",
      "layer   2  Sparsity: 71.7276%\n",
      "layer   3  Sparsity: 79.9934%\n",
      "total_backward_count 1468500 real_backward_count 84959   5.785%\n",
      "fc layer 1 self.abs_max_out: 6662.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.559207/  0.960892, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8930%\n",
      "layer   2  Sparsity: 71.5663%\n",
      "layer   3  Sparsity: 80.1386%\n",
      "total_backward_count 1478290 real_backward_count 85151   5.760%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.568155/  0.958473, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8962%\n",
      "layer   2  Sparsity: 71.5733%\n",
      "layer   3  Sparsity: 79.7124%\n",
      "total_backward_count 1488080 real_backward_count 85341   5.735%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.565135/  0.988249, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8876%\n",
      "layer   2  Sparsity: 71.6382%\n",
      "layer   3  Sparsity: 79.7085%\n",
      "total_backward_count 1497870 real_backward_count 85523   5.710%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.572218/  0.978037, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9041%\n",
      "layer   2  Sparsity: 71.5559%\n",
      "layer   3  Sparsity: 79.6080%\n",
      "total_backward_count 1507660 real_backward_count 85706   5.685%\n",
      "fc layer 3 self.abs_max_out: 1137.0\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.564474/  0.976300, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9013%\n",
      "layer   2  Sparsity: 71.6243%\n",
      "layer   3  Sparsity: 80.0929%\n",
      "total_backward_count 1517450 real_backward_count 85881   5.660%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.552355/  0.964855, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8789%\n",
      "layer   2  Sparsity: 71.7060%\n",
      "layer   3  Sparsity: 80.1292%\n",
      "total_backward_count 1527240 real_backward_count 86044   5.634%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.551633/  0.954839, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9048%\n",
      "layer   2  Sparsity: 71.8631%\n",
      "layer   3  Sparsity: 80.0206%\n",
      "total_backward_count 1537030 real_backward_count 86207   5.609%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.565323/  0.979738, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8799%\n",
      "layer   2  Sparsity: 71.7062%\n",
      "layer   3  Sparsity: 79.9278%\n",
      "total_backward_count 1546820 real_backward_count 86371   5.584%\n",
      "lif layer 1 self.abs_max_v: 11395.0\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.572857/  0.982964, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8899%\n",
      "layer   2  Sparsity: 71.6186%\n",
      "layer   3  Sparsity: 79.7062%\n",
      "total_backward_count 1556610 real_backward_count 86530   5.559%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.568974/  0.972106, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8941%\n",
      "layer   2  Sparsity: 71.7138%\n",
      "layer   3  Sparsity: 79.5645%\n",
      "total_backward_count 1566400 real_backward_count 86710   5.536%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.548056/  0.967829, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8932%\n",
      "layer   2  Sparsity: 71.7533%\n",
      "layer   3  Sparsity: 79.2619%\n",
      "total_backward_count 1576190 real_backward_count 86880   5.512%\n",
      "fc layer 1 self.abs_max_out: 6663.0\n",
      "fc layer 3 self.abs_max_out: 1140.0\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.543705/  0.946491, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8840%\n",
      "layer   2  Sparsity: 71.8312%\n",
      "layer   3  Sparsity: 79.6575%\n",
      "total_backward_count 1585980 real_backward_count 87035   5.488%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.534943/  0.952946, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8919%\n",
      "layer   2  Sparsity: 71.8910%\n",
      "layer   3  Sparsity: 79.6892%\n",
      "total_backward_count 1595770 real_backward_count 87162   5.462%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.543830/  0.972788, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8944%\n",
      "layer   2  Sparsity: 71.6918%\n",
      "layer   3  Sparsity: 79.8273%\n",
      "total_backward_count 1605560 real_backward_count 87314   5.438%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.552453/  0.950612, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8724%\n",
      "layer   2  Sparsity: 71.6491%\n",
      "layer   3  Sparsity: 80.1611%\n",
      "total_backward_count 1615350 real_backward_count 87477   5.415%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.542875/  0.940748, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8830%\n",
      "layer   2  Sparsity: 71.8051%\n",
      "layer   3  Sparsity: 79.9034%\n",
      "total_backward_count 1625140 real_backward_count 87602   5.390%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.532539/  0.946560, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8795%\n",
      "layer   2  Sparsity: 71.8121%\n",
      "layer   3  Sparsity: 80.1073%\n",
      "total_backward_count 1634930 real_backward_count 87725   5.366%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.553729/  0.951192, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8969%\n",
      "layer   2  Sparsity: 71.8153%\n",
      "layer   3  Sparsity: 79.9649%\n",
      "total_backward_count 1644720 real_backward_count 87888   5.344%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.542056/  0.952079, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9165%\n",
      "layer   2  Sparsity: 71.7964%\n",
      "layer   3  Sparsity: 79.5872%\n",
      "total_backward_count 1654510 real_backward_count 88059   5.322%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.539359/  0.934536, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9079%\n",
      "layer   2  Sparsity: 71.7390%\n",
      "layer   3  Sparsity: 79.6267%\n",
      "total_backward_count 1664300 real_backward_count 88222   5.301%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.533260/  0.968908, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9060%\n",
      "layer   2  Sparsity: 71.7760%\n",
      "layer   3  Sparsity: 79.4742%\n",
      "total_backward_count 1674090 real_backward_count 88378   5.279%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.535849/  0.955210, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.9159%\n",
      "layer   2  Sparsity: 71.8768%\n",
      "layer   3  Sparsity: 79.4548%\n",
      "total_backward_count 1683880 real_backward_count 88528   5.257%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.543178/  0.954922, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9124%\n",
      "layer   2  Sparsity: 72.0927%\n",
      "layer   3  Sparsity: 79.7524%\n",
      "total_backward_count 1693670 real_backward_count 88676   5.236%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.551205/  0.937779, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8983%\n",
      "layer   2  Sparsity: 72.0260%\n",
      "layer   3  Sparsity: 79.8225%\n",
      "total_backward_count 1703460 real_backward_count 88846   5.216%\n",
      "fc layer 1 self.abs_max_out: 6690.0\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.554244/  0.950153, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8894%\n",
      "layer   2  Sparsity: 71.9082%\n",
      "layer   3  Sparsity: 79.8109%\n",
      "total_backward_count 1713250 real_backward_count 89004   5.195%\n",
      "fc layer 3 self.abs_max_out: 1161.0\n",
      "fc layer 3 self.abs_max_out: 1193.0\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.551159/  0.958343, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8764%\n",
      "layer   2  Sparsity: 71.7285%\n",
      "layer   3  Sparsity: 79.6351%\n",
      "total_backward_count 1723040 real_backward_count 89140   5.173%\n",
      "fc layer 3 self.abs_max_out: 1265.0\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.539774/  0.941430, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.9005%\n",
      "layer   2  Sparsity: 71.6033%\n",
      "layer   3  Sparsity: 79.6712%\n",
      "total_backward_count 1732830 real_backward_count 89265   5.151%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.535605/  0.944081, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.8924%\n",
      "layer   2  Sparsity: 71.7203%\n",
      "layer   3  Sparsity: 79.8347%\n",
      "total_backward_count 1742620 real_backward_count 89405   5.130%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.542444/  0.988076, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9066%\n",
      "layer   2  Sparsity: 71.8135%\n",
      "layer   3  Sparsity: 79.6638%\n",
      "total_backward_count 1752410 real_backward_count 89551   5.110%\n",
      "lif layer 1 self.abs_max_v: 11419.0\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.546812/  0.965672, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8976%\n",
      "layer   2  Sparsity: 71.8365%\n",
      "layer   3  Sparsity: 79.6745%\n",
      "total_backward_count 1762200 real_backward_count 89687   5.089%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.535119/  0.957595, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9007%\n",
      "layer   2  Sparsity: 71.8929%\n",
      "layer   3  Sparsity: 79.4141%\n",
      "total_backward_count 1771990 real_backward_count 89811   5.068%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.530750/  0.946778, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8914%\n",
      "layer   2  Sparsity: 71.9269%\n",
      "layer   3  Sparsity: 79.7806%\n",
      "total_backward_count 1781780 real_backward_count 89976   5.050%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.532791/  0.953923, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8847%\n",
      "layer   2  Sparsity: 71.7689%\n",
      "layer   3  Sparsity: 79.5458%\n",
      "total_backward_count 1791570 real_backward_count 90112   5.030%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.528288/  0.965313, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9005%\n",
      "layer   2  Sparsity: 71.6457%\n",
      "layer   3  Sparsity: 79.9255%\n",
      "total_backward_count 1801360 real_backward_count 90239   5.009%\n",
      "lif layer 1 self.abs_max_v: 11570.0\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.518920/  0.951033, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9051%\n",
      "layer   2  Sparsity: 71.8083%\n",
      "layer   3  Sparsity: 79.7930%\n",
      "total_backward_count 1811150 real_backward_count 90376   4.990%\n",
      "lif layer 1 self.abs_max_v: 11781.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.518857/  0.937490, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8876%\n",
      "layer   2  Sparsity: 71.9013%\n",
      "layer   3  Sparsity: 79.9969%\n",
      "total_backward_count 1820940 real_backward_count 90521   4.971%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.506025/  0.949545, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8968%\n",
      "layer   2  Sparsity: 72.0669%\n",
      "layer   3  Sparsity: 80.2353%\n",
      "total_backward_count 1830730 real_backward_count 90629   4.950%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.519003/  0.966293, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8977%\n",
      "layer   2  Sparsity: 72.0727%\n",
      "layer   3  Sparsity: 80.1143%\n",
      "total_backward_count 1840520 real_backward_count 90756   4.931%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.512842/  0.940748, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8861%\n",
      "layer   2  Sparsity: 72.1653%\n",
      "layer   3  Sparsity: 80.0095%\n",
      "total_backward_count 1850310 real_backward_count 90878   4.912%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.509584/  0.951927, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.9014%\n",
      "layer   2  Sparsity: 72.1481%\n",
      "layer   3  Sparsity: 79.8836%\n",
      "total_backward_count 1860100 real_backward_count 91015   4.893%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.525235/  0.946367, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9021%\n",
      "layer   2  Sparsity: 71.9652%\n",
      "layer   3  Sparsity: 79.9109%\n",
      "total_backward_count 1869890 real_backward_count 91153   4.875%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.525188/  0.961986, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8909%\n",
      "layer   2  Sparsity: 71.8207%\n",
      "layer   3  Sparsity: 79.4836%\n",
      "total_backward_count 1879680 real_backward_count 91267   4.855%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.519726/  0.934078, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9125%\n",
      "layer   2  Sparsity: 71.7271%\n",
      "layer   3  Sparsity: 79.1334%\n",
      "total_backward_count 1889470 real_backward_count 91399   4.837%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.518416/  0.949412, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8877%\n",
      "layer   2  Sparsity: 71.7098%\n",
      "layer   3  Sparsity: 79.5216%\n",
      "total_backward_count 1899260 real_backward_count 91523   4.819%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.507726/  0.944457, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8836%\n",
      "layer   2  Sparsity: 71.9352%\n",
      "layer   3  Sparsity: 79.7096%\n",
      "total_backward_count 1909050 real_backward_count 91635   4.800%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.514644/  0.922786, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8839%\n",
      "layer   2  Sparsity: 71.9162%\n",
      "layer   3  Sparsity: 79.8996%\n",
      "total_backward_count 1918840 real_backward_count 91766   4.782%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.512804/  0.933411, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8905%\n",
      "layer   2  Sparsity: 71.8551%\n",
      "layer   3  Sparsity: 80.0337%\n",
      "total_backward_count 1928630 real_backward_count 91904   4.765%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.499500/  0.923854, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.9073%\n",
      "layer   2  Sparsity: 71.8143%\n",
      "layer   3  Sparsity: 80.0068%\n",
      "total_backward_count 1938420 real_backward_count 92019   4.747%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.502221/  0.914356, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.8858%\n",
      "layer   2  Sparsity: 71.7714%\n",
      "layer   3  Sparsity: 80.1784%\n",
      "total_backward_count 1948210 real_backward_count 92135   4.729%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.510947/  0.922958, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8973%\n",
      "layer   2  Sparsity: 71.7766%\n",
      "layer   3  Sparsity: 80.2428%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e77c811254d4171b175c95b1a566dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.51095</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>0.92296</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-159</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7x4xonr7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7x4xonr7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_061909-7x4xonr7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tu1jep1w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_103737-tu1jep1w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tu1jep1w' target=\"_blank\">efficient-sweep-164</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tu1jep1w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tu1jep1w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251118_103747_203', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 778.0\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 1027.0\n",
      "lif layer 1 self.abs_max_v: 1059.5\n",
      "fc layer 1 self.abs_max_out: 1028.0\n",
      "lif layer 1 self.abs_max_v: 1393.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 596.0\n",
      "fc layer 1 self.abs_max_out: 1029.0\n",
      "lif layer 2 self.abs_max_v: 810.0\n",
      "fc layer 1 self.abs_max_out: 1542.0\n",
      "lif layer 1 self.abs_max_v: 1632.5\n",
      "fc layer 2 self.abs_max_out: 642.0\n",
      "lif layer 2 self.abs_max_v: 880.0\n",
      "fc layer 1 self.abs_max_out: 1543.0\n",
      "lif layer 1 self.abs_max_v: 1635.5\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "lif layer 2 self.abs_max_v: 1134.0\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 1 self.abs_max_out: 1787.0\n",
      "lif layer 1 self.abs_max_v: 1787.0\n",
      "fc layer 1 self.abs_max_out: 1945.0\n",
      "lif layer 1 self.abs_max_v: 1945.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "lif layer 2 self.abs_max_v: 1150.0\n",
      "lif layer 1 self.abs_max_v: 2047.0\n",
      "lif layer 2 self.abs_max_v: 1179.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 2201.0\n",
      "lif layer 1 self.abs_max_v: 2274.5\n",
      "lif layer 2 self.abs_max_v: 1465.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 1 self.abs_max_out: 2397.0\n",
      "lif layer 1 self.abs_max_v: 2397.0\n",
      "lif layer 2 self.abs_max_v: 1599.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "fc layer 1 self.abs_max_out: 3165.0\n",
      "lif layer 1 self.abs_max_v: 3165.0\n",
      "lif layer 2 self.abs_max_v: 1657.5\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 1 self.abs_max_out: 3685.0\n",
      "lif layer 1 self.abs_max_v: 3685.0\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1902.0\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 1969.5\n",
      "lif layer 2 self.abs_max_v: 2143.5\n",
      "lif layer 2 self.abs_max_v: 2188.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "lif layer 2 self.abs_max_v: 2202.0\n",
      "fc layer 2 self.abs_max_out: 1395.0\n",
      "fc layer 2 self.abs_max_out: 1405.0\n",
      "lif layer 2 self.abs_max_v: 2423.0\n",
      "fc layer 2 self.abs_max_out: 1689.0\n",
      "lif layer 2 self.abs_max_v: 2900.5\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "fc layer 1 self.abs_max_out: 4388.0\n",
      "lif layer 1 self.abs_max_v: 4388.0\n",
      "fc layer 2 self.abs_max_out: 1872.0\n",
      "fc layer 2 self.abs_max_out: 1912.0\n",
      "fc layer 2 self.abs_max_out: 2021.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "lif layer 2 self.abs_max_v: 2907.0\n",
      "fc layer 2 self.abs_max_out: 2025.0\n",
      "fc layer 2 self.abs_max_out: 2148.0\n",
      "fc layer 1 self.abs_max_out: 4535.0\n",
      "lif layer 1 self.abs_max_v: 4535.0\n",
      "fc layer 1 self.abs_max_out: 4893.0\n",
      "lif layer 1 self.abs_max_v: 4893.0\n",
      "lif layer 2 self.abs_max_v: 3100.0\n",
      "lif layer 2 self.abs_max_v: 3241.0\n",
      "lif layer 2 self.abs_max_v: 3530.5\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "lif layer 2 self.abs_max_v: 3706.5\n",
      "lif layer 2 self.abs_max_v: 3846.5\n",
      "fc layer 3 self.abs_max_out: 435.0\n",
      "fc layer 2 self.abs_max_out: 2353.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 2 self.abs_max_out: 2548.0\n",
      "fc layer 3 self.abs_max_out: 503.0\n",
      "lif layer 2 self.abs_max_v: 3986.5\n",
      "lif layer 2 self.abs_max_v: 4068.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 1 self.abs_max_out: 5311.0\n",
      "lif layer 1 self.abs_max_v: 5311.0\n",
      "fc layer 3 self.abs_max_out: 534.0\n",
      "fc layer 2 self.abs_max_out: 2593.0\n",
      "fc layer 2 self.abs_max_out: 2890.0\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "fc layer 2 self.abs_max_out: 3073.0\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 2 self.abs_max_out: 3155.0\n",
      "fc layer 1 self.abs_max_out: 5552.0\n",
      "lif layer 1 self.abs_max_v: 5552.0\n",
      "fc layer 1 self.abs_max_out: 5635.0\n",
      "lif layer 1 self.abs_max_v: 5635.0\n",
      "fc layer 1 self.abs_max_out: 5979.0\n",
      "lif layer 1 self.abs_max_v: 5979.0\n",
      "fc layer 2 self.abs_max_out: 3330.0\n",
      "fc layer 3 self.abs_max_out: 589.0\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "fc layer 1 self.abs_max_out: 6075.0\n",
      "lif layer 1 self.abs_max_v: 6075.0\n",
      "fc layer 1 self.abs_max_out: 6402.0\n",
      "lif layer 1 self.abs_max_v: 6402.0\n",
      "lif layer 2 self.abs_max_v: 4073.5\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 2 self.abs_max_out: 3503.0\n",
      "fc layer 2 self.abs_max_out: 3612.0\n",
      "fc layer 1 self.abs_max_out: 6409.0\n",
      "lif layer 1 self.abs_max_v: 6409.0\n",
      "fc layer 1 self.abs_max_out: 6529.0\n",
      "lif layer 1 self.abs_max_v: 6529.0\n",
      "lif layer 2 self.abs_max_v: 4101.0\n",
      "lif layer 1 self.abs_max_v: 6582.5\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "lif layer 1 self.abs_max_v: 7041.0\n",
      "fc layer 1 self.abs_max_out: 6682.0\n",
      "fc layer 1 self.abs_max_out: 6808.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "lif layer 2 self.abs_max_v: 4209.5\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "fc layer 1 self.abs_max_out: 7422.0\n",
      "lif layer 1 self.abs_max_v: 7422.0\n",
      "fc layer 2 self.abs_max_out: 3694.0\n",
      "fc layer 2 self.abs_max_out: 3795.0\n",
      "fc layer 1 self.abs_max_out: 8217.0\n",
      "lif layer 1 self.abs_max_v: 8217.0\n",
      "lif layer 2 self.abs_max_v: 4228.0\n",
      "lif layer 2 self.abs_max_v: 4254.0\n",
      "lif layer 2 self.abs_max_v: 4478.5\n",
      "lif layer 2 self.abs_max_v: 4580.5\n",
      "lif layer 1 self.abs_max_v: 8264.0\n",
      "fc layer 2 self.abs_max_out: 3849.0\n",
      "fc layer 2 self.abs_max_out: 3988.0\n",
      "lif layer 2 self.abs_max_v: 4794.0\n",
      "lif layer 1 self.abs_max_v: 8902.0\n",
      "lif layer 1 self.abs_max_v: 9155.5\n",
      "lif layer 1 self.abs_max_v: 9356.0\n",
      "lif layer 1 self.abs_max_v: 9833.0\n",
      "fc layer 1 self.abs_max_out: 8879.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.103680/  2.144272, val:  32.92%, val_best:  32.92%, tr:  80.29%, tr_best:  80.29%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 80.0063%\n",
      "layer   3  Sparsity: 88.7312%\n",
      "total_backward_count 9790 real_backward_count 3974  40.592%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 9937.5\n",
      "lif layer 1 self.abs_max_v: 9985.5\n",
      "lif layer 1 self.abs_max_v: 10113.5\n",
      "lif layer 1 self.abs_max_v: 10583.5\n",
      "lif layer 1 self.abs_max_v: 10665.0\n",
      "fc layer 1 self.abs_max_out: 10164.0\n",
      "lif layer 1 self.abs_max_v: 10780.5\n",
      "lif layer 2 self.abs_max_v: 4831.0\n",
      "lif layer 2 self.abs_max_v: 4885.0\n",
      "lif layer 2 self.abs_max_v: 4888.5\n",
      "fc layer 2 self.abs_max_out: 4122.0\n",
      "fc layer 1 self.abs_max_out: 10243.0\n",
      "fc layer 2 self.abs_max_out: 4152.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.068467/  2.141387, val:  43.33%, val_best:  43.33%, tr:  95.81%, tr_best:  95.81%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4660%\n",
      "layer   2  Sparsity: 78.6877%\n",
      "layer   3  Sparsity: 86.2916%\n",
      "total_backward_count 19580 real_backward_count 6390  32.635%\n",
      "lif layer 1 self.abs_max_v: 10787.5\n",
      "fc layer 2 self.abs_max_out: 4192.0\n",
      "lif layer 1 self.abs_max_v: 11164.0\n",
      "fc layer 1 self.abs_max_out: 10380.0\n",
      "fc layer 2 self.abs_max_out: 4213.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "fc layer 1 self.abs_max_out: 10672.0\n",
      "fc layer 1 self.abs_max_out: 10788.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.078625/  2.143938, val:  47.92%, val_best:  47.92%, tr:  98.26%, tr_best:  98.26%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 78.2077%\n",
      "layer   3  Sparsity: 85.5399%\n",
      "total_backward_count 29370 real_backward_count 8496  28.927%\n",
      "lif layer 1 self.abs_max_v: 11949.5\n",
      "lif layer 1 self.abs_max_v: 12117.0\n",
      "lif layer 1 self.abs_max_v: 12163.0\n",
      "lif layer 1 self.abs_max_v: 12721.5\n",
      "lif layer 1 self.abs_max_v: 12761.0\n",
      "lif layer 1 self.abs_max_v: 12898.5\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "fc layer 1 self.abs_max_out: 11023.0\n",
      "lif layer 1 self.abs_max_v: 13158.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.082118/  2.156976, val:  41.67%, val_best:  47.92%, tr:  98.88%, tr_best:  98.88%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4286%\n",
      "layer   2  Sparsity: 78.4196%\n",
      "layer   3  Sparsity: 85.2302%\n",
      "total_backward_count 39160 real_backward_count 10409  26.581%\n",
      "fc layer 1 self.abs_max_out: 11460.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.091414/  2.153608, val:  45.42%, val_best:  47.92%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 77.4560%\n",
      "layer   3  Sparsity: 84.4397%\n",
      "total_backward_count 48950 real_backward_count 12174  24.870%\n",
      "lif layer 2 self.abs_max_v: 4994.5\n",
      "lif layer 2 self.abs_max_v: 5104.5\n",
      "lif layer 2 self.abs_max_v: 5260.0\n",
      "lif layer 1 self.abs_max_v: 13830.0\n",
      "fc layer 1 self.abs_max_out: 11682.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.086570/  2.163477, val:  44.58%, val_best:  47.92%, tr:  99.08%, tr_best:  99.39%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 78.0678%\n",
      "layer   3  Sparsity: 84.8463%\n",
      "total_backward_count 58740 real_backward_count 13934  23.721%\n",
      "fc layer 1 self.abs_max_out: 11988.0\n",
      "fc layer 1 self.abs_max_out: 12967.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.089911/  2.150659, val:  47.08%, val_best:  47.92%, tr:  98.88%, tr_best:  99.39%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 78.0804%\n",
      "layer   3  Sparsity: 84.5494%\n",
      "total_backward_count 68530 real_backward_count 15653  22.841%\n",
      "lif layer 2 self.abs_max_v: 5366.5\n",
      "lif layer 2 self.abs_max_v: 5503.5\n",
      "lif layer 2 self.abs_max_v: 5540.0\n",
      "lif layer 2 self.abs_max_v: 5708.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.088837/  2.142035, val:  51.67%, val_best:  51.67%, tr:  99.08%, tr_best:  99.39%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 77.8997%\n",
      "layer   3  Sparsity: 84.6016%\n",
      "total_backward_count 78320 real_backward_count 17300  22.089%\n",
      "lif layer 2 self.abs_max_v: 5844.0\n",
      "fc layer 2 self.abs_max_out: 4413.0\n",
      "lif layer 1 self.abs_max_v: 13865.0\n",
      "fc layer 1 self.abs_max_out: 13392.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.092203/  2.145993, val:  48.33%, val_best:  51.67%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4599%\n",
      "layer   2  Sparsity: 77.9044%\n",
      "layer   3  Sparsity: 85.1189%\n",
      "total_backward_count 88110 real_backward_count 19077  21.651%\n",
      "lif layer 2 self.abs_max_v: 5874.5\n",
      "fc layer 2 self.abs_max_out: 4438.0\n",
      "lif layer 2 self.abs_max_v: 6192.5\n",
      "fc layer 2 self.abs_max_out: 4614.0\n",
      "fc layer 1 self.abs_max_out: 13477.0\n",
      "lif layer 2 self.abs_max_v: 6359.5\n",
      "lif layer 1 self.abs_max_v: 14461.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.085634/  2.152166, val:  50.00%, val_best:  51.67%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4528%\n",
      "layer   2  Sparsity: 77.7156%\n",
      "layer   3  Sparsity: 84.9830%\n",
      "total_backward_count 97900 real_backward_count 20698  21.142%\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.086893/  2.147498, val:  37.92%, val_best:  51.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4366%\n",
      "layer   2  Sparsity: 77.2126%\n",
      "layer   3  Sparsity: 84.3648%\n",
      "total_backward_count 107690 real_backward_count 22329  20.735%\n",
      "lif layer 2 self.abs_max_v: 6400.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.088579/  2.147487, val:  52.50%, val_best:  52.50%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4093%\n",
      "layer   2  Sparsity: 76.8970%\n",
      "layer   3  Sparsity: 84.4470%\n",
      "total_backward_count 117480 real_backward_count 23934  20.373%\n",
      "lif layer 2 self.abs_max_v: 6478.5\n",
      "lif layer 1 self.abs_max_v: 14660.5\n",
      "lif layer 1 self.abs_max_v: 15819.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.086244/  2.161472, val:  39.17%, val_best:  52.50%, tr:  99.28%, tr_best:  99.69%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4209%\n",
      "layer   2  Sparsity: 76.5321%\n",
      "layer   3  Sparsity: 85.5215%\n",
      "total_backward_count 127270 real_backward_count 25558  20.082%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.094426/  2.162955, val:  43.33%, val_best:  52.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 76.2520%\n",
      "layer   3  Sparsity: 84.7906%\n",
      "total_backward_count 137060 real_backward_count 27169  19.823%\n",
      "fc layer 1 self.abs_max_out: 13579.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.094952/  2.153769, val:  49.17%, val_best:  52.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 76.0401%\n",
      "layer   3  Sparsity: 85.0475%\n",
      "total_backward_count 146850 real_backward_count 28725  19.561%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.098934/  2.147041, val:  55.00%, val_best:  55.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 76.4555%\n",
      "layer   3  Sparsity: 84.9212%\n",
      "total_backward_count 156640 real_backward_count 30308  19.349%\n",
      "fc layer 1 self.abs_max_out: 14135.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.089016/  2.141166, val:  48.33%, val_best:  55.00%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 76.7878%\n",
      "layer   3  Sparsity: 84.7224%\n",
      "total_backward_count 166430 real_backward_count 31834  19.128%\n",
      "fc layer 2 self.abs_max_out: 4661.0\n",
      "lif layer 1 self.abs_max_v: 15953.5\n",
      "lif layer 1 self.abs_max_v: 16458.0\n",
      "fc layer 2 self.abs_max_out: 4710.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.087565/  2.138156, val:  52.92%, val_best:  55.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 76.3680%\n",
      "layer   3  Sparsity: 84.6590%\n",
      "total_backward_count 176220 real_backward_count 33387  18.946%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.075847/  2.142340, val:  52.08%, val_best:  55.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4203%\n",
      "layer   2  Sparsity: 76.4761%\n",
      "layer   3  Sparsity: 84.6450%\n",
      "total_backward_count 186010 real_backward_count 34982  18.807%\n",
      "fc layer 1 self.abs_max_out: 14274.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.088099/  2.163112, val:  46.67%, val_best:  55.00%, tr:  99.18%, tr_best:  99.80%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4281%\n",
      "layer   2  Sparsity: 76.6319%\n",
      "layer   3  Sparsity: 85.1609%\n",
      "total_backward_count 195800 real_backward_count 36500  18.641%\n",
      "lif layer 1 self.abs_max_v: 16577.5\n",
      "fc layer 2 self.abs_max_out: 4760.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.093108/  2.157136, val:  47.08%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 76.8454%\n",
      "layer   3  Sparsity: 85.7746%\n",
      "total_backward_count 205590 real_backward_count 38006  18.486%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.090873/  2.149219, val:  50.42%, val_best:  55.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4473%\n",
      "layer   2  Sparsity: 76.6389%\n",
      "layer   3  Sparsity: 86.0151%\n",
      "total_backward_count 215380 real_backward_count 39536  18.356%\n",
      "lif layer 1 self.abs_max_v: 16596.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.079935/  2.128386, val:  60.00%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4432%\n",
      "layer   2  Sparsity: 76.4265%\n",
      "layer   3  Sparsity: 84.7959%\n",
      "total_backward_count 225170 real_backward_count 41088  18.248%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.088553/  2.148340, val:  56.25%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3963%\n",
      "layer   2  Sparsity: 76.0481%\n",
      "layer   3  Sparsity: 85.3822%\n",
      "total_backward_count 234960 real_backward_count 42664  18.158%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.083294/  2.142109, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4588%\n",
      "layer   2  Sparsity: 75.9356%\n",
      "layer   3  Sparsity: 84.7799%\n",
      "total_backward_count 244750 real_backward_count 44177  18.050%\n",
      "lif layer 1 self.abs_max_v: 16622.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.091428/  2.134919, val:  60.00%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4221%\n",
      "layer   2  Sparsity: 76.3114%\n",
      "layer   3  Sparsity: 85.3779%\n",
      "total_backward_count 254540 real_backward_count 45796  17.992%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.089427/  2.158132, val:  57.92%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 76.2443%\n",
      "layer   3  Sparsity: 85.7346%\n",
      "total_backward_count 264330 real_backward_count 47325  17.904%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.087996/  2.140038, val:  60.83%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3871%\n",
      "layer   2  Sparsity: 75.9168%\n",
      "layer   3  Sparsity: 85.4541%\n",
      "total_backward_count 274120 real_backward_count 48831  17.814%\n",
      "fc layer 1 self.abs_max_out: 14631.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.091724/  2.164899, val:  42.92%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4363%\n",
      "layer   2  Sparsity: 76.1926%\n",
      "layer   3  Sparsity: 85.7542%\n",
      "total_backward_count 283910 real_backward_count 50259  17.702%\n",
      "fc layer 1 self.abs_max_out: 14858.0\n",
      "lif layer 2 self.abs_max_v: 6591.5\n",
      "lif layer 2 self.abs_max_v: 6636.0\n",
      "lif layer 1 self.abs_max_v: 17056.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.107032/  2.149974, val:  57.92%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 76.0381%\n",
      "layer   3  Sparsity: 86.1579%\n",
      "total_backward_count 293700 real_backward_count 51765  17.625%\n",
      "fc layer 2 self.abs_max_out: 4915.0\n",
      "lif layer 2 self.abs_max_v: 6741.5\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.103765/  2.154719, val:  59.17%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4707%\n",
      "layer   2  Sparsity: 76.0387%\n",
      "layer   3  Sparsity: 86.0791%\n",
      "total_backward_count 303490 real_backward_count 53266  17.551%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.106321/  2.170101, val:  43.33%, val_best:  62.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 76.0806%\n",
      "layer   3  Sparsity: 86.2435%\n",
      "total_backward_count 313280 real_backward_count 54767  17.482%\n",
      "fc layer 1 self.abs_max_out: 14921.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.104352/  2.163561, val:  51.67%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4215%\n",
      "layer   2  Sparsity: 75.9066%\n",
      "layer   3  Sparsity: 85.5696%\n",
      "total_backward_count 323070 real_backward_count 56262  17.415%\n",
      "fc layer 1 self.abs_max_out: 15110.0\n",
      "lif layer 1 self.abs_max_v: 17197.5\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  2.093711/  2.154689, val:  50.42%, val_best:  62.50%, tr:  99.08%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4620%\n",
      "layer   2  Sparsity: 75.7496%\n",
      "layer   3  Sparsity: 85.5390%\n",
      "total_backward_count 332860 real_backward_count 57767  17.355%\n",
      "lif layer 1 self.abs_max_v: 17629.5\n",
      "lif layer 1 self.abs_max_v: 17700.0\n",
      "lif layer 1 self.abs_max_v: 18210.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  2.094209/  2.160492, val:  57.92%, val_best:  62.50%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4190%\n",
      "layer   2  Sparsity: 75.8366%\n",
      "layer   3  Sparsity: 86.1620%\n",
      "total_backward_count 342650 real_backward_count 59271  17.298%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  2.099413/  2.160088, val:  54.17%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4137%\n",
      "layer   2  Sparsity: 75.0771%\n",
      "layer   3  Sparsity: 86.0989%\n",
      "total_backward_count 352440 real_backward_count 60691  17.220%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  2.094846/  2.135255, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4756%\n",
      "layer   2  Sparsity: 74.8616%\n",
      "layer   3  Sparsity: 85.7865%\n",
      "total_backward_count 362230 real_backward_count 62069  17.135%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  2.102337/  2.170751, val:  54.17%, val_best:  67.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4652%\n",
      "layer   2  Sparsity: 75.4187%\n",
      "layer   3  Sparsity: 86.2112%\n",
      "total_backward_count 372020 real_backward_count 63456  17.057%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  2.110839/  2.156636, val:  56.67%, val_best:  67.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3838%\n",
      "layer   2  Sparsity: 75.4841%\n",
      "layer   3  Sparsity: 86.6419%\n",
      "total_backward_count 381810 real_backward_count 64956  17.013%\n",
      "fc layer 1 self.abs_max_out: 15157.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  2.107197/  2.175365, val:  56.25%, val_best:  67.92%, tr:  99.18%, tr_best:  99.90%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 75.4744%\n",
      "layer   3  Sparsity: 86.3354%\n",
      "total_backward_count 391600 real_backward_count 66341  16.941%\n",
      "fc layer 1 self.abs_max_out: 15201.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  2.108900/  2.165771, val:  58.75%, val_best:  67.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4060%\n",
      "layer   2  Sparsity: 75.0066%\n",
      "layer   3  Sparsity: 86.1694%\n",
      "total_backward_count 401390 real_backward_count 67778  16.886%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  2.108405/  2.165274, val:  54.17%, val_best:  67.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4285%\n",
      "layer   2  Sparsity: 75.5006%\n",
      "layer   3  Sparsity: 86.4778%\n",
      "total_backward_count 411180 real_backward_count 69238  16.839%\n",
      "fc layer 1 self.abs_max_out: 15427.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  2.100837/  2.155553, val:  57.50%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4793%\n",
      "layer   2  Sparsity: 75.3507%\n",
      "layer   3  Sparsity: 86.0897%\n",
      "total_backward_count 420970 real_backward_count 70612  16.774%\n",
      "lif layer 2 self.abs_max_v: 7134.5\n",
      "fc layer 1 self.abs_max_out: 15550.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  2.097727/  2.156469, val:  60.42%, val_best:  67.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4836%\n",
      "layer   2  Sparsity: 75.2758%\n",
      "layer   3  Sparsity: 86.1446%\n",
      "total_backward_count 430760 real_backward_count 71984  16.711%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  2.100329/  2.155068, val:  52.08%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4907%\n",
      "layer   2  Sparsity: 75.5651%\n",
      "layer   3  Sparsity: 85.7978%\n",
      "total_backward_count 440550 real_backward_count 73398  16.661%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  2.087258/  2.138481, val:  67.08%, val_best:  67.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 75.3627%\n",
      "layer   3  Sparsity: 85.3466%\n",
      "total_backward_count 450340 real_backward_count 74806  16.611%\n",
      "fc layer 1 self.abs_max_out: 15553.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  2.091075/  2.154732, val:  59.17%, val_best:  67.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4026%\n",
      "layer   2  Sparsity: 75.5330%\n",
      "layer   3  Sparsity: 85.6206%\n",
      "total_backward_count 460130 real_backward_count 76241  16.569%\n",
      "lif layer 2 self.abs_max_v: 7226.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  2.092517/  2.157484, val:  52.50%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4800%\n",
      "layer   2  Sparsity: 75.6298%\n",
      "layer   3  Sparsity: 86.0317%\n",
      "total_backward_count 469920 real_backward_count 77613  16.516%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  2.086088/  2.153776, val:  62.50%, val_best:  67.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3908%\n",
      "layer   2  Sparsity: 75.7148%\n",
      "layer   3  Sparsity: 84.8930%\n",
      "total_backward_count 479710 real_backward_count 78990  16.466%\n",
      "fc layer 1 self.abs_max_out: 15901.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  2.087827/  2.136729, val:  56.25%, val_best:  67.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 75.4019%\n",
      "layer   3  Sparsity: 85.2167%\n",
      "total_backward_count 489500 real_backward_count 80365  16.418%\n",
      "fc layer 1 self.abs_max_out: 15937.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  2.088964/  2.148900, val:  57.50%, val_best:  67.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4516%\n",
      "layer   2  Sparsity: 75.4351%\n",
      "layer   3  Sparsity: 85.5281%\n",
      "total_backward_count 499290 real_backward_count 81713  16.366%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  2.095422/  2.149064, val:  59.58%, val_best:  67.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4048%\n",
      "layer   2  Sparsity: 75.1988%\n",
      "layer   3  Sparsity: 85.6616%\n",
      "total_backward_count 509080 real_backward_count 83075  16.319%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  2.085961/  2.156055, val:  60.42%, val_best:  67.92%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4455%\n",
      "layer   2  Sparsity: 75.1212%\n",
      "layer   3  Sparsity: 85.2406%\n",
      "total_backward_count 518870 real_backward_count 84444  16.275%\n",
      "fc layer 1 self.abs_max_out: 15962.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  2.086535/  2.140022, val:  65.83%, val_best:  67.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 75.0495%\n",
      "layer   3  Sparsity: 85.5615%\n",
      "total_backward_count 528660 real_backward_count 85830  16.235%\n",
      "fc layer 1 self.abs_max_out: 16061.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  2.099053/  2.155602, val:  61.25%, val_best:  67.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 75.1644%\n",
      "layer   3  Sparsity: 85.6643%\n",
      "total_backward_count 538450 real_backward_count 87172  16.189%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  2.095031/  2.153492, val:  58.75%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4457%\n",
      "layer   2  Sparsity: 75.3190%\n",
      "layer   3  Sparsity: 85.9702%\n",
      "total_backward_count 548240 real_backward_count 88573  16.156%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  2.101018/  2.157341, val:  52.50%, val_best:  67.92%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4068%\n",
      "layer   2  Sparsity: 75.0422%\n",
      "layer   3  Sparsity: 86.0709%\n",
      "total_backward_count 558030 real_backward_count 89945  16.118%\n",
      "fc layer 2 self.abs_max_out: 5059.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  2.091952/  2.154500, val:  63.75%, val_best:  67.92%, tr:  99.39%, tr_best:  99.90%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4583%\n",
      "layer   2  Sparsity: 75.2054%\n",
      "layer   3  Sparsity: 85.9035%\n",
      "total_backward_count 567820 real_backward_count 91324  16.083%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  2.090747/  2.153179, val:  60.00%, val_best:  67.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 75.3938%\n",
      "layer   3  Sparsity: 85.7411%\n",
      "total_backward_count 577610 real_backward_count 92634  16.037%\n",
      "fc layer 1 self.abs_max_out: 16125.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  2.085997/  2.145839, val:  50.83%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4405%\n",
      "layer   2  Sparsity: 75.2476%\n",
      "layer   3  Sparsity: 85.6693%\n",
      "total_backward_count 587400 real_backward_count 94014  16.005%\n",
      "fc layer 1 self.abs_max_out: 16161.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  2.083915/  2.149595, val:  50.83%, val_best:  67.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4515%\n",
      "layer   2  Sparsity: 75.1163%\n",
      "layer   3  Sparsity: 85.8091%\n",
      "total_backward_count 597190 real_backward_count 95428  15.980%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  2.082601/  2.136097, val:  73.75%, val_best:  73.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 75.0113%\n",
      "layer   3  Sparsity: 85.5668%\n",
      "total_backward_count 606980 real_backward_count 96824  15.952%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  2.086739/  2.171324, val:  48.33%, val_best:  73.75%, tr:  99.28%, tr_best:  99.90%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4370%\n",
      "layer   2  Sparsity: 75.1361%\n",
      "layer   3  Sparsity: 85.3058%\n",
      "total_backward_count 616770 real_backward_count 98235  15.927%\n",
      "fc layer 1 self.abs_max_out: 16520.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  2.090533/  2.149639, val:  59.17%, val_best:  73.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4600%\n",
      "layer   2  Sparsity: 75.3287%\n",
      "layer   3  Sparsity: 85.7590%\n",
      "total_backward_count 626560 real_backward_count 99535  15.886%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  2.100436/  2.176199, val:  61.25%, val_best:  73.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4439%\n",
      "layer   2  Sparsity: 75.0553%\n",
      "layer   3  Sparsity: 86.0802%\n",
      "total_backward_count 636350 real_backward_count 100864  15.850%\n",
      "lif layer 1 self.abs_max_v: 18248.0\n",
      "lif layer 1 self.abs_max_v: 18472.0\n",
      "fc layer 1 self.abs_max_out: 16585.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  2.099569/  2.155086, val:  71.67%, val_best:  73.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 74.9507%\n",
      "layer   3  Sparsity: 86.3261%\n",
      "total_backward_count 646140 real_backward_count 102202  15.817%\n",
      "lif layer 1 self.abs_max_v: 18505.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  2.098001/  2.152727, val:  67.50%, val_best:  73.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4253%\n",
      "layer   2  Sparsity: 75.1084%\n",
      "layer   3  Sparsity: 85.9817%\n",
      "total_backward_count 655930 real_backward_count 103482  15.776%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  2.086369/  2.146672, val:  61.67%, val_best:  73.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4125%\n",
      "layer   2  Sparsity: 75.0610%\n",
      "layer   3  Sparsity: 85.5797%\n",
      "total_backward_count 665720 real_backward_count 104805  15.743%\n",
      "lif layer 1 self.abs_max_v: 19784.5\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  2.089720/  2.142998, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 74.8530%\n",
      "layer   3  Sparsity: 85.7251%\n",
      "total_backward_count 675510 real_backward_count 106077  15.703%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  2.083540/  2.151671, val:  58.33%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 74.8866%\n",
      "layer   3  Sparsity: 85.2036%\n",
      "total_backward_count 685300 real_backward_count 107366  15.667%\n",
      "fc layer 1 self.abs_max_out: 16731.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  2.082283/  2.141840, val:  67.50%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4428%\n",
      "layer   2  Sparsity: 75.0690%\n",
      "layer   3  Sparsity: 85.3655%\n",
      "total_backward_count 695090 real_backward_count 108708  15.639%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  2.069982/  2.136484, val:  60.83%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4284%\n",
      "layer   2  Sparsity: 75.2814%\n",
      "layer   3  Sparsity: 85.1594%\n",
      "total_backward_count 704880 real_backward_count 110001  15.606%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  2.079912/  2.142174, val:  74.17%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4367%\n",
      "layer   2  Sparsity: 75.0999%\n",
      "layer   3  Sparsity: 85.9016%\n",
      "total_backward_count 714670 real_backward_count 111303  15.574%\n",
      "fc layer 1 self.abs_max_out: 16744.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  2.079979/  2.155225, val:  59.58%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   2  Sparsity: 74.8629%\n",
      "layer   3  Sparsity: 85.9533%\n",
      "total_backward_count 724460 real_backward_count 112622  15.546%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  2.090281/  2.154491, val:  61.25%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 75.0841%\n",
      "layer   3  Sparsity: 86.0971%\n",
      "total_backward_count 734250 real_backward_count 114020  15.529%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  2.091212/  2.139885, val:  70.42%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4329%\n",
      "layer   2  Sparsity: 74.9190%\n",
      "layer   3  Sparsity: 85.7140%\n",
      "total_backward_count 744040 real_backward_count 115249  15.490%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  2.091464/  2.129447, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 75.1983%\n",
      "layer   3  Sparsity: 85.7108%\n",
      "total_backward_count 753830 real_backward_count 116567  15.463%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  2.084508/  2.153790, val:  61.25%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4431%\n",
      "layer   2  Sparsity: 75.1015%\n",
      "layer   3  Sparsity: 85.8151%\n",
      "total_backward_count 763620 real_backward_count 117859  15.434%\n",
      "fc layer 1 self.abs_max_out: 16930.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  2.093025/  2.157312, val:  54.58%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4742%\n",
      "layer   2  Sparsity: 75.0516%\n",
      "layer   3  Sparsity: 86.4630%\n",
      "total_backward_count 773410 real_backward_count 119181  15.410%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  2.101443/  2.155684, val:  67.50%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4255%\n",
      "layer   2  Sparsity: 75.1313%\n",
      "layer   3  Sparsity: 86.5611%\n",
      "total_backward_count 783200 real_backward_count 120461  15.381%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  2.087651/  2.140362, val:  60.83%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4084%\n",
      "layer   2  Sparsity: 75.1201%\n",
      "layer   3  Sparsity: 85.7970%\n",
      "total_backward_count 792990 real_backward_count 121740  15.352%\n",
      "fc layer 1 self.abs_max_out: 17064.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  2.092468/  2.156469, val:  56.25%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 75.1579%\n",
      "layer   3  Sparsity: 86.2043%\n",
      "total_backward_count 802780 real_backward_count 122959  15.317%\n",
      "fc layer 1 self.abs_max_out: 17110.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  2.093278/  2.142229, val:  62.08%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4212%\n",
      "layer   2  Sparsity: 74.9867%\n",
      "layer   3  Sparsity: 86.0032%\n",
      "total_backward_count 812570 real_backward_count 124247  15.291%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  2.088678/  2.151562, val:  58.33%, val_best:  75.00%, tr:  99.28%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3966%\n",
      "layer   2  Sparsity: 75.0017%\n",
      "layer   3  Sparsity: 86.2797%\n",
      "total_backward_count 822360 real_backward_count 125605  15.274%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  2.084506/  2.144206, val:  62.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4206%\n",
      "layer   2  Sparsity: 74.8622%\n",
      "layer   3  Sparsity: 86.1728%\n",
      "total_backward_count 832150 real_backward_count 126886  15.248%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  2.087877/  2.152267, val:  69.17%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4530%\n",
      "layer   2  Sparsity: 75.2083%\n",
      "layer   3  Sparsity: 86.3780%\n",
      "total_backward_count 841940 real_backward_count 128165  15.223%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  2.074517/  2.133813, val:  64.17%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4248%\n",
      "layer   2  Sparsity: 75.0497%\n",
      "layer   3  Sparsity: 85.6873%\n",
      "total_backward_count 851730 real_backward_count 129452  15.199%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  2.083684/  2.153223, val:  68.33%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3822%\n",
      "layer   2  Sparsity: 75.0750%\n",
      "layer   3  Sparsity: 86.4043%\n",
      "total_backward_count 861520 real_backward_count 130745  15.176%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  2.086877/  2.143229, val:  69.58%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3983%\n",
      "layer   2  Sparsity: 75.2998%\n",
      "layer   3  Sparsity: 85.9728%\n",
      "total_backward_count 871310 real_backward_count 132008  15.151%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  2.086408/  2.149847, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4472%\n",
      "layer   2  Sparsity: 75.1873%\n",
      "layer   3  Sparsity: 85.8854%\n",
      "total_backward_count 881100 real_backward_count 133286  15.127%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  2.097155/  2.163837, val:  56.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 74.8538%\n",
      "layer   3  Sparsity: 86.6324%\n",
      "total_backward_count 890890 real_backward_count 134638  15.113%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  2.090319/  2.144446, val:  73.33%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 75.1267%\n",
      "layer   3  Sparsity: 86.1806%\n",
      "total_backward_count 900680 real_backward_count 135916  15.090%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  2.092052/  2.149200, val:  64.17%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4502%\n",
      "layer   2  Sparsity: 75.4808%\n",
      "layer   3  Sparsity: 86.5575%\n",
      "total_backward_count 910470 real_backward_count 137204  15.070%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  2.095261/  2.152592, val:  66.25%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4451%\n",
      "layer   2  Sparsity: 75.0664%\n",
      "layer   3  Sparsity: 86.4893%\n",
      "total_backward_count 920260 real_backward_count 138463  15.046%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  2.094360/  2.151695, val:  66.25%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4670%\n",
      "layer   2  Sparsity: 75.1092%\n",
      "layer   3  Sparsity: 86.1491%\n",
      "total_backward_count 930050 real_backward_count 139741  15.025%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  2.090089/  2.142063, val:  67.50%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4638%\n",
      "layer   2  Sparsity: 75.1157%\n",
      "layer   3  Sparsity: 85.9286%\n",
      "total_backward_count 939840 real_backward_count 141034  15.006%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  2.083453/  2.146567, val:  62.50%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4033%\n",
      "layer   2  Sparsity: 75.0624%\n",
      "layer   3  Sparsity: 86.3924%\n",
      "total_backward_count 949630 real_backward_count 142339  14.989%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  2.080770/  2.144570, val:  73.75%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4446%\n",
      "layer   2  Sparsity: 75.1426%\n",
      "layer   3  Sparsity: 86.0260%\n",
      "total_backward_count 959420 real_backward_count 143625  14.970%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  2.081109/  2.165972, val:  51.67%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 74.9362%\n",
      "layer   3  Sparsity: 85.8146%\n",
      "total_backward_count 969210 real_backward_count 144910  14.951%\n",
      "fc layer 1 self.abs_max_out: 17112.0\n",
      "lif layer 1 self.abs_max_v: 19862.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  2.086454/  2.153460, val:  65.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4208%\n",
      "layer   2  Sparsity: 74.7959%\n",
      "layer   3  Sparsity: 86.0514%\n",
      "total_backward_count 979000 real_backward_count 146205  14.934%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  2.082461/  2.146971, val:  65.00%, val_best:  75.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4256%\n",
      "layer   2  Sparsity: 75.1010%\n",
      "layer   3  Sparsity: 85.7661%\n",
      "total_backward_count 988790 real_backward_count 147459  14.913%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  2.085650/  2.150850, val:  75.83%, val_best:  75.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4261%\n",
      "layer   2  Sparsity: 75.0912%\n",
      "layer   3  Sparsity: 86.0416%\n",
      "total_backward_count 998580 real_backward_count 148740  14.895%\n",
      "fc layer 1 self.abs_max_out: 17155.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  2.086545/  2.141980, val:  63.33%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4798%\n",
      "layer   2  Sparsity: 74.8249%\n",
      "layer   3  Sparsity: 85.8455%\n",
      "total_backward_count 1008370 real_backward_count 149995  14.875%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  2.075457/  2.153842, val:  55.42%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4214%\n",
      "layer   2  Sparsity: 75.2057%\n",
      "layer   3  Sparsity: 85.1380%\n",
      "total_backward_count 1018160 real_backward_count 151231  14.853%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  2.084540/  2.139453, val:  69.17%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4603%\n",
      "layer   2  Sparsity: 75.0637%\n",
      "layer   3  Sparsity: 85.5987%\n",
      "total_backward_count 1027950 real_backward_count 152489  14.834%\n",
      "fc layer 1 self.abs_max_out: 17248.0\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  2.081506/  2.149410, val:  73.33%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4458%\n",
      "layer   2  Sparsity: 75.0407%\n",
      "layer   3  Sparsity: 85.7194%\n",
      "total_backward_count 1037740 real_backward_count 153703  14.811%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  2.082912/  2.157678, val:  57.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4293%\n",
      "layer   2  Sparsity: 75.0988%\n",
      "layer   3  Sparsity: 86.1227%\n",
      "total_backward_count 1047530 real_backward_count 154901  14.787%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  2.081581/  2.150792, val:  58.33%, val_best:  75.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 74.7919%\n",
      "layer   3  Sparsity: 85.4351%\n",
      "total_backward_count 1057320 real_backward_count 156200  14.773%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  2.080685/  2.140560, val:  72.08%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4353%\n",
      "layer   2  Sparsity: 74.8584%\n",
      "layer   3  Sparsity: 85.6093%\n",
      "total_backward_count 1067110 real_backward_count 157470  14.757%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  2.082515/  2.152212, val:  67.08%, val_best:  75.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3850%\n",
      "layer   2  Sparsity: 75.1660%\n",
      "layer   3  Sparsity: 86.0286%\n",
      "total_backward_count 1076900 real_backward_count 158660  14.733%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  2.090872/  2.139640, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4796%\n",
      "layer   2  Sparsity: 75.1198%\n",
      "layer   3  Sparsity: 86.5031%\n",
      "total_backward_count 1086690 real_backward_count 159846  14.709%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  2.090119/  2.145171, val:  62.92%, val_best:  78.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 75.0028%\n",
      "layer   3  Sparsity: 86.1927%\n",
      "total_backward_count 1096480 real_backward_count 160996  14.683%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  2.085594/  2.145904, val:  70.42%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4570%\n",
      "layer   2  Sparsity: 74.7630%\n",
      "layer   3  Sparsity: 86.1939%\n",
      "total_backward_count 1106270 real_backward_count 162247  14.666%\n",
      "lif layer 1 self.abs_max_v: 19975.0\n",
      "lif layer 1 self.abs_max_v: 21119.5\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  2.086077/  2.153451, val:  70.00%, val_best:  78.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3622%\n",
      "layer   2  Sparsity: 74.7680%\n",
      "layer   3  Sparsity: 85.9818%\n",
      "total_backward_count 1116060 real_backward_count 163462  14.646%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  2.085266/  2.152909, val:  62.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4309%\n",
      "layer   2  Sparsity: 75.1480%\n",
      "layer   3  Sparsity: 85.8042%\n",
      "total_backward_count 1125850 real_backward_count 164715  14.630%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  2.089878/  2.152752, val:  70.00%, val_best:  78.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 74.8553%\n",
      "layer   3  Sparsity: 86.1438%\n",
      "total_backward_count 1135640 real_backward_count 165912  14.610%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  2.085366/  2.151310, val:  62.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 75.0789%\n",
      "layer   3  Sparsity: 86.0696%\n",
      "total_backward_count 1145430 real_backward_count 167107  14.589%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  2.080366/  2.137366, val:  75.83%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4663%\n",
      "layer   2  Sparsity: 74.6698%\n",
      "layer   3  Sparsity: 85.6916%\n",
      "total_backward_count 1155220 real_backward_count 168316  14.570%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  2.083893/  2.145492, val:  56.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4193%\n",
      "layer   2  Sparsity: 74.4767%\n",
      "layer   3  Sparsity: 85.2532%\n",
      "total_backward_count 1165010 real_backward_count 169468  14.546%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  2.074534/  2.127048, val:  76.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4288%\n",
      "layer   2  Sparsity: 74.4025%\n",
      "layer   3  Sparsity: 85.4614%\n",
      "total_backward_count 1174800 real_backward_count 170707  14.531%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  2.073514/  2.144618, val:  74.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4224%\n",
      "layer   2  Sparsity: 74.6485%\n",
      "layer   3  Sparsity: 85.6371%\n",
      "total_backward_count 1184590 real_backward_count 171845  14.507%\n",
      "fc layer 2 self.abs_max_out: 5090.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  2.072881/  2.147765, val:  61.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4552%\n",
      "layer   2  Sparsity: 74.9647%\n",
      "layer   3  Sparsity: 85.9876%\n",
      "total_backward_count 1194380 real_backward_count 173052  14.489%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  2.080594/  2.137418, val:  76.67%, val_best:  78.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4233%\n",
      "layer   2  Sparsity: 74.7350%\n",
      "layer   3  Sparsity: 85.7391%\n",
      "total_backward_count 1204170 real_backward_count 174292  14.474%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  2.069657/  2.110451, val:  70.42%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 74.4463%\n",
      "layer   3  Sparsity: 85.1282%\n",
      "total_backward_count 1213960 real_backward_count 175534  14.460%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  2.045423/  2.117077, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4392%\n",
      "layer   2  Sparsity: 74.7463%\n",
      "layer   3  Sparsity: 85.2372%\n",
      "total_backward_count 1223750 real_backward_count 176727  14.441%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  2.058836/  2.122486, val:  67.08%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3975%\n",
      "layer   2  Sparsity: 74.7751%\n",
      "layer   3  Sparsity: 85.3035%\n",
      "total_backward_count 1233540 real_backward_count 177946  14.426%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  2.070027/  2.139813, val:  63.75%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4549%\n",
      "layer   2  Sparsity: 74.8448%\n",
      "layer   3  Sparsity: 86.1751%\n",
      "total_backward_count 1243330 real_backward_count 179126  14.407%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  2.066035/  2.129624, val:  68.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 74.8381%\n",
      "layer   3  Sparsity: 85.8424%\n",
      "total_backward_count 1253120 real_backward_count 180250  14.384%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  2.057294/  2.127344, val:  76.67%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4191%\n",
      "layer   2  Sparsity: 74.8528%\n",
      "layer   3  Sparsity: 85.5831%\n",
      "total_backward_count 1262910 real_backward_count 181428  14.366%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  2.057015/  2.131617, val:  80.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4315%\n",
      "layer   2  Sparsity: 74.8129%\n",
      "layer   3  Sparsity: 85.5913%\n",
      "total_backward_count 1272700 real_backward_count 182559  14.344%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  2.067816/  2.125288, val:  64.17%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4136%\n",
      "layer   2  Sparsity: 74.5006%\n",
      "layer   3  Sparsity: 85.4916%\n",
      "total_backward_count 1282490 real_backward_count 183748  14.327%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  2.065414/  2.128824, val:  73.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4198%\n",
      "layer   2  Sparsity: 74.4578%\n",
      "layer   3  Sparsity: 85.2029%\n",
      "total_backward_count 1292280 real_backward_count 184915  14.309%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  2.063872/  2.131985, val:  67.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 74.5278%\n",
      "layer   3  Sparsity: 85.5646%\n",
      "total_backward_count 1302070 real_backward_count 186071  14.290%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  2.069946/  2.135664, val:  60.83%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 74.7266%\n",
      "layer   3  Sparsity: 85.5426%\n",
      "total_backward_count 1311860 real_backward_count 187269  14.275%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  2.062057/  2.132795, val:  60.83%, val_best:  80.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4386%\n",
      "layer   2  Sparsity: 75.0255%\n",
      "layer   3  Sparsity: 85.3986%\n",
      "total_backward_count 1321650 real_backward_count 188428  14.257%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  2.071410/  2.142085, val:  72.08%, val_best:  80.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3999%\n",
      "layer   2  Sparsity: 74.8061%\n",
      "layer   3  Sparsity: 85.9047%\n",
      "total_backward_count 1331440 real_backward_count 189644  14.244%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  2.071916/  2.119451, val:  77.92%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4437%\n",
      "layer   2  Sparsity: 74.8224%\n",
      "layer   3  Sparsity: 85.5022%\n",
      "total_backward_count 1341230 real_backward_count 190829  14.228%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  2.063595/  2.134240, val:  65.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4640%\n",
      "layer   2  Sparsity: 74.7772%\n",
      "layer   3  Sparsity: 85.1155%\n",
      "total_backward_count 1351020 real_backward_count 191965  14.209%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  2.064393/  2.140191, val:  65.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 74.8380%\n",
      "layer   3  Sparsity: 85.4917%\n",
      "total_backward_count 1360810 real_backward_count 193170  14.195%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  2.067421/  2.122355, val:  73.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4500%\n",
      "layer   2  Sparsity: 74.7455%\n",
      "layer   3  Sparsity: 85.2913%\n",
      "total_backward_count 1370600 real_backward_count 194358  14.181%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  2.060093/  2.139068, val:  63.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3876%\n",
      "layer   2  Sparsity: 74.4240%\n",
      "layer   3  Sparsity: 85.0953%\n",
      "total_backward_count 1380390 real_backward_count 195520  14.164%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  2.061575/  2.126377, val:  66.67%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 74.8820%\n",
      "layer   3  Sparsity: 85.1417%\n",
      "total_backward_count 1390180 real_backward_count 196687  14.148%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  2.059041/  2.128770, val:  64.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 74.9067%\n",
      "layer   3  Sparsity: 84.7562%\n",
      "total_backward_count 1399970 real_backward_count 197819  14.130%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  2.059736/  2.129729, val:  63.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4159%\n",
      "layer   2  Sparsity: 74.6258%\n",
      "layer   3  Sparsity: 85.1289%\n",
      "total_backward_count 1409760 real_backward_count 199002  14.116%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  2.063658/  2.133615, val:  68.33%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4419%\n",
      "layer   2  Sparsity: 75.0024%\n",
      "layer   3  Sparsity: 85.2457%\n",
      "total_backward_count 1419550 real_backward_count 200212  14.104%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  2.068511/  2.142904, val:  62.08%, val_best:  80.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4641%\n",
      "layer   2  Sparsity: 74.8979%\n",
      "layer   3  Sparsity: 85.4282%\n",
      "total_backward_count 1429340 real_backward_count 201383  14.089%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  2.069000/  2.129279, val:  71.67%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4555%\n",
      "layer   2  Sparsity: 74.8367%\n",
      "layer   3  Sparsity: 84.9541%\n",
      "total_backward_count 1439130 real_backward_count 202587  14.077%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  2.065105/  2.127151, val:  62.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4359%\n",
      "layer   2  Sparsity: 74.6831%\n",
      "layer   3  Sparsity: 85.2742%\n",
      "total_backward_count 1448920 real_backward_count 203695  14.058%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  2.058737/  2.117948, val:  66.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4338%\n",
      "layer   2  Sparsity: 74.7490%\n",
      "layer   3  Sparsity: 84.5587%\n",
      "total_backward_count 1458710 real_backward_count 204859  14.044%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  2.049988/  2.134441, val:  68.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4263%\n",
      "layer   2  Sparsity: 74.9906%\n",
      "layer   3  Sparsity: 84.6640%\n",
      "total_backward_count 1468500 real_backward_count 206006  14.028%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  2.052186/  2.126924, val:  63.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4890%\n",
      "layer   2  Sparsity: 74.9160%\n",
      "layer   3  Sparsity: 85.1505%\n",
      "total_backward_count 1478290 real_backward_count 207091  14.009%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  2.051937/  2.119250, val:  62.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4407%\n",
      "layer   2  Sparsity: 74.5897%\n",
      "layer   3  Sparsity: 84.6637%\n",
      "total_backward_count 1488080 real_backward_count 208221  13.993%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  2.060920/  2.140948, val:  62.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4509%\n",
      "layer   2  Sparsity: 74.1158%\n",
      "layer   3  Sparsity: 85.2142%\n",
      "total_backward_count 1497870 real_backward_count 209329  13.975%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  2.069425/  2.137870, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4051%\n",
      "layer   2  Sparsity: 74.4252%\n",
      "layer   3  Sparsity: 85.1079%\n",
      "total_backward_count 1507660 real_backward_count 210495  13.962%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  2.065853/  2.120959, val:  67.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 74.4917%\n",
      "layer   3  Sparsity: 84.9246%\n",
      "total_backward_count 1517450 real_backward_count 211605  13.945%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  2.050358/  2.119221, val:  72.08%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4632%\n",
      "layer   2  Sparsity: 74.7327%\n",
      "layer   3  Sparsity: 84.7747%\n",
      "total_backward_count 1527240 real_backward_count 212688  13.926%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  2.060228/  2.134652, val:  62.50%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4156%\n",
      "layer   2  Sparsity: 74.5061%\n",
      "layer   3  Sparsity: 85.5994%\n",
      "total_backward_count 1537030 real_backward_count 213834  13.912%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  2.064604/  2.116796, val:  75.42%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4274%\n",
      "layer   2  Sparsity: 74.5712%\n",
      "layer   3  Sparsity: 85.7064%\n",
      "total_backward_count 1546820 real_backward_count 214949  13.896%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  2.049319/  2.123438, val:  67.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4578%\n",
      "layer   2  Sparsity: 74.7077%\n",
      "layer   3  Sparsity: 84.7317%\n",
      "total_backward_count 1556610 real_backward_count 216095  13.882%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  2.044315/  2.111631, val:  70.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4591%\n",
      "layer   2  Sparsity: 74.5616%\n",
      "layer   3  Sparsity: 84.7251%\n",
      "total_backward_count 1566400 real_backward_count 217154  13.863%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  2.043642/  2.115275, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4628%\n",
      "layer   2  Sparsity: 74.5417%\n",
      "layer   3  Sparsity: 84.6870%\n",
      "total_backward_count 1576190 real_backward_count 218287  13.849%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  2.054794/  2.119741, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4337%\n",
      "layer   2  Sparsity: 74.4302%\n",
      "layer   3  Sparsity: 84.2335%\n",
      "total_backward_count 1585980 real_backward_count 219436  13.836%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  2.048155/  2.111774, val:  72.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 74.4724%\n",
      "layer   3  Sparsity: 84.0969%\n",
      "total_backward_count 1595770 real_backward_count 220582  13.823%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  2.039334/  2.135395, val:  70.42%, val_best:  80.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4257%\n",
      "layer   2  Sparsity: 74.5156%\n",
      "layer   3  Sparsity: 84.3375%\n",
      "total_backward_count 1605560 real_backward_count 221651  13.805%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  2.056074/  2.116613, val:  72.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4210%\n",
      "layer   2  Sparsity: 74.7938%\n",
      "layer   3  Sparsity: 84.8956%\n",
      "total_backward_count 1615350 real_backward_count 222816  13.794%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  2.044231/  2.124415, val:  74.58%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4491%\n",
      "layer   2  Sparsity: 74.9009%\n",
      "layer   3  Sparsity: 84.8363%\n",
      "total_backward_count 1625140 real_backward_count 224001  13.783%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  2.044899/  2.112428, val:  70.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4731%\n",
      "layer   2  Sparsity: 74.6059%\n",
      "layer   3  Sparsity: 84.4466%\n",
      "total_backward_count 1634930 real_backward_count 225125  13.770%\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  2.044369/  2.120432, val:  55.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3998%\n",
      "layer   2  Sparsity: 74.5236%\n",
      "layer   3  Sparsity: 84.3032%\n",
      "total_backward_count 1644720 real_backward_count 226273  13.758%\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  2.038267/  2.114821, val:  55.42%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4671%\n",
      "layer   2  Sparsity: 74.5319%\n",
      "layer   3  Sparsity: 84.1254%\n",
      "total_backward_count 1654510 real_backward_count 227421  13.746%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  2.040179/  2.120940, val:  70.00%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4466%\n",
      "layer   2  Sparsity: 74.6533%\n",
      "layer   3  Sparsity: 84.7676%\n",
      "total_backward_count 1664300 real_backward_count 228497  13.729%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  2.047092/  2.116476, val:  65.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4414%\n",
      "layer   2  Sparsity: 74.7196%\n",
      "layer   3  Sparsity: 84.6389%\n",
      "total_backward_count 1674090 real_backward_count 229608  13.715%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  2.042542/  2.122900, val:  66.25%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3944%\n",
      "layer   2  Sparsity: 74.9240%\n",
      "layer   3  Sparsity: 84.9325%\n",
      "total_backward_count 1683880 real_backward_count 230713  13.701%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  2.049570/  2.114821, val:  64.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 74.9886%\n",
      "layer   3  Sparsity: 85.7012%\n",
      "total_backward_count 1693670 real_backward_count 231857  13.690%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  2.051710/  2.127030, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 74.9331%\n",
      "layer   3  Sparsity: 85.3227%\n",
      "total_backward_count 1703460 real_backward_count 232958  13.676%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  2.051289/  2.109306, val:  77.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4142%\n",
      "layer   2  Sparsity: 74.5662%\n",
      "layer   3  Sparsity: 84.7826%\n",
      "total_backward_count 1713250 real_backward_count 234042  13.661%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  2.048623/  2.104735, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3973%\n",
      "layer   2  Sparsity: 74.7192%\n",
      "layer   3  Sparsity: 85.2034%\n",
      "total_backward_count 1723040 real_backward_count 235128  13.646%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  2.046848/  2.109902, val:  78.33%, val_best:  82.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4291%\n",
      "layer   2  Sparsity: 74.8529%\n",
      "layer   3  Sparsity: 85.2434%\n",
      "total_backward_count 1732830 real_backward_count 236248  13.634%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  2.051268/  2.119829, val:  66.67%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 74.8965%\n",
      "layer   3  Sparsity: 85.4525%\n",
      "total_backward_count 1742620 real_backward_count 237343  13.620%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  2.054277/  2.126117, val:  72.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 74.7906%\n",
      "layer   3  Sparsity: 85.2370%\n",
      "total_backward_count 1752410 real_backward_count 238500  13.610%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  2.056847/  2.121090, val:  70.42%, val_best:  82.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4441%\n",
      "layer   2  Sparsity: 74.7816%\n",
      "layer   3  Sparsity: 85.0053%\n",
      "total_backward_count 1762200 real_backward_count 239631  13.598%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  2.048632/  2.113480, val:  68.33%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4087%\n",
      "layer   2  Sparsity: 74.7495%\n",
      "layer   3  Sparsity: 84.9077%\n",
      "total_backward_count 1771990 real_backward_count 240733  13.585%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  2.048887/  2.125426, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 74.6663%\n",
      "layer   3  Sparsity: 84.8586%\n",
      "total_backward_count 1781780 real_backward_count 241814  13.571%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  2.048659/  2.110676, val:  82.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 74.7716%\n",
      "layer   3  Sparsity: 84.6849%\n",
      "total_backward_count 1791570 real_backward_count 242861  13.556%\n",
      "fc layer 1 self.abs_max_out: 17261.0\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  2.048498/  2.114425, val:  65.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4260%\n",
      "layer   2  Sparsity: 74.7281%\n",
      "layer   3  Sparsity: 85.1198%\n",
      "total_backward_count 1801360 real_backward_count 243956  13.543%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  2.043549/  2.101592, val:  71.25%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4424%\n",
      "layer   2  Sparsity: 74.7376%\n",
      "layer   3  Sparsity: 84.8768%\n",
      "total_backward_count 1811150 real_backward_count 245027  13.529%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  2.033141/  2.117129, val:  69.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 74.6159%\n",
      "layer   3  Sparsity: 84.4822%\n",
      "total_backward_count 1820940 real_backward_count 246132  13.517%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  2.035891/  2.107218, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4092%\n",
      "layer   2  Sparsity: 74.8080%\n",
      "layer   3  Sparsity: 84.7994%\n",
      "total_backward_count 1830730 real_backward_count 247226  13.504%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  2.024561/  2.113133, val:  74.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4323%\n",
      "layer   2  Sparsity: 74.7958%\n",
      "layer   3  Sparsity: 84.2771%\n",
      "total_backward_count 1840520 real_backward_count 248332  13.492%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  2.031317/  2.106013, val:  67.92%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4132%\n",
      "layer   2  Sparsity: 74.4873%\n",
      "layer   3  Sparsity: 84.4636%\n",
      "total_backward_count 1850310 real_backward_count 249418  13.480%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  2.035580/  2.104882, val:  62.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 74.5075%\n",
      "layer   3  Sparsity: 84.8215%\n",
      "total_backward_count 1860100 real_backward_count 250493  13.467%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  2.035879/  2.096907, val:  70.83%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3947%\n",
      "layer   2  Sparsity: 74.4105%\n",
      "layer   3  Sparsity: 84.3741%\n",
      "total_backward_count 1869890 real_backward_count 251534  13.452%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  2.030012/  2.103918, val:  63.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4354%\n",
      "layer   2  Sparsity: 74.6190%\n",
      "layer   3  Sparsity: 84.1644%\n",
      "total_backward_count 1879680 real_backward_count 252617  13.439%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  2.027636/  2.113962, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 74.7096%\n",
      "layer   3  Sparsity: 84.1973%\n",
      "total_backward_count 1889470 real_backward_count 253716  13.428%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  2.033045/  2.109844, val:  72.50%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.4487%\n",
      "layer   2  Sparsity: 74.8001%\n",
      "layer   3  Sparsity: 84.9055%\n",
      "total_backward_count 1899260 real_backward_count 254758  13.414%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  2.036853/  2.102047, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4382%\n",
      "layer   2  Sparsity: 74.7935%\n",
      "layer   3  Sparsity: 84.6066%\n",
      "total_backward_count 1909050 real_backward_count 255820  13.400%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  2.031958/  2.116113, val:  61.67%, val_best:  82.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3872%\n",
      "layer   2  Sparsity: 74.6454%\n",
      "layer   3  Sparsity: 84.2495%\n",
      "total_backward_count 1918840 real_backward_count 256950  13.391%\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  2.036880/  2.106708, val:  69.17%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4464%\n",
      "layer   2  Sparsity: 74.8406%\n",
      "layer   3  Sparsity: 84.4329%\n",
      "total_backward_count 1928630 real_backward_count 257996  13.377%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  2.027281/  2.101966, val:  75.83%, val_best:  82.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.5142%\n",
      "layer   2  Sparsity: 74.8771%\n",
      "layer   3  Sparsity: 84.0276%\n",
      "total_backward_count 1938420 real_backward_count 259061  13.365%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  2.038042/  2.119091, val:  74.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 74.6453%\n",
      "layer   3  Sparsity: 84.6381%\n",
      "total_backward_count 1948210 real_backward_count 260151  13.353%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  2.036917/  2.121399, val:  64.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4394%\n",
      "layer   2  Sparsity: 74.6141%\n",
      "layer   3  Sparsity: 84.8802%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caab0f5555cc41e9850659c0215cab57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñá‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>2.03692</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.64167</td></tr><tr><td>val_loss</td><td>2.1214</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-164</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tu1jep1w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tu1jep1w</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_103737-tu1jep1w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ad2uuwr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_145817-8ad2uuwr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8ad2uuwr' target=\"_blank\">expert-sweep-169</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8ad2uuwr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8ad2uuwr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251118_145827_218', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 5, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 391.0\n",
      "lif layer 1 self.abs_max_v: 391.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 712.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 287.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 525.5\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "lif layer 2 self.abs_max_v: 1078.0\n",
      "fc layer 3 self.abs_max_out: 340.0\n",
      "fc layer 1 self.abs_max_out: 517.0\n",
      "lif layer 1 self.abs_max_v: 687.5\n",
      "fc layer 2 self.abs_max_out: 870.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 1 self.abs_max_out: 572.0\n",
      "fc layer 2 self.abs_max_out: 899.0\n",
      "lif layer 2 self.abs_max_v: 1448.5\n",
      "fc layer 1 self.abs_max_out: 912.0\n",
      "lif layer 1 self.abs_max_v: 912.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "lif layer 2 self.abs_max_v: 1568.5\n",
      "fc layer 1 self.abs_max_out: 1070.0\n",
      "lif layer 1 self.abs_max_v: 1070.0\n",
      "lif layer 2 self.abs_max_v: 1687.5\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "fc layer 3 self.abs_max_out: 520.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "lif layer 1 self.abs_max_v: 1251.0\n",
      "fc layer 1 self.abs_max_out: 1125.0\n",
      "lif layer 1 self.abs_max_v: 1303.5\n",
      "lif layer 1 self.abs_max_v: 1327.5\n",
      "lif layer 1 self.abs_max_v: 1374.0\n",
      "fc layer 1 self.abs_max_out: 1228.0\n",
      "lif layer 1 self.abs_max_v: 1424.5\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 1 self.abs_max_out: 1283.0\n",
      "lif layer 1 self.abs_max_v: 1717.0\n",
      "fc layer 1 self.abs_max_out: 1381.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "fc layer 1 self.abs_max_out: 1517.0\n",
      "lif layer 1 self.abs_max_v: 1822.5\n",
      "fc layer 1 self.abs_max_out: 1715.0\n",
      "lif layer 1 self.abs_max_v: 2037.5\n",
      "fc layer 1 self.abs_max_out: 2019.0\n",
      "lif layer 1 self.abs_max_v: 2319.0\n",
      "fc layer 1 self.abs_max_out: 2614.0\n",
      "lif layer 1 self.abs_max_v: 2935.5\n",
      "lif layer 2 self.abs_max_v: 1778.0\n",
      "fc layer 2 self.abs_max_out: 1164.0\n",
      "lif layer 2 self.abs_max_v: 1897.0\n",
      "fc layer 3 self.abs_max_out: 674.0\n",
      "fc layer 2 self.abs_max_out: 1295.0\n",
      "lif layer 2 self.abs_max_v: 2198.0\n",
      "fc layer 2 self.abs_max_out: 1302.0\n",
      "lif layer 2 self.abs_max_v: 2389.0\n",
      "lif layer 2 self.abs_max_v: 2426.5\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "lif layer 1 self.abs_max_v: 3007.5\n",
      "fc layer 2 self.abs_max_out: 1545.0\n",
      "lif layer 2 self.abs_max_v: 2676.5\n",
      "lif layer 2 self.abs_max_v: 2709.5\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1984.0\n",
      "lif layer 1 self.abs_max_v: 3344.0\n",
      "lif layer 1 self.abs_max_v: 4028.0\n",
      "lif layer 1 self.abs_max_v: 4133.5\n",
      "lif layer 1 self.abs_max_v: 4158.0\n",
      "lif layer 2 self.abs_max_v: 2749.5\n",
      "fc layer 1 self.abs_max_out: 2966.0\n",
      "lif layer 1 self.abs_max_v: 4337.5\n",
      "lif layer 2 self.abs_max_v: 2813.5\n",
      "lif layer 1 self.abs_max_v: 4510.5\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "fc layer 3 self.abs_max_out: 830.0\n",
      "fc layer 3 self.abs_max_out: 865.0\n",
      "fc layer 3 self.abs_max_out: 867.0\n",
      "lif layer 1 self.abs_max_v: 4580.5\n",
      "lif layer 1 self.abs_max_v: 4663.5\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 1 self.abs_max_out: 3042.0\n",
      "lif layer 1 self.abs_max_v: 4665.5\n",
      "lif layer 1 self.abs_max_v: 4763.0\n",
      "lif layer 1 self.abs_max_v: 4789.0\n",
      "lif layer 2 self.abs_max_v: 3207.5\n",
      "lif layer 1 self.abs_max_v: 4833.0\n",
      "lif layer 1 self.abs_max_v: 4917.0\n",
      "lif layer 1 self.abs_max_v: 5013.0\n",
      "fc layer 1 self.abs_max_out: 3101.0\n",
      "fc layer 1 self.abs_max_out: 3172.0\n",
      "lif layer 1 self.abs_max_v: 5367.5\n",
      "fc layer 1 self.abs_max_out: 3385.0\n",
      "fc layer 1 self.abs_max_out: 3527.0\n",
      "lif layer 1 self.abs_max_v: 6078.5\n",
      "fc layer 1 self.abs_max_out: 3582.0\n",
      "lif layer 2 self.abs_max_v: 3212.0\n",
      "lif layer 2 self.abs_max_v: 3216.5\n",
      "lif layer 2 self.abs_max_v: 3279.0\n",
      "lif layer 2 self.abs_max_v: 3436.5\n",
      "lif layer 2 self.abs_max_v: 3525.5\n",
      "fc layer 2 self.abs_max_out: 2041.0\n",
      "fc layer 2 self.abs_max_out: 2049.0\n",
      "lif layer 2 self.abs_max_v: 3543.5\n",
      "lif layer 2 self.abs_max_v: 3591.0\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "fc layer 3 self.abs_max_out: 1013.0\n",
      "lif layer 2 self.abs_max_v: 3688.5\n",
      "fc layer 3 self.abs_max_out: 1057.0\n",
      "fc layer 3 self.abs_max_out: 1063.0\n",
      "fc layer 3 self.abs_max_out: 1068.0\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "lif layer 2 self.abs_max_v: 3794.0\n",
      "fc layer 1 self.abs_max_out: 3810.0\n",
      "lif layer 1 self.abs_max_v: 6188.0\n",
      "lif layer 1 self.abs_max_v: 6206.0\n",
      "lif layer 1 self.abs_max_v: 6271.0\n",
      "lif layer 1 self.abs_max_v: 6385.0\n",
      "lif layer 1 self.abs_max_v: 6795.0\n",
      "fc layer 1 self.abs_max_out: 4020.0\n",
      "lif layer 1 self.abs_max_v: 6890.0\n",
      "lif layer 1 self.abs_max_v: 6973.0\n",
      "fc layer 1 self.abs_max_out: 4439.0\n",
      "lif layer 1 self.abs_max_v: 7707.0\n",
      "lif layer 1 self.abs_max_v: 7831.5\n",
      "fc layer 2 self.abs_max_out: 2181.0\n",
      "lif layer 2 self.abs_max_v: 3821.5\n",
      "lif layer 2 self.abs_max_v: 3947.0\n",
      "fc layer 3 self.abs_max_out: 1095.0\n",
      "fc layer 3 self.abs_max_out: 1108.0\n",
      "lif layer 2 self.abs_max_v: 3967.5\n",
      "fc layer 2 self.abs_max_out: 2215.0\n",
      "fc layer 3 self.abs_max_out: 1131.0\n",
      "fc layer 3 self.abs_max_out: 1181.0\n",
      "fc layer 2 self.abs_max_out: 2242.0\n",
      "fc layer 3 self.abs_max_out: 1232.0\n",
      "fc layer 3 self.abs_max_out: 1235.0\n",
      "fc layer 3 self.abs_max_out: 1252.0\n",
      "fc layer 3 self.abs_max_out: 1261.0\n",
      "fc layer 3 self.abs_max_out: 1323.0\n",
      "fc layer 2 self.abs_max_out: 2312.0\n",
      "fc layer 3 self.abs_max_out: 1324.0\n",
      "fc layer 3 self.abs_max_out: 1369.0\n",
      "lif layer 2 self.abs_max_v: 3978.5\n",
      "lif layer 2 self.abs_max_v: 4093.5\n",
      "lif layer 1 self.abs_max_v: 8007.5\n",
      "fc layer 1 self.abs_max_out: 4510.0\n",
      "lif layer 1 self.abs_max_v: 8344.0\n",
      "fc layer 2 self.abs_max_out: 2375.0\n",
      "fc layer 1 self.abs_max_out: 4558.0\n",
      "fc layer 1 self.abs_max_out: 4562.0\n",
      "lif layer 2 self.abs_max_v: 4176.5\n",
      "lif layer 2 self.abs_max_v: 4254.5\n",
      "fc layer 3 self.abs_max_out: 1373.0\n",
      "fc layer 3 self.abs_max_out: 1382.0\n",
      "fc layer 3 self.abs_max_out: 1398.0\n",
      "fc layer 3 self.abs_max_out: 1469.0\n",
      "fc layer 3 self.abs_max_out: 1487.0\n",
      "fc layer 2 self.abs_max_out: 2388.0\n",
      "lif layer 2 self.abs_max_v: 4315.5\n",
      "lif layer 2 self.abs_max_v: 4362.5\n",
      "fc layer 2 self.abs_max_out: 2491.0\n",
      "lif layer 2 self.abs_max_v: 4367.5\n",
      "lif layer 2 self.abs_max_v: 4413.0\n",
      "lif layer 2 self.abs_max_v: 4445.5\n",
      "lif layer 2 self.abs_max_v: 4605.5\n",
      "fc layer 1 self.abs_max_out: 5052.0\n",
      "lif layer 1 self.abs_max_v: 8833.0\n",
      "fc layer 1 self.abs_max_out: 5476.0\n",
      "lif layer 1 self.abs_max_v: 9771.0\n",
      "lif layer 1 self.abs_max_v: 10233.5\n",
      "lif layer 2 self.abs_max_v: 4687.0\n",
      "lif layer 2 self.abs_max_v: 4711.5\n",
      "lif layer 2 self.abs_max_v: 4807.0\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.432537/  1.884816, val:  30.83%, val_best:  30.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 71.2272%\n",
      "layer   3  Sparsity: 64.9297%\n",
      "total_backward_count 9790 real_backward_count 1293  13.207%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 5900.0\n",
      "lif layer 1 self.abs_max_v: 10991.0\n",
      "fc layer 3 self.abs_max_out: 1575.0\n",
      "fc layer 3 self.abs_max_out: 1603.0\n",
      "fc layer 2 self.abs_max_out: 2600.0\n",
      "fc layer 3 self.abs_max_out: 1651.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "fc layer 3 self.abs_max_out: 1797.0\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.316167/  1.785212, val:  30.42%, val_best:  30.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4660%\n",
      "layer   2  Sparsity: 73.3950%\n",
      "layer   3  Sparsity: 67.9215%\n",
      "total_backward_count 19580 real_backward_count 2475  12.640%\n",
      "lif layer 2 self.abs_max_v: 5029.0\n",
      "fc layer 2 self.abs_max_out: 2755.0\n",
      "fc layer 2 self.abs_max_out: 2790.0\n",
      "fc layer 1 self.abs_max_out: 6397.0\n",
      "lif layer 1 self.abs_max_v: 11606.0\n",
      "fc layer 2 self.abs_max_out: 2793.0\n",
      "fc layer 2 self.abs_max_out: 2950.0\n",
      "lif layer 2 self.abs_max_v: 5031.0\n",
      "lif layer 2 self.abs_max_v: 5117.0\n",
      "fc layer 2 self.abs_max_out: 3004.0\n",
      "fc layer 3 self.abs_max_out: 1921.0\n",
      "fc layer 2 self.abs_max_out: 3050.0\n",
      "lif layer 1 self.abs_max_v: 11631.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.255054/  1.708046, val:  37.50%, val_best:  37.50%, tr:  99.28%, tr_best:  99.90%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4776%\n",
      "layer   2  Sparsity: 73.1871%\n",
      "layer   3  Sparsity: 67.0698%\n",
      "total_backward_count 29370 real_backward_count 3663  12.472%\n",
      "fc layer 1 self.abs_max_out: 6446.0\n",
      "fc layer 1 self.abs_max_out: 6459.0\n",
      "lif layer 1 self.abs_max_v: 11944.5\n",
      "lif layer 2 self.abs_max_v: 5129.0\n",
      "lif layer 2 self.abs_max_v: 5414.0\n",
      "lif layer 2 self.abs_max_v: 5437.0\n",
      "fc layer 2 self.abs_max_out: 3134.0\n",
      "fc layer 1 self.abs_max_out: 6739.0\n",
      "fc layer 1 self.abs_max_out: 6812.0\n",
      "fc layer 1 self.abs_max_out: 6901.0\n",
      "fc layer 1 self.abs_max_out: 6915.0\n",
      "lif layer 1 self.abs_max_v: 12823.0\n",
      "lif layer 1 self.abs_max_v: 13019.5\n",
      "lif layer 1 self.abs_max_v: 13029.0\n",
      "lif layer 1 self.abs_max_v: 13234.5\n",
      "lif layer 1 self.abs_max_v: 13361.5\n",
      "lif layer 1 self.abs_max_v: 13425.0\n",
      "lif layer 2 self.abs_max_v: 5461.0\n",
      "lif layer 2 self.abs_max_v: 5491.5\n",
      "lif layer 2 self.abs_max_v: 5608.0\n",
      "lif layer 2 self.abs_max_v: 5704.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.189682/  1.705292, val:  37.50%, val_best:  37.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4286%\n",
      "layer   2  Sparsity: 75.6464%\n",
      "layer   3  Sparsity: 67.2391%\n",
      "total_backward_count 39160 real_backward_count 4885  12.474%\n",
      "lif layer 2 self.abs_max_v: 5713.0\n",
      "fc layer 3 self.abs_max_out: 1955.0\n",
      "fc layer 2 self.abs_max_out: 3225.0\n",
      "lif layer 2 self.abs_max_v: 5743.0\n",
      "lif layer 2 self.abs_max_v: 5889.5\n",
      "fc layer 1 self.abs_max_out: 7036.0\n",
      "fc layer 1 self.abs_max_out: 7651.0\n",
      "fc layer 1 self.abs_max_out: 8011.0\n",
      "fc layer 1 self.abs_max_out: 8012.0\n",
      "lif layer 1 self.abs_max_v: 13434.5\n",
      "lif layer 1 self.abs_max_v: 13907.5\n",
      "lif layer 1 self.abs_max_v: 14322.0\n",
      "lif layer 1 self.abs_max_v: 14614.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.165820/  1.668109, val:  40.42%, val_best:  40.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 76.1613%\n",
      "layer   3  Sparsity: 67.7248%\n",
      "total_backward_count 48950 real_backward_count 6101  12.464%\n",
      "fc layer 3 self.abs_max_out: 2037.0\n",
      "fc layer 3 self.abs_max_out: 2085.0\n",
      "lif layer 2 self.abs_max_v: 5967.0\n",
      "lif layer 2 self.abs_max_v: 6076.5\n",
      "lif layer 2 self.abs_max_v: 6117.5\n",
      "lif layer 2 self.abs_max_v: 6123.0\n",
      "lif layer 2 self.abs_max_v: 6127.5\n",
      "fc layer 2 self.abs_max_out: 3252.0\n",
      "fc layer 2 self.abs_max_out: 3444.0\n",
      "fc layer 1 self.abs_max_out: 8288.0\n",
      "lif layer 1 self.abs_max_v: 15061.5\n",
      "lif layer 1 self.abs_max_v: 15128.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.173055/  1.699171, val:  40.83%, val_best:  40.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 76.0301%\n",
      "layer   3  Sparsity: 69.5422%\n",
      "total_backward_count 58740 real_backward_count 7292  12.414%\n",
      "fc layer 2 self.abs_max_out: 3450.0\n",
      "fc layer 1 self.abs_max_out: 8846.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.167393/  1.630540, val:  47.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 76.3023%\n",
      "layer   3  Sparsity: 70.1994%\n",
      "total_backward_count 68530 real_backward_count 8475  12.367%\n",
      "lif layer 2 self.abs_max_v: 6167.5\n",
      "lif layer 2 self.abs_max_v: 6286.0\n",
      "lif layer 1 self.abs_max_v: 15218.0\n",
      "lif layer 1 self.abs_max_v: 15738.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.124323/  1.572635, val:  49.17%, val_best:  49.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4523%\n",
      "layer   2  Sparsity: 77.6369%\n",
      "layer   3  Sparsity: 70.4561%\n",
      "total_backward_count 78320 real_backward_count 9625  12.289%\n",
      "fc layer 1 self.abs_max_out: 9415.0\n",
      "lif layer 1 self.abs_max_v: 15877.0\n",
      "lif layer 1 self.abs_max_v: 16331.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.127252/  1.490314, val:  57.08%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4599%\n",
      "layer   2  Sparsity: 78.0454%\n",
      "layer   3  Sparsity: 71.3253%\n",
      "total_backward_count 88110 real_backward_count 10850  12.314%\n",
      "lif layer 1 self.abs_max_v: 16498.5\n",
      "lif layer 1 self.abs_max_v: 17019.0\n",
      "lif layer 1 self.abs_max_v: 17405.5\n",
      "lif layer 1 self.abs_max_v: 17669.0\n",
      "fc layer 1 self.abs_max_out: 9605.0\n",
      "lif layer 1 self.abs_max_v: 18439.5\n",
      "lif layer 1 self.abs_max_v: 18517.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.136513/  1.596726, val:  46.25%, val_best:  57.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4528%\n",
      "layer   2  Sparsity: 77.7688%\n",
      "layer   3  Sparsity: 72.0854%\n",
      "total_backward_count 97900 real_backward_count 11990  12.247%\n",
      "lif layer 2 self.abs_max_v: 6430.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.133610/  1.638328, val:  42.92%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4366%\n",
      "layer   2  Sparsity: 78.1649%\n",
      "layer   3  Sparsity: 72.3465%\n",
      "total_backward_count 107690 real_backward_count 13181  12.240%\n",
      "fc layer 1 self.abs_max_out: 9993.0\n",
      "lif layer 1 self.abs_max_v: 18521.5\n",
      "lif layer 1 self.abs_max_v: 18745.0\n",
      "fc layer 1 self.abs_max_out: 10089.0\n",
      "lif layer 1 self.abs_max_v: 19461.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.145256/  1.588805, val:  44.17%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4093%\n",
      "layer   2  Sparsity: 78.3618%\n",
      "layer   3  Sparsity: 72.8844%\n",
      "total_backward_count 117480 real_backward_count 14294  12.167%\n",
      "fc layer 1 self.abs_max_out: 10365.0\n",
      "fc layer 3 self.abs_max_out: 2131.0\n",
      "fc layer 2 self.abs_max_out: 3451.0\n",
      "lif layer 1 self.abs_max_v: 19541.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.111616/  1.540724, val:  48.33%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4209%\n",
      "layer   2  Sparsity: 78.7237%\n",
      "layer   3  Sparsity: 73.9486%\n",
      "total_backward_count 127270 real_backward_count 15424  12.119%\n",
      "fc layer 2 self.abs_max_out: 3489.0\n",
      "fc layer 2 self.abs_max_out: 3510.0\n",
      "lif layer 2 self.abs_max_v: 6451.0\n",
      "fc layer 1 self.abs_max_out: 10526.0\n",
      "lif layer 1 self.abs_max_v: 19801.0\n",
      "lif layer 1 self.abs_max_v: 20155.5\n",
      "fc layer 1 self.abs_max_out: 10853.0\n",
      "lif layer 1 self.abs_max_v: 20931.0\n",
      "fc layer 2 self.abs_max_out: 3540.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.125147/  1.532986, val:  45.42%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 78.3613%\n",
      "layer   3  Sparsity: 72.9771%\n",
      "total_backward_count 137060 real_backward_count 16498  12.037%\n",
      "lif layer 2 self.abs_max_v: 6660.5\n",
      "lif layer 2 self.abs_max_v: 6789.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.116114/  1.553806, val:  52.92%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 78.2841%\n",
      "layer   3  Sparsity: 73.0961%\n",
      "total_backward_count 146850 real_backward_count 17573  11.967%\n",
      "fc layer 2 self.abs_max_out: 3584.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.125935/  1.551201, val:  49.17%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 78.6412%\n",
      "layer   3  Sparsity: 73.2943%\n",
      "total_backward_count 156640 real_backward_count 18727  11.955%\n",
      "fc layer 2 self.abs_max_out: 3722.0\n",
      "lif layer 2 self.abs_max_v: 6816.5\n",
      "fc layer 1 self.abs_max_out: 10887.0\n",
      "lif layer 1 self.abs_max_v: 20955.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.111003/  1.503344, val:  50.00%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 78.3180%\n",
      "layer   3  Sparsity: 74.6144%\n",
      "total_backward_count 166430 real_backward_count 19838  11.920%\n",
      "lif layer 2 self.abs_max_v: 6841.5\n",
      "fc layer 1 self.abs_max_out: 11247.0\n",
      "lif layer 1 self.abs_max_v: 21022.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.140154/  1.524394, val:  55.83%, val_best:  57.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 77.4989%\n",
      "layer   3  Sparsity: 74.5470%\n",
      "total_backward_count 176220 real_backward_count 20965  11.897%\n",
      "lif layer 2 self.abs_max_v: 6921.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.159635/  1.513742, val:  55.83%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4203%\n",
      "layer   2  Sparsity: 77.5475%\n",
      "layer   3  Sparsity: 74.8037%\n",
      "total_backward_count 186010 real_backward_count 22077  11.869%\n",
      "fc layer 2 self.abs_max_out: 3792.0\n",
      "lif layer 2 self.abs_max_v: 7010.5\n",
      "lif layer 2 self.abs_max_v: 7052.0\n",
      "fc layer 2 self.abs_max_out: 3984.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.131958/  1.632012, val:  35.83%, val_best:  57.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4281%\n",
      "layer   2  Sparsity: 77.2475%\n",
      "layer   3  Sparsity: 74.8726%\n",
      "total_backward_count 195800 real_backward_count 23146  11.821%\n",
      "lif layer 2 self.abs_max_v: 7403.0\n",
      "lif layer 1 self.abs_max_v: 21167.5\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.162280/  1.610325, val:  44.58%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 77.1269%\n",
      "layer   3  Sparsity: 75.7039%\n",
      "total_backward_count 205590 real_backward_count 24198  11.770%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.170305/  1.580928, val:  46.67%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4473%\n",
      "layer   2  Sparsity: 77.1641%\n",
      "layer   3  Sparsity: 75.7773%\n",
      "total_backward_count 215380 real_backward_count 25244  11.721%\n",
      "lif layer 1 self.abs_max_v: 21320.5\n",
      "lif layer 1 self.abs_max_v: 21696.5\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.121453/  1.462411, val:  56.25%, val_best:  57.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4432%\n",
      "layer   2  Sparsity: 76.7740%\n",
      "layer   3  Sparsity: 74.5389%\n",
      "total_backward_count 225170 real_backward_count 26330  11.693%\n",
      "fc layer 2 self.abs_max_out: 4215.0\n",
      "lif layer 2 self.abs_max_v: 7515.0\n",
      "fc layer 2 self.abs_max_out: 4431.0\n",
      "lif layer 2 self.abs_max_v: 8028.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.109813/  1.518758, val:  56.25%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3963%\n",
      "layer   2  Sparsity: 76.7570%\n",
      "layer   3  Sparsity: 72.8558%\n",
      "total_backward_count 234960 real_backward_count 27442  11.679%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.108721/  1.484374, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4588%\n",
      "layer   2  Sparsity: 76.6979%\n",
      "layer   3  Sparsity: 73.8808%\n",
      "total_backward_count 244750 real_backward_count 28469  11.632%\n",
      "lif layer 1 self.abs_max_v: 21732.5\n",
      "fc layer 1 self.abs_max_out: 11261.0\n",
      "lif layer 1 self.abs_max_v: 22127.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.087088/  1.426414, val:  58.33%, val_best:  64.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4221%\n",
      "layer   2  Sparsity: 77.4715%\n",
      "layer   3  Sparsity: 73.1010%\n",
      "total_backward_count 254540 real_backward_count 29613  11.634%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.093922/  1.490894, val:  58.33%, val_best:  64.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 76.1411%\n",
      "layer   3  Sparsity: 72.4158%\n",
      "total_backward_count 264330 real_backward_count 30725  11.624%\n",
      "fc layer 1 self.abs_max_out: 11316.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.051352/  1.401358, val:  61.67%, val_best:  64.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3871%\n",
      "layer   2  Sparsity: 76.1874%\n",
      "layer   3  Sparsity: 70.3393%\n",
      "total_backward_count 274120 real_backward_count 31774  11.591%\n",
      "fc layer 3 self.abs_max_out: 2167.0\n",
      "fc layer 3 self.abs_max_out: 2190.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.038038/  1.511703, val:  48.33%, val_best:  64.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4363%\n",
      "layer   2  Sparsity: 76.9195%\n",
      "layer   3  Sparsity: 72.3347%\n",
      "total_backward_count 283910 real_backward_count 32792  11.550%\n",
      "fc layer 1 self.abs_max_out: 11456.0\n",
      "lif layer 1 self.abs_max_v: 22169.5\n",
      "fc layer 1 self.abs_max_out: 11709.0\n",
      "lif layer 1 self.abs_max_v: 22794.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.069530/  1.438421, val:  57.08%, val_best:  64.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 77.2609%\n",
      "layer   3  Sparsity: 73.0340%\n",
      "total_backward_count 293700 real_backward_count 33845  11.524%\n",
      "fc layer 3 self.abs_max_out: 2191.0\n",
      "fc layer 3 self.abs_max_out: 2222.0\n",
      "fc layer 1 self.abs_max_out: 11951.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.053102/  1.430428, val:  55.83%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4707%\n",
      "layer   2  Sparsity: 76.9686%\n",
      "layer   3  Sparsity: 72.0872%\n",
      "total_backward_count 303490 real_backward_count 34875  11.491%\n",
      "fc layer 1 self.abs_max_out: 12137.0\n",
      "lif layer 1 self.abs_max_v: 22971.5\n",
      "lif layer 1 self.abs_max_v: 23227.0\n",
      "lif layer 1 self.abs_max_v: 23598.5\n",
      "fc layer 1 self.abs_max_out: 12363.0\n",
      "lif layer 1 self.abs_max_v: 24162.5\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.039797/  1.535087, val:  50.00%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 77.0594%\n",
      "layer   3  Sparsity: 71.7736%\n",
      "total_backward_count 313280 real_backward_count 35961  11.479%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.017523/  1.484237, val:  56.25%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4215%\n",
      "layer   2  Sparsity: 77.3508%\n",
      "layer   3  Sparsity: 70.8588%\n",
      "total_backward_count 323070 real_backward_count 36979  11.446%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.021301/  1.482757, val:  47.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4620%\n",
      "layer   2  Sparsity: 77.6752%\n",
      "layer   3  Sparsity: 71.7142%\n",
      "total_backward_count 332860 real_backward_count 38065  11.436%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  0.978503/  1.426428, val:  50.83%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4190%\n",
      "layer   2  Sparsity: 77.8044%\n",
      "layer   3  Sparsity: 70.6949%\n",
      "total_backward_count 342650 real_backward_count 39079  11.405%\n",
      "fc layer 3 self.abs_max_out: 2253.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  0.933699/  1.462522, val:  42.08%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4137%\n",
      "layer   2  Sparsity: 78.9792%\n",
      "layer   3  Sparsity: 70.2865%\n",
      "total_backward_count 352440 real_backward_count 40100  11.378%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  0.909967/  1.323741, val:  57.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4756%\n",
      "layer   2  Sparsity: 78.8401%\n",
      "layer   3  Sparsity: 70.5526%\n",
      "total_backward_count 362230 real_backward_count 41083  11.342%\n",
      "fc layer 3 self.abs_max_out: 2302.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  0.903871/  1.342802, val:  55.00%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4652%\n",
      "layer   2  Sparsity: 78.0832%\n",
      "layer   3  Sparsity: 70.4419%\n",
      "total_backward_count 372020 real_backward_count 42045  11.302%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  0.937864/  1.387420, val:  61.67%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3838%\n",
      "layer   2  Sparsity: 77.1014%\n",
      "layer   3  Sparsity: 69.9846%\n",
      "total_backward_count 381810 real_backward_count 43055  11.277%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  0.926735/  1.405619, val:  52.50%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 76.7852%\n",
      "layer   3  Sparsity: 70.1961%\n",
      "total_backward_count 391600 real_backward_count 44024  11.242%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  0.937725/  1.368266, val:  57.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4060%\n",
      "layer   2  Sparsity: 77.2309%\n",
      "layer   3  Sparsity: 69.5048%\n",
      "total_backward_count 401390 real_backward_count 44982  11.207%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  0.926747/  1.392621, val:  59.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4285%\n",
      "layer   2  Sparsity: 77.0215%\n",
      "layer   3  Sparsity: 70.2317%\n",
      "total_backward_count 411180 real_backward_count 45973  11.181%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  0.927084/  1.368953, val:  60.42%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 88.4793%\n",
      "layer   2  Sparsity: 76.2393%\n",
      "layer   3  Sparsity: 69.1991%\n",
      "total_backward_count 420970 real_backward_count 46976  11.159%\n",
      "fc layer 1 self.abs_max_out: 12366.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  0.902096/  1.383415, val:  57.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4836%\n",
      "layer   2  Sparsity: 77.2657%\n",
      "layer   3  Sparsity: 69.3673%\n",
      "total_backward_count 430760 real_backward_count 47939  11.129%\n",
      "fc layer 3 self.abs_max_out: 2343.0\n",
      "fc layer 3 self.abs_max_out: 2366.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  0.883506/  1.331519, val:  59.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4907%\n",
      "layer   2  Sparsity: 77.7195%\n",
      "layer   3  Sparsity: 69.1422%\n",
      "total_backward_count 440550 real_backward_count 48950  11.111%\n",
      "fc layer 3 self.abs_max_out: 2381.0\n",
      "fc layer 3 self.abs_max_out: 2494.0\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  0.894410/  1.346805, val:  54.17%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 78.1157%\n",
      "layer   3  Sparsity: 68.3567%\n",
      "total_backward_count 450340 real_backward_count 49939  11.089%\n",
      "fc layer 1 self.abs_max_out: 12477.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  0.880594/  1.393248, val:  50.83%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4026%\n",
      "layer   2  Sparsity: 78.6110%\n",
      "layer   3  Sparsity: 68.2447%\n",
      "total_backward_count 460130 real_backward_count 50908  11.064%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  0.827330/  1.477335, val:  42.50%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4800%\n",
      "layer   2  Sparsity: 78.9966%\n",
      "layer   3  Sparsity: 67.8411%\n",
      "total_backward_count 469920 real_backward_count 51873  11.039%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  0.852848/  1.377923, val:  53.33%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3908%\n",
      "layer   2  Sparsity: 78.9285%\n",
      "layer   3  Sparsity: 68.3262%\n",
      "total_backward_count 479710 real_backward_count 52870  11.021%\n",
      "fc layer 1 self.abs_max_out: 13183.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  0.853639/  1.356717, val:  60.00%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 79.1959%\n",
      "layer   3  Sparsity: 69.2360%\n",
      "total_backward_count 489500 real_backward_count 53898  11.011%\n",
      "fc layer 2 self.abs_max_out: 4497.0\n",
      "lif layer 2 self.abs_max_v: 8367.0\n",
      "lif layer 2 self.abs_max_v: 8541.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  0.877909/  1.279392, val:  61.67%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4516%\n",
      "layer   2  Sparsity: 78.7096%\n",
      "layer   3  Sparsity: 69.5248%\n",
      "total_backward_count 499290 real_backward_count 54906  10.997%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  0.860627/  1.305482, val:  62.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4048%\n",
      "layer   2  Sparsity: 78.5909%\n",
      "layer   3  Sparsity: 68.9962%\n",
      "total_backward_count 509080 real_backward_count 55905  10.982%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  0.886764/  1.405598, val:  52.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4455%\n",
      "layer   2  Sparsity: 78.1812%\n",
      "layer   3  Sparsity: 69.6220%\n",
      "total_backward_count 518870 real_backward_count 56920  10.970%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  0.881019/  1.351195, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 78.2438%\n",
      "layer   3  Sparsity: 69.6638%\n",
      "total_backward_count 528660 real_backward_count 57902  10.953%\n",
      "fc layer 1 self.abs_max_out: 13268.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  0.870931/  1.348614, val:  62.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4605%\n",
      "layer   2  Sparsity: 78.7926%\n",
      "layer   3  Sparsity: 68.8111%\n",
      "total_backward_count 538450 real_backward_count 58828  10.925%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  0.846670/  1.340702, val:  57.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4457%\n",
      "layer   2  Sparsity: 78.6766%\n",
      "layer   3  Sparsity: 68.8673%\n",
      "total_backward_count 548240 real_backward_count 59797  10.907%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  0.839816/  1.466671, val:  50.00%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4068%\n",
      "layer   2  Sparsity: 78.5177%\n",
      "layer   3  Sparsity: 68.5263%\n",
      "total_backward_count 558030 real_backward_count 60813  10.898%\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  0.869215/  1.256659, val:  62.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4583%\n",
      "layer   2  Sparsity: 78.3395%\n",
      "layer   3  Sparsity: 68.3810%\n",
      "total_backward_count 567820 real_backward_count 61798  10.883%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  0.828850/  1.293745, val:  68.33%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4401%\n",
      "layer   2  Sparsity: 78.5071%\n",
      "layer   3  Sparsity: 68.6437%\n",
      "total_backward_count 577610 real_backward_count 62713  10.857%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  0.832749/  1.479290, val:  45.83%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4405%\n",
      "layer   2  Sparsity: 78.7795%\n",
      "layer   3  Sparsity: 69.5978%\n",
      "total_backward_count 587400 real_backward_count 63675  10.840%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  0.844140/  1.385066, val:  50.83%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4515%\n",
      "layer   2  Sparsity: 78.9194%\n",
      "layer   3  Sparsity: 68.4054%\n",
      "total_backward_count 597190 real_backward_count 64631  10.823%\n",
      "fc layer 3 self.abs_max_out: 2556.0\n",
      "fc layer 3 self.abs_max_out: 2598.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  0.812953/  1.190785, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 79.1762%\n",
      "layer   3  Sparsity: 68.2961%\n",
      "total_backward_count 606980 real_backward_count 65624  10.812%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  0.789670/  1.417926, val:  47.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4370%\n",
      "layer   2  Sparsity: 78.9207%\n",
      "layer   3  Sparsity: 67.9694%\n",
      "total_backward_count 616770 real_backward_count 66582  10.795%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  0.788058/  1.411816, val:  51.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4600%\n",
      "layer   2  Sparsity: 78.6937%\n",
      "layer   3  Sparsity: 67.2285%\n",
      "total_backward_count 626560 real_backward_count 67485  10.771%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  0.798678/  1.333326, val:  61.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4439%\n",
      "layer   2  Sparsity: 78.8554%\n",
      "layer   3  Sparsity: 67.9418%\n",
      "total_backward_count 636350 real_backward_count 68404  10.749%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  0.753046/  1.203061, val:  65.00%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4351%\n",
      "layer   2  Sparsity: 78.9956%\n",
      "layer   3  Sparsity: 66.3201%\n",
      "total_backward_count 646140 real_backward_count 69335  10.731%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  0.759156/  1.266620, val:  61.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4253%\n",
      "layer   2  Sparsity: 79.6486%\n",
      "layer   3  Sparsity: 66.7543%\n",
      "total_backward_count 655930 real_backward_count 70266  10.712%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  0.773848/  1.210916, val:  65.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4125%\n",
      "layer   2  Sparsity: 79.7329%\n",
      "layer   3  Sparsity: 68.6412%\n",
      "total_backward_count 665720 real_backward_count 71174  10.691%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  0.762107/  1.218234, val:  68.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 79.8768%\n",
      "layer   3  Sparsity: 68.0410%\n",
      "total_backward_count 675510 real_backward_count 72100  10.673%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  0.781508/  1.402536, val:  52.92%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4239%\n",
      "layer   2  Sparsity: 80.2768%\n",
      "layer   3  Sparsity: 68.1120%\n",
      "total_backward_count 685300 real_backward_count 72999  10.652%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  0.791956/  1.269237, val:  64.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4428%\n",
      "layer   2  Sparsity: 79.9651%\n",
      "layer   3  Sparsity: 68.5694%\n",
      "total_backward_count 695090 real_backward_count 73916  10.634%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  0.740349/  1.307843, val:  53.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4284%\n",
      "layer   2  Sparsity: 79.8180%\n",
      "layer   3  Sparsity: 66.6767%\n",
      "total_backward_count 704880 real_backward_count 74807  10.613%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  0.746237/  1.215088, val:  68.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4367%\n",
      "layer   2  Sparsity: 79.6460%\n",
      "layer   3  Sparsity: 67.0646%\n",
      "total_backward_count 714670 real_backward_count 75737  10.597%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  0.725892/  1.350599, val:  59.17%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4277%\n",
      "layer   2  Sparsity: 80.0208%\n",
      "layer   3  Sparsity: 67.3418%\n",
      "total_backward_count 724460 real_backward_count 76638  10.579%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  0.730289/  1.274705, val:  59.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 80.3598%\n",
      "layer   3  Sparsity: 66.7314%\n",
      "total_backward_count 734250 real_backward_count 77595  10.568%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  0.708286/  1.251495, val:  56.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4329%\n",
      "layer   2  Sparsity: 80.4697%\n",
      "layer   3  Sparsity: 66.7922%\n",
      "total_backward_count 744040 real_backward_count 78437  10.542%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  0.689769/  1.180001, val:  67.50%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4512%\n",
      "layer   2  Sparsity: 79.6664%\n",
      "layer   3  Sparsity: 65.5054%\n",
      "total_backward_count 753830 real_backward_count 79321  10.522%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  0.678196/  1.266365, val:  61.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4431%\n",
      "layer   2  Sparsity: 80.6425%\n",
      "layer   3  Sparsity: 65.7394%\n",
      "total_backward_count 763620 real_backward_count 80211  10.504%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  0.681587/  1.248708, val:  61.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4742%\n",
      "layer   2  Sparsity: 80.8126%\n",
      "layer   3  Sparsity: 67.3013%\n",
      "total_backward_count 773410 real_backward_count 81070  10.482%\n",
      "lif layer 1 self.abs_max_v: 24268.0\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  0.686546/  1.236394, val:  61.25%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4255%\n",
      "layer   2  Sparsity: 81.0272%\n",
      "layer   3  Sparsity: 66.4500%\n",
      "total_backward_count 783200 real_backward_count 81978  10.467%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  0.718823/  1.143921, val:  72.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4084%\n",
      "layer   2  Sparsity: 80.5171%\n",
      "layer   3  Sparsity: 66.6147%\n",
      "total_backward_count 792990 real_backward_count 82901  10.454%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  0.713132/  1.364136, val:  47.08%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4316%\n",
      "layer   2  Sparsity: 80.1628%\n",
      "layer   3  Sparsity: 67.5389%\n",
      "total_backward_count 802780 real_backward_count 83821  10.441%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  0.748718/  1.171173, val:  67.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4212%\n",
      "layer   2  Sparsity: 80.1448%\n",
      "layer   3  Sparsity: 67.8960%\n",
      "total_backward_count 812570 real_backward_count 84758  10.431%\n",
      "fc layer 1 self.abs_max_out: 13319.0\n",
      "lif layer 1 self.abs_max_v: 24527.5\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  0.724657/  1.308663, val:  53.33%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3966%\n",
      "layer   2  Sparsity: 79.6963%\n",
      "layer   3  Sparsity: 68.8575%\n",
      "total_backward_count 822360 real_backward_count 85711  10.423%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  0.741800/  1.250926, val:  64.58%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4206%\n",
      "layer   2  Sparsity: 80.1134%\n",
      "layer   3  Sparsity: 67.8327%\n",
      "total_backward_count 832150 real_backward_count 86632  10.411%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  0.734463/  1.201777, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4530%\n",
      "layer   2  Sparsity: 80.3388%\n",
      "layer   3  Sparsity: 66.0485%\n",
      "total_backward_count 841940 real_backward_count 87554  10.399%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  0.703558/  1.305326, val:  57.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4248%\n",
      "layer   2  Sparsity: 80.5424%\n",
      "layer   3  Sparsity: 65.6520%\n",
      "total_backward_count 851730 real_backward_count 88442  10.384%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  0.735506/  1.211219, val:  65.00%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3822%\n",
      "layer   2  Sparsity: 80.5021%\n",
      "layer   3  Sparsity: 66.8339%\n",
      "total_backward_count 861520 real_backward_count 89398  10.377%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  0.704323/  1.126384, val:  71.67%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3983%\n",
      "layer   2  Sparsity: 80.9423%\n",
      "layer   3  Sparsity: 67.5737%\n",
      "total_backward_count 871310 real_backward_count 90303  10.364%\n",
      "fc layer 1 self.abs_max_out: 13520.0\n",
      "lif layer 1 self.abs_max_v: 24957.5\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  0.719403/  1.162045, val:  70.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4472%\n",
      "layer   2  Sparsity: 80.7405%\n",
      "layer   3  Sparsity: 67.7394%\n",
      "total_backward_count 881100 real_backward_count 91253  10.357%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  0.710043/  1.318488, val:  59.58%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 80.6643%\n",
      "layer   3  Sparsity: 67.4548%\n",
      "total_backward_count 890890 real_backward_count 92177  10.347%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  0.715377/  1.171534, val:  68.33%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3989%\n",
      "layer   2  Sparsity: 80.6209%\n",
      "layer   3  Sparsity: 66.5338%\n",
      "total_backward_count 900680 real_backward_count 93042  10.330%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  0.710551/  1.190391, val:  61.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4502%\n",
      "layer   2  Sparsity: 80.4464%\n",
      "layer   3  Sparsity: 68.2382%\n",
      "total_backward_count 910470 real_backward_count 93959  10.320%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  0.700226/  1.249648, val:  61.25%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 88.4451%\n",
      "layer   2  Sparsity: 80.3994%\n",
      "layer   3  Sparsity: 67.5171%\n",
      "total_backward_count 920260 real_backward_count 94911  10.313%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  0.698981/  1.233319, val:  61.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.59 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 88.4670%\n",
      "layer   2  Sparsity: 80.4797%\n",
      "layer   3  Sparsity: 66.9441%\n",
      "total_backward_count 930050 real_backward_count 95817  10.302%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  0.712333/  1.192770, val:  69.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.02 seconds, 1.15 minutes\n",
      "layer   1  Sparsity: 88.4638%\n",
      "layer   2  Sparsity: 80.9514%\n",
      "layer   3  Sparsity: 67.5235%\n",
      "total_backward_count 939840 real_backward_count 96720  10.291%\n",
      "fc layer 1 self.abs_max_out: 13550.0\n",
      "lif layer 1 self.abs_max_v: 25010.5\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  0.676292/  1.189873, val:  65.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.68 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 88.4033%\n",
      "layer   2  Sparsity: 81.0492%\n",
      "layer   3  Sparsity: 67.2658%\n",
      "total_backward_count 949630 real_backward_count 97647  10.283%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  0.694916/  1.131916, val:  70.83%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 88.4446%\n",
      "layer   2  Sparsity: 80.9916%\n",
      "layer   3  Sparsity: 67.0079%\n",
      "total_backward_count 959420 real_backward_count 98550  10.272%\n",
      "fc layer 1 self.abs_max_out: 13584.0\n",
      "lif layer 1 self.abs_max_v: 25077.5\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  0.671689/  1.265015, val:  57.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.73 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 80.8765%\n",
      "layer   3  Sparsity: 68.0104%\n",
      "total_backward_count 969210 real_backward_count 99435  10.259%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  0.702525/  1.171161, val:  69.17%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4208%\n",
      "layer   2  Sparsity: 81.3761%\n",
      "layer   3  Sparsity: 68.0595%\n",
      "total_backward_count 979000 real_backward_count 100329  10.248%\n",
      "fc layer 1 self.abs_max_out: 13680.0\n",
      "lif layer 1 self.abs_max_v: 25260.5\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  0.711253/  1.267819, val:  60.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4256%\n",
      "layer   2  Sparsity: 80.9151%\n",
      "layer   3  Sparsity: 68.4824%\n",
      "total_backward_count 988790 real_backward_count 101221  10.237%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  0.710962/  1.188887, val:  70.83%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4261%\n",
      "layer   2  Sparsity: 80.2905%\n",
      "layer   3  Sparsity: 67.1502%\n",
      "total_backward_count 998580 real_backward_count 102105  10.225%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  0.692851/  1.199435, val:  60.00%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4798%\n",
      "layer   2  Sparsity: 80.0102%\n",
      "layer   3  Sparsity: 64.6081%\n",
      "total_backward_count 1008370 real_backward_count 102983  10.213%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  0.696723/  1.122666, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4214%\n",
      "layer   2  Sparsity: 79.8082%\n",
      "layer   3  Sparsity: 66.5773%\n",
      "total_backward_count 1018160 real_backward_count 103855  10.200%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  0.671525/  1.258280, val:  60.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4603%\n",
      "layer   2  Sparsity: 79.4773%\n",
      "layer   3  Sparsity: 65.4217%\n",
      "total_backward_count 1027950 real_backward_count 104713  10.187%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  0.676756/  1.167888, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4458%\n",
      "layer   2  Sparsity: 79.7326%\n",
      "layer   3  Sparsity: 66.2676%\n",
      "total_backward_count 1037740 real_backward_count 105600  10.176%\n",
      "fc layer 1 self.abs_max_out: 13773.0\n",
      "lif layer 1 self.abs_max_v: 25432.0\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  0.663072/  1.140509, val:  68.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4293%\n",
      "layer   2  Sparsity: 79.7678%\n",
      "layer   3  Sparsity: 67.2370%\n",
      "total_backward_count 1047530 real_backward_count 106440  10.161%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  0.683998/  1.196009, val:  63.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 79.0637%\n",
      "layer   3  Sparsity: 66.2932%\n",
      "total_backward_count 1057320 real_backward_count 107324  10.151%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  0.670033/  1.189914, val:  68.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4353%\n",
      "layer   2  Sparsity: 79.2159%\n",
      "layer   3  Sparsity: 66.7633%\n",
      "total_backward_count 1067110 real_backward_count 108222  10.142%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  0.657501/  1.246977, val:  64.17%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3850%\n",
      "layer   2  Sparsity: 78.6726%\n",
      "layer   3  Sparsity: 66.6223%\n",
      "total_backward_count 1076900 real_backward_count 109074  10.129%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  0.646591/  1.224272, val:  65.83%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4796%\n",
      "layer   2  Sparsity: 78.3862%\n",
      "layer   3  Sparsity: 67.6004%\n",
      "total_backward_count 1086690 real_backward_count 109936  10.117%\n",
      "fc layer 3 self.abs_max_out: 2647.0\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  0.669153/  1.182547, val:  60.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 78.4002%\n",
      "layer   3  Sparsity: 67.6248%\n",
      "total_backward_count 1096480 real_backward_count 110730  10.099%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  0.618819/  1.171092, val:  62.50%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4570%\n",
      "layer   2  Sparsity: 79.4440%\n",
      "layer   3  Sparsity: 66.6076%\n",
      "total_backward_count 1106270 real_backward_count 111555  10.084%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  0.630582/  1.174747, val:  66.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3622%\n",
      "layer   2  Sparsity: 79.5416%\n",
      "layer   3  Sparsity: 66.7969%\n",
      "total_backward_count 1116060 real_backward_count 112351  10.067%\n",
      "fc layer 3 self.abs_max_out: 2651.0\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  0.631394/  1.179645, val:  63.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4309%\n",
      "layer   2  Sparsity: 79.0398%\n",
      "layer   3  Sparsity: 66.4668%\n",
      "total_backward_count 1125850 real_backward_count 113196  10.054%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  0.657453/  1.199928, val:  66.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4430%\n",
      "layer   2  Sparsity: 78.6576%\n",
      "layer   3  Sparsity: 67.0995%\n",
      "total_backward_count 1135640 real_backward_count 114058  10.043%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  0.641016/  1.107661, val:  65.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 79.0348%\n",
      "layer   3  Sparsity: 66.8286%\n",
      "total_backward_count 1145430 real_backward_count 114869  10.028%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  0.665412/  1.143576, val:  71.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.4663%\n",
      "layer   2  Sparsity: 78.6494%\n",
      "layer   3  Sparsity: 67.9447%\n",
      "total_backward_count 1155220 real_backward_count 115711  10.016%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  0.676527/  1.271463, val:  54.58%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4193%\n",
      "layer   2  Sparsity: 78.7721%\n",
      "layer   3  Sparsity: 68.2300%\n",
      "total_backward_count 1165010 real_backward_count 116561  10.005%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  0.687815/  1.233893, val:  62.92%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4288%\n",
      "layer   2  Sparsity: 78.3119%\n",
      "layer   3  Sparsity: 67.6516%\n",
      "total_backward_count 1174800 real_backward_count 117464   9.999%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  0.669855/  1.184769, val:  71.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4224%\n",
      "layer   2  Sparsity: 78.7864%\n",
      "layer   3  Sparsity: 67.3496%\n",
      "total_backward_count 1184590 real_backward_count 118325   9.989%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.634266/  1.123444, val:  62.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4552%\n",
      "layer   2  Sparsity: 79.3389%\n",
      "layer   3  Sparsity: 66.1427%\n",
      "total_backward_count 1194380 real_backward_count 119128   9.974%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.660249/  1.223262, val:  63.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4233%\n",
      "layer   2  Sparsity: 78.7357%\n",
      "layer   3  Sparsity: 67.6226%\n",
      "total_backward_count 1204170 real_backward_count 119996   9.965%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  0.651875/  1.081117, val:  71.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 79.1258%\n",
      "layer   3  Sparsity: 67.7063%\n",
      "total_backward_count 1213960 real_backward_count 120851   9.955%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  0.652933/  1.193787, val:  67.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4392%\n",
      "layer   2  Sparsity: 79.3404%\n",
      "layer   3  Sparsity: 69.2993%\n",
      "total_backward_count 1223750 real_backward_count 121711   9.946%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  0.666966/  1.130961, val:  72.08%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3975%\n",
      "layer   2  Sparsity: 78.9982%\n",
      "layer   3  Sparsity: 68.9666%\n",
      "total_backward_count 1233540 real_backward_count 122565   9.936%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.653306/  1.335861, val:  56.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4549%\n",
      "layer   2  Sparsity: 79.2838%\n",
      "layer   3  Sparsity: 66.3572%\n",
      "total_backward_count 1243330 real_backward_count 123401   9.925%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  0.605074/  1.211218, val:  65.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4613%\n",
      "layer   2  Sparsity: 79.3153%\n",
      "layer   3  Sparsity: 65.1487%\n",
      "total_backward_count 1253120 real_backward_count 124190   9.910%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  0.609468/  1.155260, val:  65.00%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4191%\n",
      "layer   2  Sparsity: 79.1870%\n",
      "layer   3  Sparsity: 66.5970%\n",
      "total_backward_count 1262910 real_backward_count 125045   9.901%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  0.596789/  1.067092, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4315%\n",
      "layer   2  Sparsity: 79.7450%\n",
      "layer   3  Sparsity: 67.7432%\n",
      "total_backward_count 1272700 real_backward_count 125840   9.888%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  0.603424/  1.184040, val:  63.33%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4136%\n",
      "layer   2  Sparsity: 79.5141%\n",
      "layer   3  Sparsity: 66.1069%\n",
      "total_backward_count 1282490 real_backward_count 126666   9.877%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.606882/  1.183729, val:  62.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4198%\n",
      "layer   2  Sparsity: 79.4479%\n",
      "layer   3  Sparsity: 66.4932%\n",
      "total_backward_count 1292280 real_backward_count 127536   9.869%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.629747/  1.171125, val:  69.58%, val_best:  77.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4465%\n",
      "layer   2  Sparsity: 80.5573%\n",
      "layer   3  Sparsity: 68.2983%\n",
      "total_backward_count 1302070 real_backward_count 128350   9.857%\n",
      "fc layer 3 self.abs_max_out: 2747.0\n",
      "fc layer 3 self.abs_max_out: 2795.0\n",
      "fc layer 3 self.abs_max_out: 2808.0\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.614865/  1.193197, val:  57.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4330%\n",
      "layer   2  Sparsity: 80.5365%\n",
      "layer   3  Sparsity: 66.4030%\n",
      "total_backward_count 1311860 real_backward_count 129188   9.848%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.606207/  1.171703, val:  60.42%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4386%\n",
      "layer   2  Sparsity: 80.6726%\n",
      "layer   3  Sparsity: 66.1400%\n",
      "total_backward_count 1321650 real_backward_count 130001   9.836%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.656660/  1.236461, val:  61.25%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3999%\n",
      "layer   2  Sparsity: 80.8664%\n",
      "layer   3  Sparsity: 66.6672%\n",
      "total_backward_count 1331440 real_backward_count 130865   9.829%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.646137/  1.148539, val:  69.58%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4437%\n",
      "layer   2  Sparsity: 80.9753%\n",
      "layer   3  Sparsity: 69.0568%\n",
      "total_backward_count 1341230 real_backward_count 131743   9.823%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.615098/  1.159611, val:  64.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4640%\n",
      "layer   2  Sparsity: 80.9173%\n",
      "layer   3  Sparsity: 67.5632%\n",
      "total_backward_count 1351020 real_backward_count 132523   9.809%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.609600/  1.233628, val:  60.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4173%\n",
      "layer   2  Sparsity: 81.0248%\n",
      "layer   3  Sparsity: 65.9121%\n",
      "total_backward_count 1360810 real_backward_count 133390   9.802%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.616142/  1.144014, val:  72.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4500%\n",
      "layer   2  Sparsity: 80.6751%\n",
      "layer   3  Sparsity: 66.5103%\n",
      "total_backward_count 1370600 real_backward_count 134224   9.793%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.603644/  1.225022, val:  55.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3876%\n",
      "layer   2  Sparsity: 80.2782%\n",
      "layer   3  Sparsity: 65.2366%\n",
      "total_backward_count 1380390 real_backward_count 135062   9.784%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.574375/  1.145758, val:  65.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4177%\n",
      "layer   2  Sparsity: 80.7223%\n",
      "layer   3  Sparsity: 64.9409%\n",
      "total_backward_count 1390180 real_backward_count 135850   9.772%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.570423/  1.108810, val:  72.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4245%\n",
      "layer   2  Sparsity: 81.2122%\n",
      "layer   3  Sparsity: 65.7475%\n",
      "total_backward_count 1399970 real_backward_count 136630   9.759%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.571949/  1.210848, val:  68.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4159%\n",
      "layer   2  Sparsity: 80.9444%\n",
      "layer   3  Sparsity: 67.1529%\n",
      "total_backward_count 1409760 real_backward_count 137446   9.750%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.603249/  1.181262, val:  65.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4419%\n",
      "layer   2  Sparsity: 80.4864%\n",
      "layer   3  Sparsity: 65.7650%\n",
      "total_backward_count 1419550 real_backward_count 138242   9.738%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.606016/  1.276943, val:  58.75%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4641%\n",
      "layer   2  Sparsity: 80.6468%\n",
      "layer   3  Sparsity: 66.6964%\n",
      "total_backward_count 1429340 real_backward_count 139026   9.727%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.626009/  1.138509, val:  64.58%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4555%\n",
      "layer   2  Sparsity: 80.1486%\n",
      "layer   3  Sparsity: 66.8368%\n",
      "total_backward_count 1439130 real_backward_count 139869   9.719%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.578610/  1.291831, val:  54.17%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4359%\n",
      "layer   2  Sparsity: 80.1411%\n",
      "layer   3  Sparsity: 64.6157%\n",
      "total_backward_count 1448920 real_backward_count 140657   9.708%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.597350/  1.182989, val:  65.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4338%\n",
      "layer   2  Sparsity: 80.2232%\n",
      "layer   3  Sparsity: 63.6976%\n",
      "total_backward_count 1458710 real_backward_count 141513   9.701%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.575854/  1.152914, val:  67.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4263%\n",
      "layer   2  Sparsity: 80.5268%\n",
      "layer   3  Sparsity: 63.2217%\n",
      "total_backward_count 1468500 real_backward_count 142363   9.694%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.555895/  1.094043, val:  70.42%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4890%\n",
      "layer   2  Sparsity: 80.7516%\n",
      "layer   3  Sparsity: 64.2430%\n",
      "total_backward_count 1478290 real_backward_count 143138   9.683%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.575909/  1.129935, val:  62.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4407%\n",
      "layer   2  Sparsity: 80.2899%\n",
      "layer   3  Sparsity: 64.5050%\n",
      "total_backward_count 1488080 real_backward_count 143997   9.677%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.581123/  1.171572, val:  57.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 88.4509%\n",
      "layer   2  Sparsity: 80.0006%\n",
      "layer   3  Sparsity: 64.8732%\n",
      "total_backward_count 1497870 real_backward_count 144796   9.667%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.588878/  1.080697, val:  69.17%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4051%\n",
      "layer   2  Sparsity: 80.4813%\n",
      "layer   3  Sparsity: 64.6489%\n",
      "total_backward_count 1507660 real_backward_count 145643   9.660%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.561024/  1.032242, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4294%\n",
      "layer   2  Sparsity: 80.4351%\n",
      "layer   3  Sparsity: 65.8016%\n",
      "total_backward_count 1517450 real_backward_count 146449   9.651%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.558516/  1.041934, val:  73.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4632%\n",
      "layer   2  Sparsity: 80.2667%\n",
      "layer   3  Sparsity: 65.9834%\n",
      "total_backward_count 1527240 real_backward_count 147256   9.642%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.555289/  1.208744, val:  57.92%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4156%\n",
      "layer   2  Sparsity: 80.0553%\n",
      "layer   3  Sparsity: 65.9419%\n",
      "total_backward_count 1537030 real_backward_count 148072   9.634%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.577481/  1.117888, val:  70.42%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4274%\n",
      "layer   2  Sparsity: 80.0410%\n",
      "layer   3  Sparsity: 66.3861%\n",
      "total_backward_count 1546820 real_backward_count 148870   9.624%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.566241/  1.158096, val:  66.67%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4578%\n",
      "layer   2  Sparsity: 80.3180%\n",
      "layer   3  Sparsity: 65.1688%\n",
      "total_backward_count 1556610 real_backward_count 149727   9.619%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.541683/  1.161947, val:  62.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4591%\n",
      "layer   2  Sparsity: 80.4177%\n",
      "layer   3  Sparsity: 64.8766%\n",
      "total_backward_count 1566400 real_backward_count 150468   9.606%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.570441/  1.059574, val:  72.08%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4628%\n",
      "layer   2  Sparsity: 80.1835%\n",
      "layer   3  Sparsity: 65.9994%\n",
      "total_backward_count 1576190 real_backward_count 151258   9.596%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.575729/  1.049442, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4337%\n",
      "layer   2  Sparsity: 80.2287%\n",
      "layer   3  Sparsity: 67.1471%\n",
      "total_backward_count 1585980 real_backward_count 152075   9.589%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.572958/  1.224886, val:  57.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4225%\n",
      "layer   2  Sparsity: 79.9135%\n",
      "layer   3  Sparsity: 66.1422%\n",
      "total_backward_count 1595770 real_backward_count 152909   9.582%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.566981/  1.155709, val:  67.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4257%\n",
      "layer   2  Sparsity: 79.9539%\n",
      "layer   3  Sparsity: 65.5021%\n",
      "total_backward_count 1605560 real_backward_count 153720   9.574%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.550313/  1.023588, val:  72.92%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4210%\n",
      "layer   2  Sparsity: 79.5422%\n",
      "layer   3  Sparsity: 65.1486%\n",
      "total_backward_count 1615350 real_backward_count 154488   9.564%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.548985/  1.038915, val:  73.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4491%\n",
      "layer   2  Sparsity: 80.0188%\n",
      "layer   3  Sparsity: 66.7230%\n",
      "total_backward_count 1625140 real_backward_count 155223   9.551%\n",
      "fc layer 3 self.abs_max_out: 2816.0\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.542142/  1.118553, val:  67.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4731%\n",
      "layer   2  Sparsity: 79.8767%\n",
      "layer   3  Sparsity: 66.4775%\n",
      "total_backward_count 1634930 real_backward_count 156004   9.542%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.554486/  1.351660, val:  48.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3998%\n",
      "layer   2  Sparsity: 80.0807%\n",
      "layer   3  Sparsity: 65.1654%\n",
      "total_backward_count 1644720 real_backward_count 156799   9.533%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.557115/  1.110721, val:  63.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4671%\n",
      "layer   2  Sparsity: 79.6966%\n",
      "layer   3  Sparsity: 64.4781%\n",
      "total_backward_count 1654510 real_backward_count 157599   9.525%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.546472/  1.082445, val:  67.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4466%\n",
      "layer   2  Sparsity: 79.7702%\n",
      "layer   3  Sparsity: 64.5543%\n",
      "total_backward_count 1664300 real_backward_count 158397   9.517%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.545450/  1.037266, val:  73.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 88.4414%\n",
      "layer   2  Sparsity: 79.8719%\n",
      "layer   3  Sparsity: 65.1059%\n",
      "total_backward_count 1674090 real_backward_count 159160   9.507%\n",
      "fc layer 1 self.abs_max_out: 13866.0\n",
      "fc layer 1 self.abs_max_out: 14088.0\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.547231/  1.177759, val:  58.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3944%\n",
      "layer   2  Sparsity: 80.2840%\n",
      "layer   3  Sparsity: 63.6003%\n",
      "total_backward_count 1683880 real_backward_count 159950   9.499%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.529663/  1.185572, val:  62.50%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4352%\n",
      "layer   2  Sparsity: 80.4651%\n",
      "layer   3  Sparsity: 63.5312%\n",
      "total_backward_count 1693670 real_backward_count 160746   9.491%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.531800/  1.109699, val:  70.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4129%\n",
      "layer   2  Sparsity: 80.0128%\n",
      "layer   3  Sparsity: 64.1918%\n",
      "total_backward_count 1703460 real_backward_count 161463   9.479%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.533170/  1.092351, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4142%\n",
      "layer   2  Sparsity: 79.6667%\n",
      "layer   3  Sparsity: 63.6089%\n",
      "total_backward_count 1713250 real_backward_count 162254   9.471%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.536212/  1.095041, val:  67.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3973%\n",
      "layer   2  Sparsity: 79.5539%\n",
      "layer   3  Sparsity: 63.0911%\n",
      "total_backward_count 1723040 real_backward_count 163034   9.462%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.498524/  1.056512, val:  69.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4291%\n",
      "layer   2  Sparsity: 79.9404%\n",
      "layer   3  Sparsity: 64.0520%\n",
      "total_backward_count 1732830 real_backward_count 163757   9.450%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.500258/  1.141493, val:  62.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4387%\n",
      "layer   2  Sparsity: 79.9895%\n",
      "layer   3  Sparsity: 64.9235%\n",
      "total_backward_count 1742620 real_backward_count 164499   9.440%\n",
      "fc layer 3 self.abs_max_out: 2839.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.525757/  1.144913, val:  70.42%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 79.7863%\n",
      "layer   3  Sparsity: 63.6453%\n",
      "total_backward_count 1752410 real_backward_count 165294   9.432%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.502957/  0.986853, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4441%\n",
      "layer   2  Sparsity: 80.1616%\n",
      "layer   3  Sparsity: 64.5293%\n",
      "total_backward_count 1762200 real_backward_count 166028   9.422%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.484788/  0.988314, val:  70.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4087%\n",
      "layer   2  Sparsity: 80.5846%\n",
      "layer   3  Sparsity: 64.7049%\n",
      "total_backward_count 1771990 real_backward_count 166723   9.409%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.472571/  1.050475, val:  68.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4333%\n",
      "layer   2  Sparsity: 80.6097%\n",
      "layer   3  Sparsity: 64.8245%\n",
      "total_backward_count 1781780 real_backward_count 167481   9.400%\n",
      "fc layer 1 self.abs_max_out: 14384.0\n",
      "lif layer 1 self.abs_max_v: 25598.5\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.500281/  1.105425, val:  71.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 80.8459%\n",
      "layer   3  Sparsity: 63.4595%\n",
      "total_backward_count 1791570 real_backward_count 168259   9.392%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.510370/  1.165021, val:  58.33%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4260%\n",
      "layer   2  Sparsity: 80.8274%\n",
      "layer   3  Sparsity: 63.3256%\n",
      "total_backward_count 1801360 real_backward_count 169006   9.382%\n",
      "fc layer 3 self.abs_max_out: 2865.0\n",
      "fc layer 3 self.abs_max_out: 2891.0\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.479738/  1.151448, val:  63.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4424%\n",
      "layer   2  Sparsity: 80.9358%\n",
      "layer   3  Sparsity: 63.2537%\n",
      "total_backward_count 1811150 real_backward_count 169782   9.374%\n",
      "fc layer 1 self.abs_max_out: 14412.0\n",
      "lif layer 1 self.abs_max_v: 25650.0\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.495624/  1.035613, val:  67.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4128%\n",
      "layer   2  Sparsity: 80.8913%\n",
      "layer   3  Sparsity: 64.1007%\n",
      "total_backward_count 1820940 real_backward_count 170556   9.366%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.491458/  1.080286, val:  70.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4092%\n",
      "layer   2  Sparsity: 80.8836%\n",
      "layer   3  Sparsity: 64.5399%\n",
      "total_backward_count 1830730 real_backward_count 171330   9.359%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.509352/  1.164356, val:  62.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 88.4323%\n",
      "layer   2  Sparsity: 80.7332%\n",
      "layer   3  Sparsity: 64.3548%\n",
      "total_backward_count 1840520 real_backward_count 172104   9.351%\n",
      "fc layer 1 self.abs_max_out: 14490.0\n",
      "lif layer 1 self.abs_max_v: 25815.0\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.500216/  1.191101, val:  60.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4132%\n",
      "layer   2  Sparsity: 80.6165%\n",
      "layer   3  Sparsity: 65.3597%\n",
      "total_backward_count 1850310 real_backward_count 172862   9.342%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.519699/  1.083964, val:  65.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3926%\n",
      "layer   2  Sparsity: 80.9469%\n",
      "layer   3  Sparsity: 65.3984%\n",
      "total_backward_count 1860100 real_backward_count 173638   9.335%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.502552/  1.019543, val:  77.08%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.3947%\n",
      "layer   2  Sparsity: 81.2300%\n",
      "layer   3  Sparsity: 64.0103%\n",
      "total_backward_count 1869890 real_backward_count 174383   9.326%\n",
      "lif layer 1 self.abs_max_v: 25914.5\n",
      "fc layer 3 self.abs_max_out: 3159.0\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.522368/  1.141931, val:  66.67%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4354%\n",
      "layer   2  Sparsity: 80.9619%\n",
      "layer   3  Sparsity: 65.8073%\n",
      "total_backward_count 1879680 real_backward_count 175163   9.319%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.493198/  1.000559, val:  80.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4223%\n",
      "layer   2  Sparsity: 80.8821%\n",
      "layer   3  Sparsity: 64.3760%\n",
      "total_backward_count 1889470 real_backward_count 175926   9.311%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.488483/  1.072392, val:  75.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 88.4487%\n",
      "layer   2  Sparsity: 80.7236%\n",
      "layer   3  Sparsity: 63.9447%\n",
      "total_backward_count 1899260 real_backward_count 176625   9.300%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.494453/  1.048547, val:  71.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4382%\n",
      "layer   2  Sparsity: 81.1491%\n",
      "layer   3  Sparsity: 64.3696%\n",
      "total_backward_count 1909050 real_backward_count 177400   9.293%\n",
      "fc layer 1 self.abs_max_out: 14672.0\n",
      "lif layer 1 self.abs_max_v: 26197.5\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.501113/  1.090926, val:  71.25%, val_best:  80.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.3872%\n",
      "layer   2  Sparsity: 81.1374%\n",
      "layer   3  Sparsity: 63.9933%\n",
      "total_backward_count 1918840 real_backward_count 178181   9.286%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.484715/  1.078161, val:  65.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4464%\n",
      "layer   2  Sparsity: 80.5949%\n",
      "layer   3  Sparsity: 63.9446%\n",
      "total_backward_count 1928630 real_backward_count 178946   9.278%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.475591/  1.143140, val:  63.33%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.5142%\n",
      "layer   2  Sparsity: 80.4696%\n",
      "layer   3  Sparsity: 63.3441%\n",
      "total_backward_count 1938420 real_backward_count 179737   9.272%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.475151/  1.160131, val:  69.17%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4200%\n",
      "layer   2  Sparsity: 80.8216%\n",
      "layer   3  Sparsity: 63.2713%\n",
      "total_backward_count 1948210 real_backward_count 180518   9.266%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.481809/  1.040250, val:  72.08%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 88.4394%\n",
      "layer   2  Sparsity: 81.2229%\n",
      "layer   3  Sparsity: 64.3294%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18315e76d84b46d686383a69c992deda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.48181</td></tr><tr><td>val_acc_best</td><td>0.80833</td></tr><tr><td>val_acc_now</td><td>0.72083</td></tr><tr><td>val_loss</td><td>1.04025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">expert-sweep-169</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8ad2uuwr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8ad2uuwr</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251118_145817-8ad2uuwr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ursptz3w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_191845-ursptz3w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ursptz3w' target=\"_blank\">cerulean-sweep-173</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hcapkd0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ursptz3w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ursptz3w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251118_191853_799', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 1, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 125.0\n",
      "lif layer 1 self.abs_max_v: 125.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 193.0\n",
      "fc layer 2 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 143.0\n",
      "fc layer 3 self.abs_max_out: 17.0\n",
      "fc layer 1 self.abs_max_out: 214.0\n",
      "lif layer 1 self.abs_max_v: 284.0\n",
      "fc layer 2 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 284.5\n",
      "fc layer 3 self.abs_max_out: 63.0\n",
      "fc layer 1 self.abs_max_out: 217.0\n",
      "fc layer 2 self.abs_max_out: 249.0\n",
      "lif layer 2 self.abs_max_v: 295.5\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 1 self.abs_max_out: 325.0\n",
      "lif layer 1 self.abs_max_v: 325.0\n",
      "lif layer 2 self.abs_max_v: 319.0\n",
      "fc layer 1 self.abs_max_out: 422.0\n",
      "lif layer 1 self.abs_max_v: 422.0\n",
      "fc layer 2 self.abs_max_out: 284.0\n",
      "lif layer 2 self.abs_max_v: 361.5\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 1 self.abs_max_out: 530.0\n",
      "lif layer 1 self.abs_max_v: 530.0\n",
      "fc layer 2 self.abs_max_out: 297.0\n",
      "lif layer 2 self.abs_max_v: 444.0\n",
      "fc layer 1 self.abs_max_out: 543.0\n",
      "lif layer 1 self.abs_max_v: 543.0\n",
      "lif layer 2 self.abs_max_v: 462.0\n",
      "fc layer 2 self.abs_max_out: 350.0\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "lif layer 2 self.abs_max_v: 500.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 528.5\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 566.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 753.0\n",
      "fc layer 1 self.abs_max_out: 828.0\n",
      "lif layer 1 self.abs_max_v: 828.0\n",
      "fc layer 2 self.abs_max_out: 485.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "lif layer 2 self.abs_max_v: 581.5\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 588.0\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "lif layer 2 self.abs_max_v: 607.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 635.5\n",
      "lif layer 2 self.abs_max_v: 664.0\n",
      "lif layer 2 self.abs_max_v: 670.0\n",
      "lif layer 2 self.abs_max_v: 702.5\n",
      "lif layer 2 self.abs_max_v: 705.5\n",
      "lif layer 2 self.abs_max_v: 709.5\n",
      "lif layer 2 self.abs_max_v: 745.0\n",
      "fc layer 2 self.abs_max_out: 569.0\n",
      "lif layer 2 self.abs_max_v: 888.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 914.0\n",
      "lif layer 2 self.abs_max_v: 962.0\n",
      "fc layer 2 self.abs_max_out: 601.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "fc layer 2 self.abs_max_out: 611.0\n",
      "fc layer 1 self.abs_max_out: 914.0\n",
      "lif layer 1 self.abs_max_v: 927.5\n",
      "fc layer 1 self.abs_max_out: 938.0\n",
      "lif layer 1 self.abs_max_v: 938.0\n",
      "fc layer 1 self.abs_max_out: 983.0\n",
      "lif layer 1 self.abs_max_v: 983.0\n",
      "fc layer 1 self.abs_max_out: 1053.0\n",
      "lif layer 1 self.abs_max_v: 1053.0\n",
      "fc layer 1 self.abs_max_out: 1316.0\n",
      "lif layer 1 self.abs_max_v: 1316.0\n",
      "fc layer 2 self.abs_max_out: 636.0\n",
      "fc layer 1 self.abs_max_out: 1349.0\n",
      "lif layer 1 self.abs_max_v: 1349.0\n",
      "lif layer 1 self.abs_max_v: 1371.5\n",
      "fc layer 2 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 1508.5\n",
      "fc layer 2 self.abs_max_out: 700.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "lif layer 2 self.abs_max_v: 1002.0\n",
      "lif layer 2 self.abs_max_v: 1015.0\n",
      "lif layer 2 self.abs_max_v: 1023.5\n",
      "lif layer 2 self.abs_max_v: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1165.5\n",
      "fc layer 2 self.abs_max_out: 769.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 2 self.abs_max_out: 773.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 3 self.abs_max_out: 390.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "lif layer 1 self.abs_max_v: 1527.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "fc layer 1 self.abs_max_out: 1443.0\n",
      "lif layer 1 self.abs_max_v: 1658.5\n",
      "lif layer 1 self.abs_max_v: 1712.5\n",
      "lif layer 1 self.abs_max_v: 1853.5\n",
      "lif layer 2 self.abs_max_v: 1228.5\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "lif layer 2 self.abs_max_v: 1239.5\n",
      "lif layer 2 self.abs_max_v: 1308.5\n",
      "fc layer 2 self.abs_max_out: 1001.0\n",
      "lif layer 2 self.abs_max_v: 1347.0\n",
      "lif layer 2 self.abs_max_v: 1359.0\n",
      "lif layer 2 self.abs_max_v: 1401.0\n",
      "lif layer 1 self.abs_max_v: 2004.0\n",
      "fc layer 1 self.abs_max_out: 1660.0\n",
      "fc layer 1 self.abs_max_out: 1742.0\n",
      "lif layer 2 self.abs_max_v: 1440.0\n",
      "lif layer 2 self.abs_max_v: 1471.5\n",
      "lif layer 2 self.abs_max_v: 1491.0\n",
      "fc layer 2 self.abs_max_out: 1020.0\n",
      "lif layer 2 self.abs_max_v: 1566.5\n",
      "lif layer 2 self.abs_max_v: 1677.5\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "fc layer 2 self.abs_max_out: 1064.0\n",
      "lif layer 2 self.abs_max_v: 1709.0\n",
      "lif layer 2 self.abs_max_v: 1876.0\n",
      "lif layer 1 self.abs_max_v: 2064.5\n",
      "lif layer 2 self.abs_max_v: 1888.0\n",
      "fc layer 2 self.abs_max_out: 1092.0\n",
      "fc layer 2 self.abs_max_out: 1095.0\n",
      "fc layer 1 self.abs_max_out: 1760.0\n",
      "lif layer 1 self.abs_max_v: 2196.5\n",
      "lif layer 1 self.abs_max_v: 2273.0\n",
      "fc layer 2 self.abs_max_out: 1096.0\n",
      "fc layer 2 self.abs_max_out: 1115.0\n",
      "fc layer 2 self.abs_max_out: 1120.0\n",
      "lif layer 2 self.abs_max_v: 1936.0\n",
      "lif layer 2 self.abs_max_v: 1948.0\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "lif layer 2 self.abs_max_v: 1966.0\n",
      "lif layer 2 self.abs_max_v: 2050.0\n",
      "lif layer 2 self.abs_max_v: 2063.5\n",
      "lif layer 2 self.abs_max_v: 2109.0\n",
      "lif layer 2 self.abs_max_v: 2157.5\n",
      "fc layer 1 self.abs_max_out: 1808.0\n",
      "fc layer 1 self.abs_max_out: 2013.0\n",
      "lif layer 2 self.abs_max_v: 2181.5\n",
      "fc layer 2 self.abs_max_out: 1173.0\n",
      "fc layer 2 self.abs_max_out: 1227.0\n",
      "lif layer 1 self.abs_max_v: 2273.5\n",
      "fc layer 1 self.abs_max_out: 2077.0\n",
      "lif layer 1 self.abs_max_v: 2380.0\n",
      "lif layer 1 self.abs_max_v: 2606.0\n",
      "lif layer 1 self.abs_max_v: 2609.5\n",
      "fc layer 1 self.abs_max_out: 2195.0\n",
      "fc layer 1 self.abs_max_out: 2220.0\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "fc layer 1 self.abs_max_out: 2232.0\n",
      "lif layer 1 self.abs_max_v: 2671.5\n",
      "lif layer 1 self.abs_max_v: 2801.0\n",
      "lif layer 1 self.abs_max_v: 2875.5\n",
      "fc layer 2 self.abs_max_out: 1364.0\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "lif layer 1 self.abs_max_v: 3103.5\n",
      "lif layer 1 self.abs_max_v: 3349.0\n",
      "lif layer 1 self.abs_max_v: 3470.5\n",
      "lif layer 1 self.abs_max_v: 3565.5\n",
      "lif layer 2 self.abs_max_v: 2194.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.585289/  1.916843, val:  27.50%, val_best:  27.50%, tr:  98.77%, tr_best:  98.77%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.4546%\n",
      "layer   3  Sparsity: 74.7708%\n",
      "total_backward_count 9790 real_backward_count 1753  17.906%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1386.0\n",
      "lif layer 2 self.abs_max_v: 2290.5\n",
      "lif layer 2 self.abs_max_v: 2363.5\n",
      "fc layer 2 self.abs_max_out: 1421.0\n",
      "fc layer 2 self.abs_max_out: 1513.0\n",
      "lif layer 2 self.abs_max_v: 2386.0\n",
      "lif layer 2 self.abs_max_v: 2402.0\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "fc layer 3 self.abs_max_out: 582.0\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "fc layer 1 self.abs_max_out: 2372.0\n",
      "lif layer 2 self.abs_max_v: 2559.0\n",
      "fc layer 2 self.abs_max_out: 1528.0\n",
      "fc layer 1 self.abs_max_out: 2378.0\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "fc layer 2 self.abs_max_out: 1581.0\n",
      "fc layer 1 self.abs_max_out: 2504.0\n",
      "fc layer 1 self.abs_max_out: 2675.0\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "lif layer 1 self.abs_max_v: 3601.5\n",
      "lif layer 1 self.abs_max_v: 3666.0\n",
      "lif layer 1 self.abs_max_v: 3684.0\n",
      "lif layer 1 self.abs_max_v: 3910.0\n",
      "lif layer 1 self.abs_max_v: 4126.0\n",
      "lif layer 2 self.abs_max_v: 2574.0\n",
      "lif layer 2 self.abs_max_v: 2636.0\n",
      "lif layer 2 self.abs_max_v: 2645.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.462036/  1.851936, val:  35.83%, val_best:  35.83%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.6888%\n",
      "layer   3  Sparsity: 74.5810%\n",
      "total_backward_count 19580 real_backward_count 3197  16.328%\n",
      "lif layer 2 self.abs_max_v: 2664.5\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "fc layer 3 self.abs_max_out: 624.0\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "fc layer 1 self.abs_max_out: 2716.0\n",
      "lif layer 2 self.abs_max_v: 2702.5\n",
      "lif layer 1 self.abs_max_v: 4219.0\n",
      "fc layer 3 self.abs_max_out: 637.0\n",
      "lif layer 1 self.abs_max_v: 4420.5\n",
      "lif layer 1 self.abs_max_v: 4475.5\n",
      "lif layer 2 self.abs_max_v: 2812.0\n",
      "lif layer 2 self.abs_max_v: 2831.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.404520/  1.758547, val:  43.75%, val_best:  43.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.2995%\n",
      "layer   3  Sparsity: 74.5149%\n",
      "total_backward_count 29370 real_backward_count 4598  15.655%\n",
      "lif layer 2 self.abs_max_v: 2881.0\n",
      "fc layer 1 self.abs_max_out: 2789.0\n",
      "fc layer 1 self.abs_max_out: 2940.0\n",
      "fc layer 2 self.abs_max_out: 1681.0\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "lif layer 2 self.abs_max_v: 2894.5\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "fc layer 2 self.abs_max_out: 1731.0\n",
      "fc layer 2 self.abs_max_out: 1747.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "lif layer 2 self.abs_max_v: 3020.0\n",
      "lif layer 1 self.abs_max_v: 4587.5\n",
      "lif layer 1 self.abs_max_v: 4944.0\n",
      "lif layer 1 self.abs_max_v: 5310.0\n",
      "lif layer 2 self.abs_max_v: 3175.0\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "fc layer 2 self.abs_max_out: 1794.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.364274/  1.798672, val:  37.08%, val_best:  43.75%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 74.7258%\n",
      "layer   3  Sparsity: 75.1762%\n",
      "total_backward_count 39160 real_backward_count 6002  15.327%\n",
      "fc layer 3 self.abs_max_out: 698.0\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 1 self.abs_max_out: 3005.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "lif layer 1 self.abs_max_v: 5448.0\n",
      "lif layer 1 self.abs_max_v: 5462.0\n",
      "fc layer 1 self.abs_max_out: 3070.0\n",
      "lif layer 1 self.abs_max_v: 5577.0\n",
      "lif layer 1 self.abs_max_v: 5795.5\n",
      "lif layer 1 self.abs_max_v: 5915.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.288314/  1.666372, val:  54.17%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.5316%\n",
      "layer   3  Sparsity: 74.9712%\n",
      "total_backward_count 48950 real_backward_count 7378  15.073%\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 2 self.abs_max_out: 2091.0\n",
      "fc layer 1 self.abs_max_out: 3313.0\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "fc layer 2 self.abs_max_out: 2176.0\n",
      "fc layer 2 self.abs_max_out: 2180.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.296968/  1.662365, val:  50.00%, val_best:  54.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.3366%\n",
      "layer   3  Sparsity: 75.6212%\n",
      "total_backward_count 58740 real_backward_count 8679  14.775%\n",
      "fc layer 1 self.abs_max_out: 3318.0\n",
      "fc layer 1 self.abs_max_out: 3331.0\n",
      "lif layer 1 self.abs_max_v: 6173.5\n",
      "lif layer 1 self.abs_max_v: 6236.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.278888/  1.664756, val:  49.17%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.8403%\n",
      "layer   3  Sparsity: 75.7840%\n",
      "total_backward_count 68530 real_backward_count 9946  14.513%\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "lif layer 1 self.abs_max_v: 6336.0\n",
      "lif layer 1 self.abs_max_v: 6420.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.265598/  1.602057, val:  48.75%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.4971%\n",
      "layer   3  Sparsity: 76.5632%\n",
      "total_backward_count 78320 real_backward_count 11241  14.353%\n",
      "fc layer 1 self.abs_max_out: 3630.0\n",
      "fc layer 3 self.abs_max_out: 763.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.224351/  1.574338, val:  57.08%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.9014%\n",
      "layer   3  Sparsity: 76.5463%\n",
      "total_backward_count 88110 real_backward_count 12561  14.256%\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "fc layer 3 self.abs_max_out: 781.0\n",
      "fc layer 3 self.abs_max_out: 792.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "lif layer 1 self.abs_max_v: 6484.5\n",
      "lif layer 1 self.abs_max_v: 6550.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.198921/  1.594162, val:  51.25%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.1111%\n",
      "layer   3  Sparsity: 76.4727%\n",
      "total_backward_count 97900 real_backward_count 13772  14.067%\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "fc layer 3 self.abs_max_out: 861.0\n",
      "fc layer 1 self.abs_max_out: 3781.0\n",
      "lif layer 2 self.abs_max_v: 3206.5\n",
      "lif layer 1 self.abs_max_v: 6945.0\n",
      "lif layer 1 self.abs_max_v: 7008.5\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.141759/  1.549690, val:  60.00%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.1728%\n",
      "layer   3  Sparsity: 76.1361%\n",
      "total_backward_count 107690 real_backward_count 14926  13.860%\n",
      "lif layer 2 self.abs_max_v: 3256.5\n",
      "lif layer 2 self.abs_max_v: 3299.0\n",
      "fc layer 1 self.abs_max_out: 3838.0\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.134091/  1.495625, val:  63.75%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.1838%\n",
      "layer   3  Sparsity: 76.0095%\n",
      "total_backward_count 117480 real_backward_count 16074  13.682%\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "lif layer 1 self.abs_max_v: 7026.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.073103/  1.469643, val:  59.58%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.9180%\n",
      "layer   3  Sparsity: 76.0766%\n",
      "total_backward_count 127270 real_backward_count 17189  13.506%\n",
      "fc layer 2 self.abs_max_out: 2211.0\n",
      "fc layer 3 self.abs_max_out: 934.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "fc layer 3 self.abs_max_out: 976.0\n",
      "fc layer 2 self.abs_max_out: 2229.0\n",
      "fc layer 1 self.abs_max_out: 3870.0\n",
      "lif layer 1 self.abs_max_v: 7133.0\n",
      "fc layer 1 self.abs_max_out: 3955.0\n",
      "lif layer 1 self.abs_max_v: 7490.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.033257/  1.467982, val:  57.08%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.9706%\n",
      "layer   3  Sparsity: 75.2443%\n",
      "total_backward_count 137060 real_backward_count 18290  13.345%\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "lif layer 2 self.abs_max_v: 3329.0\n",
      "fc layer 2 self.abs_max_out: 2322.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.030486/  1.416936, val:  60.83%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.6708%\n",
      "layer   3  Sparsity: 76.3495%\n",
      "total_backward_count 146850 real_backward_count 19367  13.188%\n",
      "lif layer 2 self.abs_max_v: 3364.0\n",
      "fc layer 1 self.abs_max_out: 4133.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.016961/  1.437681, val:  57.50%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.0251%\n",
      "layer   3  Sparsity: 77.3623%\n",
      "total_backward_count 156640 real_backward_count 20408  13.029%\n",
      "lif layer 2 self.abs_max_v: 3391.5\n",
      "lif layer 2 self.abs_max_v: 3398.0\n",
      "lif layer 2 self.abs_max_v: 3404.0\n",
      "lif layer 2 self.abs_max_v: 3517.0\n",
      "lif layer 2 self.abs_max_v: 3553.5\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  0.993531/  1.380906, val:  67.08%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.9014%\n",
      "layer   3  Sparsity: 77.5360%\n",
      "total_backward_count 166430 real_backward_count 21439  12.882%\n",
      "lif layer 2 self.abs_max_v: 3563.5\n",
      "lif layer 2 self.abs_max_v: 3684.0\n",
      "lif layer 2 self.abs_max_v: 3740.5\n",
      "fc layer 1 self.abs_max_out: 4221.0\n",
      "lif layer 1 self.abs_max_v: 7671.5\n",
      "lif layer 1 self.abs_max_v: 7883.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  0.990889/  1.351694, val:  71.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.5628%\n",
      "layer   3  Sparsity: 77.1511%\n",
      "total_backward_count 176220 real_backward_count 22405  12.714%\n",
      "lif layer 2 self.abs_max_v: 3807.0\n",
      "lif layer 2 self.abs_max_v: 3891.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.000630/  1.404037, val:  64.17%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.5872%\n",
      "layer   3  Sparsity: 77.2715%\n",
      "total_backward_count 186010 real_backward_count 23366  12.562%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  0.976046/  1.410809, val:  53.75%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.7534%\n",
      "layer   3  Sparsity: 77.5820%\n",
      "total_backward_count 195800 real_backward_count 24277  12.399%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  0.934843/  1.399799, val:  59.58%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.0667%\n",
      "layer   3  Sparsity: 77.9292%\n",
      "total_backward_count 205590 real_backward_count 25138  12.227%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  0.919974/  1.322760, val:  75.83%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 76.0082%\n",
      "layer   3  Sparsity: 77.4660%\n",
      "total_backward_count 215380 real_backward_count 26028  12.085%\n",
      "fc layer 3 self.abs_max_out: 1066.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  0.923166/  1.258243, val:  75.42%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.7404%\n",
      "layer   3  Sparsity: 77.1323%\n",
      "total_backward_count 225170 real_backward_count 26874  11.935%\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  0.878960/  1.294876, val:  68.33%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.5662%\n",
      "layer   3  Sparsity: 76.4014%\n",
      "total_backward_count 234960 real_backward_count 27644  11.765%\n",
      "lif layer 2 self.abs_max_v: 3898.5\n",
      "lif layer 2 self.abs_max_v: 4027.0\n",
      "lif layer 2 self.abs_max_v: 4191.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  0.876938/  1.256979, val:  72.92%, val_best:  75.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.7970%\n",
      "layer   3  Sparsity: 76.2635%\n",
      "total_backward_count 244750 real_backward_count 28434  11.618%\n",
      "lif layer 2 self.abs_max_v: 4197.5\n",
      "lif layer 2 self.abs_max_v: 4251.0\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  0.859489/  1.217669, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.9806%\n",
      "layer   3  Sparsity: 75.6705%\n",
      "total_backward_count 254540 real_backward_count 29221  11.480%\n",
      "fc layer 2 self.abs_max_out: 2418.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  0.833762/  1.206036, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.7476%\n",
      "layer   3  Sparsity: 75.6184%\n",
      "total_backward_count 264330 real_backward_count 29941  11.327%\n",
      "fc layer 2 self.abs_max_out: 2480.0\n",
      "fc layer 3 self.abs_max_out: 1081.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  0.819831/  1.210610, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.6745%\n",
      "layer   3  Sparsity: 76.0585%\n",
      "total_backward_count 274120 real_backward_count 30691  11.196%\n",
      "fc layer 3 self.abs_max_out: 1082.0\n",
      "fc layer 3 self.abs_max_out: 1136.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  0.777641/  1.180697, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.5526%\n",
      "layer   3  Sparsity: 76.4732%\n",
      "total_backward_count 283910 real_backward_count 31389  11.056%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  0.797638/  1.208058, val:  75.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 92.7136%\n",
      "layer   2  Sparsity: 75.6391%\n",
      "layer   3  Sparsity: 76.5877%\n",
      "total_backward_count 293700 real_backward_count 32045  10.911%\n",
      "fc layer 1 self.abs_max_out: 4222.0\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [10,15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \n",
    "        \"random_select_ratio\": {\"values\": [1,2,3,4,5]},\n",
    "        \"leaky_temporal_filter\": {\"values\": [0.0, 0.25, 0.5, 0.75, 1.0]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"0\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "        random_select_ratio  =  wandb.config.random_select_ratio,\n",
    "        leaky_temporal_filter  =  wandb.config.leaky_temporal_filter,\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'hcapkd0n'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
